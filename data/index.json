{
  "generated_at": "2026-02-23T02:02:23.529648+00:00",
  "count": 651,
  "dates": [
    "2026-02-20",
    "2026-02-19",
    "2026-02-18",
    "2026-02-17",
    "2026-02-16",
    "2026-02-13",
    "2026-02-12",
    "2026-02-11",
    "2026-02-10",
    "2026-02-09",
    "2026-02-06",
    "2026-02-05",
    "2026-02-04",
    "2026-02-03",
    "2026-02-02"
  ],
  "daily_summary": {
    "date": "2026-02-20",
    "content": "Overview\n- Date: 2026-02-20\n- Total Papers: 23\n- Total Upvotes: 151\n- Papers with GitHub: 14\n\nKey Takeaways\n1. ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¸åŠ¨æ€è®¡ç®—ä¼˜åŒ–æˆä¸º Diffusion Transformer æ•ˆç‡æå‡çš„æ ¸å¿ƒæ–¹å‘ï¼ŒSpargeAttention2 å’Œ DDiT åˆ†åˆ«é€šè¿‡å¯è®­ç»ƒç¨€ç–æ©ç ä¸åŠ¨æ€ Patch è°ƒåº¦å®ç°æ€§èƒ½çªç ´ã€‚\n2. GUI Agent å‘å¤šå¹³å°é€šç”¨åŒ–æ¼”è¿›ï¼ŒMobile-Agent-v3.5 å±•ç¤ºäº†è·¨å¹³å°å›¾å½¢ç•Œé¢è‡ªåŠ¨åŒ–ä¸å·¥å…·è°ƒç”¨çš„æœ€æ–°è¿›å±•ã€‚\n3. æ½œç©ºé—´è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æŒç»­åˆ›æ–°ï¼ŒUnified Latents æ¡†æ¶é€šè¿‡ Diffusion å…ˆéªŒæ­£åˆ™åŒ–å®ç°è”åˆæ½œç©ºé—´å­¦ä¹ ï¼Œä¼˜åŒ–ç”Ÿæˆè´¨é‡ä¸è®­ç»ƒç¨³å®šæ€§ã€‚\n4. LLM Agent åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æˆæœ¬æ•ˆç›Šä¸äº¤äº’è®¾è®¡å—åˆ°å…³æ³¨ï¼Œæ¶µç›–è½¦è½½åŠ©æ‰‹çš„è‡ªé€‚åº”åé¦ˆæœºåˆ¶ä¸æˆæœ¬æ„ŸçŸ¥çš„æ¢ç´¢ç­–ç•¥ä¼˜åŒ–ã€‚\n\nNotable Papers\n- [2602.13515] SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning (ğŸ‘24): æå‡ºå¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œé€šè¿‡æ··åˆ Top-k+Top-p æ©ç ä¸è’¸é¦å¾®è°ƒåœ¨ Diffusion æ¨¡å‹ä¸­å®ç°é«˜ç¨€ç–åº¦å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚\n- [2602.16855] Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents (ğŸ‘22): å‘å¸ƒå¤šå¹³å° GUI Agent æ¨¡å‹ GUI-Owl-1.5ï¼Œåœ¨ GUI è‡ªåŠ¨åŒ–ã€è§†è§‰å®šä½ä¸å·¥å…·è°ƒç”¨ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA æ€§èƒ½ã€‚\n- [2602.17270] Unified Latents (UL): How to train your latents (ğŸ‘21): æå‡º Unified Latents æ¡†æ¶ï¼Œåˆ©ç”¨ Diffusion å…ˆéªŒæ­£åˆ™åŒ–ä¸æ‰©æ•£æ¨¡å‹è§£ç å­¦ä¹ è”åˆæ½œç©ºé—´è¡¨ç¤ºï¼Œå–å¾—æœ‰ç«äº‰åŠ›çš„ FID åˆ†æ•°ã€‚\n- [2602.15569] \"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing (ğŸ‘12): å‘ç°ç”¨æˆ·åå¥½è½¦è½½ AI åŠ©æ‰‹é‡‡ç”¨è‡ªé€‚åº”åé¦ˆæœºåˆ¶ï¼ŒåˆæœŸé«˜é€æ˜åº¦å»ºç«‹ä¿¡ä»»åé€æ­¥é™ä½å†—ä½™åº¦ä»¥æå‡å¯é æ€§æ„ŸçŸ¥ã€‚\n- [2602.16699] Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents (ğŸ‘11): æå‡º Calibrate-Then-Act æ¡†æ¶ï¼Œä½¿ LLM é€šè¿‡æ˜¾å¼æ¨ç†æˆæœ¬-ä¸ç¡®å®šæ€§æƒè¡¡æ¥ä¼˜åŒ–å¤æ‚ä»»åŠ¡ä¸­çš„æ¢ç´¢ç­–ç•¥ã€‚",
    "source": "openrouter",
    "model": "moonshotai/kimi-k2.5",
    "generated_at": "2026-02-21T01:54:29.439566+00:00"
  },
  "daily_summaries": {
    "2026-02-20": {
      "date": "2026-02-20",
      "content": "Overview\n- Date: 2026-02-20\n- Total Papers: 23\n- Total Upvotes: 151\n- Papers with GitHub: 14\n\nKey Takeaways\n1. ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¸åŠ¨æ€è®¡ç®—ä¼˜åŒ–æˆä¸º Diffusion Transformer æ•ˆç‡æå‡çš„æ ¸å¿ƒæ–¹å‘ï¼ŒSpargeAttention2 å’Œ DDiT åˆ†åˆ«é€šè¿‡å¯è®­ç»ƒç¨€ç–æ©ç ä¸åŠ¨æ€ Patch è°ƒåº¦å®ç°æ€§èƒ½çªç ´ã€‚\n2. GUI Agent å‘å¤šå¹³å°é€šç”¨åŒ–æ¼”è¿›ï¼ŒMobile-Agent-v3.5 å±•ç¤ºäº†è·¨å¹³å°å›¾å½¢ç•Œé¢è‡ªåŠ¨åŒ–ä¸å·¥å…·è°ƒç”¨çš„æœ€æ–°è¿›å±•ã€‚\n3. æ½œç©ºé—´è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æŒç»­åˆ›æ–°ï¼ŒUnified Latents æ¡†æ¶é€šè¿‡ Diffusion å…ˆéªŒæ­£åˆ™åŒ–å®ç°è”åˆæ½œç©ºé—´å­¦ä¹ ï¼Œä¼˜åŒ–ç”Ÿæˆè´¨é‡ä¸è®­ç»ƒç¨³å®šæ€§ã€‚\n4. LLM Agent åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æˆæœ¬æ•ˆç›Šä¸äº¤äº’è®¾è®¡å—åˆ°å…³æ³¨ï¼Œæ¶µç›–è½¦è½½åŠ©æ‰‹çš„è‡ªé€‚åº”åé¦ˆæœºåˆ¶ä¸æˆæœ¬æ„ŸçŸ¥çš„æ¢ç´¢ç­–ç•¥ä¼˜åŒ–ã€‚\n\nNotable Papers\n- [2602.13515] SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning (ğŸ‘24): æå‡ºå¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œé€šè¿‡æ··åˆ Top-k+Top-p æ©ç ä¸è’¸é¦å¾®è°ƒåœ¨ Diffusion æ¨¡å‹ä¸­å®ç°é«˜ç¨€ç–åº¦å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚\n- [2602.16855] Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents (ğŸ‘22): å‘å¸ƒå¤šå¹³å° GUI Agent æ¨¡å‹ GUI-Owl-1.5ï¼Œåœ¨ GUI è‡ªåŠ¨åŒ–ã€è§†è§‰å®šä½ä¸å·¥å…·è°ƒç”¨ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA æ€§èƒ½ã€‚\n- [2602.17270] Unified Latents (UL): How to train your latents (ğŸ‘21): æå‡º Unified Latents æ¡†æ¶ï¼Œåˆ©ç”¨ Diffusion å…ˆéªŒæ­£åˆ™åŒ–ä¸æ‰©æ•£æ¨¡å‹è§£ç å­¦ä¹ è”åˆæ½œç©ºé—´è¡¨ç¤ºï¼Œå–å¾—æœ‰ç«äº‰åŠ›çš„ FID åˆ†æ•°ã€‚\n- [2602.15569] \"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing (ğŸ‘12): å‘ç°ç”¨æˆ·åå¥½è½¦è½½ AI åŠ©æ‰‹é‡‡ç”¨è‡ªé€‚åº”åé¦ˆæœºåˆ¶ï¼ŒåˆæœŸé«˜é€æ˜åº¦å»ºç«‹ä¿¡ä»»åé€æ­¥é™ä½å†—ä½™åº¦ä»¥æå‡å¯é æ€§æ„ŸçŸ¥ã€‚\n- [2602.16699] Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents (ğŸ‘11): æå‡º Calibrate-Then-Act æ¡†æ¶ï¼Œä½¿ LLM é€šè¿‡æ˜¾å¼æ¨ç†æˆæœ¬-ä¸ç¡®å®šæ€§æƒè¡¡æ¥ä¼˜åŒ–å¤æ‚ä»»åŠ¡ä¸­çš„æ¢ç´¢ç­–ç•¥ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-21T01:54:29.439566+00:00"
    },
    "2026-02-19": {
      "date": "2026-02-19",
      "content": "Overview\n- Date: 2026-02-19\n- Total Papers: 21\n- Total Upvotes: 171\n- Papers with GitHub: 13\n\nKey Takeaways\n1. Embodied AIä¸æœºå™¨äººæ§åˆ¶æˆä¸ºç ”ç©¶ç„¦ç‚¹ï¼Œæ¶µç›–äººå½¢æœºå™¨äººæ“ä½œã€å…·èº«åŸºç¡€æ¨¡å‹åŠWeb Agentç¯å¢ƒåˆæˆã€‚\n2. æ¨¡å‹æ•ˆç‡ä¼˜åŒ–æŒç»­æ·±å…¥ï¼ŒåŒ…æ‹¬Diffusionæ¨¡å‹çš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›æ”¹è¿›ä¸é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„å¿«é€Ÿæƒé‡å¼ºåŒ–å­¦ä¹ ã€‚\n3. ç¨‹åºåŒ–å†…å®¹ç”Ÿæˆä¸å¤šæ¨¡æ€å­¦ä¹ è¿›å±•æ˜¾è‘—ï¼Œæ¶‰åŠCADç¨‹åºè¿›åŒ–ç”Ÿæˆä¸å¤§è§„æ¨¡éŸ³é¢‘åµŒå…¥åŸºå‡†è¯„ä¼°ã€‚\n4. äº‹å®æ£€ç´¢ä¸ä¸ªæ€§åŒ–Agentæœºåˆ¶å—åˆ°å…³æ³¨ï¼Œæ­ç¤ºLLMçŸ¥è¯†å¬å›ç“¶é¢ˆå¹¶æ¢ç´¢åŸºäºäººç±»åé¦ˆçš„æŒç»­ä¸ªæ€§åŒ–æ–¹æ³•ã€‚\n\nNotable Papers\n- [2602.14296] AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines (ğŸ‘33): åŸºäºæœ‰é™çŠ¶æ€æœºå’Œç¼–ç¨‹Agentåˆæˆå¯éªŒè¯çš„æ— é™Webç¯å¢ƒï¼Œä¸ºè‡ªä¸»Web GUI Agentè®­ç»ƒæä¾›é«˜æ•ˆæ•°æ®æ”¯æŒã€‚\n- [2602.12675] SLA2: Sparse-Linear Attention with Learnable Routing and QAT (ğŸ‘25): é€šè¿‡å¯å­¦ä¹ è·¯ç”±ã€ç›´æ¥æ³¨æ„åŠ›å…¬å¼åŒ–åŠé‡åŒ–æ„ŸçŸ¥å¾®è°ƒï¼Œæå‡Diffusionæ¨¡å‹çš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›æ•ˆç‡ä¸æ€§èƒ½ã€‚\n- [2602.16705] Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation (ğŸ‘20): HEROæ¡†æ¶ç»“åˆç²¾ç¡®æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶ä¸å¼€æ”¾è¯æ±‡è§†è§‰æ„ŸçŸ¥ï¼Œä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çœŸå®ç¯å¢ƒä¸­å®Œæˆç‰©ä½“æ“ä½œä»»åŠ¡ã€‚\n- [2602.16317] CADEvolve: Creating Realistic CAD via Program Evolution (ğŸ‘19): é‡‡ç”¨åŸºäºVLMå¼•å¯¼ç¼–è¾‘çš„è¿›åŒ–æ–¹æ³•ï¼Œä»ç®€å•åŸºå…ƒç”Ÿæˆå¤æ‚CADç¨‹åºï¼Œæ„å»ºå¤§è§„æ¨¡CADæ•°æ®é›†æ¨åŠ¨ç¨‹åºåŒ–å»ºæ¨¡å‘å±•ã€‚\n- [2602.16008] MAEB: Massive Audio Embedding Benchmark (ğŸ‘13): å¤§è§„æ¨¡éŸ³é¢‘åŸºå‡†æµ‹è¯•æ¶µç›–è¯­éŸ³ã€éŸ³ä¹åŠç¯å¢ƒéŸ³ç­‰30é¡¹ä»»åŠ¡ï¼Œç³»ç»Ÿè¯„ä¼°50ä½™ç§æ¨¡å‹å¹¶æ­ç¤ºä¸åŒæ¶æ„åœ¨éŸ³é¢‘ç†è§£ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-20T05:32:43.570501+00:00"
    },
    "2026-02-18": {
      "date": "2026-02-18",
      "content": "Overview\n- Date: 2026-02-18\n- Total Papers: 25\n- Total Upvotes: 234\n- Papers with GitHub: 13\n\nKey Takeaways\n1. 2026-02-18 has 25 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 234).\n3. 13 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.14111] Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines? (ğŸ‘51)\n- [2602.12670] SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks (ğŸ‘38)\n- [2602.15763] GLM-5: from Vibe Coding to Agentic Engineering (ğŸ‘31)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T07:01:15.350488+00:00"
    },
    "2026-02-17": {
      "date": "2026-02-17",
      "content": "Overview\n- Date: 2026-02-17\n- Total Papers: 28\n- Total Upvotes: 156\n- Papers with GitHub: 17\n\nKey Takeaways\n1. å¤šæ¨¡æ€Agentä¸è§†è§‰æ£€ç´¢ç³»ç»Ÿæˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚\n2. å¼ºåŒ–å­¦ä¹ èŒƒå¼æŒç»­æ¼”è¿›ï¼Œä»ç»éªŒåæ€åˆ°æ¨ç†é©±åŠ¨çš„è¡¨å¾å­¦ä¹ ï¼Œæå‡ç¨€ç–å¥–åŠ±åœºæ™¯ä¸‹çš„å­¦ä¹ æ•ˆç‡ã€‚\n3. æ¨¡å‹æ•ˆç‡ä¸æ¶æ„åˆ›æ–°å¹¶é‡ï¼ŒäºŒè¿›åˆ¶Tokenè¡¨ç¤ºï¼ˆBinary Tokensï¼‰å’Œå°è§„æ¨¡é«˜æ€§èƒ½æ¨¡å‹ï¼ˆ3Bå‚æ•°ï¼‰å—åˆ°å…³æ³¨ã€‚\n4. è·¨é¢†åŸŸåº”ç”¨æ‹“å±•æ˜¾è‘—ï¼Œæ¶µç›–é‡å­æ•°æ®åº“ï¼ˆQuantum-Native Databaseï¼‰ã€ç§‘ç ”æ•°æ®è¯„ä¼°ä¸è‡ªåŠ¨åŒ–æœç´¢æ¡†æ¶ã€‚\n\nNotable Papers\n- [2602.10809] DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories (ğŸ‘24): æå‡ºæ¨¡å—åŒ–Agentæ¡†æ¶ï¼Œé€šè¿‡å¤šæ­¥æ¨ç†è§£å†³ä¼ ç»Ÿè¯­ä¹‰åŒ¹é…åœ¨è§†è§‰å†å²æ£€ç´¢ä¸­çš„å±€é™æ€§ã€‚\n- [2602.14492] Query as Anchor: Scenario-Adaptive User Representation via Large Language Model (ğŸ‘14): å°†ç”¨æˆ·å»ºæ¨¡ä»é™æ€ç¼–ç è½¬å˜ä¸ºåŸºäºLLMçš„åŠ¨æ€æŸ¥è¯¢æ„ŸçŸ¥åˆæˆï¼Œå®ç°åœºæ™¯è‡ªé€‚åº”è¡¨ç¤ºã€‚\n- [2602.13949] Experiential Reinforcement Learning (ğŸ‘14): å¼•å…¥æ˜¾å¼ç»éªŒ-åæ€-å·©å›ºå¾ªç¯æœºåˆ¶ï¼Œæ˜¾è‘—æå‡ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚\n- [2602.14699] Qute: Towards Quantum-Native Database (ğŸ‘13): é¦–ä¸ªå°†é‡å­è®¡ç®—ä½œä¸ºä¸€ç­‰æ‰§è¡Œé€‰é¡¹çš„æ•°æ®åº“ç³»ç»Ÿï¼Œæ”¯æŒæ‰©å±•SQLåˆ°é—¨çº§é‡å­ç”µè·¯çš„é«˜æ•ˆç¼–è¯‘ä¸æ··åˆä¼˜åŒ–ã€‚\n- [2602.14234] REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents (ğŸ‘13): é€šè¿‡ä»»åŠ¡åˆæˆã€å·¥å…·å¢å¼ºæŸ¥è¯¢å’Œæ¨¡æ‹Ÿç¯å¢ƒä¼˜åŒ–ï¼Œæ„å»ºå¯æ‰©å±•çš„é•¿ç¨‹æœç´¢Agentæ¡†æ¶ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-17T16:24:46.833682+00:00"
    },
    "2026-02-16": {
      "date": "2026-02-16",
      "content": "Overview\n- Date: 2026-02-16\n- Total Papers: 32\n- Total Upvotes: 639\n- Papers with GitHub: 20\n\nKey Takeaways\n1. å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰æŒç»­å‘å‚ç›´é¢†åŸŸæ·±åŒ–ï¼ŒåŒ»ç–—è¯Šæ–­ä¸ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æˆä¸ºé‡ç‚¹çªç ´æ–¹å‘ã€‚\n2. å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ç»“åˆç´§å¯†ï¼Œåœ¨æœºå™¨äººæ“æ§å’Œè§†è§‰æ¨ç†ä¸­æ¢ç´¢èƒ½åŠ›è¾¹ç•Œã€‚\n3. æ•°æ®æ•ˆç‡ç ”ç©¶å—å…³æ³¨ï¼Œç‰¹å¾ç©ºé—´æ•°æ®åˆæˆä¸ç¼–è§£ç å™¨å¯¹é½çš„ç¨€ç–æ€§æ–¹æ³•æå‡è®­ç»ƒä¸æ¨ç†æ•ˆç‡ã€‚\n4. é²æ£’æ€§è¯„ä¼°åŸºå‡†å»ºè®¾åŠ é€Ÿï¼Œé’ˆå¯¹è¯­éŸ³æ£€ç´¢å’Œåœ°ç†å®šä½ç­‰åœºæ™¯çš„ç³»ç»ŸåŒ–æµ‹è¯•æ¡†æ¶ç›¸ç»§æå‡ºã€‚\n\nNotable Papers\n- [2602.10388] Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs (ğŸ‘203): æå‡ºFeature Activation CoverageæŒ‡æ ‡ï¼Œåœ¨LLMå¯è§£é‡Šç‰¹å¾ç©ºé—´åˆæˆå¤šæ ·åŒ–æ•°æ®ï¼Œæ— éœ€å¢åŠ æ•°æ®é‡å³å¯æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚\n- [2602.12783] SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise (ğŸ‘134): æ„å»ºåŒ…å«37,317ä¸ªç‹¬ç‰¹æŸ¥è¯¢çš„è·¨è¯­è¨€é²æ£’æ€§åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°è¯­éŸ³æŸ¥è¯¢æ£€ç´¢åœ¨å£°å­¦å™ªå£°ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚\n- [2602.12705] MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs (ğŸ‘56): é€šè¿‡å®ä½“æ„ŸçŸ¥æŒç»­é¢„è®­ç»ƒã€RLHFå’Œå·¥å…·å¢å¼ºæ™ºèƒ½ä½“è®­ç»ƒï¼Œæ„å»ºé¢å‘å¯é åŒ»ç–—è¯Šæ–­çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚\n- [2602.11858] Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception (ğŸ‘52): æå‡ºRegion-to-Image Distillationæ–¹æ³•ï¼Œä½¿MLLMåœ¨æ¨ç†æ—¶å†…éƒ¨æ‰§è¡Œè¿­ä»£ç¼©æ”¾ï¼Œæ— éœ€é‡å¤å·¥å…·è°ƒç”¨å³å¯å®ç°ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥ã€‚\n- [2602.08683] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence (ğŸ‘40): åŸºäºè§†é¢‘å‹ç¼©çš„ä¿¡æ¯è®ºåŸç†ï¼Œåˆ©ç”¨Codecå¯¹é½çš„ç¨€ç–æ€§é©±åŠ¨ç¼–ç ï¼Œåœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§ä¸Šè¶…è¶Šä¼ ç»Ÿè§†è§‰ç¼–ç æ–¹æ³•ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-18T11:40:58.973200+00:00"
    },
    "2026-02-13": {
      "date": "2026-02-13",
      "content": "Overview\n- Date: 2026-02-13\n- Total Papers: 41\n- Total Upvotes: 950\n- Papers with GitHub: 26\n\nKey Takeaways\n1. 2026-02-13 has 41 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 950).\n3. 26 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.09877] The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Socâ€¦ (ğŸ‘187)\n- [2602.12036] Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Lanâ€¦ (ğŸ‘91)\n- [2602.12205] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation andâ€¦ (ğŸ‘77)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T06:51:27.952920+00:00"
    },
    "2026-02-12": {
      "date": "2026-02-12",
      "content": "Overview\n- Date: 2026-02-12\n- Total Papers: 48\n- Total Upvotes: 867\n- Papers with GitHub: 26\n\nKey Takeaways\n1. 2026-02-12 has 48 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 867).\n3. 26 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.10604] Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters (ğŸ‘176)\n- [2602.08099] VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval (ğŸ‘120)\n- [2602.11144] GENIUS: Generative Fluid Intelligence Evaluation Suite (ğŸ‘53)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T06:51:27.953115+00:00"
    },
    "2026-02-11": {
      "date": "2026-02-11",
      "content": "Overview\n- Date: 2026-02-11\n- Total Papers: 57\n- Total Upvotes: 1385\n- Papers with GitHub: 35\n\nKey Takeaways\n1. 2026-02-11 has 57 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 1385).\n3. 35 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.05400] OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-trainâ€¦ (ğŸ‘316)\n- [2602.09856] Code2World: A GUI World Model via Renderable Code Generation (ğŸ‘189)\n- [2602.09082] UI-Venus-1.5 Technical Report (ğŸ‘149)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T06:51:27.953169+00:00"
    },
    "2026-02-10": {
      "date": "2026-02-10",
      "content": "Overview\n- Date: 2026-02-10\n- Total Papers: 58\n- Total Upvotes: 1744\n- Papers with GitHub: 34\n\nKey Takeaways\n1. 2026-02-10 has 58 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 1744).\n3. 34 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.08222] Weak-Driven Learning: How Weak Agents make Strong Agents Stronger (ğŸ‘256)\n- [2602.07274] TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents (ğŸ‘197)\n- [2602.07085] QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining (ğŸ‘181)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T06:51:27.953237+00:00"
    },
    "2026-02-09": {
      "date": "2026-02-09",
      "content": "Overview\n- Date: 2026-02-09\n- Total Papers: 31\n- Total Upvotes: 474\n- Papers with GitHub: 12\n\nKey Takeaways\n1. 2026-02-09 has 31 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 474).\n3. 12 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.05027] AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders (ğŸ‘59)\n- [2602.05843] OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductiveâ€¦ (ğŸ‘57)\n- [2602.03392] On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models (ğŸ‘53)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T05:49:28.111676+00:00"
    },
    "2026-02-06": {
      "date": "2026-02-06",
      "content": "Overview\n- Date: 2026-02-06\n- Total Papers: 47\n- Total Upvotes: 810\n- Papers with GitHub: 24\n\nKey Takeaways\n1. 2026-02-06 has 47 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 810).\n3. 24 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2601.22027] CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-Worlâ€¦ (ğŸ‘81)\n- [2602.05386] Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adapâ€¦ (ğŸ‘69)\n- [2602.02474] MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents (ğŸ‘55)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T05:49:28.111778+00:00"
    },
    "2026-02-05": {
      "date": "2026-02-05",
      "content": "Overview\n- Date: 2026-02-05\n- Total Papers: 53\n- Total Upvotes: 1276\n- Papers with GitHub: 26\n\nKey Takeaways\n1. 2026-02-05 has 53 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 1276).\n3. 26 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.04705] ERNIE 5.0 Technical Report (ğŸ‘251)\n- [2602.03152] FASA: Frequency-aware Sparse Attention (ğŸ‘146)\n- [2602.04634] WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinâ€¦ (ğŸ‘93)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T05:49:28.111805+00:00"
    },
    "2026-02-04": {
      "date": "2026-02-04",
      "content": "Overview\n- Date: 2026-02-04\n- Total Papers: 53\n- Total Upvotes: 933\n- Papers with GitHub: 30\n\nKey Takeaways\n1. 2026-02-04 has 53 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 933).\n3. 30 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.01785] CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding (ğŸ‘93)\n- [2602.03786] AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration (ğŸ‘85)\n- [2602.02103] No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs (ğŸ‘70)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T05:49:28.111834+00:00"
    },
    "2026-02-03": {
      "date": "2026-02-03",
      "content": "Overview\n- Date: 2026-02-03\n- Total Papers: 73\n- Total Upvotes: 1874\n- Papers with GitHub: 40\n\nKey Takeaways\n1. 2026-02-03 has 73 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 1874).\n3. 40 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2602.00919] Green-VLA: Staged Vision-Language-Action Model for Generalist Robots (ğŸ‘280)\n- [2602.02276] Kimi K2.5: Visual Agentic Intelligence (ğŸ‘233)\n- [2601.22060] Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Languageâ€¦ (ğŸ‘153)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T05:49:28.111867+00:00"
    },
    "2026-02-02": {
      "date": "2026-02-02",
      "content": "Overview\n- Date: 2026-02-02\n- Total Papers: 41\n- Total Upvotes: 829\n- Papers with GitHub: 19\n\nKey Takeaways\n1. 2026-02-02 has 41 papers with broad coverage across multiple AI subfields.\n2. Community attention is concentrated on a few papers (total ğŸ‘ 829).\n3. 19 papers provide GitHub links, indicating practical reproducibility focus.\n\nNotable Papers\n- [2601.23265] PaperBanana: Automating Academic Illustration for AI Scientists (ğŸ‘188)\n- [2601.22975] Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Interâ€¦ (ğŸ‘100)\n- [2601.21558] ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas (ğŸ‘58)",
      "source": "fallback",
      "model": "",
      "generated_at": "2026-02-19T05:49:28.111890+00:00"
    }
  },
  "papers": [
    {
      "date": "2026-02-20",
      "paper_id": "2602.13515",
      "title": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning",
      "authors": [
        "Jintao Zhang",
        "Kai Jiang",
        "Chendong Xiang",
        "Weiqi Feng",
        "Yuezhou Hu",
        "Haocheng Xi",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "abstract": "A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning. Many training-free sparse attention methods are effective for accelerating diffusion models . Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p , fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention . Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.",
      "summary_en": "A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åä¸ºSpargeAttention2çš„å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ··åˆæ©ç è§„åˆ™å’Œè’¸é¦å¯å‘çš„å¾®è°ƒï¼Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°é«˜ç¨€ç–æ€§çš„åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13515",
      "arxiv_url": "https://arxiv.org/abs/2602.13515",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13515",
      "github_url": "https://github.com/thu-ml/SpargeAttn",
      "upvotes": 24,
      "fetched_at": "2026-02-21T01:52:46.205050+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16855",
      "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents",
      "authors": [
        "Haiyang Xu",
        "Xi Zhang",
        "Haowei Liu",
        "Junyang Wang",
        "Zhaozai Zhu",
        "Shengjie Zhou",
        "Xuhao Hu",
        "Feiyu Gao",
        "Junjie Cao",
        "Zihua Wang",
        "Zhiyuan Chen",
        "Jitong Liao",
        "Qi Zheng",
        "Jiahui Zeng",
        "Ze Xu",
        "Shuai Bai",
        "Junyang Lin",
        "Jingren Zhou",
        "Ming Yan"
      ],
      "abstract": "GUI-Owl-1.5 is a multi-platform GUI agent model with multiple sizes that achieves state-of-the-art performance on GUI automation, grounding, tool-calling, and memory tasks through innovations in hybrid data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning. The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments , in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation ; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm , MRPO , to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.",
      "summary_en": "GUI-Owl-1.5 is a multi-platform GUI agent model with multiple sizes that achieves state-of-the-art performance on GUI automation, grounding, tool-calling, and memory tasks through innovations in hybrid data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning.",
      "summary_zh": "GUI-Owl-1.5 æ˜¯ä¸€æ¬¾å…·å¤‡å¤šç§å°ºå¯¸çš„å¤šå¹³å° GUI æ™ºèƒ½ä½“æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ··åˆæ•°æ®æµæ°´çº¿ã€ç»Ÿä¸€æ¨ç†å¢å¼ºå’Œå¤šå¹³å°å¼ºåŒ–å­¦ä¹ æ–¹é¢çš„åˆ›æ–°ï¼Œåœ¨ GUI è‡ªåŠ¨åŒ–ã€groundingã€å·¥å…·è°ƒç”¨å’Œè®°å¿†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16855",
      "arxiv_url": "https://arxiv.org/abs/2602.16855",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16855",
      "github_url": "https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3.5",
      "upvotes": 22,
      "fetched_at": "2026-02-21T01:52:56.793327+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17270",
      "title": "Unified Latents (UL): How to train your latents",
      "authors": [
        "Jonathan Heek",
        "Emiel Hoogeboom",
        "Thomas Mensink",
        "Tim Salimans"
      ],
      "abstract": "Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute. We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model . By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate . On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality ( PSNR ) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.",
      "summary_en": "Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.",
      "summary_zh": "Unified Latents æ¡†æ¶é€šè¿‡æ‰©æ•£å…ˆéªŒæ­£åˆ™åŒ–ä¸æ‰©æ•£æ¨¡å‹è§£ç å­¦ä¹ è”åˆæ½œåœ¨è¡¨ç¤ºï¼Œä»¥æ›´ä½çš„è®­ç»ƒè®¡ç®—é‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ FID åˆ†æ•°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17270",
      "arxiv_url": "https://arxiv.org/abs/2602.17270",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17270",
      "github_url": "",
      "upvotes": 21,
      "fetched_at": "2026-02-21T01:53:03.694814+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.15569",
      "title": "\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing",
      "authors": [
        "Johannes Kirmayr",
        "Raphael Wennmacher",
        "Khanh Huynh",
        "Lukas Stappen",
        "Elisabeth AndrÃ©",
        "Florian Alt"
      ],
      "abstract": "Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios. Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience : how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant , we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts . Interviews further revealed user preferences for an adaptive approach : high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency .",
      "summary_en": "Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios.",
      "summary_zh": "ç”¨æˆ·åå¥½åœ¨è½¦è½½AIåŠ©æ‰‹ä¸­é‡‡ç”¨è‡ªé€‚åº”åé¦ˆæœºåˆ¶ï¼Œå³åˆæœŸé€šè¿‡é«˜é€æ˜åº¦å»ºç«‹ä¿¡ä»»ï¼Œéšåéšå¯é æ€§æå‡è€Œé™ä½åé¦ˆè¯¦ç»†ç¨‹åº¦ï¼Œå°¤å…¶åœ¨æ³¨æ„åŠ›å…³é”®å‹é©¾é©¶åœºæ™¯ä¸­ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15569",
      "arxiv_url": "https://arxiv.org/abs/2602.15569",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15569",
      "github_url": "https://github.com/johanneskirmayr/agentic_llm_feedback",
      "upvotes": 12,
      "fetched_at": "2026-02-21T01:52:50.074003+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16699",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "authors": [
        "Wenxuan Ding",
        "Nicholas Tomlin",
        "Greg Durrett"
      ],
      "abstract": "Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments. LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs , then perform more optimal environment exploration . We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "summary_en": "Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.",
      "summary_zh": "é€šè¿‡ Calibrate-Then-Act æ¡†æ¶æ˜¾å¼æ¨ç†æˆæœ¬-ä¸ç¡®å®šæ€§æƒè¡¡ï¼Œå¯åœ¨å¤æ‚ä»»åŠ¡ä¸Šæ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¢å¼ºåºåˆ—ç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16699",
      "arxiv_url": "https://arxiv.org/abs/2602.16699",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16699",
      "github_url": "https://github.com/Wenwen-D/env-explorer",
      "upvotes": 11,
      "fetched_at": "2026-02-21T01:52:52.124534+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17004",
      "title": "Arcee Trinity Large Technical Report",
      "authors": [
        "Varun Singh",
        "Lucas Krauss",
        "Sami Jaghouar",
        "Matej Sirovatka",
        "Charles Goddard",
        "Fares Obied",
        "Jack Min Ong",
        "Jannik Straube",
        "Fern",
        "Aria Harley",
        "Conner Stewart",
        "Colin Kealty",
        "Maziyar Panahi",
        "Simon Kirsten",
        "Anushka Deshpande",
        "Anneketh Vij",
        "Arthur Bresnu",
        "Pranav Veldurthi",
        "Raghav Ravishankar",
        "Hardik Bishnoi",
        "DatologyAI Team",
        "Arcee AI Team"
      ],
      "abstract": "Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.",
      "summary_en": "Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.",
      "summary_zh": "Arcee Trinity æ¨¡å‹ä¸ºç¨€ç–æ··åˆä¸“å®¶æ¶æ„ï¼Œå‚æ•°é‡ä¸æ¿€æ´»æ¨¡å¼å„å¼‚ï¼Œé‡‡ç”¨å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶ä¸è®­ç»ƒä¼˜åŒ–æŠ€æœ¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17004",
      "arxiv_url": "https://arxiv.org/abs/2602.17004",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17004",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-21T01:53:01.183988+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.13579",
      "title": "TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment",
      "authors": [
        "Youngsun Wi",
        "Jessica Yin",
        "Elvis Xiang",
        "Akash Sharma",
        "Jitendra Malik",
        "Mustafa Mukadam",
        "Nima Fazeli",
        "Tess Hellebrekers"
      ],
      "abstract": "TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels. Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow , without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs . We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).",
      "summary_en": "TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.",
      "summary_zh": "TactAlign é€šè¿‡è·¨å…·èº«è§¦è§‰å¯¹é½ï¼Œå®ç°äº†äººç±»è§¦è§‰æ¼”ç¤ºå‘ä¸åŒå…·èº«æœºå™¨äººçš„è¿ç§»ï¼Œæ— éœ€é…å¯¹æ•°æ®æˆ–äººå·¥æ ‡æ³¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13579",
      "arxiv_url": "https://arxiv.org/abs/2602.13579",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13579",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-21T01:52:47.098792+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16968",
      "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers",
      "authors": [
        "Dahye Kim",
        "Deepti Ghadiyaram",
        "Raghudeep Gadde"
      ],
      "abstract": "Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss. Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase , regardless of the content's complexity. We propose dynamic tokenization , an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep . Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference , our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.",
      "summary_en": "Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.",
      "summary_zh": "åŠ¨æ€tokenåŒ–é€šè¿‡åŸºäºå†…å®¹å¤æ‚åº¦å’Œå»å™ªæ—¶é—´æ­¥è°ƒæ•´patchå°ºå¯¸æ¥æå‡æ‰©æ•£Transformerçš„æ•ˆç‡ï¼Œå®ç°äº†æ— æŸè´¨é‡ä¸‹çš„æ˜¾è‘—åŠ é€Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16968",
      "arxiv_url": "https://arxiv.org/abs/2602.16968",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16968",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-21T01:52:59.961025+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.14457",
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
      "authors": [
        "Dongrui Liu",
        "Yi Yu",
        "Jie Zhang",
        "Guanxu Chen",
        "Qihao Lin",
        "Hanxi Zhu",
        "Lige Huang",
        "Yijin Zhou",
        "Peng Wang",
        "Shuai Shao",
        "Boxuan Zhang",
        "Zicheng Liu",
        "Jingwei Sun",
        "Yu Li",
        "Yuejin Xie",
        "Jiaxuan Guo",
        "Jia Xu",
        "Chaochao Lu",
        "Bowen Zhou",
        "Xia Hu",
        "Jing Shao"
      ],
      "abstract": "Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.",
      "summary_en": "Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.",
      "summary_zh": "å‰æ²¿AIé£é™©åˆ†æè¯„ä¼°ç½‘ç»œæ”»å‡»ã€è¯´æœä¸æ“çºµã€æˆ˜ç•¥æ¬ºéª—ã€ä¸å—æ§çš„AIç ”å‘å’Œè‡ªæˆ‘å¤åˆ¶ç­‰å…³é”®ç»´åº¦ï¼Œæå‡ºç¼“è§£ç­–ç•¥ä»¥ä¿éšœå…ˆè¿›AIç³»ç»Ÿçš„å®‰å…¨éƒ¨ç½²ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14457",
      "arxiv_url": "https://arxiv.org/abs/2602.14457",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14457",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-21T01:52:47.741657+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17288",
      "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
      "authors": [
        "Anuj Gupta"
      ],
      "abstract": "Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources. While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization , and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability , scaling behavior , data yield losses , and infrastructure bottlenecks . Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
      "summary_en": "Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.",
      "summary_zh": "åŸºäºåŸå§‹arXiv LaTeXæºè®­ç»ƒ13.6äº¿å‚æ•°çš„ç§‘å­¦è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†é¢„å¤„ç†å†³ç­–ã€åˆ†è¯å’ŒåŸºç¡€è®¾æ–½çº¦æŸåœ¨è®¡ç®—èµ„æºæœ‰é™æƒ…å†µä¸‹å¯¹æ¨¡å‹å¼€å‘çš„å½±å“ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17288",
      "arxiv_url": "https://arxiv.org/abs/2602.17288",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17288",
      "github_url": "https://github.com/kitefishai/KiteFish-A1-1.5B-Math",
      "upvotes": 4,
      "fetched_at": "2026-02-21T01:53:04.592102+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16928",
      "title": "Discovering Multiagent Learning Algorithms with Large Language Models",
      "authors": [
        "Zun Li",
        "John Schultz",
        "Daniel Hennes",
        "Marc Lanctot"
      ],
      "abstract": "AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.",
      "summary_en": "AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.",
      "summary_zh": "AlphaEvolve æ˜¯ä¸€ç§ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„è¿›åŒ–å¼ç¼–ç æ™ºèƒ½ä½“ï¼Œé€šè¿‡æ¼”åŒ–é—æ†¾æœ€å°åŒ–ä¸åŸºäºç§ç¾¤è®­ç»ƒçš„å˜ä½“ï¼Œè‡ªåŠ¨å‘ç°é¢å‘ä¸å®Œå…¨ä¿¡æ¯åšå¼ˆçš„æ–°å‹å¤šæ™ºèƒ½ä½“å­¦ä¹ ç®—æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16928",
      "arxiv_url": "https://arxiv.org/abs/2602.16928",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16928",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-21T01:52:58.910993+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17365",
      "title": "Computer-Using World Model",
      "authors": [
        "Yiming Guan",
        "Rui Yu",
        "John Zhang",
        "Lu Wang",
        "Chaoyun Zhang",
        "Liqun Li",
        "Bo Qiao",
        "Si Qin",
        "He Huang",
        "Fangkai Yang",
        "Pu Zhao",
        "Lukas Wutschitz",
        "Samuel Kessler",
        "Huseyin A Inan",
        "Robert Sim",
        "Saravan Rajmohan",
        "Qingwei Lin",
        "Dongmei Zhang"
      ],
      "abstract": "A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks. Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments . We evaluate CUWM via test-time action search , where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
      "summary_en": "A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.",
      "summary_zh": "ä¸€ç§ç”¨äºæ¡Œé¢è½¯ä»¶çš„ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡å…ˆæ–‡æœ¬æè¿°åè§†è§‰åˆæˆçš„æ–¹å¼é¢„æµ‹ UI çŠ¶æ€å˜åŒ–ï¼Œè¿›è€Œæå‡è®¡ç®—æœºä½¿ç”¨ä»»åŠ¡çš„å†³ç­–è´¨é‡ä¸æ‰§è¡Œé²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17365",
      "arxiv_url": "https://arxiv.org/abs/2602.17365",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17365",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-21T01:53:07.596474+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17259",
      "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
      "authors": [
        "Han Zhao",
        "Jingbo Wang",
        "Wenxuan Song",
        "Shuai Chen",
        "Yang Liu",
        "Yan Wang",
        "Haoang Li",
        "Donglin Wang"
      ],
      "abstract": "FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models. Enabling VLA models to predict environmental dynamics, known as world modeling , has been recognized as essential for improving robotic reasoning and generalization . However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction , which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation . To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy : In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models . By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies . Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.",
      "summary_en": "FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models.",
      "summary_zh": "FRAPPE é‡‡ç”¨å¹¶è¡Œæ¸è¿›æ‰©å±•æ¥æ”¹å–„è¡¨å¾å¯¹é½å¹¶é™ä½é¢„æµ‹æ¨¡å‹çš„è¯¯å·®ç´¯ç§¯ï¼Œä»è€Œè§£å†³æœºå™¨äººä¸–ç•Œå»ºæ¨¡ä¸­çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17259",
      "arxiv_url": "https://arxiv.org/abs/2602.17259",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17259",
      "github_url": "https://github.com/OpenHelix-Team/frappe",
      "upvotes": 3,
      "fetched_at": "2026-02-21T01:53:02.222425+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16849",
      "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
      "authors": [
        "Jianliang He",
        "Leda Wang",
        "Siyu Chen",
        "Zhuoran Yang"
      ],
      "abstract": "Two-layer neural networks solve modular addition by learning Fourier features through phase symmetry and frequency diversification, enabling robust computation via majority voting despite individual neuron noise. We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized , consisting of two parts: phase symmetry and frequency diversification . We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism . Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma . Finally, we use these insights to demystify grokking , characterizing it as a three-stage process involving memorization followed by two generalization phases , driven by the competition between loss minimization and weight decay .",
      "summary_en": "Two-layer neural networks solve modular addition by learning Fourier features through phase symmetry and frequency diversification, enabling robust computation via majority voting despite individual neuron noise.",
      "summary_zh": "ä¸¤å±‚ç¥ç»ç½‘ç»œé€šè¿‡ç›¸ä½å¯¹ç§°å’Œé¢‘ç‡å¤šæ ·åŒ–å­¦ä¹ å‚…é‡Œå¶ç‰¹å¾æ¥è§£å†³æ¨¡åŠ æ³•ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨åœ¨å•ä¸ªç¥ç»å…ƒå™ªå£°å­˜åœ¨çš„æƒ…å†µä¸‹å®ç°ç¨³å¥è®¡ç®—ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16849",
      "arxiv_url": "https://arxiv.org/abs/2602.16849",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16849",
      "github_url": "https://github.com/Y-Agent/modular-addition-feature-learning",
      "upvotes": 3,
      "fetched_at": "2026-02-21T01:52:55.865295+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17363",
      "title": "2Mamba2Furious: Linear in Complexity, Competitive in Accuracy",
      "authors": [
        "Gabriel Mongaras",
        "Eric C. Larson"
      ],
      "abstract": "Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences. Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention . To bridge the accuracy gap between softmax attention and linear attention , we manipulate Mamba-2 , a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant ( Mamba-2 S), we improve the A-mask and increase the order of the hidden state , resulting in a method, which we call 2Mamba , that is nearly as accurate as softmax attention , yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments",
      "summary_en": "Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.",
      "summary_zh": "ç ”ç©¶äººå‘˜é€šè¿‡ç®€åŒ– Mamba-2 å¹¶æ”¹è¿›å…¶æ¶æ„ç»„ä»¶æ¥å¢å¼ºçº¿æ€§æ³¨æ„åŠ›ï¼Œåœ¨ä¿æŒé•¿åºåˆ—å†…å­˜æ•ˆç‡çš„åŒæ—¶å®ç°äº†æ¥è¿‘ softmax çš„ç²¾åº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17363",
      "arxiv_url": "https://arxiv.org/abs/2602.17363",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17363",
      "github_url": "https://github.com/gmongaras/2Mamba2Furious",
      "upvotes": 2,
      "fetched_at": "2026-02-21T01:53:06.090207+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.15823",
      "title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
      "authors": [
        "Zarif Ikram",
        "Arad Firouzkouhi",
        "Stephen Tu",
        "Mahdi Soltanolkotabi",
        "Paria Rashidinejad"
      ],
      "abstract": "CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations. A central challenge in large language model (LLM) editing is capability preservation : methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence , whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.",
      "summary_en": "CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.",
      "summary_zh": "CrispEditæ˜¯ä¸€ç§é¢å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„äºŒé˜¶ç¼–è¾‘ç®—æ³•ï¼Œå…¶é€šè¿‡ä½¿ç”¨Bregmanæ•£åº¦å’Œé«˜æ•ˆçš„Kroneckeråˆ†è§£è¿‘ä¼¼ï¼Œå°†æ›´æ–°çº¦æŸåœ¨èƒ½åŠ›æŸå¤±æ™¯è§‚çš„ä½æ›²ç‡å­ç©ºé—´ä¸­ï¼Œä»¥ä¿ç•™æ¨¡å‹èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15823",
      "arxiv_url": "https://arxiv.org/abs/2602.15823",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15823",
      "github_url": "https://github.com/zarifikram/CrispEdit",
      "upvotes": 2,
      "fetched_at": "2026-02-21T01:52:50.688676+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.17588",
      "title": "Modeling Distinct Human Interaction in Web Agents",
      "authors": [
        "Faria Huq",
        "Zora Zhiruo Wang",
        "Zhanqiu Guo",
        "Venu Arvind Arangarajan",
        "Tianyue Ou",
        "Frank Xu",
        "Shuyan Zhou",
        "Graham Neubig",
        "Jeffrey P. Bigham"
      ],
      "abstract": "Human intervention patterns in web navigation are modeled to improve agent adaptability and collaboration, with language models achieving better intervention prediction and user satisfaction. Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness . Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.",
      "summary_en": "Human intervention patterns in web navigation are modeled to improve agent adaptability and collaboration, with language models achieving better intervention prediction and user satisfaction.",
      "summary_zh": "ç½‘é¡µå¯¼èˆªä¸­çš„äººç±»å¹²é¢„æ¨¡å¼è¢«å»ºæ¨¡ä»¥æå‡æ™ºèƒ½ä½“é€‚åº”æ€§ä¸åä½œèƒ½åŠ›ï¼Œè¯­è¨€æ¨¡å‹å®ç°äº†æ›´ä¼˜çš„å¹²é¢„é¢„æµ‹å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.17588",
      "arxiv_url": "https://arxiv.org/abs/2602.17588",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17588",
      "github_url": "https://github.com/oaishi/PlowPilot",
      "upvotes": 1,
      "fetched_at": "2026-02-21T01:53:08.208943+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16756",
      "title": "NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist",
      "authors": [
        "Johannes Bertram",
        "Jonas Geiping"
      ],
      "abstract": "NESSiE benchmark reveals safety vulnerabilities in large language models through simple security tests, demonstrating that even state-of-the-art models fail basic safety requirements without adversarial attacks. We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security , NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety , even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.",
      "summary_en": "NESSiE benchmark reveals safety vulnerabilities in large language models through simple security tests, demonstrating that even state-of-the-art models fail basic safety requirements without adversarial attacks.",
      "summary_zh": "NESSiE åŸºå‡†æµ‹è¯•é€šè¿‡ç®€å•å®‰å…¨æµ‹è¯•æ­ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ¼æ´ï¼Œè¡¨æ˜å³ä½¿æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ— éœ€å¯¹æŠ—æ€§æ”»å‡»çš„æƒ…å†µä¸‹ä¹Ÿæ— æ³•æ»¡è¶³åŸºæœ¬å®‰å…¨è¦æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16756",
      "arxiv_url": "https://arxiv.org/abs/2602.16756",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16756",
      "github_url": "https://github.com/JohannesBertram/NESSiE",
      "upvotes": 1,
      "fetched_at": "2026-02-21T01:52:52.787014+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.14857",
      "title": "World Models for Policy Refinement in StarCraft II",
      "authors": [
        "Yixin Zhang",
        "Ziyi Wang",
        "Yiming Rong",
        "Haoxi Wang",
        "Jinling Jiang",
        "Shuang Xu",
        "Haoran Wu",
        "Shiyu Zhou",
        "Bo Xu"
      ],
      "abstract": "StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability. Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability , is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability . To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k , the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement . Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.",
      "summary_en": "StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.",
      "summary_zh": "StarWM æ˜¯é¢å‘ StarCraft II çš„ä¸–ç•Œæ¨¡å‹ï¼Œåˆ©ç”¨ç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤ºåœ¨éƒ¨åˆ†å¯è§‚æµ‹æ¡ä»¶ä¸‹é¢„æµ‹æœªæ¥è§‚æµ‹ï¼Œå¹¶åœ¨èƒœç‡å’Œå†³ç­–ç¨³å®šæ€§æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14857",
      "arxiv_url": "https://arxiv.org/abs/2602.14857",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14857",
      "github_url": "https://github.com/yxzzhang/StarWM",
      "upvotes": 1,
      "fetched_at": "2026-02-21T01:52:49.150379+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.10377",
      "title": "Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs",
      "authors": [
        "Luoyang Sun",
        "Jiwen Jiang",
        "Yifeng Ding",
        "Fengfa Li",
        "Yan Song",
        "Haifeng Zhang",
        "Jian Ying",
        "Lei Ren",
        "Kun Zhan",
        "Wei Chen",
        "Yan Xie",
        "Cheng Deng"
      ],
      "abstract": "A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance. Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling . We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.",
      "summary_en": "A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.",
      "summary_zh": "ä¸€ç§ç”¨äºç«¯ä¾§å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²çš„ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒæŸå¤±å»ºæ¨¡å’Œå±‹é¡¶çº¿åˆ†æå»ºç«‹å‡†ç¡®ç‡-å»¶è¿Ÿå…³ç³»ï¼Œå®ç°å¿«é€Ÿæ¶æ„é€‰æ‹©ä¸æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10377",
      "arxiv_url": "https://arxiv.org/abs/2602.10377",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10377",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-21T01:52:44.812399+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16915",
      "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation",
      "authors": [
        "Zeyu Ren",
        "Xiang Li",
        "Yiran Wang",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with a selective state space ConvSS2D operator for efficient long-range propagation and introduces a large-scale synthetic underwater dataset. Stereo depth estimation is fundamental to underwater robotic perception , yet suffers from severe domain shift s caused by wavelength-dependent light attenuation , scattering , and refraction . Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation , limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models . The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID , with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.",
      "summary_en": "StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with a selective state space ConvSS2D operator for efficient long-range propagation and introduces a large-scale synthetic underwater dataset.",
      "summary_zh": "StereoAdapter-2 é€šè¿‡ä»¥é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´ ConvSS2D ç®—å­æ›¿æ¢ ConvGRU å®ç°é«˜æ•ˆé•¿è·ç¦»ä¼ æ’­ï¼Œå¹¶å¼•å…¥å¤§è§„æ¨¡åˆæˆæ°´ä¸‹æ•°æ®é›†ï¼Œæ”¹è¿›äº†æ°´ä¸‹ç«‹ä½“æ·±åº¦ä¼°è®¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16915",
      "arxiv_url": "https://arxiv.org/abs/2602.16915",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16915",
      "github_url": "https://github.com/AIGeeksGroup/StereoAdapter-2",
      "upvotes": 0,
      "fetched_at": "2026-02-21T01:52:57.701265+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16835",
      "title": "NeST: Neuron Selective Tuning for LLM Safety",
      "authors": [
        "Sasha Behrouzi",
        "Lichao Wu",
        "Mohamadreza Rostami",
        "Ahmad-Reza Sadeghi"
      ],
      "abstract": "NeST is a lightweight safety alignment framework that selectively adapts safety-relevant neurons while keeping the rest of the model frozen, achieving significant reductions in unsafe generations with minimal trainable parameters. Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains. We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.",
      "summary_en": "NeST is a lightweight safety alignment framework that selectively adapts safety-relevant neurons while keeping the rest of the model frozen, achieving significant reductions in unsafe generations with minimal trainable parameters.",
      "summary_zh": "NeST æ˜¯ä¸€ç§è½»é‡çº§å®‰å…¨å¯¹é½æ¡†æ¶ï¼Œé€‰æ‹©æ€§è°ƒæ•´å®‰å…¨ç›¸å…³ç¥ç»å…ƒå¹¶ä¿æŒæ¨¡å‹å…¶ä½™éƒ¨åˆ†å†»ç»“ï¼Œä»¥æå°‘çš„å¯è®­ç»ƒå‚æ•°æ˜¾è‘—å‡å°‘ä¸å®‰å…¨ç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16835",
      "arxiv_url": "https://arxiv.org/abs/2602.16835",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16835",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-21T01:52:54.399708+00:00"
    },
    {
      "date": "2026-02-20",
      "paper_id": "2602.16802",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains",
      "authors": [
        "Kejian Shi",
        "Yixin Liu",
        "Peifeng Wang",
        "Alexander R. Fabbri",
        "Shafiq Joty",
        "Arman Cohan"
      ],
      "abstract": "Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques. While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment . In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM , a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard . These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "summary_en": "Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.",
      "summary_zh": "å‚è€ƒå¼•å¯¼çš„LLM-evaluatorä½œä¸ºè½¯éªŒè¯å™¨å¢å¼ºLLMå¯¹é½ï¼Œæå‡è¯„åˆ¤å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡è‡ªæ”¹è¿›æŠ€æœ¯åœ¨ä¸å¯éªŒè¯é¢†åŸŸå®ç°æœ‰æ•ˆçš„åè®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16802",
      "arxiv_url": "https://arxiv.org/abs/2602.16802",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16802",
      "github_url": "https://github.com/yale-nlp/RLRR",
      "upvotes": 0,
      "fetched_at": "2026-02-21T01:52:53.385009+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14296",
      "title": "AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines",
      "authors": [
        "Yifan Wu",
        "Yiran Peng",
        "Yiyu Chen",
        "Jianhao Ruan",
        "Zijie Zhuang",
        "Cheng Yang",
        "Jiayi Zhang",
        "Man Chen",
        "Yenchi Tseng",
        "Zhaoyang Yu",
        "Liang Chen",
        "Yuyao Zhai",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "abstract": "AutoWebWorld synthesizes verifiable web environments using finite state machines and coding agents, enabling efficient training of autonomous Web GUI agents through automated trajectory generation and verification. The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline , generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.",
      "summary_en": "AutoWebWorld synthesizes verifiable web environments using finite state machines and coding agents, enabling efficient training of autonomous Web GUI agents through automated trajectory generation and verification.",
      "summary_zh": "AutoWebWorld ä½¿ç”¨æœ‰é™çŠ¶æ€æœºå’Œç¼–ç æ™ºèƒ½ä½“åˆæˆå¯éªŒè¯çš„ç½‘é¡µç¯å¢ƒï¼Œé€šè¿‡è‡ªåŠ¨åŒ–è½¨è¿¹ç”Ÿæˆä¸éªŒè¯å®ç°è‡ªä¸» Web GUI æ™ºèƒ½ä½“çš„é«˜æ•ˆè®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14296",
      "arxiv_url": "https://arxiv.org/abs/2602.14296",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14296",
      "github_url": "",
      "upvotes": 33,
      "fetched_at": "2026-02-20T05:30:40.900245+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.12675",
      "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT",
      "authors": [
        "Jintao Zhang",
        "Haoxu Wang",
        "Kai Jiang",
        "Kaiwen Zheng",
        "Youhe Jiang",
        "Ion Stoica",
        "Jianfei Chen",
        "Jun Zhu",
        "Joseph E. Gonzalez"
      ],
      "abstract": "SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality. Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models , SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.",
      "summary_en": "SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.",
      "summary_zh": "SLA2 é€šè¿‡å¼•å…¥å¯å­¦ä¹ è·¯ç”±ã€ç›´æ¥æ³¨æ„åŠ›å½¢å¼åŠé‡åŒ–æ„ŸçŸ¥å¾®è°ƒï¼Œæ”¹è¿›äº†æ‰©æ•£æ¨¡å‹ä¸­çš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸è´¨é‡ä¸Šå®ç°æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12675",
      "arxiv_url": "https://arxiv.org/abs/2602.12675",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12675",
      "github_url": "https://github.com/thu-ml/SLA",
      "upvotes": 25,
      "fetched_at": "2026-02-19T06:47:22.414393+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16705",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "authors": [
        "Runpei Dong",
        "Ziyan Li",
        "Xialin He",
        "Saurabh Gupta"
      ],
      "abstract": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding. Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment , and d) replanning . Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation , where we use open-vocabulary large vision models for strong visual generalization . Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
      "summary_en": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.",
      "summary_zh": "HEROç»“åˆç²¾ç¡®çš„æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶ä¸å¼€æ”¾è¯æ±‡è§†è§‰æ¨¡å‹ï¼Œå®ç°å¯æ³›åŒ–çš„åœºæ™¯ç†è§£ï¼Œä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æ‰§è¡Œç‰©ä½“æ“ä½œã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16705",
      "arxiv_url": "https://arxiv.org/abs/2602.16705",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16705",
      "github_url": "",
      "upvotes": 20,
      "fetched_at": "2026-02-19T06:47:41.667558+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16317",
      "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
      "authors": [
        "Maksim Elistratov",
        "Marina Barannikov",
        "Gregory Ivanov",
        "Valentin Khrulkov",
        "Anton Konushin",
        "Andrey Kuznetsov",
        "Dmitrii Zhemchuzhnikov"
      ],
      "abstract": "CADEvolve presents an evolution-based approach using VLM-guided edits to generate complex CAD programs from simple primitives, creating a large dataset for improved Image2CAD performance. Computer-Aided Design ( CAD ) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLM s often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CAD Evolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM -guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators . After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CAD Evolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD , Fusion 360 , and MCB benchmarks.",
      "summary_en": "CADEvolve presents an evolution-based approach using VLM-guided edits to generate complex CAD programs from simple primitives, creating a large dataset for improved Image2CAD performance.",
      "summary_zh": "CADEvolveæå‡ºäº†ä¸€ç§åŸºäºè¿›åŒ–çš„æ–¹æ³•ï¼Œåˆ©ç”¨VLMå¼•å¯¼çš„ç¼–è¾‘ä»ç®€å•åŸºå…ƒç”Ÿæˆå¤æ‚CADç¨‹åºï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ä»¥æå‡Image2CADæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16317",
      "arxiv_url": "https://arxiv.org/abs/2602.16317",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16317",
      "github_url": "https://github.com/zhemdi/CADEvolve",
      "upvotes": 19,
      "fetched_at": "2026-02-20T05:30:48.449501+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16008",
      "title": "MAEB: Massive Audio Embedding Benchmark",
      "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
      ],
      "abstract": "MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance. We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks , and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models . MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.",
      "summary_en": "MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.",
      "summary_zh": "MAEBæ˜¯ä¸€é¡¹å¤§è§„æ¨¡éŸ³é¢‘åŸºå‡†ï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒå£°éŸ³é¢†åŸŸçš„30é¡¹ä»»åŠ¡ï¼Œå¯¹50ä½™ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå¹¶å»ºç«‹äº†ä¸audio LLMæ€§èƒ½çš„å…³è”ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16008",
      "arxiv_url": "https://arxiv.org/abs/2602.16008",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16008",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-20T05:30:46.096894+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14979",
      "title": "RynnBrain: Open Embodied Foundation Models",
      "authors": [
        "Ronghao Dang",
        "Jiayan Guo",
        "Bohan Hou",
        "Sicong Leng",
        "Kehan Li",
        "Xin Li",
        "Jiangpin Liu",
        "Yunxuan Mao",
        "Zhikai Wang",
        "Yuqian Yuan",
        "Minghao Zhu",
        "Xiao Lin",
        "Yang Bai",
        "Qian Jiang",
        "Yaxi Zhao",
        "Minghua Zeng",
        "Junlong Gao",
        "Yuming Jiang",
        "Jun Cen",
        "Siteng Huang",
        "Liuyi Wang",
        "Wenqiao Zhang"
      ],
      "abstract": "RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants. Despite rapid progress in multimodal foundation models , embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence . RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding , diverse spatiotemporal localization , physically grounded reasoning , and physics-aware planning . The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE ) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks , our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.",
      "summary_en": "RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.",
      "summary_zh": "RynnBrainæ˜¯ä¸€ä¸ªé¢å‘å…·èº«æ™ºèƒ½çš„å¼€æºæ—¶ç©ºåŸºç¡€æ¨¡å‹ï¼Œç»Ÿä¸€äº†å¤šå°ºåº¦åŠä»»åŠ¡ç‰¹å®šå˜ä½“ä¸‹çš„æ„ŸçŸ¥ã€æ¨ç†ä¸è§„åˆ’èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14979",
      "arxiv_url": "https://arxiv.org/abs/2602.14979",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14979",
      "github_url": "https://github.com/alibaba-damo-academy/RynnBrain",
      "upvotes": 10,
      "fetched_at": "2026-02-19T06:47:26.457151+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16704",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "authors": [
        "Hee Seung Hwang",
        "Xindi Wu",
        "Sanghyuk Chun",
        "Olga Russakovsky"
      ],
      "abstract": "REFINE is a reinforcement learning framework that improves fast weight models for long-context modeling by training under next-sequence prediction instead of next-token prediction, enhancing their ability to capture long-range dependencies. Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy , generates multi-token rollouts , assigns self-supervised sequence-level rewards , and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval , long-context question answering , and diverse tasks in LongBench . REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures .",
      "summary_en": "REFINE is a reinforcement learning framework that improves fast weight models for long-context modeling by training under next-sequence prediction instead of next-token prediction, enhancing their ability to capture long-range dependencies.",
      "summary_zh": "REFINEæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸€åºåˆ—é¢„æµ‹ä»£æ›¿ä¸‹ä¸€tokené¢„æµ‹è¿›è¡Œè®­ç»ƒï¼Œæ”¹è¿›ç”¨äºé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„å¿«é€Ÿæƒé‡æ¨¡å‹ï¼Œä»è€Œå¢å¼ºå…¶æ•è·é•¿ç¨‹ä¾èµ–çš„èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16704",
      "arxiv_url": "https://arxiv.org/abs/2602.16704",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16704",
      "github_url": "https://github.com/princetonvisualai/ReFINE",
      "upvotes": 9,
      "fetched_at": "2026-02-20T05:30:49.540159+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14080",
      "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality",
      "authors": [
        "Nitay Calderon",
        "Eyal Ben-David",
        "Zorik Gekhman",
        "Eran Ofek",
        "Gal Yona"
      ],
      "abstract": "LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information. Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation ( thinking ). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs , we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions . Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.",
      "summary_en": "LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹å‡ ä¹å®Œæ•´ç¼–ç äº†äº‹å®ï¼Œä½†åœ¨æ£€ç´¢å¯åŠæ€§ä¸Šå­˜åœ¨å›°éš¾ï¼›å…¶é”™è¯¯æºäºè®¿é—®é™åˆ¶è€ŒéçŸ¥è¯†ç¼ºå£ï¼Œæ¨ç†å¯æå‡å¯¹å·²ç¼–ç ä¿¡æ¯çš„å¬å›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14080",
      "arxiv_url": "https://arxiv.org/abs/2602.14080",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14080",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:47:24.459955+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16301",
      "title": "Multi-agent cooperation through in-context co-player inference",
      "authors": [
        "Marissa A. Weis",
        "Maciej WoÅ‚czyk",
        "Rajai Nasser",
        "Rif A. Saurous",
        "Blaise AgÃ¼era y Arcas",
        "JoÃ£o Sacramento",
        "Alexander Meulemans"
      ],
      "abstract": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation. Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning . Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescale s and \" meta-learners \" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior . Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behavior s.",
      "summary_en": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.",
      "summary_zh": "åºåˆ—æ¨¡å‹é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ— éœ€ç¡¬ç¼–ç å‡è®¾æˆ–æ—¶é—´å°ºåº¦åˆ†ç¦»ï¼Œå³å¯åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­å®ç°åˆä½œè¡Œä¸ºçš„æ¶Œç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16301",
      "arxiv_url": "https://arxiv.org/abs/2602.16301",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16301",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:47:33.270961+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16173",
      "title": "Learning Personalized Agents from Human Feedback",
      "authors": [
        "Kaiqu Liang",
        "Julia Kruk",
        "Shengyi Qian",
        "Xianjun Yang",
        "Shengjie Bi",
        "Yuanshun Yao",
        "Shaoliang Nie",
        "Mingyang Zhang",
        "Lijuan Liu",
        "Jaime FernÃ¡ndez Fisac",
        "Shuyan Zhou",
        "Saghar Hosseini"
      ],
      "abstract": "PAHF framework enables continual personalization of AI agents through explicit user memory and dual feedback channels, allowing rapid adaptation to changing user preferences. Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping . These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.",
      "summary_en": "PAHF framework enables continual personalization of AI agents through explicit user memory and dual feedback channels, allowing rapid adaptation to changing user preferences.",
      "summary_zh": "PAHFæ¡†æ¶é€šè¿‡æ˜¾å¼ç”¨æˆ·è®°å¿†å’ŒåŒé‡åé¦ˆé€šé“ï¼Œå®ç°AIæ™ºèƒ½ä½“çš„æŒç»­ä¸ªæ€§åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ç”¨æˆ·åå¥½çš„å˜åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16173",
      "arxiv_url": "https://arxiv.org/abs/2602.16173",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16173",
      "github_url": "https://github.com/facebookresearch/PAHF",
      "upvotes": 5,
      "fetched_at": "2026-02-20T05:30:47.386082+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15922",
      "title": "World Action Models are Zero-shot Policies",
      "authors": [
        "Seonghyeon Ye",
        "Yunhao Ge",
        "Kaiyuan Zheng",
        "Shenyuan Gao",
        "Sihyun Yu",
        "George Kurian",
        "Suneel Indupuru",
        "You Liang Tan",
        "Chuning Zhu",
        "Jiannan Xiang",
        "Ayaan Malik",
        "Kyungmin Lee",
        "William Liang",
        "Nadun Ranawaka",
        "Jiasheng Gu",
        "Yinzhen Xu",
        "Guanzhi Wang",
        "Fengyuan Hu",
        "Avnish Narayan",
        "Johan Bjorck",
        "Jing Wang",
        "Gwanghyun Kim"
      ],
      "abstract": "DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models. State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer : video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation , transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
      "summary_en": "DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.",
      "summary_zh": "DreamZeroæ˜¯ä¸€ç§World Action Modelï¼Œåˆ©ç”¨video diffusionï¼Œç›¸æ¯”vision-language-actionæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ–°ç¯å¢ƒå’Œå…·èº«å½¢æ€ä¸Šæ›´å¥½åœ°æ³›åŒ–ç‰©ç†åŠ¨ä½œã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15922",
      "arxiv_url": "https://arxiv.org/abs/2602.15922",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15922",
      "github_url": "https://github.com/dreamzero0/dreamzero",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:47:29.149441+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16682",
      "title": "Learning Situated Awareness in the Real World",
      "authors": [
        "Chuhan Li",
        "Ruilin Han",
        "Joy Hsu",
        "Yongyuan Liang",
        "Rajiv Dhawan",
        "Jiajun Wu",
        "Ming-Hsuan Yang",
        "Xin Eric Wang"
      ],
      "abstract": "SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks. A core aspect of human perception is situated awareness , the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench ( Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos . SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs . It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos , they often fail to infer a coherent camera geometry , leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
      "summary_en": "SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.",
      "summary_zh": "SAW-Bench æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œé€šè¿‡åŒ…å«äººå·¥æ ‡æ³¨é—®ç­”å¯¹çš„çœŸå®ä¸–ç•Œè§†é¢‘æ•°æ®é›†ï¼Œè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­çš„ç¬¬ä¸€äººç§°æƒ…å¢ƒæ„ŸçŸ¥ï¼Œèšç„¦äºä»¥è§‚å¯Ÿè€…ä¸ºä¸­å¿ƒçš„ç©ºé—´æ¨ç†ä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16682",
      "arxiv_url": "https://arxiv.org/abs/2602.16682",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16682",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:47:39.228337+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16493",
      "title": "MMA: Multimodal Memory Agent",
      "authors": [
        "Yihao Lu",
        "Wanru Cheng",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems. Long-horizon multimodal agents depend on external memory ; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility , temporal decay , and conflict-aware network consensus , and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench , a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \" Visual Placebo Effect \", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench , MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "summary_en": "Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.",
      "summary_zh": "å¤šæ¨¡æ€è®°å¿†æ™ºèƒ½ä½“ï¼ˆMMAï¼‰é€šè¿‡åŠ¨æ€è¯„åˆ†è®°å¿†å¯é æ€§å¹¶å¤„ç†æ£€ç´¢å¢å¼ºç³»ç»Ÿä¸­çš„è§†è§‰åå·®ï¼Œæå‡äº†é•¿ç¨‹æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16493",
      "arxiv_url": "https://arxiv.org/abs/2602.16493",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16493",
      "github_url": "https://github.com/AIGeeksGroup/MMA",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:47:35.146522+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15989",
      "title": "SAM 3D Body: Robust Full-Body Human Mesh Recovery",
      "authors": [
        "Xitong Yang",
        "Devansh Kukreja",
        "Don Pinkus",
        "Anushka Sagar",
        "Taosha Fan",
        "Jinhyung Park",
        "Soyong Shin",
        "Jinkun Cao",
        "Jiawei Liu",
        "Nicolas Ugrinovic",
        "Matt Feiszli",
        "Jitendra Malik",
        "Piotr Dollar",
        "Kris Kitani"
      ],
      "abstract": "A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions. We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation , Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts , including 2D keypoints and masks , enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization , multi-view geometry , and dense keypoint detection . Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis . Both 3DB and MHR are open-source.",
      "summary_en": "A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.",
      "summary_zh": "ä¸€ç§å¯æç¤ºçš„3Däººä½“ç½‘æ ¼æ¢å¤æ¨¡å‹é‡‡ç”¨æ–°å‹å‚æ•°åŒ–è¡¨ç¤ºä¸ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œåœ¨å¤šæ ·åŒ–æ¡ä»¶ä¸‹è¾¾åˆ°state-of-the-artæ€§èƒ½å¹¶å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15989",
      "arxiv_url": "https://arxiv.org/abs/2602.15989",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15989",
      "github_url": "https://github.com/facebookresearch/sam-3d-body",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:47:31.401695+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15927",
      "title": "Visual Memory Injection Attacks for Multi-Turn Conversations",
      "authors": [
        "Christian Schlarmann",
        "Matthias Hein"
      ],
      "abstract": "Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations. Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion . Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection",
      "summary_en": "Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.",
      "summary_zh": "Visual Memory Injection attacké€šè¿‡ä»…åœ¨å¤šè½®å¯¹è¯ä¸­ç‰¹å®šæç¤ºè¯ä¸‹æ‰ä¼šè§¦å‘ç›®æ ‡å“åº”çš„ç»è¿‡æ“æ§çš„å›¾åƒï¼Œå®ç°äº†å¯¹ç”Ÿæˆå¼è§†è§‰-è¯­è¨€æ¨¡å‹çš„éšè”½æ“æ§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15927",
      "arxiv_url": "https://arxiv.org/abs/2602.15927",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15927",
      "github_url": "https://github.com/chs20/visual-memory-injection",
      "upvotes": 3,
      "fetched_at": "2026-02-20T05:30:44.573979+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14498",
      "title": "Uncertainty-Aware Vision-Language Segmentation for Medical Imaging",
      "authors": [
        "Aryan Das",
        "Tanishq Rachamalla",
        "Koushik Biswas",
        "Swalpa Kumar Roy",
        "Vinay Kumar Verma"
      ],
      "abstract": "A novel uncertainty-aware multimodal segmentation framework uses radiological images and clinical text with Modality Decoding Attention Blocks and Spectral-Entropic Uncertainty Loss for improved medical diagnosis accuracy. We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling . To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS",
      "summary_en": "A novel uncertainty-aware multimodal segmentation framework uses radiological images and clinical text with Modality Decoding Attention Blocks and Spectral-Entropic Uncertainty Loss for improved medical diagnosis accuracy.",
      "summary_zh": "ä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å¤šæ¨¡æ€åˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨æ”¾å°„å­¦å½±åƒä¸ä¸´åºŠæ–‡æœ¬ï¼Œç»“åˆ Modality Decoding Attention Blocks å’Œ Spectral-Entropic Uncertainty Lossï¼Œä»¥æå‡åŒ»å­¦è¯Šæ–­å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14498",
      "arxiv_url": "https://arxiv.org/abs/2602.14498",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14498",
      "github_url": "https://github.com/arya-domain/UA-VLS",
      "upvotes": 3,
      "fetched_at": "2026-02-20T05:30:41.568113+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.16666",
      "title": "Towards a Science of AI Agent Reliability",
      "authors": [
        "Stephan Rabanser",
        "Sayash Kapoor",
        "Peter Kirgis",
        "Kangheng Liu",
        "Saiteja Utpala",
        "Arvind Narayanan"
      ],
      "abstract": "Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions. AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety -critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency , robustness , predictability , and safety . Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability . By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
      "summary_en": "Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.",
      "summary_zh": "å¯¹AIæ™ºèƒ½ä½“çš„ä¼ ç»ŸåŸºå‡†è¯„ä¼°æœªèƒ½æ•æ‰å…³é”®çš„å¯é æ€§é—®é¢˜ï¼Œä»è€Œæ¨åŠ¨äº†ç»¼åˆæŒ‡æ ‡çš„å¼€å‘ï¼Œä»¥åœ¨å¤šç»´åº¦ä¸Šè¯„ä¼°ä¸€è‡´æ€§ã€é²æ£’æ€§ã€å¯é¢„æµ‹æ€§å’Œå®‰å…¨æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.16666",
      "arxiv_url": "https://arxiv.org/abs/2602.16666",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16666",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:47:37.106379+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14514",
      "title": "Efficient Text-Guided Convolutional Adapter for the Diffusion Model",
      "authors": [
        "Aryan Das",
        "Koushik Biswas",
        "Swalpa Kumar Roy",
        "Badri Narayana Patro",
        "Vinay Kumar Verma"
      ],
      "abstract": "Nexus Adapters are efficient text-guided adapters for diffusion models that preserve structure while enhancing prompt understanding through cross-attention mechanisms, achieving state-of-the-art results with significantly fewer parameters. We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning . Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter . Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter , which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters",
      "summary_en": "Nexus Adapters are efficient text-guided adapters for diffusion models that preserve structure while enhancing prompt understanding through cross-attention mechanisms, achieving state-of-the-art results with significantly fewer parameters.",
      "summary_zh": "Nexus Adapters æ˜¯ç”¨äºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ–‡æœ¬å¼•å¯¼é€‚é…å™¨ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¿ç•™ç»“æ„çš„åŒæ—¶å¢å¼ºæç¤ºç†è§£ï¼Œä»¥æ˜¾è‘—æ›´å°‘çš„å‚æ•°å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14514",
      "arxiv_url": "https://arxiv.org/abs/2602.14514",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14514",
      "github_url": "https://github.com/arya-domain/Nexus-Adapters",
      "upvotes": 2,
      "fetched_at": "2026-02-20T05:30:42.340429+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.08392",
      "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
      "authors": [
        "Xin Wu",
        "Zhixuan Liang",
        "Yue Ma",
        "Mengkang Hu",
        "Zhiyuan Qin",
        "Xiu Li"
      ],
      "abstract": "BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.",
      "summary_en": "BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.",
      "summary_zh": "BiManiBench åœ¨åŒè‡‚æœºå™¨äººä»»åŠ¡ä¸Šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºå…¶åœ¨ç©ºé—´å®šä½å’Œæ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å…·å¤‡è¾ƒå¼ºçš„é«˜å±‚æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08392",
      "arxiv_url": "https://arxiv.org/abs/2602.08392",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08392",
      "github_url": "https://github.com/bimanibench/BiManiBench",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:47:19.949818+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.07345",
      "title": "Optimizing Few-Step Generation with Adaptive Matching Distillation",
      "authors": [
        "Lichen Bai",
        "Zikai Zhou",
        "Shitong Shao",
        "Wenliang Zhong",
        "Shuo Yang",
        "Shuo Chen",
        "Bojun Chen",
        "Zeke Xie"
      ],
      "abstract": "Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape. Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone , regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zone s. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zone s is essential for pushing the performance ceiling of few-step generative models .",
      "summary_en": "Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.",
      "summary_zh": "è‡ªé€‚åº”åŒ¹é…è’¸é¦å¼•å…¥è‡ªæ ¡æ­£æœºåˆ¶ï¼Œé€šè¿‡æ£€æµ‹å¹¶é€ƒç¦»ä¼˜åŒ–æ™¯è§‚ä¸­çš„ä¸ç¨³å®šåŒºåŸŸæ¥æ”¹è¿›ç”Ÿæˆæ¨¡å‹è®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07345",
      "arxiv_url": "https://arxiv.org/abs/2602.07345",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07345",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:47:17.425548+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14602",
      "title": "OPBench: A Graph Benchmark to Combat the Opioid Crisis",
      "authors": [
        "Tianyi Ma",
        "Yiyang Li",
        "Yiyue Qian",
        "Zheyuan Zhang",
        "Zehong Wang",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.",
      "summary_en": "Despite the potential of graph learning methods for addressing the opioid epidemic, no comprehensive benchmark exists for evaluating these methods across real-world opioid crisis scenarios. This work introduces OPBench, the first comprehensive opioid benchmark featuring five datasets across three domainsâ€”opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patternsâ€”incorporating heterogeneous graphs and hypergraphs alongside a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines. Extensive experiments using OPBench analyze the strengths and limitations of existing graph learning methods, providing actionable insights for future research in combating the opioid crisis.",
      "summary_zh": "å°½ç®¡å›¾å­¦ä¹ æ–¹æ³•åœ¨åº”å¯¹é˜¿ç‰‡ç±»è¯ç‰©æµè¡Œæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†ç›®å‰å°šç¼ºä¹ç”¨äºåœ¨çœŸå®ä¸–ç•Œé˜¿ç‰‡ç±»è¯ç‰©å±æœºåœºæ™¯ä¸‹è¯„ä¼°è¿™äº›æ–¹æ³•çš„å…¨é¢åŸºå‡†ã€‚æœ¬å·¥ä½œæå‡ºäº†OPBenchï¼Œé¦–ä¸ªå…¨é¢çš„é˜¿ç‰‡ç±»è¯ç‰©åŸºå‡†ï¼Œæ¶µç›–ä¸‰ä¸ªé¢†åŸŸçš„äº”ä¸ªæ•°æ®é›†â€”â€”åŸºäºåŒ»ç–—ç´¢èµ”çš„é˜¿ç‰‡ç±»è¯ç‰©è¿‡é‡æ£€æµ‹ã€åŸºäºæ•°å­—å¹³å°çš„éæ³•è¯ç‰©è´©è¿æ£€æµ‹ï¼Œä»¥åŠåŸºäºé¥®é£Ÿæ¨¡å¼çš„è¯ç‰©æ»¥ç”¨é¢„æµ‹â€”â€”åŒ…å«å¼‚æ„å›¾å’Œè¶…å›¾ï¼Œä»¥åŠå…·æœ‰æ ‡å‡†åŒ–åè®®ã€é¢„å®šä¹‰æ•°æ®åˆ’åˆ†å’Œå¯å¤ç°åŸºçº¿çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ã€‚ä½¿ç”¨OPBenchè¿›è¡Œçš„å¹¿æ³›å®éªŒåˆ†æäº†ç°æœ‰å›¾å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œä¸ºåº”å¯¹é˜¿ç‰‡ç±»è¯ç‰©å±æœºçš„æœªæ¥ç ”ç©¶æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14602",
      "arxiv_url": "https://arxiv.org/abs/2602.14602",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14602",
      "github_url": "https://github.com/Tianyi-Billy-Ma/OPBench",
      "upvotes": 0,
      "fetched_at": "2026-02-20T05:30:43.764889+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14111",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "authors": [
        "Anton Korznikov",
        "Andrey Galichin",
        "Alexey Dontsov",
        "Oleg Rogov",
        "Ivan Oseledets",
        "Elena Tutubalina"
      ],
      "abstract": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations. Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance , showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations , we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.",
      "summary_en": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.",
      "summary_zh": "ç¨€ç–è‡ªç¼–ç å™¨å°½ç®¡å…·æœ‰è¾ƒå¼ºçš„é‡å»ºæ€§èƒ½ï¼Œå´æ— æ³•å¯é åœ°åˆ†è§£ç¥ç»ç½‘ç»œå†…éƒ¨ç»“æ„ï¼Œè¿™ä¸€ç‚¹åœ¨åˆæˆä¸çœŸå®æ¿€æ´»è¯„ä¼°ä¸­å¾—åˆ°äº†è¯å®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14111",
      "arxiv_url": "https://arxiv.org/abs/2602.14111",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14111",
      "github_url": "",
      "upvotes": 51,
      "fetched_at": "2026-02-19T01:55:38.241233+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12670",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "authors": [
        "Xiangyi Li",
        "Wenbo Chen",
        "Yimin Liu",
        "Shenghan Zheng",
        "Xiaokun Chen",
        "Yifeng He",
        "Yubo Li",
        "Bingran You",
        "Haotian Shen",
        "Jiankai Sun",
        "Shuyi Wang",
        "Qunhong Zeng",
        "Di Wang",
        "Xuandong Zhao",
        "Yuanli Wang",
        "Roey Ben Chaim",
        "Zonglin Di",
        "Yipeng Gao",
        "Junwei He",
        "Yizhuo He",
        "Liqiang Jing",
        "Luyang Kong"
      ],
      "abstract": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions. Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench , a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills , and self-generated Skills . We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "summary_en": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.",
      "summary_zh": "SkillsBenchåœ¨86é¡¹ä»»åŠ¡ä¸­è¯„ä¼°äº†æ™ºèƒ½ä½“æŠ€èƒ½ï¼Œå‘ç°ç²¾é€‰æŠ€èƒ½è™½èƒ½æ˜¾è‘—æå‡æ€§èƒ½ä½†æ•ˆæœä¸ä¸€ï¼Œè€Œè‡ªç”ŸæˆæŠ€èƒ½åˆ™æ¯«æ— åŠ©ç›Šï¼Œè¿™è¡¨æ˜å°½ç®¡æ¨¡å‹èƒ½ä»ç²¾é€‰ç‰ˆæœ¬ä¸­è·ç›Šï¼Œå´éš¾ä»¥åˆ›å»ºæœ‰ç”¨çš„ç¨‹åºæ€§çŸ¥è¯†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12670",
      "arxiv_url": "https://arxiv.org/abs/2602.12670",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12670",
      "github_url": "https://github.com/benchflow-ai/skillsbench",
      "upvotes": 38,
      "fetched_at": "2026-02-19T01:55:35.285668+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15763",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "authors": [
        "GLM-5 Team",
        "Aohan Zeng",
        "Xin Lv",
        "Zhenyu Hou",
        "Zhengxiao Du",
        "Qinkai Zheng",
        "Bin Chen",
        "Da Yin",
        "Chendi Ge",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Gengzheng Pan",
        "Hao Zeng",
        "Haoke Zhang",
        "Haoran Wang",
        "Huilong Chen",
        "Jiajie Zhang",
        "Jian Jiao",
        "Jiaqi Guo",
        "Jingsen Wang",
        "Jingzhao Du",
        "Jinzhu Wu"
      ],
      "abstract": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering. We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering . Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks . Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "summary_en": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.",
      "summary_zh": "GLM-5é€šè¿‡DSAé™ä½æˆæœ¬ï¼Œåˆ©ç”¨å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æ”¹è¿›å¯¹é½ï¼Œå¹¶å¢å¼ºç¼–ç¨‹èƒ½åŠ›ä»¥æ”¯æŒçœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ï¼Œä»è€Œæ¨è¿›åŸºç¡€æ¨¡å‹å‘å±•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15763",
      "arxiv_url": "https://arxiv.org/abs/2602.15763",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15763",
      "github_url": "https://github.com/zai-org/GLM-5",
      "upvotes": 31,
      "fetched_at": "2026-02-19T01:55:54.127746+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14299",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "authors": [
        "Ming Li",
        "Xirui Li",
        "Tianyi Zhou"
      ],
      "abstract": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures. As large language model agents increasingly populate networked environments , a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies , measuring semantic stabilization , lexical turnover , individual inertia , influence persistence , and collective consensus . Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover , defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies .",
      "summary_en": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.",
      "summary_zh": "ç½‘ç»œç¯å¢ƒä¸­çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è¡¨ç°å‡ºåŠ¨æ€ç¨³å®šæ€§ï¼Œä½†æœªå®ç°çœŸæ­£çš„ç¤¾ä¼šæ”¶æ•›ï¼Œåœ¨ä¿æŒä¸ªä½“å¤šæ ·æ€§çš„åŒæ—¶ç¼ºä¹é›†ä½“å½±å“ç»“æ„ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14299",
      "arxiv_url": "https://arxiv.org/abs/2602.14299",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14299",
      "github_url": "https://github.com/MingLiiii/Moltbook_Socialization",
      "upvotes": 21,
      "fetched_at": "2026-02-19T01:55:39.715212+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12279",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha",
        "Xiaoliang Dai",
        "Jialiang Wang",
        "Zecheng He",
        "Jianwei Yang",
        "Chunyuan Li",
        "Junzhe Sun",
        "Chu Wang",
        "Serena Yeung-Levy",
        "Felix Juefei-Xu"
      ],
      "abstract": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities. Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis , unified model training , and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning . These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models .",
      "summary_en": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.",
      "summary_zh": "UniTæ¡†æ¶ä½¿ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ€ç»´é“¾æµ‹è¯•æ—¶ç¼©æ”¾è¿›è¡Œè¿­ä»£æ¨ç†ä¸ä¼˜åŒ–ï¼Œä»è€Œæå‡ç”Ÿæˆä¸ç†è§£èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12279",
      "arxiv_url": "https://arxiv.org/abs/2602.12279",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12279",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T01:55:33.664750+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15112",
      "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi Patwardhan",
        "Arman Cohan"
      ],
      "abstract": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance. We introduce ResearchGym , a benchmark and execution environment for evaluating AI agents on end-to-end research . To instantiate this, we repurpose five oral and spotlight papers from ICML , ICLR , and ACL . From each paper's repository, we preserve the datasets , evaluation harness , and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5 , we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length . Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex ( GPT-5 .2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
      "summary_en": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.",
      "summary_zh": "ResearchGymæå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“ç«¯åˆ°ç«¯ç ”ç©¶ä»»åŠ¡çš„åŸºå‡†ç¯å¢ƒï¼Œæ­ç¤ºäº†å½“å‰è‡ªä¸»æ™ºèƒ½ä½“å°½ç®¡å¶å°”èƒ½å–å¾—æœ€ä¼˜æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„èƒ½åŠ›-å¯é æ€§å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15112",
      "arxiv_url": "https://arxiv.org/abs/2602.15112",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15112",
      "github_url": "https://github.com/Anikethh/ResearchGym",
      "upvotes": 14,
      "fetched_at": "2026-02-19T01:55:43.087568+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15547",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
      "authors": [
        "Mohammad Kalim Akram",
        "Saba Sturua",
        "Nastia Havriushenko",
        "Quentin Herreros",
        "Michael GÃ¼nther",
        "Maximilian Werk",
        "Han Xiao"
      ],
      "abstract": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization. Text embedding models are widely used for semantic similarity tasks, including information retrieval , clustering , and classification . General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models . Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization . Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "summary_en": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization.",
      "summary_zh": "ç´§å‡‘å‹æ–‡æœ¬åµŒå…¥æ¨¡å‹é€šè¿‡è’¸é¦ä¸å¯¹æ¯”æŸå¤±ç›¸ç»“åˆçš„è”åˆè®­ç»ƒæ–¹æ³•å¼€å‘ï¼Œåœ¨å®ç°æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶æ”¯æŒé•¿ä¸Šä¸‹æ–‡åºåˆ—å’Œé«˜æ•ˆé‡åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15547",
      "arxiv_url": "https://arxiv.org/abs/2602.15547",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15547",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T01:55:51.191144+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14486",
      "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
      "authors": [
        "Fabian GrÃ¶ger",
        "Shuo Wen",
        "Maria BrbiÄ‡"
      ],
      "abstract": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics. The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity , but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis : representations in neural networks are converging to shared local neighborhood relationships.",
      "summary_en": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.",
      "summary_zh": "ç»æ ¡å‡†çš„ç›¸ä¼¼æ€§åº¦é‡æ˜¾ç¤ºï¼Œç½‘ç»œè¡¨å¾æ”¶æ•›äºå…±äº«çš„å±€éƒ¨é‚»åŸŸå…³ç³»ï¼Œè€Œéå…¨å±€ç»Ÿè®¡æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14486",
      "arxiv_url": "https://arxiv.org/abs/2602.14486",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14486",
      "github_url": "https://github.com/mlbio-epfl/aristotelian",
      "upvotes": 7,
      "fetched_at": "2026-02-19T01:55:42.035681+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15772",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
      "authors": [
        "Sen Ye",
        "Mengde Xu",
        "Shuyang Gu",
        "Di He",
        "Liwei Wang",
        "Han Hu"
      ],
      "abstract": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation. Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma , achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models . Code is available at https://github.com/sen-ye/R3.",
      "summary_en": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.",
      "summary_zh": "æ¨ç†-åæ€-ç²¾ç‚¼æ¡†æ¶é€šè¿‡å°†å•æ­¥ç”Ÿæˆé‡æ–°è¡¨è¿°ä¸ºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¾å¼èå…¥ç†è§£çš„å¤šæ­¥è¿‡ç¨‹ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15772",
      "arxiv_url": "https://arxiv.org/abs/2602.15772",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15772",
      "github_url": "https://github.com/sen-ye/R3",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:55.413014+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15322",
      "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
      "authors": [
        "Taejong Joo",
        "Wenhan Xia",
        "Cheolmin Kim",
        "Ming Zhang",
        "Eugene Ie"
      ],
      "abstract": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers. Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners . We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment . Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
      "summary_en": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.",
      "summary_zh": "éšæœºå‚æ•°æ›´æ–°æ©ç é€šè¿‡è¯±å¯¼æ›²ç‡ç›¸å…³æ­£åˆ™åŒ–ï¼Œåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ›´ä¼˜çš„ä¼˜åŒ–ï¼Œå…¶åŠ¨é‡å¯¹é½å˜ä½“ç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15322",
      "arxiv_url": "https://arxiv.org/abs/2602.15322",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15322",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:46.990692+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15200",
      "title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
      "authors": [
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Ammar Ali",
        "Baher Mohammad",
        "Stamatios Lefkimmiatis"
      ],
      "abstract": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods. Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization . COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.",
      "summary_en": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.",
      "summary_zh": "COMPOT æ˜¯ä¸€ç§é¢å‘ Transformer æ¨¡å‹çš„å…è®­ç»ƒå‹ç¼©æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ­£äº¤å­—å…¸ä¸é—­å¼æ›´æ–°çš„ç¨€ç–å­—å…¸å­¦ä¹ ï¼Œå®ç°äº†ä¼˜äºä¼ ç»Ÿä½ç§©æ–¹æ³•çš„è´¨é‡-å‹ç¼©æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15200",
      "arxiv_url": "https://arxiv.org/abs/2602.15200",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15200",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:44.392487+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15156",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "abstract": "Panini enables efficient and accurate language model reasoning through a non-parametric continual learning framework that uses generative semantic workspaces to store and retrieve knowledge, achieving superior performance with reduced computational overhead.",
      "summary_en": "Panini enables efficient and accurate language model reasoning through a non-parametric continual learning framework that uses generative semantic workspaces to store and retrieve knowledge, achieving superior performance with reduced computational overhead.",
      "summary_zh": "Paninié€šè¿‡éå‚æ•°åŒ–æŒç»­å­¦ä¹ æ¡†æ¶å®ç°é«˜æ•ˆå‡†ç¡®çš„è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆå¼è¯­ä¹‰å·¥ä½œç©ºé—´å­˜å‚¨å’Œæ£€ç´¢çŸ¥è¯†ï¼Œåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶å–å¾—å“è¶Šæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15156",
      "arxiv_url": "https://arxiv.org/abs/2602.15156",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15156",
      "github_url": "https://github.com/roychowdhuryresearch/gsw-memory",
      "upvotes": 5,
      "fetched_at": "2026-02-19T02:57:18.247922+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15449",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
      ],
      "abstract": "Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure. Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "summary_en": "Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure.",
      "summary_zh": "é¢å‘ä»£ç ç”Ÿæˆçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å››å±‚æµ‹è¯•å¥—ä»¶ç»“æ„åŸºäºæ¨¡å‹èƒ½åŠ›è¿›è¡Œè‡ªé€‚åº”è¯¾ç¨‹è®¾è®¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15449",
      "arxiv_url": "https://arxiv.org/abs/2602.15449",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15449",
      "github_url": "https://github.com/deep-diver/TAROT",
      "upvotes": 4,
      "fetched_at": "2026-02-19T01:55:50.538737+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15620",
      "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
      "authors": [
        "Shiqi Liu",
        "Zeyu He",
        "Guojian Zhan",
        "Letian Tao",
        "Zhilong Zheng",
        "Jiang Wu",
        "Yinuo Wang",
        "Yang Guan",
        "Kehua Sheng",
        "Bo Zhang",
        "Keqiang Li",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "abstract": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance. Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy . Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens . When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates . Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO , 20- Entropy and JustRL .",
      "summary_en": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.",
      "summary_zh": "ç ”ç©¶å‘ç°ï¼Œå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§æºäºè™šå‡ tokenï¼Œå¹¶æå‡ºé€šè¿‡é€‰æ‹©æ€§æ©ç é—®é¢˜æ¢¯åº¦æ›´æ–°ä»¥æå‡æ¨ç†æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15620",
      "arxiv_url": "https://arxiv.org/abs/2602.15620",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15620",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T01:55:52.727674+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15278",
      "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
      "authors": [
        "Manuel Cherep",
        "Pranav M R",
        "Pattie Maes",
        "Nikhil Singh"
      ],
      "abstract": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns. The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference : choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization , adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities , safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
      "summary_en": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.",
      "summary_zh": "é€šè¿‡å—æ§çš„å›¾åƒé€‰æ‹©ä»»åŠ¡ä¸ç³»ç»Ÿæ€§è¾“å…¥æ‰°åŠ¨ï¼Œç ”ç©¶è§†è§‰-è¯­è¨€æ¨¡å‹çš„å†³ç­–åå¥½ï¼Œæ­ç¤ºå…¶è§†è§‰æ¼æ´ä¸å®‰å…¨é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15278",
      "arxiv_url": "https://arxiv.org/abs/2602.15278",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15278",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T01:55:45.581697+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15382",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
      "authors": [
        "Xiaoze Liu",
        "Ruowang Zhang",
        "Weichen Yu",
        "Siheng Xiong",
        "Liu He",
        "Feijie Wu",
        "Hoin Jung",
        "Matt Fredrikson",
        "Xiaoqian Wang",
        "Jing Gao"
      ],
      "abstract": "A Vision Wormhole framework enables efficient, model-agnostic communication in multi-agent systems by using visual-language models to transfer reasoning states through a shared latent space, reducing computational overhead while maintaining reasoning accuracy. Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec , we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway , effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas",
      "summary_en": "A Vision Wormhole framework enables efficient, model-agnostic communication in multi-agent systems by using visual-language models to transfer reasoning states through a shared latent space, reducing computational overhead while maintaining reasoning accuracy.",
      "summary_zh": "Vision Wormholeæ¡†æ¶é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å…±äº«éšç©ºé—´ä¸­ä¼ é€’æ¨ç†çŠ¶æ€ï¼Œå®ç°äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é«˜æ•ˆä¸”ä¸æ¨¡å‹æ— å…³çš„é€šä¿¡ï¼Œåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15382",
      "arxiv_url": "https://arxiv.org/abs/2602.15382",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15382",
      "github_url": "https://github.com/xz-liu/heterogeneous-latent-mas",
      "upvotes": 2,
      "fetched_at": "2026-02-19T02:57:19.262455+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15327",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "authors": [
        "Hanlin Zhang",
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade"
      ],
      "abstract": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks. For deploying foundation models, practitioners increasingly need prescriptive scaling laws : given a pre training compute budget , what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance , we estimate capability boundaries , high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization . We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budget s into reliable performance expectations and for monitoring when capability boundaries shift across time.",
      "summary_en": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.",
      "summary_zh": "å¤§è§„æ¨¡è§‚å¯Ÿæ€§åˆ†æä½¿ç”¨åˆ†ä½æ•°å›å½’ä¼°è®¡åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œå’Œæ€§èƒ½é¢„æµ‹ï¼Œå¹¶è¯„ä¼°è·¨ä»»åŠ¡çš„æ—¶é—´ç¨³å®šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15327",
      "arxiv_url": "https://arxiv.org/abs/2602.15327",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15327",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:47.959989+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.13964",
      "title": "HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam",
      "authors": [
        "Weiqi Zhai",
        "Zhihai Wang",
        "Jinghang Wang",
        "Boyu Yang",
        "Xiaogang Li",
        "Xiang Xu",
        "Bohan Wang",
        "Peng Wang",
        "Xingzhe Wu",
        "Anfeng Li",
        "Qiyuan Feng",
        "Yuhao Zhou",
        "Shoulin Han",
        "Wenjie Luo",
        "Yiyuan Li",
        "Yaxuan Wang",
        "Ruixian Luo",
        "Guojie Lin",
        "Peiyao Xiao",
        "Chengliang Xu",
        "Ben Wang",
        "Zeyu Wang"
      ],
      "abstract": "HLE-Verified presents a validated and revised version of the HLE benchmark with improved reliability through expert review and model-based checks, demonstrating significant accuracy improvements in language model evaluations. Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy . Our construction follows a two-stage validation -and-repair workflow resulting in a certified benchmark . In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks , yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs , model-assisted auditing , and final adjudication , resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified",
      "summary_en": "HLE-Verified presents a validated and revised version of the HLE benchmark with improved reliability through expert review and model-based checks, demonstrating significant accuracy improvements in language model evaluations.",
      "summary_zh": "HLE-Verifiedæå‡ºäº†HLEåŸºå‡†æµ‹è¯•çš„éªŒè¯ä¸ä¿®è®¢ç‰ˆæœ¬ï¼Œé€šè¿‡ä¸“å®¶å®¡æ ¸å’ŒåŸºäºæ¨¡å‹çš„æ£€æŸ¥æå‡äº†å¯é æ€§ï¼Œåœ¨è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­å±•ç°å‡ºæ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13964",
      "arxiv_url": "https://arxiv.org/abs/2602.13964",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13964",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T02:57:17.057340+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12978",
      "title": "Learning Native Continuation for Action Chunking Flow Policies",
      "authors": [
        "Yufeng Liu",
        "Hang Yu",
        "Juntu Zhao",
        "Bocheng Li",
        "Di Zhang",
        "Mingzhu Li",
        "Wenxuan Wu",
        "Yingdong Hu",
        "Junyuan Xie",
        "Junliang Guo",
        "Dequan Wang",
        "Yang Gao"
      ],
      "abstract": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution. Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time . Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time .",
      "summary_en": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.",
      "summary_zh": "Legato åˆ©ç”¨è®­ç»ƒæ—¶å»¶ç»­æ–¹æ³•æ”¹è¿›åŠ¨ä½œåˆ†å—çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç¡®ä¿è½¨è¿¹å¹³æ»‘å¹¶å‡å°‘å®æ—¶æ‰§è¡Œä¸­çš„å¤šæ¨¡æ€åˆ‡æ¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12978",
      "arxiv_url": "https://arxiv.org/abs/2602.12978",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12978",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:36.092392+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "authors": [
        "Heejeong Nam",
        "Quentin Le Lidec",
        "Lucas Maes",
        "Yann LeCun",
        "Randall Balestriero"
      ],
      "abstract": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.",
      "summary_en": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.",
      "summary_zh": "C-JEPAå°†æ©ç è”åˆåµŒå…¥é¢„æµ‹æ‰©å±•è‡³ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨å¾ï¼Œé€šè¿‡å¼•å…¥å› æœå½’çº³åç½®çš„å¯¹è±¡çº§æ©ç å®ç°ç¨³å¥çš„å…³ç³»ç†è§£ï¼Œå¹¶æå‡æ¨ç†ä¸æ§åˆ¶ä»»åŠ¡çš„è¡¨ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11389",
      "arxiv_url": "https://arxiv.org/abs/2602.11389",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11389",
      "github_url": "https://github.com/galilai-group/cjepa",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:32.256322+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.09653",
      "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
      "authors": [
        "Shiwei Lyu",
        "Xidong Wang",
        "Lei Liu",
        "Hao Zhu",
        "Chaohe Zhang",
        "Jian Wang",
        "Jinjie Gu",
        "Benyou Wang",
        "Yue Shen"
      ],
      "abstract": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning. Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics , a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples : 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision . A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard , outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
      "summary_en": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.",
      "summary_zh": "ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶é€šè¿‡åŒ»ç”ŸéªŒè¯çš„ç¤ºä¾‹å’Œè’¸é¦å¾—åˆ°çš„ä¸´åºŠåŸåˆ™ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹ä¸ä¸´åºŠåŒ»ç”Ÿåå¥½çš„å¯¹é½ï¼Œä»¥æ”¹è¿›åŒ»å­¦æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09653",
      "arxiv_url": "https://arxiv.org/abs/2602.09653",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09653",
      "github_url": "https://github.com/AQ-MedAI/ClinAlign",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:30.816675+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.07854",
      "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model",
      "authors": [
        "Chendong Xiang",
        "Jiajun Liu",
        "Jintao Zhang",
        "Xiao Yang",
        "Zhengwei Fang",
        "Shizun Wang",
        "Zijun Wang",
        "Yingtian Zou",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization. Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings , which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers . By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention , which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift . Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.",
      "summary_en": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.",
      "summary_zh": "ViewRopeæ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡å°†ç›¸æœºå°„çº¿æ–¹å‘æ³¨å…¥è§†é¢‘Transformerçš„æ³¨æ„åŠ›å±‚æ¥å¢å¼ºé¢„æµ‹æ€§ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨ç›¸å¯¹å°„çº¿å‡ ä½•å‚æ•°åŒ–è§£å†³ç©ºé—´æŒä¹…æ€§é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07854",
      "arxiv_url": "https://arxiv.org/abs/2602.07854",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07854",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:29.881857+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14364",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "authors": [
        "Tianyu Chen",
        "Dongrui Liu",
        "Xia Hu",
        "Jingyi Yu",
        "Wenjie Wang"
      ],
      "abstract": "Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks. Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench ) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge ( AgentDoG-Qwen3-4B ) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "summary_en": "Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks.",
      "summary_zh": "Clawdbotæ˜¯ä¸€æ¬¾å…·å¤‡å¤šæ ·åŒ–å·¥å…·èƒ½åŠ›çš„è‡ªæ‰˜ç®¡AIæ™ºèƒ½ä½“ï¼Œå…¶åœ¨ä¸åŒé£é™©ç»´åº¦ä¸Šçš„å®‰å…¨æ€§èƒ½è¡¨ç°å„å¼‚ï¼Œå°¤å…¶åœ¨å¤„ç†æ¨¡ç³Šæˆ–å¯¹æŠ—æ€§è¾“å…¥æ—¶å­˜åœ¨å›°éš¾ï¼Œå°½ç®¡åœ¨æŒ‡å®šä»»åŠ¡ä¸­ä¿æŒä¸€è‡´çš„å¯é æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14364",
      "arxiv_url": "https://arxiv.org/abs/2602.14364",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14364",
      "github_url": "https://github.com/tychenn/clawdbot_report",
      "upvotes": 1,
      "fetched_at": "2026-02-19T01:55:40.666586+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12235",
      "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
      "authors": [
        "Julia Belikova",
        "Danila Rozhevskii",
        "Dennis Svirin",
        "Konstantin Polev",
        "Alexander Panchenko"
      ],
      "abstract": "Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors. Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens . Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA , SQuADv2 , and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
      "summary_en": "Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors.",
      "summary_zh": "é¢å‘é•¿ä¸Šä¸‹æ–‡LLMçš„è½¯å‹ç¼©æ¶æ„ä½¿ç”¨æŸ¥è¯¢æ„ŸçŸ¥æ¢æµ‹åˆ†ç±»å™¨æ£€æµ‹tokenæº¢å‡ºå¹¶ç¼“è§£å‹ç¼©å¼•èµ·çš„é”™è¯¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12235",
      "arxiv_url": "https://arxiv.org/abs/2602.12235",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12235",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T01:55:32.934470+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.10210",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "authors": [
        "Junhong Lin",
        "Bing Zhang",
        "Song Wang",
        "Ziyan Liu",
        "Dan Gutfreund",
        "Julian Shun",
        "Yada Zhu"
      ],
      "abstract": "HybridRAG-Bench evaluates retrieval-intensive multi-hop reasoning in large language models by combining unstructured text and structured knowledge graphs from recent scientific literature, providing a contamination-aware benchmark that distinguishes genuine retrieval and reasoning from parametric recall. Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning . Augmenting LLMs with hybrid external knowledge , such as unstructured text and structured knowledge graphs , offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall . We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall , offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "summary_en": "HybridRAG-Bench evaluates retrieval-intensive multi-hop reasoning in large language models by combining unstructured text and structured knowledge graphs from recent scientific literature, providing a contamination-aware benchmark that distinguishes genuine retrieval and reasoning from parametric recall.",
      "summary_zh": "HybridRAG-Benché€šè¿‡ç»“åˆæœ€æ–°ç§‘å­¦æ–‡çŒ®ä¸­çš„éç»“æ„åŒ–æ–‡æœ¬å’Œç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¯†é›†å‹å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ä¸ªæ±¡æŸ“æ„ŸçŸ¥åŸºå‡†ï¼Œç”¨äºåŒºåˆ†çœŸæ­£çš„æ£€ç´¢ä¸æ¨ç†å’Œå‚æ•°åŒ–è®°å¿†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10210",
      "arxiv_url": "https://arxiv.org/abs/2602.10210",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10210",
      "github_url": "https://github.com/junhongmit/HybridRAG-Bench",
      "upvotes": 1,
      "fetched_at": "2026-02-19T02:57:15.482763+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.10809",
      "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories",
      "authors": [
        "Chenlong Deng",
        "Mengjie Deng",
        "Junjie Wu",
        "Dun Zeng",
        "Teng Wang",
        "Qingsong Xie",
        "Jiadeng Huang",
        "Shengjie Ma",
        "Changwang Zhang",
        "Zhaoxiang Wang",
        "Jun Wang",
        "Yutao Zhu",
        "Zhicheng Dou"
      ],
      "abstract": "DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system. Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams , where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues . We construct DISBench , a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations , effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation . Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.",
      "summary_en": "DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system.",
      "summary_zh": "DeepImageSearch æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒæ£€ç´¢çš„æ™ºèƒ½ä½“æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é…å¤‡åŒè®°å¿†ç³»ç»Ÿçš„æ¨¡å—åŒ–æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ç°å¯¹è§†è§‰å†å²çš„å¤šæ­¥æ¨ç†ï¼Œä»è€Œè§£å†³ä¼ ç»Ÿè¯­ä¹‰åŒ¹é…çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10809",
      "arxiv_url": "https://arxiv.org/abs/2602.10809",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10809",
      "github_url": "https://github.com/RUC-NLPIR/DeepImageSearch",
      "upvotes": 24,
      "fetched_at": "2026-02-17T15:57:40.909289+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14265",
      "title": "STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts",
      "authors": [
        "Zachary Bamberger",
        "Till R. Saenger",
        "Gilad Morad",
        "Ofra Amir",
        "Brandon M. Stewart",
        "Amir Feder"
      ],
      "abstract": "STATe presents an interpretable inference-time compute method that uses discrete textual interventions to generate diverse, high-quality, and explainable text by searching over reasoning patterns rather than relying on stochastic sampling.",
      "summary_en": "STATe presents an interpretable inference-time compute method that uses discrete textual interventions to generate diverse, high-quality, and explainable text by searching over reasoning patterns rather than relying on stochastic sampling.",
      "summary_zh": "STATe æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„æ¨ç†æ—¶è®¡ç®—æ–¹æ³•ï¼Œåˆ©ç”¨ç¦»æ•£æ–‡æœ¬å¹²é¢„ï¼Œé€šè¿‡å¯¹æ¨ç†æ¨¡å¼çš„æœç´¢è€Œééšæœºé‡‡æ ·ï¼Œç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡ä¸”å¯è§£é‡Šçš„æ–‡æœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14265",
      "arxiv_url": "https://arxiv.org/abs/2602.14265",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14265",
      "github_url": "https://github.com/zbambergerNLP/state-of-thoughts",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:45:31.617066+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14492",
      "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
      "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Ziyi Gao",
        "Xiaotong Lin",
        "Yun Liu",
        "Xing Fu",
        "Yu Cheng",
        "Yongchao Liu",
        "Weiqiang Wang",
        "Zhongle Xie"
      ],
      "abstract": "A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods. Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU , an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
      "summary_en": "A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods.",
      "summary_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Query-as-Anchor çš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å…·æœ‰ä¸“é—¨æ¶æ„å’Œè®­ç»ƒæ–¹æ³•çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå°†ç”¨æˆ·å»ºæ¨¡ä»é™æ€ç¼–ç è½¬å˜ä¸ºåŠ¨æ€ã€æŸ¥è¯¢æ„ŸçŸ¥çš„åˆæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14492",
      "arxiv_url": "https://arxiv.org/abs/2602.14492",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14492",
      "github_url": "https://github.com/JhCircle/Q-Anchor",
      "upvotes": 14,
      "fetched_at": "2026-02-17T15:57:58.685056+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13949",
      "title": "Experiential Reinforcement Learning",
      "authors": [
        "Taiwei Shi",
        "Sihao Chen",
        "Bowen Jiang",
        "Linxin Song",
        "Longqi Yang",
        "Jieyu Zhao"
      ],
      "abstract": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs. Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback , and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision , improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.",
      "summary_en": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.",
      "summary_zh": "Experiential Reinforcement Learning å¼•å…¥äº†ä¸€ç§æ˜¾å¼çš„ experience-reflection-consolidation å¾ªç¯ï¼Œé€šè¿‡åœ¨ä¸å¢åŠ é¢å¤–æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹å®ç°ç»“æ„åŒ–è¡Œä¸ºä¿®æ­£ï¼Œæå‡äº†ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13949",
      "arxiv_url": "https://arxiv.org/abs/2602.13949",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13949",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-17T15:57:50.538181+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.11574",
      "title": "Learning to Configure Agentic AI Systems",
      "authors": [
        "Aditya Taparia",
        "Som Sagar",
        "Ransalu Senanayake"
      ],
      "abstract": "Learning per-query agent configurations through reinforcement learning improves task accuracy while reducing computational costs compared to fixed templates and hand-tuned heuristics. Configuring LLM-based agent systems involves choosing workflows, tools, token budget s, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering , the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configuration s is a powerful alternative to \"one size fits all\" designs.",
      "summary_en": "Learning per-query agent configurations through reinforcement learning improves task accuracy while reducing computational costs compared to fixed templates and hand-tuned heuristics.",
      "summary_zh": "é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¹ å¾—é€æŸ¥è¯¢æ™ºèƒ½ä½“é…ç½®ï¼Œç›¸è¾ƒäºå›ºå®šæ¨¡æ¿ä¸æ‰‹å·¥è°ƒä¼˜å¯å‘å¼æ–¹æ³•ï¼Œå¯åœ¨æå‡ä»»åŠ¡å‡†ç¡®ç‡çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11574",
      "arxiv_url": "https://arxiv.org/abs/2602.11574",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11574",
      "github_url": "https://github.com/somsagar07/Context_Optimization",
      "upvotes": 14,
      "fetched_at": "2026-02-19T06:45:24.066984+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14699",
      "title": "Qute: Towards Quantum-Native Database",
      "authors": [
        "Muzhi Chen",
        "Xuanhe Zhou",
        "Wei Zhou",
        "Bangrui Xu",
        "Surui Tang",
        "Guoliang Li",
        "Bingsheng He",
        "Yeye He",
        "Yitong Song",
        "Fan Wu"
      ],
      "abstract": "This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.",
      "summary_en": "This paper presents Qute, a quantum database that treats quantum computation as a first-class execution option. Qute compiles extended SQL into gate-efficient quantum circuits and employs a hybrid optimizer, selective quantum indexing, and fidelity-preserving storage to mitigate qubit constraints. Deployed on the origin_wukong quantum processor, Qute outperforms classical baselines at scale, and the authors release an open-source prototype.",
      "summary_zh": "æœ¬æ–‡æå‡ºQuteï¼Œä¸€ç§å°†é‡å­è®¡ç®—è§†ä¸ºä¸€çº§æ‰§è¡Œé€‰é¡¹çš„é‡å­æ•°æ®åº“ã€‚Quteå°†æ‰©å±•SQLç¼–è¯‘ä¸ºé—¨é«˜æ•ˆçš„é‡å­çº¿è·¯ï¼Œå¹¶é‡‡ç”¨æ··åˆä¼˜åŒ–å™¨ã€é€‰æ‹©æ€§é‡å­ç´¢å¼•å’Œä¿çœŸåº¦ä¿æŒå­˜å‚¨æ¥ç¼“è§£é‡å­æ¯”ç‰¹çº¦æŸã€‚éƒ¨ç½²äºorigin_wukongé‡å­å¤„ç†å™¨åï¼ŒQuteåœ¨å¤§è§„æ¨¡ä¸‹æ€§èƒ½ä¼˜äºç»å…¸åŸºçº¿ï¼Œä¸”ä½œè€…å‘å¸ƒäº†å¼€æºåŸå‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14699",
      "arxiv_url": "https://arxiv.org/abs/2602.14699",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14699",
      "github_url": "https://github.com/weAIDB/Qute",
      "upvotes": 13,
      "fetched_at": "2026-02-17T15:58:02.921371+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14234",
      "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
      "authors": [
        "Zheng Chu",
        "Xiao Wang",
        "Jack Hong",
        "Huiming Fan",
        "Yuqi Huang",
        "Yue Yang",
        "Guohai Xu",
        "Chenxiao Zhao",
        "Cheng Xiang",
        "Shengchao Hu",
        "Dongdong Kuang",
        "Ming Liu",
        "Bing Qin",
        "Xing Yu"
      ],
      "abstract": "REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks. Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis , midtraining , and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion , allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining , we strengthen core atomic capabilities knowledge , planning , and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents , we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.",
      "summary_en": "REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.",
      "summary_zh": "REDSearcher æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡æ”¹è¿›çš„ä»»åŠ¡åˆæˆã€å·¥å…·å¢å¼ºæŸ¥è¯¢ã€ä¸­æœŸè®­ç»ƒèƒ½åŠ›å¢å¼ºå’Œæ¨¡æ‹Ÿç¯å¢ƒæ¥ä¼˜åŒ–æœç´¢æ™ºèƒ½ä½“ï¼Œä»¥åº”å¯¹é•¿ç¨‹æœç´¢ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14234",
      "arxiv_url": "https://arxiv.org/abs/2602.14234",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14234",
      "github_url": "https://github.com/RedSearchAgent/REDSearcher",
      "upvotes": 13,
      "fetched_at": "2026-02-17T15:57:56.424041+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14367",
      "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem",
      "authors": [
        "Shuofei Qiao",
        "Yunxiang Wei",
        "Xuehai Wang",
        "Bin Wu",
        "Boyang Xue",
        "Ningyu Zhang",
        "Hossein A. Rahmani",
        "Yanshan Wang",
        "Qiang Zhang",
        "Keyan Ding",
        "Jeff Z. Pan",
        "Huajun Chen",
        "Emine Yilmaz"
      ],
      "abstract": "InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation. The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation . The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.",
      "summary_en": "InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.",
      "summary_zh": "InnoEval æ˜¯ä¸€ç§æ·±åº¦åˆ›æ–°è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åŸºäºçŸ¥è¯†çš„å¤šè§†è§’æ¨ç†ã€å¼‚æ„æ·±åº¦çŸ¥è¯†æ£€ç´¢ä¸å¤šç»´è§£è€¦è¯„ä¼°ï¼Œæ¨¡æ‹Ÿäººç±»æ°´å¹³çš„åˆ›æ„è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14367",
      "arxiv_url": "https://arxiv.org/abs/2602.14367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14367",
      "github_url": "https://github.com/zjunlp/InnoEval",
      "upvotes": 12,
      "fetched_at": "2026-02-17T15:57:57.913951+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13294",
      "title": "VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction",
      "authors": [
        "Jiarong Liang",
        "Max Ku",
        "Ka-Hei Hui",
        "Ping Nie",
        "Wenhu Chen"
      ],
      "abstract": "VisPhyWorld framework evaluates physical reasoning in MLLMs by requiring executable simulator code generation from visual observations, separating physical reasoning from rendering and enabling inspectable, falsifiable world representations. Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding , they struggle to accurately infer physical parameters and to simulate consistent physical dynamics .",
      "summary_en": "VisPhyWorld framework evaluates physical reasoning in MLLMs by requiring executable simulator code generation from visual observations, separating physical reasoning from rendering and enabling inspectable, falsifiable world representations.",
      "summary_zh": "VisPhyWorldæ¡†æ¶é€šè¿‡è¦æ±‚åŸºäºè§†è§‰è§‚å¯Ÿç”Ÿæˆå¯æ‰§è¡Œçš„æ¨¡æ‹Ÿå™¨ä»£ç æ¥è¯„ä¼°MLLMçš„ç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œå°†ç‰©ç†æ¨ç†ä¸æ¸²æŸ“åˆ†ç¦»ï¼Œå¹¶å®ç°å¯æ£€æŸ¥ã€å¯è¯ä¼ªçš„ä¸–ç•Œè¡¨å¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13294",
      "arxiv_url": "https://arxiv.org/abs/2602.13294",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13294",
      "github_url": "https://github.com/TIGER-AI-Lab/VisPhyWorld",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:45:26.467001+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14041",
      "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens",
      "authors": [
        "Yuang Ai",
        "Jiaming Han",
        "Shaobin Zhuang",
        "Weijia Mao",
        "Xuefeng Hu",
        "Ziyan Yang",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen"
      ],
      "abstract": "BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance. We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents , BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head : instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion , a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion , BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation , BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.",
      "summary_en": "BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance.",
      "summary_zh": "BitDanceæ˜¯ä¸€ç§å¯æ‰©å±•çš„è‡ªå›å½’å›¾åƒç”Ÿæˆå™¨ï¼Œåˆ©ç”¨äºŒè¿›åˆ¶è§†è§‰ä»¤ç‰Œå’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆï¼Œå¹¶åœ¨é€Ÿåº¦å’Œæ€§èƒ½æ–¹é¢å‡æœ‰æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14041",
      "arxiv_url": "https://arxiv.org/abs/2602.14041",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14041",
      "github_url": "https://github.com/shallowdream204/BitDance",
      "upvotes": 10,
      "fetched_at": "2026-02-17T15:57:52.051908+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.07824",
      "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training",
      "authors": [
        "Yiwei Qin",
        "Zhen Huang",
        "Tiantian Mi",
        "Weiye Si",
        "Chenyang Zhou",
        "Qipeng Guo",
        "Siyuan Feng",
        "Pengfei Liu"
      ],
      "abstract": "Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text. Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution : advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 ( Generative Refinement ) and L5 ( Cognitive Completion ) using frontier LLMs to explicate reasoning and terminology. To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training , Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks . Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.",
      "summary_en": "Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.",
      "summary_zh": "Data Darwinism æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„æ•°æ®-æ¨¡å‹ååŒè¿›åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åå±‚åˆ†ç±»ä½“ç³»ï¼Œè¯æ˜å…ˆè¿›å¤„ç†æŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æå‡åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦æ–‡æœ¬ä¸Šçš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07824",
      "arxiv_url": "https://arxiv.org/abs/2602.07824",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07824",
      "github_url": "https://github.com/GAIR-NLP/Data-Darwinism",
      "upvotes": 10,
      "fetched_at": "2026-02-17T15:57:36.747333+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14178",
      "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model",
      "authors": [
        "Shaobin Zhuang",
        "Yuang Ai",
        "Jiaming Han",
        "Weijia Mao",
        "Xiaohui Li",
        "Fangyikang Wang",
        "Xiao Wang",
        "Yan Li",
        "Shanchuan Lin",
        "Kun Xu",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen",
        "Yali Wang"
      ],
      "abstract": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements. Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function . SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss . We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding , image generation ( DPG Score : UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing ( GEdit Overall Score : UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
      "summary_en": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.",
      "summary_zh": "UniWeTokå¼•å…¥äº†ä¸€ç§ç»Ÿä¸€ç¦»æ•£tokenizerï¼Œå…¶é‡‡ç”¨å¤§è§„æ¨¡äºŒè¿›åˆ¶ç æœ¬å’Œæ–°é¢–çš„è®­ç»ƒæŠ€æœ¯ï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14178",
      "arxiv_url": "https://arxiv.org/abs/2602.14178",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14178",
      "github_url": "https://github.com/shallowdream204/BitDance",
      "upvotes": 6,
      "fetched_at": "2026-02-17T15:57:55.097541+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13367",
      "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
      "authors": [
        "Chen Yang",
        "Guangyue Peng",
        "Jiaying Zhu",
        "Ran Le",
        "Ruixiang Feng",
        "Tao Zhang",
        "Xiyun Xu",
        "Yang Song",
        "Yiming Jia",
        "Yuntao Wen",
        "Yunzhi Xu",
        "Zekai Wang",
        "Zhenwei An",
        "Zhicong Sun",
        "Zongchao Chen"
      ],
      "abstract": "Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques. We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior , code generation , and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling , ensuring high-quality, human-aligned responses . For code generation , we design complexity-aware rewards in Reinforcement Learning , optimizing both correctness and efficiency. In deep search , we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.",
      "summary_en": "Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques.",
      "summary_zh": "Nanbeige4.1-3Bæ˜¯ä¸€ä¸ª3Bå‚æ•°çš„ç»Ÿä¸€è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å…ˆè¿›çš„å¥–åŠ±å»ºæ¨¡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œåœ¨agenticè¡Œä¸ºã€ä»£ç ç”Ÿæˆå’Œæ¨ç†æ–¹é¢å±•ç°å‡ºç›¸è¾ƒäºæ›´å¤§æ¨¡å‹æ›´ä¼˜çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13367",
      "arxiv_url": "https://arxiv.org/abs/2602.13367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13367",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-17T15:57:47.965838+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13823",
      "title": "Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings",
      "authors": [
        "Haonan Jiang",
        "Yuji Wang",
        "Yongjie Zhu",
        "Xin Lu",
        "Wenyu Qin",
        "Meng Wang",
        "Pengfei Wan",
        "Yansong Tang"
      ],
      "abstract": "A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance. Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks . Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner , ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment , effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.",
      "summary_en": "A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.",
      "summary_zh": "ä¸€ç§æ¨ç†é©±åŠ¨çš„é€šç”¨å¤šæ¨¡æ€ embedding æ¡†æ¶æ•´åˆäº† embedder-guided å¼ºåŒ–å­¦ä¹ ä¸å¯è¿½æº¯ chain-of-thoughtï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œæ£€ç´¢æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13823",
      "arxiv_url": "https://arxiv.org/abs/2602.13823",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13823",
      "github_url": "https://github.com/ZoengHN/Embed-RL",
      "upvotes": 5,
      "fetched_at": "2026-02-17T15:57:49.818770+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.12876",
      "title": "BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "authors": [
        "Huanyao Zhang",
        "Jiepeng Zhou",
        "Bo Li",
        "Bowen Zhou",
        "Yanzhe Dan",
        "Haishan Lu",
        "Zhiyong Cao",
        "Jiaoyang Chen",
        "Yuqian Han",
        "Zinan Sheng",
        "Zhengwei Tao",
        "Hao Liang",
        "Jialong Wu",
        "Yang Shi",
        "Yuanpeng He",
        "Jiaye Lin",
        "Qintong Zhang",
        "Guochen Yan",
        "Runhao Zhao",
        "Zhengpin Li",
        "Xiaohan Yu",
        "Lang Mei"
      ],
      "abstract": "A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities. Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception . Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.",
      "summary_en": "A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.",
      "summary_zh": "ä¸€é¡¹åä¸º BrowseComp-V3 çš„æ–°åŸºå‡†é€šè¿‡éœ€è¦åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€é—´è¿›è¡Œæ·±åº¦æœç´¢çš„å¤æ‚å¤šè·³æ¨ç†ä»»åŠ¡æŒ‘æˆ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰èƒ½åŠ›çš„æ˜¾è‘—å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12876",
      "arxiv_url": "https://arxiv.org/abs/2602.12876",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12876",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-17T15:57:44.766475+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13195",
      "title": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision",
      "authors": [
        "Aadarsh Sahoo",
        "Georgia Gkioxari"
      ],
      "abstract": "Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding. Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding , and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/",
      "summary_en": "Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding.",
      "summary_zh": "å¯¹è¯å¼å›¾åƒåˆ†å‰²é€šè¿‡å¼•å…¥ç»“åˆåˆ†å‰²å…ˆéªŒä¸è¯­è¨€ç†è§£çš„æ–°åŸºå‡†å’Œæ¨¡å‹ï¼Œè§£å†³åŠŸèƒ½æ€§å’Œç‰©ç†æ€§æ¨ç†ä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13195",
      "arxiv_url": "https://arxiv.org/abs/2602.13195",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13195",
      "github_url": "https://github.com/AadSah/ConverSeg",
      "upvotes": 4,
      "fetched_at": "2026-02-17T15:57:46.229181+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14147",
      "title": "LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models",
      "authors": [
        "Shufan Li",
        "Yuchen Zhu",
        "Jiuxiang Gu",
        "Kangning Liu",
        "Zhe Lin",
        "Yongxin Chen",
        "Molei Tao",
        "Aditya Grover",
        "Jason Kuen"
      ],
      "abstract": "LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks. Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing , tree search , and complementary likelihood estimation , to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning , reason-intensive grounding , and image editing .",
      "summary_en": "LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.",
      "summary_zh": "LaViDa-R1 æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡æ–°é¢–çš„è®­ç»ƒæŠ€æœ¯ç»Ÿä¸€äº†ç›‘ç£å¾®è°ƒä¸å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œåœ¨è§†è§‰æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14147",
      "arxiv_url": "https://arxiv.org/abs/2602.14147",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14147",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T15:57:53.734573+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13344",
      "title": "FireRed-Image-Edit-1.0 Techinical Report",
      "authors": [
        "Super Intelligence Team",
        "Changhao Qiao",
        "Chao Hui",
        "Chen Li",
        "Cunzheng Wang",
        "Dejia Song",
        "Jiale Zhang",
        "Jing Li",
        "Qiang Xiang",
        "Runqi Wang",
        "Shuang Sun",
        "Wei Zhu",
        "Xu Tang",
        "Yao Hu",
        "Yibo Chen",
        "Yuhao Huang",
        "Yuxuan Duan",
        "Zhiyi Chen",
        "Ziyuan Guo"
      ],
      "abstract": "FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability. We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation , training methodology , and evaluation design . We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO , DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench , a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks ( ImgEdit and GEdit ) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.",
      "summary_en": "FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.",
      "summary_zh": "FireRed-Image-Edit é‡‡ç”¨ diffusion transformerï¼Œç»“åˆä¼˜åŒ–çš„æ•°æ®æ•´ç†ä¸è®­ç»ƒæ–¹æ³•ï¼Œåœ¨ instruction-based image editing ä¸­å®ç°äº† SOTA æ€§èƒ½ï¼Œå¹¶è¾…ä»¥ comprehensive benchmark ä»¥åŠæå‡æ•°æ®æ•ˆç‡å’Œä¼˜åŒ–ç¨³å®šæ€§çš„æ–°æŠ€æœ¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13344",
      "arxiv_url": "https://arxiv.org/abs/2602.13344",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13344",
      "github_url": "https://github.com/FireRedTeam/FireRed-Image-Edit",
      "upvotes": 3,
      "fetched_at": "2026-02-17T15:57:47.256611+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14721",
      "title": "WebWorld: A Large-Scale World Model for Web Agent Training",
      "authors": [
        "Zikai Xiao",
        "Jianhong Tu",
        "Chuhang Zou",
        "Yuxin Zuo",
        "Zhi Li",
        "Peng Wang",
        "Bowen Yu",
        "Fei Huang",
        "Junyang Lin",
        "Zuozhu Liu"
      ],
      "abstract": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open- web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation , we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation , Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search , outperforming GPT-5 as a world model . Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
      "summary_en": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open- web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation , we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation , Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search , outperforming GPT-5 as a world model . Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
      "summary_zh": "ç½‘é¡µæ™ºèƒ½ä½“éœ€è¦å¤§é‡è½¨è¿¹æ¥å®ç°æ³›åŒ–ï¼Œä½†çœŸå®ä¸–ç•Œè®­ç»ƒå—é™äºç½‘ç»œå»¶è¿Ÿã€é€Ÿç‡é™åˆ¶å’Œå®‰å…¨é£é™©ã€‚æˆ‘ä»¬æ¨å‡º WebWorld ç³»åˆ—ï¼Œé¦–ä¸ªå¤§è§„æ¨¡è®­ç»ƒçš„å¼€æ”¾ç½‘é¡µæ¨¡æ‹Ÿå™¨ã€‚ç°æœ‰æ¨¡æ‹Ÿå™¨å±€é™äºåŒ…å«æ•°åƒæ¡è½¨è¿¹çš„å°é—­ç¯å¢ƒï¼Œè€Œ WebWorld åˆ©ç”¨å¯æ‰©å±•çš„æ•°æ®æµæ°´çº¿ï¼ŒåŸºäº 100 ä¸‡+ å¼€æ”¾ç½‘é¡µäº¤äº’è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæ¨ç†ã€å¤šæ ¼å¼æ•°æ®ä»¥åŠ 30+ æ­¥çš„é•¿ç¨‹æ¨¡æ‹Ÿã€‚å¯¹äºå†…åœ¨è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥ WebWorld-Benchï¼Œå…¶åŒé‡æŒ‡æ ‡æ¶µç›–ä¹ä¸ªç»´åº¦ï¼ŒWebWorld åœ¨è¯¥åŸºå‡†ä¸Šè¾¾åˆ°ä¸ Gemini-3-Pro å¯æ¯”çš„æ¨¡æ‹Ÿæ€§èƒ½ã€‚å¯¹äºå¤–éƒ¨è¯„ä¼°ï¼Œåœ¨ WebWorld åˆæˆè½¨è¿¹ä¸Šè®­ç»ƒçš„ Qwen3-14B åœ¨ WebArena ä¸Šæå‡ +9.2%ï¼Œæ€§èƒ½è¾¾åˆ°ä¸ GPT-4o ç›¸å½“çš„æ°´å¹³ã€‚WebWorld å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æœç´¢ï¼Œä½œä¸ºä¸–ç•Œæ¨¡å‹å…¶æ€§èƒ½ä¼˜äº GPT-5ã€‚é™¤ç½‘é¡µæ¨¡æ‹Ÿå¤–ï¼ŒWebWorld è¿˜å±•ç°å‡ºå¯¹ä»£ç ã€GUI å’Œæ¸¸æˆç¯å¢ƒçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¸–ç•Œæ¨¡å‹æ„å»ºæä¾›å¯å¤ç°çš„æ–¹æ¡ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14721",
      "arxiv_url": "https://arxiv.org/abs/2602.14721",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14721",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:58:04.330698+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14534",
      "title": "MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation",
      "authors": [
        "Hongpeng Wang",
        "Zeyu Zhang",
        "Wenhao Li",
        "Hao Tang"
      ],
      "abstract": "A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility. Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards . Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets , MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.",
      "summary_en": "A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility.",
      "summary_zh": "é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç»Ÿä¸€å¤šæ¨¡æ€è¿åŠ¨æ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½ã€æ¨ç†è¿è´¯æ€§å’Œç‰©ç†åˆç†æ€§ï¼Œæå‡äº†äººä½“è¿åŠ¨ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14534",
      "arxiv_url": "https://arxiv.org/abs/2602.14534",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14534",
      "github_url": "https://github.com/AIGeeksGroup/MoRL",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:57:59.624552+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14060",
      "title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
      "authors": [
        "Yang Liu",
        "Jiaye Yang",
        "Weikang Li",
        "Jiahui Liang",
        "Yang Li",
        "Lingyong Yan"
      ],
      "abstract": "LM-Lexicon improves definition modeling through data clustering, semantic expert learning, and sparse mixture-of-experts architecture, achieving higher BLEU scores and better expert specialization. We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering , semantic expert learning , and model merging using a sparse mixture-of-experts architecture . By decomposing the definition modeling task into specialized semantic domains , where small language models are trained as domain experts , LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.",
      "summary_en": "LM-Lexicon improves definition modeling through data clustering, semantic expert learning, and sparse mixture-of-experts architecture, achieving higher BLEU scores and better expert specialization.",
      "summary_zh": "LM-Lexiconé€šè¿‡æ•°æ®èšç±»ã€è¯­ä¹‰ä¸“å®¶å­¦ä¹ å’Œç¨€ç–æ··åˆä¸“å®¶æ¶æ„æ”¹è¿›å®šä¹‰å»ºæ¨¡ï¼Œå®ç°äº†æ›´é«˜çš„BLEUåˆ†æ•°å’Œæ›´å¥½çš„ä¸“å®¶ç‰¹åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14060",
      "arxiv_url": "https://arxiv.org/abs/2602.14060",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14060",
      "github_url": "https://github.com/jacklanda/LMLexicon",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:57:52.665140+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.09185",
      "title": "AIDev: Studying AI Coding Agents on GitHub",
      "authors": [
        "Hao Li",
        "Haoxiang Zhang",
        "Ahmed E. Hassan"
      ],
      "abstract": "AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios. AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories . AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity , and human-AI collaboration in the new era of software engineering. > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering , Agentic Engineering",
      "summary_en": "AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.",
      "summary_zh": "AIDevæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ”¶å½•äº†çœŸå®GitHubä»“åº“ä¸­ç”±agentç¼–å†™çš„pull requestï¼Œæ•æ‰äº†AI coding agentåœ¨å®é™…è½¯ä»¶å¼€å‘åœºæ™¯ä¸­çš„ä½¿ç”¨æƒ…å†µã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09185",
      "arxiv_url": "https://arxiv.org/abs/2602.09185",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09185",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:57:38.181506+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.15031",
      "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
      "authors": [
        "Yehonathan Litman",
        "Shikun Liu",
        "Dario Seyb",
        "Nicholas Milef",
        "Yang Zhou",
        "Carl Marshall",
        "Shubham Tulsiani",
        "Caleb Leak"
      ],
      "abstract": "Efficient video inpainting framework that focuses computation on masked regions while maintaining global context consistency through a lightweight embedder. High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models . However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens , yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation .",
      "summary_en": "Efficient video inpainting framework that focuses computation on masked regions while maintaining global context consistency through a lightweight embedder.",
      "summary_zh": "é«˜æ•ˆçš„è§†é¢‘ä¿®å¤æ¡†æ¶ï¼Œå°†è®¡ç®—èšç„¦äºæ©ç åŒºåŸŸï¼ŒåŒæ—¶é€šè¿‡è½»é‡çº§åµŒå…¥å™¨ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15031",
      "arxiv_url": "https://arxiv.org/abs/2602.15031",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15031",
      "github_url": "https://github.com/yehonathanlitman/EditCtrl",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:45:35.949403+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14941",
      "title": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories",
      "authors": [
        "Zun Wang",
        "Han Lin",
        "Jaehong Yoon",
        "Jaemin Cho",
        "Yue Zhang",
        "Mohit Bansal"
      ],
      "abstract": "AnchorWeave addresses long-term video generation consistency by replacing global 3D scene reconstruction with multiple local geometric memories and a multi-anchor weaving controller to reconcile cross-view inconsistencies. Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation . Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment , as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval .",
      "summary_en": "AnchorWeave addresses long-term video generation consistency by replacing global 3D scene reconstruction with multiple local geometric memories and a multi-anchor weaving controller to reconcile cross-view inconsistencies.",
      "summary_zh": "AnchorWeave é€šè¿‡ä»¥å¤šä¸ªå±€éƒ¨å‡ ä½•è®°å¿†æ›¿ä»£å…¨å±€ä¸‰ç»´åœºæ™¯é‡å»ºï¼Œå¹¶å€ŸåŠ©å¤šé”šç‚¹ç¼–ç»‡æ§åˆ¶å™¨è°ƒå’Œè·¨è§†å›¾ä¸ä¸€è‡´æ€§ï¼Œè§£å†³é•¿æœŸè§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14941",
      "arxiv_url": "https://arxiv.org/abs/2602.14941",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14941",
      "github_url": "https://github.com/wz0919/AnchorWeave",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:45:33.753963+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14689",
      "title": "Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks",
      "authors": [
        "Lukas Struppek",
        "Adam Gleave",
        "Kellin Pelrine"
      ],
      "abstract": "Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models. As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses , open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling , which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models . Our results show that prefill attacks are consistently effective against all major contemporary open-weight models , revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling , they remain vulnerable to tailored, model-specific strategies . Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.",
      "summary_en": "Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models.",
      "summary_zh": "Prefill attacks ä»£è¡¨äº†å¼€æ”¾æƒé‡è¯­è¨€æ¨¡å‹ä¸­ä¸€ç§é‡å¤§ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„æ¼æ´ï¼Œå½±å“ç€ä¸»æµå½“ä»£æ¨¡å‹ï¼Œå°½ç®¡å¤§å‹æ¨ç†æ¨¡å‹å¯¹æ­¤è¡¨ç°å‡ºä¸€å®šçš„æŠµæŠ—æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14689",
      "arxiv_url": "https://arxiv.org/abs/2602.14689",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14689",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:58:01.257492+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13346",
      "title": "CellMaster: Collaborative Cell Type Annotation in Single-Cell Analysis",
      "authors": [
        "Zhen Wang",
        "Yiming Gao",
        "Jieyuan Liu",
        "Enze Ma",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Zhiting Hu",
        "Wei Wang",
        "Trey Ideker",
        "Eric P. Xing"
      ],
      "abstract": "CellMaster uses LLM-encoded knowledge for zero-shot cell-type annotation in single-cell RNA sequencing, improving accuracy over existing tools through interpretable rationales without pre-training. Single-cell RNA-seq (scRNA-seq) enables atlas-scale profiling of complex tissues, revealing rare lineages and transient states. Yet, assigning biologically valid cell identities remains a bottleneck because markers are tissue- and state-dependent, and novel states lack references. We present CellMaster, an AI agent that mimics expert practice for zero-shot cell-type annotation . Unlike existing automated tools, CellMaster leverages LLM-encoded knowledge (e.g., GPT-4o) to perform on-the-fly annotation with interpretable rationales , without pre-training or fixed marker databases. Across 9 datasets spanning 8 tissues, CellMaster improved accuracy by 7.1% over best-performing baselines (including CellTypist and scTab ) in automatic mode. With human-in-the-loop refinement, this advantage increased to 18.6%, with a 22.1% gain on subtype populations. The system demonstrates particular strength in rare and novel cell states where baselines often fail. Source code and the web application are available at https://github.com/AnonymousGym/CellMaster{https://github.com/AnonymousGym/CellMaster}.",
      "summary_en": "CellMaster uses LLM-encoded knowledge for zero-shot cell-type annotation in single-cell RNA sequencing, improving accuracy over existing tools through interpretable rationales without pre-training.",
      "summary_zh": "CellMasteråˆ©ç”¨LLMç¼–ç çš„çŸ¥è¯†å®ç°å•ç»†èƒRNAæµ‹åºçš„é›¶æ ·æœ¬ç»†èƒç±»å‹æ³¨é‡Šï¼Œé€šè¿‡å¯è§£é‡Šçš„ä¾æ®åœ¨ä¸ç»è¿‡é¢„è®­ç»ƒçš„æƒ…å†µä¸‹å–å¾—äº†ä¼˜äºç°æœ‰å·¥å…·çš„å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13346",
      "arxiv_url": "https://arxiv.org/abs/2602.13346",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13346",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:45:28.805594+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.12586",
      "title": "Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models",
      "authors": [
        "Joshua Ong Jun Leang",
        "Yu Zhao",
        "Mihaela CÄƒtÄƒlina Stoian",
        "Wenda Li",
        "Shay B. Cohen",
        "Eleonora Giunchiglia"
      ],
      "abstract": "McDiffuSE enhances Masked Diffusion Models by optimizing slot infilling order through Monte Carlo Tree Search, improving reasoning task performance through strategic exploration of generation sequences. While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders . Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.",
      "summary_en": "McDiffuSE enhances Masked Diffusion Models by optimizing slot infilling order through Monte Carlo Tree Search, improving reasoning task performance through strategic exploration of generation sequences.",
      "summary_zh": "McDiffuSEé€šè¿‡Monte Carlo Tree Searchä¼˜åŒ–slot infilling orderä»¥å¢å¼ºMasked Diffusion Modelsï¼Œå¹¶é€šè¿‡strategic exploration of generation sequencesæå‡reasoning task performanceã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12586",
      "arxiv_url": "https://arxiv.org/abs/2602.12586",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12586",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:43.762632+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.12299",
      "title": "Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization",
      "authors": [
        "Mandip Goswami"
      ],
      "abstract": "Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorous signal processing with intuitive visualization remain scarce. This paper presents AcoustiVision Pro, an open-source web-based platform for comprehensive room impulse response (RIR) analysis. The system computes twelve distinct acoustic parameters from uploaded or dataset-sourced RIRs, provides interactive 3D visualizations of early reflections, generates frequency-dependent decay characteristics through waterfall plots, and checks compliance against international standards including ANSI S12.60 and ISO 3382. We introduce the accompanying RIRMega and RIRMega Speech datasets hosted on Hugging Face, containing thousands of simulated room impulse responses with full metadata. The platform supports real-time auralization through FFT-based convolution, exports detailed PDF reports suitable for engineering documentation, and provides CSV data export for further analysis. We describe the mathematical foundations underlying each acoustic metric, detail the system architecture, and present preliminary case studies demonstrating the platform's utility across diverse application domains including classroom acoustics, healthcare facility design, and recording studio evaluation.",
      "summary_en": "This paper presents AcoustiVision Pro, an open-source web-based platform for room impulse response (RIR) analysis that computes twelve acoustic parameters, provides interactive 3D visualizations of early reflections, generates frequency-dependent waterfall plots, and checks compliance against ANSI S12.60 and ISO 3382 standards. The system supports real-time auralization through FFT-based convolution, exports PDF reports and CSV data, and is accompanied by the RIRMega and RIRMega Speech datasets hosted on Hugging Face containing thousands of simulated RIRs with full metadata. We describe the mathematical foundations underlying each metric, detail the system architecture, and present case studies demonstrating the platform's utility in classroom acoustics, healthcare facility design, and recording studio evaluation.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº† AcoustiVision Proï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„åŸºäº Web çš„æˆ¿é—´è„‰å†²å“åº” (RIR) åˆ†æå¹³å°ï¼Œå¯è®¡ç®—åäºŒé¡¹å£°å­¦å‚æ•°ï¼Œæä¾›æ—©æœŸåå°„çš„äº¤äº’å¼ 3D å¯è§†åŒ–ï¼Œç”Ÿæˆé¢‘åŸŸ waterfall plotsï¼Œå¹¶æ£€æŸ¥æ˜¯å¦ç¬¦åˆ ANSI S12.60 å’Œ ISO 3382 æ ‡å‡†ã€‚è¯¥ç³»ç»Ÿæ”¯æŒé€šè¿‡åŸºäº FFT çš„å·ç§¯å®ç°å®æ—¶ auralizationï¼Œå¯å¯¼å‡º PDF æŠ¥å‘Šå’Œ CSV æ•°æ®ï¼Œå¹¶é™„å¸¦æ‰˜ç®¡äº Hugging Face çš„ RIRMega å’Œ RIRMega Speech æ•°æ®é›†ï¼ŒåŒ…å«æ•°åƒæ¡å¸¦æœ‰å®Œæ•´å…ƒæ•°æ®çš„æ¨¡æ‹Ÿ RIRã€‚æˆ‘ä»¬æè¿°äº†å„é¡¹æŒ‡æ ‡èƒŒåçš„æ•°å­¦åŸºç¡€ï¼Œè¯¦ç»†ä»‹ç»äº†ç³»ç»Ÿæ¶æ„ï¼Œå¹¶å±•ç¤ºäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥å¹³å°åœ¨æ•™å®¤å£°å­¦ã€åŒ»ç–—è®¾æ–½è®¾è®¡å’Œå½•éŸ³æ£šè¯„ä¼°ä¸­çš„å®ç”¨æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12299",
      "arxiv_url": "https://arxiv.org/abs/2602.12299",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12299",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:42.450227+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.11968",
      "title": "DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling",
      "authors": [
        "Mariia Fedorova",
        "Andrey Kutuzov",
        "Khonzoda Umarova"
      ],
      "abstract": "In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.",
      "summary_en": "DHPLT is an open collection of diachronic corpora covering 41 diverse languages across three time periods (2011-2015, 2020-2021, and 2024-present), derived from HPLT web-crawled data using timestamps as temporal signals with one million documents per period per language. The resource provides pre-computed word type and token embeddings and lexical substitutions for target words while allowing researchers to define custom targets, aiming to fill the gap in multilingual diachronic corpora for semantic change modelling beyond high-resource languages. All datasets are available at https://data.hplt-project.org/three/diachronic/.",
      "summary_zh": "DHPLTæ˜¯ä¸€ä¸ªæ¶µç›–41ç§ä¸åŒè¯­è¨€ã€è·¨è¶Šä¸‰ä¸ªæ—¶æœŸï¼ˆ2011-2015ã€2020-2021å’Œ2024è‡³ä»Šï¼‰çš„å¼€æ”¾å†æ—¶è¯­æ–™åº“é›†åˆï¼ŒåŸºäºæ—¶é—´æˆ³ä½œä¸ºæ—¶é—´ä¿¡å·ä»HPLTç½‘ç»œçˆ¬å–æ•°æ®ä¸­æå–ï¼Œæ¯ç§è¯­è¨€æ¯ä¸ªæ—¶æœŸåŒ…å«ä¸€ç™¾ä¸‡ç¯‡æ–‡æ¡£ã€‚è¯¥èµ„æºæä¾›é¢„è®¡ç®—çš„word typeå’Œtoken embeddingsä»¥åŠç›®æ ‡è¯çš„è¯æ±‡æ›¿æ¢ï¼ŒåŒæ—¶å…è®¸ç ”ç©¶è€…å®šä¹‰è‡ªå®šä¹‰ç›®æ ‡ï¼Œæ—¨åœ¨å¡«è¡¥é’ˆå¯¹é«˜èµ„æºè¯­è¨€ä¹‹å¤–çš„è¯­è¨€ã€ç”¨äºè¯­ä¹‰å˜åŒ–å»ºæ¨¡çš„å¤šè¯­è¨€å†æ—¶è¯­æ–™åº“çš„ç©ºç™½ã€‚æ‰€æœ‰æ•°æ®é›†å‡å¯åœ¨ https://data.hplt-project.org/three/diachronic/ è·å–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11968",
      "arxiv_url": "https://arxiv.org/abs/2602.11968",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11968",
      "github_url": "https://github.com/ltgoslo/scdisc_hplt",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:41.607451+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.09319",
      "title": "Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation",
      "authors": [
        "Zhisheng Qi",
        "Utkarsh Sahu",
        "Li Ma",
        "Haoyu Han",
        "Ryan Rossi",
        "Franck Dernoncourt",
        "Mahantesh Halappanavar",
        "Nesreen Ahmed",
        "Yushun Dong",
        "Yue Zhao",
        "Yu Zhang",
        "Yu Wang"
      ],
      "abstract": "A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols. Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models , and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies , representative retrieval embedding models , and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.",
      "summary_en": "A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.",
      "summary_zh": "ä»‹ç»äº†ä¸€ç§ç”¨äºè¯„ä¼°é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„çŸ¥è¯†æå–æ”»å‡»ï¼ˆknowledge-extraction attacksï¼‰çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§æ”»å‡»ä¸é˜²å¾¡ç­–ç•¥ï¼Œè·¨è¶Šå¤šç§æ£€ç´¢ä¸ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09319",
      "arxiv_url": "https://arxiv.org/abs/2602.09319",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09319",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:39.635562+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.07673",
      "title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation",
      "authors": [
        "Jiangnan Fang",
        "Cheng-Tse Liu",
        "Hanieh Deilamsalehy",
        "Nesreen K. Ahmed",
        "Puneet Mathur",
        "Nedim Lipka",
        "Franck Dernoncourt",
        "Ryan A. Rossi"
      ],
      "abstract": "LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.",
      "summary_en": "LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.",
      "summary_zh": "LLMè¯„åˆ¤å™¨åå‘ä¸å…¶è‡ªèº«ç”Ÿæˆç›¸ä¼¼çš„æ‘˜è¦ï¼Œä¸”éšç€æ‘˜è¦ä¸äººç±»å‚è€ƒçš„é‡å åº¦é™ä½ï¼Œå…¶æ€§èƒ½ä¼šä¸‹é™ï¼Œè¿™ä¸€ç°è±¡åœ¨å¤šç§æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸­å‡å­˜åœ¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07673",
      "arxiv_url": "https://arxiv.org/abs/2602.07673",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07673",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:35.290034+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.15259",
      "title": "Knowing Isn't Understanding: Re-grounding Generative Proactivity with Epistemic and Behavioral Insight",
      "authors": [
        "Kirandeep Kaur",
        "Xingda Lyu",
        "Chirag Shah"
      ],
      "abstract": "Generative AI agents equate understanding with resolving explicit queries, an assumption that confines interaction to what users can articulate. This assumption breaks down when users themselves lack awareness of what is missing, risky, or worth considering. In such conditions, proactivity is not merely an efficiency enhancement, but an epistemic necessity. We refer to this condition as epistemic incompleteness: where progress depends on engaging with unknown unknowns for effective partnership. Existing approaches to proactivity remain narrowly anticipatory, extrapolating from past behavior and presuming that goals are already well defined, thereby failing to support users meaningfully. However, surfacing possibilities beyond a user's current awareness is not inherently beneficial. Unconstrained proactive interventions can misdirect attention, overwhelm users, or introduce harm. Proactive agents, therefore, require behavioral grounding: principled constraints on when, how, and to what extent an agent should intervene. We advance the position that generative proactivity must be grounded both epistemically and behaviorally. Drawing on the philosophy of ignorance and research on proactive behavior, we argue that these theories offer critical guidance for designing agents that can engage responsibly and foster meaningful partnerships.",
      "summary_en": "Generative AI agents conventionally equate understanding with resolving explicit queries, an assumption that fails under epistemic incompleteness where users lack awareness of unknown unknowns required for progress. The authors argue that effective proactivity requires dual grounding: epistemically to surface possibilities beyond current user awareness, and behaviorally through principled constraints on when and how to intervene, drawing on philosophy of ignorance and proactive behavior research. Existing anticipatory approaches fail by assuming well-defined goals, while unconstrained proactivity risks misdirection or harm, but epistemically and behaviorally grounded agents can engage responsibly to foster meaningful partnerships.",
      "summary_zh": "ç”Ÿæˆå¼AIæ™ºèƒ½ä½“ä¼ ç»Ÿä¸Šå°†ç†è§£ç­‰åŒäºè§£å†³æ˜¾å¼æŸ¥è¯¢ï¼Œè¿™ä¸€å‡è®¾åœ¨è®¤çŸ¥ä¸å®Œå¤‡æ€§æƒ…å¢ƒä¸‹å¤±æ•ˆâ€”â€”ç”¨æˆ·ç¼ºä¹å¯¹è¿›æ­¥æ‰€éœ€çš„\"æœªçŸ¥çš„æœªçŸ¥\"çš„æ„è¯†ã€‚ä½œè€…è®¤ä¸ºï¼Œæœ‰æ•ˆçš„ä¸»åŠ¨æ€§éœ€è¦åŒé‡åŸºç¡€ï¼šåœ¨è®¤è¯†è®ºå±‚é¢ï¼Œæ­ç¤ºè¶…å‡ºå½“å‰ç”¨æˆ·æ„è¯†çš„å¯èƒ½æ€§ï¼›åœ¨è¡Œä¸ºå±‚é¢ï¼Œé€šè¿‡åŸåˆ™æ€§çº¦æŸè§„èŒƒå¹²é¢„çš„æ—¶æœºä¸æ–¹å¼ï¼Œå…¶ç†è®ºä¾æ®æ¥è‡ªæ— çŸ¥å“²å­¦ä¸ä¸»åŠ¨è¡Œä¸ºç ”ç©¶ã€‚ç°æœ‰çš„é¢„æµ‹æ€§æ–¹æ³•å› å‡è®¾ç›®æ ‡æ˜ç¡®å®šä¹‰è€Œå¤±æ•ˆï¼Œè€Œæ— çº¦æŸçš„ä¸»åŠ¨æ€§åˆ™å­˜åœ¨è¯¯å¯¼æˆ–ä¼¤å®³çš„é£é™©ï¼›å”¯æœ‰åœ¨è®¤è¯†è®ºå’Œè¡Œä¸ºå±‚é¢å‡å…·å¤‡åŸºç¡€çš„æ™ºèƒ½ä½“ï¼Œæ‰èƒ½è´Ÿè´£ä»»åœ°å‚ä¸äº’åŠ¨ï¼Œä¿ƒæˆæœ‰æ„ä¹‰çš„ä¼™ä¼´å…³ç³»ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15259",
      "arxiv_url": "https://arxiv.org/abs/2602.15259",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15259",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:45:37.877481+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14696",
      "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
      "authors": [
        "Nihal V. Nayak",
        "Paula Rodriguez-Diaz",
        "Neha Hulkund",
        "Sara Beery",
        "David Alvarez-Melis"
      ],
      "abstract": "Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets. Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms . Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representation s choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds . More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
      "summary_en": "Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.",
      "summary_zh": "é’ˆå¯¹LLMå¾®è°ƒçš„æŒ‡ä»¤é€‰æ‹©å¯é€šè¿‡ç³»ç»Ÿåˆ†ææ•°æ®è¡¨ç¤ºä¸é€‰æ‹©ç®—æ³•åŠ ä»¥æ”¹è¿›ï¼Œå…¶ä¸­åŸºäºæ¢¯åº¦çš„è¡¨ç¤ºä¸è´ªå¿ƒè½®è¯¢é€‰æ‹©åœ¨ä½é¢„ç®—ä¸‹è¡¨ç°æœ€ä½³ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14696",
      "arxiv_url": "https://arxiv.org/abs/2602.14696",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14696",
      "github_url": "https://github.com/dcml-lab/targeted-instruction-selection",
      "upvotes": 0,
      "fetched_at": "2026-02-17T15:58:01.965017+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14560",
      "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
      "authors": [
        "Sandy H. S. Herho",
        "Rusmawan Suwarman",
        "Nurjanna J. Trilaksono",
        "Iwan P. Anwar",
        "Faiz R. Fajary"
      ],
      "abstract": "Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales. Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems . We treat El NiÃ±o-Southern Oscillation ( ENSO ), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the NiÃ±o 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems ( pelog and slendro ) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space . Recurrence-based diagnostics , convex hull geometry , and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO 's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro . Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
      "summary_en": "Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales.",
      "summary_zh": "ENSOæ•°æ®çš„å‚æ•°æ˜ å°„å£°åŒ–é€šè¿‡å£°å­¦ç›¸ç©ºé—´åˆ†æä¿ç•™äº†åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œåœ¨ä¼ ç»ŸéŸ³é˜¶ä¸­æ­ç¤ºäº†ä¸åŒçš„è€¦åˆæœºåˆ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14560",
      "arxiv_url": "https://arxiv.org/abs/2602.14560",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14560",
      "github_url": "https://github.com/sandyherho/suppl-enso-javanese-sonification",
      "upvotes": 0,
      "fetched_at": "2026-02-17T15:58:00.286351+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13516",
      "title": "SPILLage: Agentic Oversharing on the Web",
      "authors": [
        "Jaechul Roh",
        "Eugene Bagdasarian",
        "Hamed Haddadi",
        "Ali Shahin Shamsabadi"
      ],
      "abstract": "Web agents inadvertently disclose user information through both content and behavioral traces, with behavioral oversharing being more prevalent than content oversharing, and this issue persists despite mitigation efforts. LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage , a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation . However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success . Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/ SPILLage .",
      "summary_en": "Web agents inadvertently disclose user information through both content and behavioral traces, with behavioral oversharing being more prevalent than content oversharing, and this issue persists despite mitigation efforts.",
      "summary_zh": "ç½‘é¡µæ™ºèƒ½ä½“é€šè¿‡å†…å®¹ç—•è¿¹å’Œè¡Œä¸ºç—•è¿¹æ— æ„ä¸­æ³„éœ²ç”¨æˆ·ä¿¡æ¯ï¼Œå…¶ä¸­è¡Œä¸ºè¿‡åº¦åˆ†äº«æ¯”å†…å®¹è¿‡åº¦åˆ†äº«æ›´ä¸ºæ™®éï¼Œä¸”å°½ç®¡é‡‡å–äº†ç¼“è§£æªæ–½ï¼Œè¯¥é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13516",
      "arxiv_url": "https://arxiv.org/abs/2602.13516",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13516",
      "github_url": "https://github.com/jrohsc/SPILLage",
      "upvotes": 0,
      "fetched_at": "2026-02-17T15:57:48.870947+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.10458",
      "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
      "authors": [
        "Yansong Qu",
        "Zihao Sheng",
        "Zilin Huang",
        "Jiancong Chen",
        "Yuhao Luo",
        "Tianyi Wang",
        "Yiheng Feng",
        "Samuel Labi",
        "Sikai Chen"
      ],
      "abstract": "Found-RL integrates vision-language models with reinforcement learning for autonomous driving, addressing sample efficiency and latency issues through asynchronous inference and specialized supervision mechanisms.",
      "summary_en": "Found-RL integrates vision-language models with reinforcement learning for autonomous driving, addressing sample efficiency and latency issues through asynchronous inference and specialized supervision mechanisms.",
      "summary_zh": "Found-RLå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆç”¨äºè‡ªåŠ¨é©¾é©¶ï¼Œé€šè¿‡å¼‚æ­¥æ¨ç†å’Œä¸“é—¨çš„ç›‘ç£æœºåˆ¶è§£å†³æ ·æœ¬æ•ˆç‡ä¸å»¶è¿Ÿé—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10458",
      "arxiv_url": "https://arxiv.org/abs/2602.10458",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10458",
      "github_url": "https://github.com/ys-qu/found-rl",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:45:22.148671+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.10388",
      "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
      "authors": [
        "Zhongzhi Li",
        "Xuansheng Wu",
        "Yijiang Li",
        "Lijie Hu",
        "Ninghao Liu"
      ],
      "abstract": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures. The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance . In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following , toxicity detection , reward modeling , and behavior steering . Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer . Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
      "summary_en": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.",
      "summary_zh": "Feature Activation Coverage åœ¨å¯è§£é‡Šçš„ç‰¹å¾ç©ºé—´ä¸­è¡¡é‡æ•°æ®å¤šæ ·æ€§ï¼Œå¹¶å®ç°å¤šæ ·æ€§é©±åŠ¨çš„æ•°æ®åˆæˆï¼Œä»è€Œæå‡è·¨å¤šç§è¯­è¨€æ¨¡å‹æ¶æ„çš„ä¸‹æ¸¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10388",
      "arxiv_url": "https://arxiv.org/abs/2602.10388",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10388",
      "github_url": "https://github.com/Zhongzhi660/FAC-Synthesis",
      "upvotes": 203,
      "fetched_at": "2026-02-17T09:52:03.950719+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12783",
      "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise",
      "authors": [
        "Yuejie Li",
        "Ke Yang",
        "Yueying Hua",
        "Berlin Chen",
        "Jianhao Nie",
        "Yueping He",
        "Caixin Kang"
      ],
      "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.",
      "summary_en": "SQuTR is a robustness benchmark for spoken query retrieval that addresses limitations of existing datasets by aggregating 37,317 unique queries from six English and Chinese text retrieval datasets, synthesized using voice profiles from 200 real speakers and mixed with 17 categories of real-world environmental noise under controlled SNR levels. The benchmark provides a unified evaluation protocol for assessing both cascaded and end-to-end retrieval systems across conditions ranging from quiet to highly noisy environments. Experimental results demonstrate that retrieval performance degrades substantially as noise increases, with significant variation across different systems, and even large-scale models struggle under extreme noise, indicating that robustness remains a critical bottleneck in spoken query to text retrieval.",
      "summary_zh": "SQuTR æ˜¯ä¸€ä¸ªé¢å‘å£è¯­æŸ¥è¯¢æ£€ç´¢çš„é²æ£’æ€§åŸºå‡†æµ‹è¯•ï¼Œå®ƒé€šè¿‡ä»å…­ä¸ªè‹±æ–‡å’Œä¸­æ–‡æ–‡æœ¬æ£€ç´¢æ•°æ®é›†ä¸­èšåˆ 37,317 æ¡ç‹¬ç‰¹æŸ¥è¯¢ï¼Œä½¿ç”¨ 200 ä½çœŸå®è¯´è¯äººçš„è¯­éŸ³ç‰¹å¾åˆæˆï¼Œå¹¶åœ¨å—æ§ SNR æ°´å¹³ä¸‹æ··å…¥ 17 ç±»çœŸå®ç¯å¢ƒå™ªå£°ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†æä¾›äº†ç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼Œç”¨äºåœ¨ä»å®‰é™åˆ°é«˜å™ªå£°ç¯å¢ƒçš„å¤šç§æ¡ä»¶ä¸‹è¯„ä¼°çº§è”ï¼ˆcascadedï¼‰å’Œç«¯åˆ°ç«¯ï¼ˆend-to-endï¼‰æ£€ç´¢ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€å™ªå£°å¢åŠ ï¼Œæ£€ç´¢æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸åŒç³»ç»Ÿé—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”å³ä½¿å¤§è§„æ¨¡æ¨¡å‹åœ¨æç«¯å™ªå£°ä¸‹ä¹Ÿè¡¨ç°ä¸ä½³ï¼Œè¿™è¡¨æ˜é²æ£’æ€§ä»ç„¶æ˜¯å£è¯­æŸ¥è¯¢åˆ°æ–‡æœ¬æ£€ç´¢ä¸­çš„å…³é”®ç“¶é¢ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12783",
      "arxiv_url": "https://arxiv.org/abs/2602.12783",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12783",
      "github_url": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval",
      "upvotes": 134,
      "fetched_at": "2026-02-17T09:52:23.869416+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12705",
      "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
      "authors": [
        "Baorong Shi",
        "Bo Cui",
        "Boyuan Jiang",
        "Deli Yu",
        "Fang Qian",
        "Haihua Yang",
        "Huichao Wang",
        "Jiale Chen",
        "Jianfei Pan",
        "Jieqiong Cao",
        "Jinghao Lin",
        "Kai Wu",
        "Lin Yang",
        "Shengsheng Yao",
        "Tao Chen",
        "Xiaojun Xiao",
        "Xiaozhong Ji",
        "Xu Wang",
        "Yijun He",
        "Zhixiong Yang"
      ],
      "abstract": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "summary_en": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "summary_zh": "MedXIAOHE æ˜¯ä¸€ç§åŒ»ç–—è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ entity-aware continual pretrainingã€reinforcement learning å’Œ tool-augmented agentic training æ¥å¢å¼ºä¸´åºŠç†è§£ï¼Œå®ç°å¯é çš„è¯Šæ–­æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12705",
      "arxiv_url": "https://arxiv.org/abs/2602.12705",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12705",
      "github_url": "",
      "upvotes": 56,
      "fetched_at": "2026-02-17T09:52:22.809484+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11858",
      "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
      "authors": [
        "Lai Wei",
        "Liangbo He",
        "Jun Lan",
        "Lingzhong Dong",
        "Yutong Cai",
        "Siyuan Li",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Linghe Kong",
        "Yue Wang",
        "Zhuosheng Zhang",
        "Weiran Huang"
      ],
      "abstract": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks. Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception , where decisive evidence is small and easily overwhelmed by global context. Recent \" Thinking-with-Images \" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation , which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench , a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents . We further discuss when \" Thinking-with-Images \" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
      "summary_en": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.",
      "summary_zh": "Region-to-Image Distillation é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å†…éƒ¨æ‰§è¡Œè¿­ä»£ç¼©æ”¾ï¼Œä½¿ MLLMs èƒ½å¤Ÿå®ç°ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥ï¼Œæ¶ˆé™¤äº†é‡å¤å·¥å…·è°ƒç”¨å’Œè§†è§‰é‡ç¼–ç çš„éœ€æ±‚ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11858",
      "arxiv_url": "https://arxiv.org/abs/2602.11858",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11858",
      "github_url": "https://github.com/inclusionAI/Zooming-without-Zooming",
      "upvotes": 52,
      "fetched_at": "2026-02-17T09:52:10.555002+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.08683",
      "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
      "authors": [
        "Feilong Tang",
        "Xiang An",
        "Yunyao Yan",
        "Yin Xie",
        "Bin Qin",
        "Kaicheng Yang",
        "Yifei Shen",
        "Yuanhan Zhang",
        "Chunyuan Li",
        "Shikun Feng",
        "Changrui Chen",
        "Huajie Tan",
        "Ming Hu",
        "Manyuan Zhang",
        "Bo Li",
        "Ziyong Feng",
        "Ziwei Liu",
        "Zongyuan Ge",
        "Jiankang Deng"
      ],
      "abstract": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy. Hypothesis. Artificial general intelligence is, at its core, a compression problem . Effective compression demands resonance : deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information , the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification , OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts , jointly capturing object permanence and motion dynamics . Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM , it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data . Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
      "summary_en": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy.",
      "summary_zh": "è§†è§‰ç†è§£å¯é€šè¿‡å°†æ¶æ„ä¸è§†é¢‘å‹ç¼©çš„ä¿¡æ¯è®ºåŸç†å¯¹é½æ¥æ”¹è¿›ï¼Œåˆ©ç”¨åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç¨€ç–é©±åŠ¨ç¼–ç ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08683",
      "arxiv_url": "https://arxiv.org/abs/2602.08683",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08683",
      "github_url": "https://github.com/EvolvingLMMs-Lab/OneVision-Encoder",
      "upvotes": 40,
      "fetched_at": "2026-02-17T09:51:59.876130+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13191",
      "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
      "authors": [
        "Sayan Deb Sarkar",
        "RÃ©mi Pautrat",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Iro Armeni",
        "Mahdi Rad",
        "Mihai Dusmanu"
      ],
      "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
      "summary_en": "Current Video Language Models (VideoLMs) rely on keyframe sampling that misses temporal details and incurs high computational costs from full-image processing. We propose leveraging video codec primitives (motion vectors and residuals) to encode sparsity without full-image encoding, using lightweight transformer encoders that aggregate these primitives and align them with image embeddings through pre-training. This approach reduces time-to-first-token by up to 86% and token usage by up to 93% while maintaining or exceeding performance on 14 video understanding benchmarks spanning question answering, temporal reasoning, and spatial understanding.",
      "summary_zh": "ç°æœ‰çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVideoLMsï¼‰ä¾èµ–å…³é”®å¸§é‡‡æ ·ï¼Œè¿™ä¼šä¸¢å¤±æ—¶åºç»†èŠ‚ï¼Œå¹¶å› å…¨å›¾å¤„ç†è€Œäº§ç”Ÿé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨è§†é¢‘ç¼–è§£ç åŸè¯­ï¼ˆè¿åŠ¨å‘é‡å’Œæ®‹å·®ï¼‰æ¥ç¼–ç ç¨€ç–æ€§ï¼Œæ— éœ€å®Œæ•´å›¾åƒç¼–ç ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§ transformer ç¼–ç å™¨èšåˆè¿™äº›åŸè¯­ï¼Œé€šè¿‡é¢„è®­ç»ƒå°†å…¶ä¸å›¾åƒåµŒå…¥å¯¹é½ã€‚è¯¥æ–¹æ³•å°† time-to-first-token é™ä½å¤šè¾¾ 86%ï¼Œtoken ä½¿ç”¨é‡å‡å°‘å¤šè¾¾ 93%ï¼ŒåŒæ—¶åœ¨æ¶µç›–é—®ç­”ã€æ—¶åºæ¨ç†å’Œç©ºé—´ç†è§£çš„ 14 ä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šä¿æŒæˆ–è¶…è¶Šæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13191",
      "arxiv_url": "https://arxiv.org/abs/2602.13191",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13191",
      "github_url": "",
      "upvotes": 25,
      "fetched_at": "2026-02-17T09:52:30.980694+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12617",
      "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
      "authors": [
        "Modi Jin",
        "Yiming Zhang",
        "Boyuan Sun",
        "Dingwen Zhang",
        "MingMing Cheng",
        "Qibin Hou"
      ],
      "abstract": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency. This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics . To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process . Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
      "summary_en": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency.",
      "summary_zh": "GeoAgent é€šè¿‡ä¸“é—¨çš„æ•°æ®é›†å’Œå¥–åŠ±æœºåˆ¶å®ç°å“è¶Šçš„åœ°ç†å®šä½æ¨ç†æ€§èƒ½ï¼Œç¡®ä¿åœ°ç†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12617",
      "arxiv_url": "https://arxiv.org/abs/2602.12617",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12617",
      "github_url": "https://github.com/HVision-NKU/GeoAgent",
      "upvotes": 19,
      "fetched_at": "2026-02-17T09:52:19.372799+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.09146",
      "title": "SemanticMoments: Training-Free Motion Similarity via Third Moment Features",
      "authors": [
        "Saar Huberman",
        "Kfir Goldberg",
        "Or Patashnik",
        "Sagie Benaim",
        "Ron Mokady"
      ],
      "abstract": "Temporal statistics in semantic feature space provide a scalable approach for motion-centric video understanding, outperforming existing RGB, flow, and text-supervised methods.",
      "summary_en": "Temporal statistics in semantic feature space provide a scalable approach for motion-centric video understanding, outperforming existing RGB, flow, and text-supervised methods.",
      "summary_zh": "è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­çš„æ—¶åºç»Ÿè®¡ä¸ºä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„RGBã€flowå’Œæ–‡æœ¬ç›‘ç£æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09146",
      "arxiv_url": "https://arxiv.org/abs/2602.09146",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09146",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-17T09:52:01.205751+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12395",
      "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
      "authors": [
        "Xirui Li",
        "Ming Li",
        "Tianyi Zhou"
      ],
      "abstract": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
      "summary_en": "While reinforcement learning (RL) with verifiable rewards is widely used to enhance visual reasoning in vision-language models, end-to-end benchmarks obscure what specific capabilities improve beyond supervised fine-tuning initialization. To disentangle these effects, the authors propose a Frankenstein-style framework combining causal probing, parameter comparison, and model merging to analyze functional localization, update characterization, and transferability. Their findings demonstrate that RL induces consistent inference-time shifts primarily in mid-to-late transformer layers, with these refinements proving both transferable via merging and necessary via freezing for RL gains. These results suggest RL systematically refines mid-to-late computation to improve vision-to-reasoning alignment rather than uniformly enhancing visual perception, highlighting limitations of benchmark-only evaluation for assessing multimodal reasoning.",
      "summary_zh": "è™½ç„¶åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«å¹¿æ³›ç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•æ©ç›–äº†ç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆå§‹åŒ–å…·ä½“å“ªäº›èƒ½åŠ›å¾—åˆ°äº†æå‡ã€‚ä¸ºäº†è§£è€¦è¿™äº›æ•ˆåº”ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ Frankenstein-style æ¡†æ¶ï¼Œç»“åˆå› æœæ¢æµ‹ï¼ˆcausal probingï¼‰ã€å‚æ•°æ¯”è¾ƒå’Œæ¨¡å‹åˆå¹¶ï¼ˆmodel mergingï¼‰ï¼Œä»¥åˆ†æåŠŸèƒ½å®šä½ï¼ˆfunctional localizationï¼‰ã€æ›´æ–°ç‰¹å¾åˆ»ç”»ï¼ˆupdate characterizationï¼‰å’Œå¯è¿ç§»æ€§ï¼ˆtransferabilityï¼‰ã€‚ä»–ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒRL ä¸»è¦åœ¨ä¸­åå±‚ Transformer å±‚å¼•å‘ä¸€è‡´çš„æ¨ç†æ—¶åç§»ï¼ˆinference-time shiftsï¼‰ï¼Œè¿™äº›ä¼˜åŒ–æ—¢å¯é€šè¿‡ merging å®ç°è¿ç§»ï¼Œä¹Ÿå¯é€šè¿‡ freezing éªŒè¯å…¶å¯¹ RL æ”¶ç›Šçš„å¿…è¦æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRL ç³»ç»Ÿåœ°ä¼˜åŒ–ä¸­åå±‚è®¡ç®—ä»¥æ”¹è¿› vision-to-reasoning alignmentï¼Œè€Œéå‡åŒ€åœ°å¢å¼ºè§†è§‰æ„ŸçŸ¥ï¼Œå‡¸æ˜¾äº†ä»…ä¾èµ–åŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€æ¨ç†çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12395",
      "arxiv_url": "https://arxiv.org/abs/2602.12395",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12395",
      "github_url": "https://github.com/tianyi-lab/Frankenstein",
      "upvotes": 13,
      "fetched_at": "2026-02-17T09:52:15.581155+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11865",
      "title": "Intelligent AI Delegation",
      "authors": [
        "Nenad TomaÅ¡ev",
        "Matija Franklin",
        "Simon Osindero"
      ],
      "abstract": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms. AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web .",
      "summary_en": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms.",
      "summary_zh": "AIæ™ºèƒ½ä½“éœ€è¦è‡ªé€‚åº”æ¡†æ¶æ¥å®ç°ä»»åŠ¡åˆ†è§£ä¸å§”æ‰˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€å“åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–æƒé™è½¬ç§»ä¸ä¿¡ä»»æœºåˆ¶å¤„ç†æ„å¤–æ•…éšœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11865",
      "arxiv_url": "https://arxiv.org/abs/2602.11865",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11865",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-17T09:52:11.573500+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11236",
      "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
      "authors": [
        "Yandan Yang",
        "Shuang Zeng",
        "Tong Lin",
        "Xinyuan Chang",
        "Dekang Qi",
        "Junjin Xiao",
        "Haoyun Liu",
        "Ronghan Chen",
        "Yuzhi Chen",
        "Dongjie Huo",
        "Feng Xiong",
        "Xing Wei",
        "Zhiheng Ma",
        "Mu Xu"
      ],
      "abstract": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability. Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies , enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis : effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit , enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
      "summary_en": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability.",
      "summary_zh": "ABot-M0 æå‡ºäº†ä¸€ä¸ªç”¨äºå…·èº«æ™ºèƒ½ä½“å¼€å‘çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ‡å‡†åŒ–äº†å¤šæ ·åŒ–çš„æœºå™¨äººæ•°æ®ï¼Œå¹¶é‡‡ç”¨åŠ¨ä½œæµå½¢å­¦ä¹ ï¼ˆaction manifold learningï¼‰æ¥æå‡é¢„æµ‹æ•ˆç‡å’Œç¨³å®šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11236",
      "arxiv_url": "https://arxiv.org/abs/2602.11236",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11236",
      "github_url": "https://github.com/amap-cvlab/ABot-Manipulation",
      "upvotes": 10,
      "fetched_at": "2026-02-17T09:52:05.064119+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12628",
      "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
      "authors": [
        "Liangzhi Shi",
        "Shuaihang Chen",
        "Feng Gao",
        "Yinuo Chen",
        "Kang Chen",
        "Tonghe Zhang",
        "Hongzhi Zhang",
        "Weinan Zhang",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring. Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\textit{RL}-based sim-real \\textit{Co}-training (RL-Co) framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting . We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and Ï€_{0.5}, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on Ï€_{0.5}. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency , providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
      "summary_en": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring.",
      "summary_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„sim-realååŒè®­ç»ƒæ¡†æ¶é€šè¿‡äº¤äº’å¼ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œæ•°æ®é”šå®šæå‡vision-language-actionç­–ç•¥æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12628",
      "arxiv_url": "https://arxiv.org/abs/2602.12628",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12628",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-17T09:52:20.604775+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13013",
      "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "authors": [
        "Yunheng Li",
        "Hengrui Zhang",
        "Meng-Hao Guo",
        "Wenzhao Gao",
        "Shaoyong Jia",
        "Shaohui Jiao",
        "Qibin Hou",
        "Ming-Ming Cheng"
      ],
      "abstract": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning. Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning , attribute-wise captioning , caption-based QA , and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.",
      "summary_en": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé¢å‘ç»†ç²’åº¦è§†å¬ç†è§£çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸æ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–æ ‡æ³¨å’Œç›‘ç£å¾®è°ƒï¼Œåœ¨æå‡æè¿°è´¨é‡çš„åŒæ—¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13013",
      "arxiv_url": "https://arxiv.org/abs/2602.13013",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13013",
      "github_url": "https://github.com/ASID-Caption/ASID-Caption",
      "upvotes": 7,
      "fetched_at": "2026-02-17T09:52:27.207390+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.04163",
      "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
      "authors": [
        "Junyu Chen",
        "Jungang Li",
        "Jing Xiong",
        "Wenjie Wang",
        "Qingyao Yang",
        "He Xiao",
        "Zhen Li",
        "Taiqiang Wu",
        "Mengzhao Chen",
        "Zhen Peng",
        "Chaofan Tao",
        "Long Shi",
        "Hongxia Yang",
        "Ngai Wong"
      ],
      "abstract": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference. Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients , and iteratively refines them using approximate second-order information while progressively compensating quantization error s to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry . Code: github.com/KingdalfGoodman/BPDQ.",
      "summary_en": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference.",
      "summary_zh": "Bit-Plane Decomposition Quantization (BPDQ) åˆ©ç”¨æºè‡ªä½å¹³é¢å’Œæ ‡é‡ç³»æ•°çš„å¯å˜é‡åŒ–ç½‘æ ¼æ”¹è¿›ä½æ¯”ç‰¹é‡åŒ–ï¼Œåœ¨èµ„æºå—é™çš„ LLM æ¨ç†ä¸­å–å¾—äº†ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç²¾åº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04163",
      "arxiv_url": "https://arxiv.org/abs/2602.04163",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04163",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-17T09:51:56.455376+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11715",
      "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
      "authors": [
        "Haolei Bai",
        "Lingcheng Kong",
        "Xueyi Chen",
        "Jianmian Wang",
        "Zhiqiang Tao",
        "Huan Wang"
      ],
      "abstract": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework. Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation . This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation , spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation .",
      "summary_en": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework.",
      "summary_zh": "ç”¨äº CUDA å†…æ ¸ç”Ÿæˆçš„æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ (dLLMs) é€šè¿‡ä¸“é—¨çš„æ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶å®ç°äº†æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11715",
      "arxiv_url": "https://arxiv.org/abs/2602.11715",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11715",
      "github_url": "https://github.com/deadlykitten4/DICE",
      "upvotes": 5,
      "fetched_at": "2026-02-17T09:52:07.720244+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12984",
      "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
      "authors": [
        "Yujiong Shen",
        "Yajie Yang",
        "Zhiheng Xi",
        "Binze Hu",
        "Huayu Sha",
        "Jiazheng Zhang",
        "Qiyuan Peng",
        "Junlin Shang",
        "Jixuan Huang",
        "Yutao Fan",
        "Jingqi Tong",
        "Shihan Dou",
        "Ming Zhang",
        "Lei Bai",
        "Zhenfei Yin",
        "Tao Gui",
        "Xingjun Ma",
        "Qi Zhang",
        "Xuanjing Huang",
        "Yu-Gang Jiang"
      ],
      "abstract": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "summary_en": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "summary_zh": "SciAgentGym å’Œ SciAgentBench æ”¯æŒç§‘å­¦å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œ SciForge åˆ™é€šè¿‡ä¾èµ–å›¾å»ºæ¨¡å·¥å…·äº¤äº’æ¥æå‡æ™ºèƒ½ä½“æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12984",
      "arxiv_url": "https://arxiv.org/abs/2602.12984",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12984",
      "github_url": "https://github.com/CMarsRover/SciAgentGYM",
      "upvotes": 4,
      "fetched_at": "2026-02-17T09:52:25.767440+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12829",
      "title": "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching",
      "authors": [
        "Lei Lv",
        "Yunfei Li",
        "Yu Luo",
        "Fuchun Sun",
        "Xiao Ma"
      ],
      "abstract": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized SchrÃ¶dinger bridge formulation. Iterative generative policies, such as diffusion models and flow matching , offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field . Our key insight is to formulate policy optimization as a Generalized SchrÃ¶dinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism . Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.",
      "summary_en": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized SchrÃ¶dinger bridge formulation.",
      "summary_zh": "Field Least-Energy Actor-Critic (FLAC) é€šè¿‡å¹¿ä¹‰è–›å®šè°”æ¡¥å½¢å¼åŒ–ï¼Œå°†åŠ¨èƒ½ä½œä¸ºç­–ç•¥éšæœºæ€§è°ƒèŠ‚çš„ä»£ç†ï¼Œè§£å†³äº†æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ä¸­è¿­ä»£ç”Ÿæˆç­–ç•¥æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12829",
      "arxiv_url": "https://arxiv.org/abs/2602.12829",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12829",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:24.838753+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12684",
      "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
      "authors": [
        "Rui Cai",
        "Jun Guo",
        "Xinze He",
        "Piaopiao Jin",
        "Jie Li",
        "Bingxuan Lin",
        "Futeng Liu",
        "Wei Liu",
        "Fei Ma",
        "Kun Ma",
        "Feng Qiu",
        "Heng Qu",
        "Yifei Su",
        "Qiao Sun",
        "Dong Wang",
        "Donghao Wang",
        "Yunhong Wang",
        "Rujie Wu",
        "Diyun Xiang",
        "Yu Yang",
        "Hangjun Ye",
        "Yuan Zhang"
      ],
      "abstract": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "summary_en": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "summary_zh": "é¢å‘æœºå™¨äººå­¦çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒä¸ä¸“é—¨è®­ç»ƒæŠ€æœ¯ï¼Œå®ç°å®æ—¶æ‰§è¡Œå’Œé«˜æ€§èƒ½æ“ä½œä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12684",
      "arxiv_url": "https://arxiv.org/abs/2602.12684",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12684",
      "github_url": "https://github.com/XiaomiRobotics/Xiaomi-Robotics-0",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:21.667396+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12506",
      "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
      "authors": [
        "Rosie Zhao",
        "Anshul Shah",
        "Xiaoyu Zhu",
        "Xinke Deng",
        "Zhongyu Jiang",
        "Yang Yang",
        "Joerg Liebelt",
        "Arnab Mondal"
      ],
      "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
      "summary_en": "RL fine-tuning improves VLMs on visual reasoning benchmarks but leaves them vulnerable to weak visual grounding and hallucinations, where simple textual perturbations cause robustness drops that entropy-based metrics trace to reshaped uncertainty and probability mass, particularly when CoT consistency is evaluated. Analysis of RL dynamics reveals an accuracy-faithfulness trade-off: fine-tuning boosts accuracy while eroding CoT reliability and robustness to contextual shifts, and although adversarial augmentation improves robustness, it does not prevent faithfulness drift. Faithfulness-aware rewards restore alignment between answers and reasoning but risk training collapse when paired with augmentation, leaving robustness elusive. These findings highlight the limitations of accuracy-only evaluations and motivate protocols that jointly emphasize correctness, robustness, and faithfulness in visually grounded reasoning.",
      "summary_zh": "RLå¾®è°ƒæå‡äº†VLMåœ¨è§†è§‰æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œä½†ä½¿å…¶æ˜“å—è§†è§‰æ¥åœ°è–„å¼±å’Œå¹»è§‰çš„å½±å“ï¼Œå…¶ä¸­ç®€å•çš„æ–‡æœ¬æ‰°åŠ¨ä¼šå¯¼è‡´é²æ£’æ€§ä¸‹é™ï¼Œè€ŒåŸºäºç†µçš„æŒ‡æ ‡å°†è¿™ç§ä¸‹é™è¿½æº¯è‡³ä¸ç¡®å®šæ€§å’Œæ¦‚ç‡è´¨é‡çš„é‡å¡‘ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°CoTä¸€è‡´æ€§æ—¶ã€‚å¯¹RLåŠ¨æ€çš„åˆ†ææ­ç¤ºäº†å‡†ç¡®æ€§ä¸å¿ å®åº¦ä¹‹é—´çš„æƒè¡¡ï¼šå¾®è°ƒæå‡äº†å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¾µèš€äº†CoTçš„å¯é æ€§ä»¥åŠå¯¹ä¸Šä¸‹æ–‡åç§»çš„é²æ£’æ€§ï¼›å°½ç®¡å¯¹æŠ—å¢å¼ºæå‡äº†é²æ£’æ€§ï¼Œä½†æ— æ³•é˜»æ­¢å¿ å®åº¦æ¼‚ç§»ã€‚å¿ å®åº¦æ„ŸçŸ¥å¥–åŠ±æ¢å¤äº†ç­”æ¡ˆä¸æ¨ç†ä¹‹é—´çš„å¯¹é½ï¼Œä½†åœ¨ä¸å¢å¼ºæ–¹æ³•ç»“åˆæ—¶å­˜åœ¨è®­ç»ƒå´©æºƒçš„é£é™©ï¼Œä½¿å¾—é²æ£’æ€§éš¾ä»¥å®ç°ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä»…å…³æ³¨å‡†ç¡®æ€§è¯„ä¼°çš„å±€é™æ€§ï¼Œå¹¶æ¨åŠ¨å»ºç«‹èƒ½å¤Ÿåœ¨è§†è§‰æ¥åœ°æ¨ç†ä¸­è”åˆå¼ºè°ƒæ­£ç¡®æ€§ã€é²æ£’æ€§å’Œå¿ å®åº¦çš„è¯„ä¼°åè®®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12506",
      "arxiv_url": "https://arxiv.org/abs/2602.12506",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12506",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:17.485935+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11757",
      "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
      "authors": [
        "Yi Zhang",
        "Yunshuang Wang",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement. Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation . First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration . Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code . Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.",
      "summary_en": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement.",
      "summary_zh": "Code2Worldsé€šè¿‡å°†4DåŠ¨æ€åœºæ™¯ç”Ÿæˆå½¢å¼åŒ–ä¸ºè¯­è¨€åˆ°æ¨¡æ‹Ÿä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨åŒæµæ¶æ„ä¸ç‰©ç†æ„ŸçŸ¥é—­ç¯ä¼˜åŒ–æ¥å®ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11757",
      "arxiv_url": "https://arxiv.org/abs/2602.11757",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11757",
      "github_url": "https://github.com/AIGeeksGroup/Code2Worlds",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:08.380082+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12612",
      "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
      "authors": [
        "Sein Kim",
        "Sangwu Park",
        "Hongseok Kang",
        "Wonjoong Kim",
        "Jimin Seo",
        "Yeonjun In",
        "Kanghoon Yoon",
        "Chanyoung Park"
      ],
      "abstract": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
      "summary_en": "Traditional Neural Architecture Search (NAS) methods are constrained by fixed search spaces defined by human priors, while recent LLM-driven code evolution frameworks rely on scalar metrics such as NDCG and Hit Ratio that lack qualitative insights for directional improvement. We propose Self-EvolveRec, which integrates a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative verification to establish directional feedback, and introduces a Diagnosis Tool - Model Co-Evolution strategy to dynamically adapt evaluation criteria as recommendation architectures evolve. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in recommendation performance and user satisfaction.",
      "summary_zh": "ä¼ ç»Ÿçš„ Neural Architecture Search (NAS) æ–¹æ³•å—é™äºç”±äººå·¥å…ˆéªŒå®šä¹‰çš„å›ºå®šæœç´¢ç©ºé—´ï¼Œè€Œè¿‘æœŸçš„ LLM-driven code evolution æ¡†æ¶ä¾èµ–äº NDCG å’Œ Hit Ratio ç­‰æ ‡é‡æŒ‡æ ‡ï¼Œç¼ºä¹ç”¨äºå®šå‘æ”¹è¿›çš„å®šæ€§æ´å¯Ÿã€‚æˆ‘ä»¬æå‡ºäº† Self-EvolveRecï¼Œå…¶æ•´åˆ User Simulator ä»¥æä¾›å®šæ€§æ‰¹è¯„ï¼Œå¹¶é›†æˆ Model Diagnosis Tool è¿›è¡Œå®šé‡éªŒè¯ï¼Œä»è€Œå»ºç«‹å®šå‘åé¦ˆï¼ŒåŒæ—¶å¼•å…¥ Diagnosis Tool - Model Co-Evolution ç­–ç•¥ï¼Œä»¥åœ¨æ¨èæ¶æ„æ¼”è¿›è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è¯„ä¼°æ ‡å‡†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSelf-EvolveRec åœ¨æ¨èæ€§èƒ½å’Œç”¨æˆ·æ»¡æ„åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ NAS å’Œ LLM-driven code evolution åŸºçº¿ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12612",
      "arxiv_url": "https://arxiv.org/abs/2602.12612",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12612",
      "github_url": "https://github.com/Sein-Kim/self_evolverec",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:18.321404+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12221",
      "title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
      "authors": [
        "Onkar Susladkar",
        "Tushar Prakash",
        "Gayatri Deshmukh",
        "Kiet A. Nguyen",
        "Jiaxun Zhang",
        "Adheesh Juvekar",
        "Tianshu Bao",
        "Lin Chai",
        "Sparsh Mittal",
        "Inderjit S Dhillon",
        "Ismini Lourentzou"
      ],
      "abstract": "UniDFlow is a unified discrete flow-matching framework that decouples understanding and generation through low-rank adapters and uses reference-based alignment to improve multimodal tasks without retraining. We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding , generation , and editing . It decouples understanding and generation via task-specific low-rank adapters , avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting , in-context image generation , reference-based editing , and compositional generation , despite no explicit task-specific training.",
      "summary_en": "UniDFlow is a unified discrete flow-matching framework that decouples understanding and generation through low-rank adapters and uses reference-based alignment to improve multimodal tasks without retraining.",
      "summary_zh": "UniDFlow æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡ low-rank adapters è§£è€¦ç†è§£ä¸ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨ reference-based alignment æ”¹è¿›å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12221",
      "arxiv_url": "https://arxiv.org/abs/2602.12221",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12221",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:14.257513+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Åukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "abstract": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering. Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts , such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "summary_en": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering.",
      "summary_zh": "ç ”ç©¶è¡¨æ˜ï¼ŒéŸ³é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å±‚æ§åˆ¶ç€ä¸åŒçš„éŸ³ä¹æ¦‚å¿µï¼Œä»è€Œèƒ½å¤Ÿé€šè¿‡æ¿€æ´»å¼•å¯¼ç²¾ç¡®æ“æ§éŸ³é¢‘ç‰¹å¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11910",
      "arxiv_url": "https://arxiv.org/abs/2602.11910",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11910",
      "github_url": "https://github.com/luk-st/steer-audio",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:13.145126+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11769",
      "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
      "authors": [
        "Zhenghuang Wu",
        "Kang Chen",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms. Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance , a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity . Second, to reinforce temporal consistency , we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.",
      "summary_en": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms.",
      "summary_zh": "Light4Dé€šè¿‡è§£è€¦å…‰æµå¼•å¯¼å’Œæ—¶é—´ä¸€è‡´æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†åœ¨ç›®æ ‡å…‰ç…§ä¸‹çš„ä¸€è‡´æ€§4Dè§†é¢‘åˆæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11769",
      "arxiv_url": "https://arxiv.org/abs/2602.11769",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11769",
      "github_url": "https://github.com/AIGeeksGroup/Light4D",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:09.718197+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13022",
      "title": "Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels",
      "authors": [
        "Julius Pesonen",
        "Stefan Rua",
        "Josef Taher",
        "Niko KoivumÃ¤ki",
        "Xiaowei Yu",
        "Eija Honkavaara"
      ],
      "abstract": "Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.",
      "summary_en": "Individual tree crown mapping supports urban inventories and forest health monitoring but is challenging to automate from aerial imagery due to texture variations and crown overlaps. This study presents a method to train deep learning segmentation models on RGB and multispectral images using pseudo-labels derived from aerial laser scanning (ALS) data, which are enhanced by the zero-shot Segment Anything Model 2 (SAM 2). The approach eliminates manual annotation costs while producing domain-specific models that outperform general-domain alternatives on this task.",
      "summary_zh": "å•æœ¨æ ‘å† åˆ¶å›¾æ”¯æŒåŸå¸‚æ¸…æŸ¥å’Œæ£®æ—å¥åº·ç›‘æµ‹ï¼Œä½†ç”±äºçº¹ç†å˜åŒ–å’Œæ ‘å† é‡å ï¼Œéš¾ä»¥ä»èˆªç©ºå½±åƒä¸­å®ç°è‡ªåŠ¨åŒ–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æºè‡ªæœºè½½æ¿€å…‰é›·è¾¾ï¼ˆALSï¼‰æ•°æ®çš„ä¼ªæ ‡ç­¾åœ¨RGBå’Œå¤šå…‰è°±å½±åƒä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™äº›ä¼ªæ ‡ç­¾ç”±é›¶æ ·æœ¬Segment Anything Model 2ï¼ˆSAM 2ï¼‰å¢å¼ºã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†äººå·¥æ ‡æ³¨æˆæœ¬ï¼ŒåŒæ—¶ç”Ÿæˆäº†åœ¨è¯¥ä»»åŠ¡ä¸Šä¼˜äºé€šç”¨é¢†åŸŸæ›¿ä»£æ–¹æ¡ˆçš„é¢†åŸŸç‰¹å®šæ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13022",
      "arxiv_url": "https://arxiv.org/abs/2602.13022",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13022",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:28.645454+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12500",
      "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
      "authors": [
        "AndrÃ© Storhaug",
        "Jiamou Sun",
        "Jingyue Li"
      ],
      "abstract": "Favia is a forensic, agent-based framework that combines scalable candidate ranking with deep semantic reasoning to accurately identify vulnerability-fixing commits by leveraging LLM agents with specialized tools and environmental context. Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs . Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning . Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent . By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.",
      "summary_en": "Favia is a forensic, agent-based framework that combines scalable candidate ranking with deep semantic reasoning to accurately identify vulnerability-fixing commits by leveraging LLM agents with specialized tools and environmental context.",
      "summary_zh": "Favia æ˜¯ä¸€ä¸ªå–è¯å‹ã€åŸºäº agent çš„æ¡†æ¶ï¼Œç»“åˆå¯æ‰©å±•çš„å€™é€‰æ’åºä¸æ·±åº¦è¯­ä¹‰æ¨ç†ï¼Œåˆ©ç”¨é…å¤‡ä¸“ç”¨å·¥å…·å’Œç¯å¢ƒä¸Šä¸‹æ–‡çš„ LLM agent ä»¥å‡†ç¡®è¯†åˆ«æ¼æ´ä¿®å¤æäº¤ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12500",
      "arxiv_url": "https://arxiv.org/abs/2602.12500",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12500",
      "github_url": "https://github.com/andstor/agentic-security-patch-classification-replication-package",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:16.385253+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11609",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "abstract": "scPilot presents a framework for omics-native reasoning where large language models directly analyze single-cell RNA-seq data through step-by-step reasoning processes, improving accuracy and interpretability in cell-type annotation and developmental trajectory reconstruction. We present scPilot, the first systematic framework to practice omics-native reasoning : a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation , developmental-trajectory reconstruction , and transcription-factor targeting , into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence. To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic . By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses. Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "summary_en": "scPilot presents a framework for omics-native reasoning where large language models directly analyze single-cell RNA-seq data through step-by-step reasoning processes, improving accuracy and interpretability in cell-type annotation and developmental trajectory reconstruction.",
      "summary_zh": "scPilot æ„å»ºäº†ä¸€ä¸ªç”¨äº omics-native reasoning çš„æ¡†æ¶ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é€æ­¥æ¨ç†è¿‡ç¨‹ç›´æ¥åˆ†æ single-cell RNA-seq æ•°æ®ï¼Œä»è€Œåœ¨ç»†èƒç±»å‹æ³¨é‡Šå’Œå‘è‚²è½¨è¿¹é‡å»ºæ–¹é¢æå‡å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11609",
      "arxiv_url": "https://arxiv.org/abs/2602.11609",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11609",
      "github_url": "https://github.com/maitrix-org/scPilot",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:06.359879+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.09870",
      "title": "Steer2Edit: From Activation Steering to Component-Level Editing",
      "authors": [
        "Chung-En Sun",
        "Ge Yan",
        "Zimo Wang",
        "Tsui-Wei Weng"
      ],
      "abstract": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.",
      "summary_en": "Current steering methods apply fixed global modifications during inference, which induces unfavorable attribute-utility trade-offs by ignoring that behaviors are governed by small, heterogeneous subsets of components. Steer2Edit transforms steering vectors into diagnostic signals for component-level rank-1 weight editing, selectively redistributing behavioral influence across individual attention heads and MLP neurons while preserving the standard forward pass. This training-free framework achieves more favorable trade-offs than standard steering, improving safety by up to 17.2%, truthfulness by 9.8%, and reducing reasoning length by 12.2% on average across safety alignment, hallucination mitigation, and reasoning efficiency tasks.",
      "summary_zh": "ç°æœ‰çš„ steering æ–¹æ³•åœ¨ inference æœŸé—´æ–½åŠ å›ºå®šçš„å…¨å±€ä¿®æ”¹ï¼Œå› å…¶å¿½ç•¥äº†è¡Œä¸ºå—å°å‹ã€å¼‚è´¨çš„ç»„ä»¶å­é›†è°ƒæ§çš„äº‹å®ï¼Œä»è€Œå¯¼è‡´ä¸åˆ©çš„ attribute-utility trade-offsã€‚Steer2Edit å°† steering vectors è½¬åŒ–ä¸ºç”¨äºç»„ä»¶çº§ rank-1 weight editing çš„è¯Šæ–­ä¿¡å·ï¼Œåœ¨ä¿æŒæ ‡å‡† forward pass çš„åŒæ—¶ï¼Œé€‰æ‹©æ€§åœ°å°†è¡Œä¸ºå½±å“é‡æ–°åˆ†é…è‡³å„ä¸ª attention heads å’Œ MLP neuronsã€‚è¯¥ training-free æ¡†æ¶ç›¸æ¯”æ ‡å‡† steering å®ç°äº†æ›´ä¼˜çš„ trade-offsï¼Œåœ¨ safety alignmentã€hallucination mitigation å’Œ reasoning efficiency ä»»åŠ¡ä¸­ï¼Œå¹³å‡å°† safety æå‡æœ€å¤šè¾¾ 17.2%ï¼Œtruthfulness æå‡ 9.8%ï¼Œå¹¶å°† reasoning length å‡å°‘ 12.2%ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09870",
      "arxiv_url": "https://arxiv.org/abs/2602.09870",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09870",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:02.393593+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.07298",
      "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
      "authors": [
        "Benyu Zhang",
        "Qiang Zhang",
        "Jianpeng Cheng",
        "Hong-You Chen",
        "Qifei Wang",
        "Wei Sun",
        "Shen Li",
        "Jia Li",
        "Jiahao Wu",
        "Xiangjun Fan",
        "Hong Yan"
      ],
      "abstract": "A novel layered framework generates high-quality synthetic data for large language models in recommender systems, demonstrating superior performance and predictable scaling laws compared to traditional methods. Large Language Models (LLMs) represent a promising frontier for recommender systems , yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform (+130% on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.",
      "summary_en": "A novel layered framework generates high-quality synthetic data for large language models in recommender systems, demonstrating superior performance and predictable scaling laws compared to traditional methods.",
      "summary_zh": "ä¸€ç§æ–°é¢–çš„åˆ†å±‚æ¡†æ¶å¯ä¸ºæ¨èç³»ç»Ÿä¸­çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å±•ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½å’Œå¯é¢„æµ‹çš„ scaling lawsã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07298",
      "arxiv_url": "https://arxiv.org/abs/2602.07298",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07298",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:51:59.101035+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.04315",
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "authors": [
        "Guoqing Ma",
        "Siheng Wang",
        "Zeyu Zhang",
        "Shan Yu",
        "Hao Tang"
      ],
      "abstract": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection. Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability , which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning ), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM ( Affordance Segmentation Module ) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
      "summary_en": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection.",
      "summary_zh": "GeneralVLAæ˜¯ä¸€ç§åˆ†å±‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†å¼•å¯¼çš„è½¨è¿¹è§„åˆ’å®ç°é›¶æ ·æœ¬æœºå™¨äººæ“ä½œï¼Œæ— éœ€çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04315",
      "arxiv_url": "https://arxiv.org/abs/2602.04315",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04315",
      "github_url": "https://github.com/AIGeeksGroup/GeneralVLA",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:51:57.328841+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.03120",
      "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
      "authors": [
        "Yinggan Xu",
        "Risto Miikkulainen",
        "Xin Qiu"
      ],
      "abstract": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .",
      "summary_en": "Post-Training Quantization (PTQ) enables deploying Large Language Models (LLMs) on memory-constrained devices but renders models static and incompatible with standard fine-tuning methods that require backpropagation through discrete, non-differentiable parameter spaces. This paper introduces Quantized Evolution Strategies (QES), which performs full-parameter fine-tuning directly in quantized space using accumulated error feedback to preserve high-precision gradient signals and stateless seed replay to reduce memory usage. QES significantly outperforms state-of-the-art zeroth-order methods on arithmetic reasoning tasks, enabling direct fine-tuning of quantized models and opening possibilities for scaling LLMs entirely in quantized space.",
      "summary_zh": "åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿéƒ¨ç½²äºå†…å­˜å—é™è®¾å¤‡ï¼Œä½†ä¼šä½¿æ¨¡å‹é™æ€åŒ–ï¼Œå¹¶ä¸éœ€è¦é€šè¿‡ç¦»æ•£ã€ä¸å¯å¾®å‚æ•°ç©ºé—´è¿›è¡Œåå‘ä¼ æ’­çš„æ ‡å‡†å¾®è°ƒæ–¹æ³•ä¸å…¼å®¹ã€‚æœ¬æ–‡æå‡ºé‡åŒ–è¿›åŒ–ç­–ç•¥ï¼ˆQESï¼‰ï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨é‡åŒ–ç©ºé—´ä¸­è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œåˆ©ç”¨ç´¯ç§¯è¯¯å·®åé¦ˆä¿ç•™é«˜ç²¾åº¦æ¢¯åº¦ä¿¡å·ï¼Œå¹¶ä½¿ç”¨æ— çŠ¶æ€ç§å­é‡æ”¾å‡å°‘å†…å­˜ä½¿ç”¨ã€‚QESåœ¨ç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„zeroth-orderæ–¹æ³•ï¼Œå®ç°äº†å¯¹é‡åŒ–æ¨¡å‹çš„ç›´æ¥å¾®è°ƒï¼Œå¹¶ä¸ºå®Œå…¨åœ¨é‡åŒ–ç©ºé—´ä¸­æ‰©å±•LLMså¼€è¾Ÿäº†å¯èƒ½æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03120",
      "arxiv_url": "https://arxiv.org/abs/2602.03120",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03120",
      "github_url": "https://github.com/dibbla/Quantized-Evolution-Strategies",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:51:55.095923+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13139",
      "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report",
      "authors": [
        "Mariia Fedorova",
        "Nikolay Arefyev",
        "Maja Buljan",
        "JindÅ™ich Helcl",
        "Stephan Oepen",
        "Egil RÃ¸nningstad",
        "Yves Scherrer"
      ],
      "abstract": "OpenLID-v3 improves language identification accuracy for closely related languages and low-resource variants through enhanced training data, cluster merging, and noise detection mechanisms. Language identification (LID) is an essential step in building high-quality multilingual datasets from web data . Existing LID tools (such as OpenLID or GlotLID ) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages . In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters , and introducing a special label for marking noise. We call this extended system OpenLID -v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages . OpenLID -v3 is available on https://huggingface.co/HPLT/ OpenLID -v3.",
      "summary_en": "OpenLID-v3 improves language identification accuracy for closely related languages and low-resource variants through enhanced training data, cluster merging, and noise detection mechanisms.",
      "summary_zh": "OpenLID-v3 é€šè¿‡å¢å¼ºçš„è®­ç»ƒæ•°æ®ã€èšç±»åˆå¹¶å’Œå™ªå£°æ£€æµ‹æœºåˆ¶ï¼Œæå‡äº†å¯¹è¿‘ç¼˜è¯­è¨€åŠä½èµ„æºå˜ä½“çš„è¯­è¨€è¯†åˆ«å‡†ç¡®ç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13139",
      "arxiv_url": "https://arxiv.org/abs/2602.13139",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13139",
      "github_url": "https://github.com/hplt-project/openlid",
      "upvotes": 0,
      "fetched_at": "2026-02-17T09:52:30.260440+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.09877",
      "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
      "authors": [
        "Chenxu Wang",
        "Chaozhuo Li",
        "Songyang Liu",
        "Zejian Chen",
        "Jinyu Hou",
        "Ji Qi",
        "Rui Li",
        "Litian Zhang",
        "Qiwei Ye",
        "Zheng Liu",
        "Xu Chen",
        "Xi Zhang",
        "Philip S. Yu"
      ],
      "abstract": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution. The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution . Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment --a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution , complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework , we formalize safety as the divergence degree from anthropic value distributions . We theoretically demonstrate that isolated self-evolution induces statistical blind spots , leading to the irreversible degradation of the system's safety alignment . Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms .",
      "summary_en": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.",
      "summary_zh": "å¤šæ™ºèƒ½ä½“LLMç³»ç»Ÿå› å­¤ç«‹è¿›åŒ–ä¸­çš„å›ºæœ‰ç»Ÿè®¡ç›²åŒºï¼Œåœ¨å®ç°æŒç»­è‡ªæˆ‘æ”¹è¿›çš„åŒæ—¶ä¿æŒå®‰å…¨å¯¹é½æ–¹é¢é¢ä¸´æ ¹æœ¬æ€§å±€é™ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09877",
      "arxiv_url": "https://arxiv.org/abs/2602.09877",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09877",
      "github_url": "",
      "upvotes": 187,
      "fetched_at": "2026-02-19T06:37:55.568690+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12036",
      "title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
      "authors": [
        "Xin Xu",
        "Clive Bai",
        "Kai Yang",
        "Tianhao Chen",
        "Yangkun Chen",
        "Weijie Liu",
        "Hao Chen",
        "Yang Wang",
        "Saiyong Yang",
        "Can Yang"
      ],
      "abstract": "Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training. Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.",
      "summary_en": "Composition-RL improves reasoning capabilities by automatically composing multiple problems into new verifiable questions for reinforcement learning training.",
      "summary_zh": "Composition-RL é€šè¿‡è‡ªåŠ¨å°†å¤šä¸ªé—®é¢˜ç»„åˆä¸ºæ–°çš„å¯éªŒè¯é—®é¢˜ä»¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä»è€Œæå‡æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12036",
      "arxiv_url": "https://arxiv.org/abs/2602.12036",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12036",
      "github_url": "https://github.com/XinXU-USTC/Composition-RL",
      "upvotes": 91,
      "fetched_at": "2026-02-19T06:38:44.205524+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12205",
      "title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Ruihang Li",
        "Feng Han",
        "Chaofan Ma",
        "Wei Song",
        "Siyuan Wang",
        "Yibin Wang",
        "Yi Xin",
        "Hongjian Liu",
        "Zhixiong Zhang",
        "Shengyuan Ding",
        "Tianhang Wang",
        "Zhenglin Cheng",
        "Tao Lin",
        "Cheng Jin",
        "Kaicheng Yu",
        "Jingjing Chen",
        "Wenjie Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO. Current unified multimodal models for image generation and editing typically rely on massive parameter scale s (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable ' think tokens ' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations , (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO , which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences , while maintaining stable training progress and avoiding visual artifacts . Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.",
      "summary_en": "A lightweight 5B unified multimodal model achieves competitive performance through hierarchical feature extraction, learnable think tokens, and progressive training strategies including alignment pre-training, joint supervised fine-tuning, and reinforcement learning with MR-GRPO.",
      "summary_zh": "ä¸€æ¬¾è½»é‡çº§çš„5Bç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡åˆ†å±‚ç‰¹å¾æå–ã€å¯å­¦ä¹ æ€è€ƒtokenä»¥åŠåŒ…æ‹¬å¯¹é½é¢„è®­ç»ƒã€è”åˆç›‘ç£å¾®è°ƒå’ŒMR-GRPOå¼ºåŒ–å­¦ä¹ åœ¨å†…çš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12205",
      "arxiv_url": "https://arxiv.org/abs/2602.12205",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12205",
      "github_url": "https://github.com/DeepGenTeam/DeepGen",
      "upvotes": 77,
      "fetched_at": "2026-02-19T06:39:06.795530+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12125",
      "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
      "authors": [
        "Wenkai Yang",
        "Weijie Liu",
        "Ruobing Xie",
        "Kai Yang",
        "Saiyong Yang",
        "Yankai Lin"
      ],
      "abstract": "On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques. On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation ), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings . In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
      "summary_en": "On-policy distillation is extended through a generalized framework that introduces flexible reference models and reward scaling factors, demonstrating improved performance through reward extrapolation and reward correction techniques.",
      "summary_zh": "åœ¨çº¿ç­–ç•¥è’¸é¦é€šè¿‡å¼•å…¥çµæ´»å‚è€ƒæ¨¡å‹å’Œå¥–åŠ±ç¼©æ”¾å› å­çš„å¹¿ä¹‰æ¡†æ¶å®ç°æ‰©å±•ï¼Œå¹¶ç»“åˆå¥–åŠ±å¤–æ¨ä¸å¥–åŠ±ä¿®æ­£æŠ€æœ¯å±•ç°å‡ºæ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12125",
      "arxiv_url": "https://arxiv.org/abs/2602.12125",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12125",
      "github_url": "https://github.com/RUCBM/G-OPD",
      "upvotes": 57,
      "fetched_at": "2026-02-19T06:38:54.717461+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12099",
      "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
      "authors": [
        "GigaBrain Team",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao",
        "Hao Li",
        "Jie Li",
        "Jindi Lv",
        "Jingyu Liu",
        "Lv Feng",
        "Mingming Yu",
        "Peng Li",
        "Qiuping Deng",
        "Tianze Liu",
        "Xinyu Zhou",
        "Xinze Chen",
        "Xiaofeng Wang",
        "Yang Wang",
        "Yifan Li",
        "Yifei Nie",
        "Yilong Li",
        "Yukun Zhou"
      ],
      "abstract": "A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks. Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, a VLA model trained via world model-based reinforcement learning . Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark . GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP ( Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation . Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M^* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our https://gigabrain05m.github.io{project page}.",
      "summary_en": "A vision-language-action model enhanced with world model-based reinforcement learning demonstrates improved performance and long-horizon execution capabilities for robotic manipulation tasks.",
      "summary_zh": "é€šè¿‡åŸºäºä¸–ç•Œæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å±•ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½å’Œé•¿æ—¶ç¨‹æ‰§è¡Œèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12099",
      "arxiv_url": "https://arxiv.org/abs/2602.12099",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12099",
      "github_url": "https://github.com/open-gigaai/giga-brain-0",
      "upvotes": 55,
      "fetched_at": "2026-02-19T06:38:50.611331+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.10934",
      "title": "MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models",
      "authors": [
        "Yitian Gong",
        "Kuangwei Chen",
        "Zhaoye Fei",
        "Xiaogui Yang",
        "Ke Chen",
        "Yang Wang",
        "Kexin Huang",
        "Mingshu Chen",
        "Ruixiao Li",
        "Qingyuan Cheng",
        "Shimin Li",
        "Xipeng Qiu"
      ],
      "abstract": "A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance. Discrete audio tokenizers are fundamental to empowering large language models with native audio processing and generation capabilities. Despite recent progress, existing approaches often rely on pretrained encoder s, semantic distillation , or heterogeneous CNN-based architectures . These designs introduce fixed inductive biases that limit reconstruction fidelity and hinder effective scaling. In this paper, we argue that discrete audio tokenization should be learned fully end-to-end using a homogeneous and scalable architecture. To this end, we first propose CAT ( Causal Audio Tokenizer with Transformer), a purely Transformer-based architecture that jointly optimizes the encoder , quantizer , and decoder from scratch for high-fidelity reconstruction. Building on the CAT architecture, we develop MOSS-Audio-Tokenizer, a large-scale audio tokenizer featuring 1.6 billion parameters, pre-trained on 3 million hours of diverse, general audio data. We show that this simple, fully end-to-end approach built from homogeneous, causal Transformer blocks scales gracefully and supports high-fidelity reconstruction across diverse audio domains. Across speech, sound, and music, MOSS-Audio-Tokenizer consistently outperforms prior codec s over a wide range of bitrates, while exhibiting predictable improvements with increased scale. Notably, leveraging the discrete tokens from our model, we develop the first purely autoregressive TTS model that surpasses prior non-autoregressive and cascaded systems. Furthermore, MOSS-Audio-Tokenizer enables competitive ASR performance without auxiliary encoder s. Our findings position the CAT architecture as a unified, scalable interface for the next generation of native audio foundation models.",
      "summary_en": "A fully end-to-end Transformer-based audio tokenizer architecture achieves high-fidelity reconstruction across diverse audio domains and enables superior text-to-speech and automatic speech recognition performance.",
      "summary_zh": "å®Œå…¨ç«¯åˆ°ç«¯çš„TransformeréŸ³é¢‘åˆ†è¯å™¨æ¶æ„å¯åœ¨å¤šç§éŸ³é¢‘é¢†åŸŸå®ç°é«˜ä¿çœŸé‡å»ºï¼Œå¹¶å¸¦æ¥æ›´ä¼˜çš„æ–‡æœ¬è½¬è¯­éŸ³ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10934",
      "arxiv_url": "https://arxiv.org/abs/2602.10934",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10934",
      "github_url": "https://github.com/OpenMOSS/MOSS-Audio-Tokenizer",
      "upvotes": 49,
      "fetched_at": "2026-02-19T06:38:06.334530+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.09070",
      "title": "NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control",
      "authors": [
        "Yufan Wen",
        "Zhaocheng Liu",
        "YeGuo Hua",
        "Ziyi Guo",
        "Lihua Zhang",
        "Chun Yuan",
        "Jian Wu"
      ],
      "abstract": "NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation. Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories . Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a Global Semantic Anchor ensures stylistic stability, while a surgical Token-Level Affective Adapter modulates local tension via direct element-wise residual injection . This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.",
      "summary_en": "NarraScore presents a hierarchical framework that uses frozen Vision-Language Models as affective sensors to generate coherent soundtracks for long-form videos by combining global semantic anchors with token-level adaptive modulation.",
      "summary_zh": "NarraScoreæå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å†»ç»“çš„è§†è§‰-è¯­è¨€æ¨¡å‹ä½œä¸ºæƒ…æ„Ÿä¼ æ„Ÿå™¨ï¼Œç»“åˆå…¨å±€è¯­ä¹‰é”šç‚¹ä¸tokençº§è‡ªé€‚åº”è°ƒåˆ¶ï¼Œä¸ºé•¿è§†é¢‘ç”Ÿæˆè¿è´¯éŸ³è½¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09070",
      "arxiv_url": "https://arxiv.org/abs/2602.09070",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09070",
      "github_url": "",
      "upvotes": 43,
      "fetched_at": "2026-02-19T06:37:53.440952+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12056",
      "title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
      "authors": [
        "Xinyu Yang",
        "Chenlong Deng",
        "Tongyu Wen",
        "Binyu Xie",
        "Zhicheng Dou"
      ],
      "abstract": "LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps. Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments . The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy , fact-law relevance , and procedural compliance , with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .",
      "summary_en": "LawThinker is an autonomous legal research agent that uses an Explore-Verify-Memorize strategy with a DeepVerifier module to ensure accurate and procedurally compliant legal reasoning through dynamic verification of intermediate steps.",
      "summary_zh": "LawThinkeræ˜¯ä¸€ç§è‡ªä¸»æ³•å¾‹ç ”ç©¶æ™ºèƒ½ä½“ï¼Œé‡‡ç”¨Explore-Verify-Memorizeç­–ç•¥å¹¶é…å¤‡DeepVerifieræ¨¡å—ï¼Œé€šè¿‡åŠ¨æ€éªŒè¯ä¸­é—´æ­¥éª¤æ¥ç¡®ä¿æ³•å¾‹æ¨ç†çš„å‡†ç¡®æ€§ä¸ç¨‹åºåˆè§„æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12056",
      "arxiv_url": "https://arxiv.org/abs/2602.12056",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12056",
      "github_url": "https://github.com/yxy-919/LawThinker-agent",
      "upvotes": 33,
      "fetched_at": "2026-02-19T06:38:46.041128+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11731",
      "title": "Thinking with Drafting: Optical Decompression via Logical Reconstruction",
      "authors": [
        "Jingxuan Wei",
        "Honghao He",
        "Caijun Jia",
        "Siyuan Li",
        "Zheng Sun",
        "Yuhang Xu",
        "Yuanyuan Lin",
        "Linzhuang Sun",
        "Yuchen Wu",
        "Bihui Yu",
        "Xiangxiang Zhang",
        "Cheng Tan"
      ],
      "abstract": "Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification. Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation . However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology , while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression -the process of reconstructing latent logical structures from compressed visual tokens . Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark . Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning .",
      "summary_en": "Visual reasoning is enhanced by reconstructing logical structures from compressed visual tokens through a DSL-based approach that generates deterministic visual proofs for verification.",
      "summary_zh": "é€šè¿‡åŸºäºDSLçš„æ–¹æ³•ä»å‹ç¼©è§†è§‰tokené‡å»ºé€»è¾‘ç»“æ„ï¼Œå¹¶ç”Ÿæˆç¡®å®šæ€§è§†è§‰è¯æ˜ä»¥éªŒè¯ï¼Œä»è€Œå¢å¼ºè§†è§‰æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11731",
      "arxiv_url": "https://arxiv.org/abs/2602.11731",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11731",
      "github_url": "",
      "upvotes": 32,
      "fetched_at": "2026-02-19T06:38:30.542922+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11748",
      "title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
      "authors": [
        "Futing Wang",
        "Jianhao Yan",
        "Yun Luo",
        "Ganqu Cui",
        "Zhi Wang",
        "Xiaoye Qu",
        "Yue Zhang",
        "Yu Cheng",
        "Tao Lin"
      ],
      "abstract": "Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks. Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context. Grounded in State Coverage theory , our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation , a phenomenon we term the `` Shallow Exploration Trap ''. To bridge this gap, we propose Length-Incentivized Exploration (\\method). This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty , thereby maximizing state coverage in two-step manner. Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration . As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks.",
      "summary_en": "Models require in-context exploration capabilities to scale effectively at test time, but autoregressive generation faces exponential decay in sampling long sequences, which is addressed by a length-incentivized exploration method that improves performance on both in-domain and out-of-domain tasks.",
      "summary_zh": "æ¨¡å‹éœ€è¦ä¸Šä¸‹æ–‡æ¢ç´¢èƒ½åŠ›ä»¥åœ¨æµ‹è¯•æ—¶æœ‰æ•ˆæ‰©å±•ï¼Œä½†è‡ªå›å½’ç”Ÿæˆåœ¨é‡‡æ ·é•¿åºåˆ—æ—¶é¢ä¸´æŒ‡æ•°çº§è¡°å‡ï¼Œé•¿åº¦æ¿€åŠ±æ¢ç´¢æ–¹æ³•è§£å†³äº†è¯¥é—®é¢˜ï¼Œå¹¶åœ¨é¢†åŸŸå†…ä¸é¢†åŸŸå¤–ä»»åŠ¡ä¸Šå‡æå‡äº†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11748",
      "arxiv_url": "https://arxiv.org/abs/2602.11748",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11748",
      "github_url": "https://github.com/LINs-lab/LIE",
      "upvotes": 30,
      "fetched_at": "2026-02-19T06:38:34.559836+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12280",
      "title": "Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching",
      "authors": [
        "Huai-Hsun Cheng",
        "Siang-Ling Zhang",
        "Yu-Lun Liu"
      ],
      "abstract": "Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength. Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise , a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the \"dual-constraint\": initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a \"common structural subspace \" valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/",
      "summary_en": "Progressive Semantic Illusions use a generative framework with dual-branch Score Distillation Sampling to create vector sketches that transform semantically through sequential stroke additions, achieving superior recognizability and illusion strength.",
      "summary_zh": "æ¸è¿›å¼è¯­ä¹‰å¹»è§‰é‡‡ç”¨åŒåˆ†æ”¯åˆ†æ•°è’¸é¦é‡‡æ ·çš„ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡é¡ºåºæ·»åŠ ç¬”ç”»åˆ›å»ºè¯­ä¹‰å˜æ¢çš„çŸ¢é‡è‰å›¾ï¼Œå®ç°äº†æ›´ä¼˜çš„å¯è¯†åˆ«æ€§å’Œå¹»è§‰å¼ºåº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12280",
      "arxiv_url": "https://arxiv.org/abs/2602.12280",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12280",
      "github_url": "https://github.com/stroke-of-surprise/Stroke-Of-Surprise",
      "upvotes": 29,
      "fetched_at": "2026-02-19T06:39:10.881169+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11075",
      "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
      "authors": [
        "Jiazhi Yang",
        "Kunyang Lin",
        "Jinwei Li",
        "Wencong Zhang",
        "Tianwei Lin",
        "Longyan Wu",
        "Zhizhong Su",
        "Hao Zhao",
        "Ya-Qin Zhang",
        "Li Chen",
        "Ping Luo",
        "Xiangyu Yue",
        "Hongyang Li"
      ],
      "abstract": "RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials. Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model , and (ii) evaluates imagined outcomes with a progress value model , producing informative advantages for the policy improvement . Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts , estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.",
      "summary_en": "RISE is a robotic reinforcement learning framework that uses a compositional world model to predict multi-view futures and evaluate imagined outcomes, enabling policy improvement through virtual interactions rather than physical trials.",
      "summary_zh": "RISEæ˜¯ä¸€ç§æœºå™¨äººå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç»„åˆå¼ä¸–ç•Œæ¨¡å‹é¢„æµ‹å¤šè§†è§’æœªæ¥å¹¶è¯„ä¼°æƒ³è±¡ç»“æœï¼Œé€šè¿‡è™šæ‹Ÿäº¤äº’è€Œéç‰©ç†è¯•éªŒå®ç°ç­–ç•¥æ”¹è¿›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11075",
      "arxiv_url": "https://arxiv.org/abs/2602.11075",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11075",
      "github_url": "",
      "upvotes": 28,
      "fetched_at": "2026-02-19T06:38:08.586990+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.09021",
      "title": "Ï‡_{0}: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
      "authors": [
        "Checheng Yu",
        "Chonghao Sima",
        "Gangcheng Jiang",
        "Hai Zhang",
        "Haoguang Mai",
        "Hongyang Li",
        "Huijie Wang",
        "Jin Chen",
        "Kaiyang Wu",
        "Li Chen",
        "Lirui Zhao",
        "Modi Shi",
        "Ping Luo",
        "Qingwen Bu",
        "Shijia Peng",
        "Tianyu Li",
        "Yibo Yuan"
      ],
      "abstract": "A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability. High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy , and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose Ï‡_{0}, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation . Our approach builds off three technical pillars: (i) Model Arithmetic , a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment , which bridges the distribution gap via spatio-temporal augmentation , heuristic DAgger corrections , and temporal chunk-wise smoothing . Ï‡_{0} enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that Ï‡_{0} surpasses the state-of-the-art Ï€_{0.5} in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.",
      "summary_en": "A resource-efficient robotic manipulation framework addresses distributional shifts through model arithmetic, stage-aware advantage estimation, and train-deploy alignment to achieve long-horizon task reliability.",
      "summary_zh": "ä¸€ç§èµ„æºé«˜æ•ˆçš„æœºå™¨äººæ“ä½œæ¡†æ¶é€šè¿‡æ¨¡å‹ç®—æœ¯ã€é˜¶æ®µæ„ŸçŸ¥ä¼˜åŠ¿ä¼°è®¡ä¸è®­ç»ƒ-éƒ¨ç½²å¯¹é½åº”å¯¹åˆ†å¸ƒåç§»ï¼Œå®ç°é•¿æ—¶ç¨‹ä»»åŠ¡å¯é æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09021",
      "arxiv_url": "https://arxiv.org/abs/2602.09021",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09021",
      "github_url": "https://github.com/OpenDriveLab/KAI0",
      "upvotes": 25,
      "fetched_at": "2026-02-19T06:37:51.126713+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12153",
      "title": "dVoting: Fast Voting for dLLMs",
      "authors": [
        "Sicheng Feng",
        "Zigeng Chen",
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "abstract": "Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples. Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling , offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling , which was previously constrained by severe inefficiency in autoregressive modeling . In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis , regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting",
      "summary_en": "Diffusion large language models enable parallel token generation and efficient reasoning enhancement through a voting technique that identifies and refines uncertain predictions across multiple samples.",
      "summary_zh": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹é€šè¿‡ä¸€ç§æŠ•ç¥¨æŠ€æœ¯å®ç°å¹¶è¡Œtokenç”Ÿæˆä¸é«˜æ•ˆæ¨ç†å¢å¼ºï¼Œè¯¥æŠ€æœ¯å¯è¯†åˆ«å¹¶ä¼˜åŒ–å¤šä¸ªæ ·æœ¬ä¸­çš„ä¸ç¡®å®šé¢„æµ‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12153",
      "arxiv_url": "https://arxiv.org/abs/2602.12153",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12153",
      "github_url": "https://github.com/fscdc/dVoting",
      "upvotes": 20,
      "fetched_at": "2026-02-19T06:38:57.197422+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.10106",
      "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
      "authors": [
        "Modi Shi",
        "Shijia Peng",
        "Jin Chen",
        "Haoran Jiang",
        "Yinghui Li",
        "Di Huang",
        "Ping Luo",
        "Hongyang Li",
        "Li Chen"
      ],
      "abstract": "EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques. Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
      "summary_en": "EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.",
      "summary_zh": "EgoHumanoid åˆ©ç”¨ç¬¬ä¸€äººç§°è§†è§’äººç±»æ¼”ç¤ºå’Œæœ‰é™çš„æœºå™¨äººæ•°æ®è”åˆè®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥ï¼Œå®ç°äººå½¢æœºå™¨äººç§»åŠ¨æ“ä½œï¼Œå¹¶é€šè¿‡è§†è§’ä¸åŠ¨ä½œå¯¹é½æŠ€æœ¯è§£å†³å…·èº«å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10106",
      "arxiv_url": "https://arxiv.org/abs/2602.10106",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10106",
      "github_url": "",
      "upvotes": 20,
      "fetched_at": "2026-02-19T06:37:59.483327+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.05827",
      "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
      "authors": [
        "Hai Zhang",
        "Siqi Liang",
        "Li Chen",
        "Yuxian Li",
        "Yukuan Xu",
        "Yichao Zhong",
        "Fu Zhang",
        "Hongyang Li"
      ],
      "abstract": "Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment. Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
      "summary_en": "Vision-language navigation systems traditionally require detailed instructions but can be improved by incorporating video generation models with sparse future planning for faster, more efficient real-world deployment.",
      "summary_zh": "è§†è§‰è¯­è¨€å¯¼èˆªç³»ç»Ÿä¼ ç»Ÿä¸Šéœ€è¦è¯¦ç»†æŒ‡ä»¤ï¼Œä½†å¯ä»¥é€šè¿‡ç»“åˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸ç¨€ç–æœªæ¥è§„åˆ’æ¥æ”¹è¿›ï¼Œä»¥å®ç°æ›´å¿«ã€æ›´é«˜æ•ˆçš„çœŸå®ä¸–ç•Œéƒ¨ç½²ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05827",
      "arxiv_url": "https://arxiv.org/abs/2602.05827",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05827",
      "github_url": "https://github.com/opendrivelab/sparsevideonav",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:37:42.338237+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11298",
      "title": "Voxtral Realtime",
      "authors": [
        "Alexander H. Liu",
        "Andy Ehrenberg",
        "Andy Lo",
        "Chen-Yo Sun",
        "Guillaume Lample",
        "Jean-Malo Delignon",
        "Khyathi Raghavi Chandu",
        "Patrick von Platen",
        "Pavankumar Reddy Muddireddy",
        "Rohin Arora",
        "Sanchit Gandhi",
        "Sandeep Subramanian",
        "Soham Ghosh",
        "Srijan Mishra",
        "Abhinav Rastogi",
        "Alan Jeffares",
        "Albert Jiang",
        "Alexandre Sablayrolles",
        "AmÃ©lie HÃ©liou",
        "Andrew Bai",
        "Angele Lenglemetz",
        "Anmol Agarwal"
      ],
      "abstract": "Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems. We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency . Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming , with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license.",
      "summary_en": "Voxtral Realtime is a streaming speech recognition model trained end-to-end for sub-second latency with performance matching offline systems.",
      "summary_zh": "Voxtral Realtimeæ˜¯ç«¯åˆ°ç«¯è®­ç»ƒçš„æµå¼è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œå…·æœ‰äºšç§’çº§å»¶è¿Ÿï¼Œæ€§èƒ½ä¸ç¦»çº¿ç³»ç»Ÿç›¸å½“ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11298",
      "arxiv_url": "https://arxiv.org/abs/2602.11298",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11298",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:38:11.269412+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12092",
      "title": "DeepSight: An All-in-One LM Safety Toolkit",
      "authors": [
        "Bo Zhang",
        "Jiaxuan Guo",
        "Lijun Li",
        "Dongrui Liu",
        "Sujin Chen",
        "Guanxu Chen",
        "Zhijie Zheng",
        "Qihao Lin",
        "Lewen Yan",
        "Chen Qian",
        "Yijin Zhou",
        "Yuyao Wu",
        "Shaoxiong Guo",
        "Tianyi Du",
        "Jingyi Yang",
        "Xuhao Hu",
        "Ziqi Miao",
        "Xiaoya Lu",
        "Jing Shao",
        "Xia Hu"
      ],
      "abstract": "DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits. As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation -diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan . By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "summary_en": "DeepSight is an open-source project that integrates safety evaluation and diagnosis for large language and multimodal models, enabling white-box insights through unified protocols and specialized toolkits.",
      "summary_zh": "DeepSight æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œé›†æˆäº†å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§è¯„ä¼°ä¸è¯Šæ–­ï¼Œé€šè¿‡ç»Ÿä¸€åè®®å’Œä¸“ç”¨å·¥å…·åŒ…å®ç°ç™½ç›’æ´å¯Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12092",
      "arxiv_url": "https://arxiv.org/abs/2602.12092",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12092",
      "github_url": "https://github.com/AI45Lab/DeepScan/",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:38:48.198986+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11964",
      "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
      "authors": [
        "Romain Froger",
        "Pierre Andrews",
        "Matteo Bettini",
        "Amar Budhiraja",
        "Ricardo Silveira Cabral",
        "Virginie Do",
        "Emilien Garreau",
        "Jean-Baptiste Gaya",
        "Hugo LaurenÃ§on",
        "Maxime Lecanu",
        "Kunal Malkan",
        "Dheeraj Mekala",
        "Pierre MÃ©nard",
        "Gerard Moreno-Torres Bertran",
        "Ulyana Piterbarg",
        "Mikhail Plekhanov",
        "Mathieu Rita",
        "Andrey Rusakov",
        "Vladislav Vorotilov",
        "Mengjue Wang",
        "Ian Yu",
        "Amine Benhalloum"
      ],
      "abstract": "Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness. We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments . Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints , adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier , enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards . Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1 . These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the \"sim2real\" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.",
      "summary_en": "Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.",
      "summary_zh": "Gaia2æå‡ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å…·æœ‰æ—¶é—´çº¦æŸçš„å¼‚æ­¥åŠ¨æ€å¤šæ™ºèƒ½ä½“åä½œç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œé…å¤‡äº†ç”¨äºå¼ºåŒ–å­¦ä¹ çš„å†™åŠ¨ä½œéªŒè¯å™¨ï¼Œå¹¶æ­ç¤ºäº†æ¨ç†ã€æ•ˆç‡ä¸é²æ£’æ€§ä¹‹é—´çš„æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11964",
      "arxiv_url": "https://arxiv.org/abs/2602.11964",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11964",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:38:42.089265+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11733",
      "title": "Adapting Vision-Language Models for E-commerce Understanding at Scale",
      "authors": [
        "Matteo Nulli",
        "Vladimir Orshulevich",
        "Tala Bazazo",
        "Christian Herold",
        "Michael Kozielski",
        "Marcin Mazur",
        "Szymon Tuzel",
        "Cees G. M. Snoek",
        "Seyyed Hadi Hashemi",
        "Omar Javed",
        "Yannick Versley",
        "Shahram Khadivi"
      ],
      "abstract": "General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities. E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling , yet there is no documented, well-known strategy for adapting them to the attribute-centric , multi-image , and noisy nature of e-commerce data , without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding , strict instruction following , and dynamic attribute extraction .",
      "summary_en": "General-purpose Vision-Language Models can be effectively adapted for e-commerce applications through targeted techniques that enhance product understanding while maintaining broad multimodal capabilities.",
      "summary_zh": "é€šç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å¯é€šè¿‡é’ˆå¯¹æ€§æŠ€æœ¯æœ‰æ•ˆé€‚é…ç”µå­å•†åŠ¡åº”ç”¨ï¼Œåœ¨å¢å¼ºå•†å“ç†è§£çš„åŒæ—¶ä¿æŒå¹¿æ³›çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11733",
      "arxiv_url": "https://arxiv.org/abs/2602.11733",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11733",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:38:32.529715+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.08277",
      "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
      "authors": [
        "Xiangbo Gao",
        "Renjie Li",
        "Xinghao Chen",
        "Yuheng Wu",
        "Suofei Feng",
        "Qing Yin",
        "Zhengzhong Tu"
      ],
      "abstract": "Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking. The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion , which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing , this task demands several requirements: precise spatial-temporal placement , physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control . PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion model s, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation , together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.",
      "summary_en": "Video diffusion model PISCO enables precise instance insertion with sparse keyframe control through variable-information guidance and distribution-preserving temporal masking.",
      "summary_zh": "è§†é¢‘æ‰©æ•£æ¨¡å‹PISCOé€šè¿‡å¯å˜ä¿¡æ¯å¼•å¯¼å’Œåˆ†å¸ƒä¿æŒçš„æ—¶é—´æ©ç ï¼Œå®ç°äº†åŸºäºç¨€ç–å…³é”®å¸§æ§åˆ¶çš„ç²¾ç¡®å®ä¾‹æ’å…¥ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08277",
      "arxiv_url": "https://arxiv.org/abs/2602.08277",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08277",
      "github_url": "https://github.com/taco-group/PISCO",
      "upvotes": 11,
      "fetched_at": "2026-02-19T06:37:48.778539+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.05548",
      "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
      "authors": [
        "Zhiqi Yu",
        "Zhangquan Chen",
        "Mengting Liu",
        "Heye Zhang",
        "Liangqiong Qu"
      ],
      "abstract": "Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.",
      "summary_en": "Asymmetric Group Relative Advantage Estimation addresses exploration and difficulty adaptation challenges in reinforcement learning with large language models by dynamically modulating exploration incentives and sample difficulty focus.",
      "summary_zh": "éå¯¹ç§°ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡é€šè¿‡åŠ¨æ€è°ƒèŠ‚æ¢ç´¢æ¿€åŠ±å’Œæ ·æœ¬éš¾åº¦èšç„¦ï¼Œè§£å†³å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ä¸éš¾åº¦é€‚åº”æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05548",
      "arxiv_url": "https://arxiv.org/abs/2602.05548",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05548",
      "github_url": "https://github.com/HKU-HealthAI/A-GRAE",
      "upvotes": 11,
      "fetched_at": "2026-02-19T06:37:39.686735+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12262",
      "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
      "authors": [
        "Tunyu Zhang",
        "Xinxi Zhang",
        "Ligong Han",
        "Haizhou Shi",
        "Xiaoxiao He",
        "Zhuowei Li",
        "Hao Wang",
        "Kai Xu",
        "Akash Srivastava",
        "Vladimir Pavlovic",
        "Dimitris N. Metaxas"
      ],
      "abstract": "A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality. Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories . We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.",
      "summary_en": "A trajectory self-distillation framework with direct discriminative optimization improves few-step decoding efficiency in diffusion large language models while maintaining generation quality.",
      "summary_zh": "è½¨è¿¹è‡ªè’¸é¦æ¡†æ¶ç»“åˆç›´æ¥åˆ¤åˆ«ä¼˜åŒ–ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„å°‘æ­¥è§£ç æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12262",
      "arxiv_url": "https://arxiv.org/abs/2602.12262",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12262",
      "github_url": "https://github.com/Tyrion58/T3D",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:39:08.866949+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12176",
      "title": "Single-minus gluon tree amplitudes are nonzero",
      "authors": [
        "Alfredo Guevara",
        "Alexandru Lupsasca",
        "David Skinner",
        "Andrew Strominger",
        "Kevin Weil"
      ],
      "abstract": "Single-minus tree-level n-gluon scattering amplitudes are reconsidered. Often presumed to vanish, they are shown here to be nonvanishing for certain \"half-collinear\" configurations existing in Klein space or for complexified momenta. We derive a piecewise-constant closed-form expression for the decay of a single minus-helicity gluon into n-1 plus-helicity gluons as a function of their momenta. This formula nontrivially satisfies multiple consistency conditions including Weinberg's soft theorem.",
      "summary_en": "Single-minus tree-level n-gluon scattering amplitudes, often presumed to vanish, are shown to be nonvanishing for \"half-collinear\" configurations in Klein space or for complexified momenta. The authors derive a piecewise-constant closed-form expression for the decay of a single minus-helicity gluon into n-1 plus-helicity gluons as a function of their momenta. This formula nontrivially satisfies multiple consistency conditions including Weinberg's soft theorem.",
      "summary_zh": "é€šå¸¸è¢«è®¤ä¸ºæ¶ˆå¤±çš„å•è´Ÿå·æ ‘å›¾çº§nèƒ¶å­æ•£å°„æŒ¯å¹…ï¼Œåœ¨Kleinç©ºé—´çš„â€œåŠå…±çº¿â€æ„å‹æˆ–å¤åŒ–åŠ¨é‡ä¸‹è¢«è¯æ˜éé›¶ã€‚ä½œè€…æ¨å¯¼å‡ºäº†å•è´Ÿèºæ—‹åº¦èƒ¶å­è¡°å˜ä¸ºn-1ä¸ªæ­£èºæ—‹åº¦èƒ¶å­çš„åˆ†æ®µå¸¸æ•°é—­å¼è¡¨è¾¾å¼ï¼Œè¯¥å¼æ˜¯è¿™äº›èƒ¶å­åŠ¨é‡çš„å‡½æ•°ã€‚è¯¥å…¬å¼éå¹³å‡¡åœ°æ»¡è¶³åŒ…æ‹¬Weinbergè½¯å®šç†åœ¨å†…çš„å¤šç§ä¸€è‡´æ€§æ¡ä»¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12176",
      "arxiv_url": "https://arxiv.org/abs/2602.12176",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12176",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:39:02.024004+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11683",
      "title": "ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces",
      "authors": [
        "Xin Xu",
        "Tong Yu",
        "Xiang Chen",
        "Haoliang Wang",
        "Julian McAuley",
        "Saayan Mitra"
      ],
      "abstract": "ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation. Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space , yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter , an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT , random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1 , while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning , and accelerates end-of-thinking token generation by globally lowering model confidence .",
      "summary_en": "ThinkRouter is a confidence-aware routing mechanism that improves reasoning efficiency by switching between discrete token and latent spaces based on model confidence, achieving better accuracy and faster generation.",
      "summary_zh": "ThinkRouteræ˜¯ä¸€ç§ç½®ä¿¡åº¦æ„ŸçŸ¥çš„è·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡åŸºäºæ¨¡å‹ç½®ä¿¡åº¦åœ¨ç¦»æ•£tokenç©ºé—´ä¸éšç©ºé—´ä¹‹é—´åˆ‡æ¢æ¥æé«˜æ¨ç†æ•ˆç‡ï¼Œå®ç°æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11683",
      "arxiv_url": "https://arxiv.org/abs/2602.11683",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11683",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:38:28.063011+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.07885",
      "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck",
      "authors": [
        "Zhenyuan Zhang",
        "Xianzhang Jia",
        "Zhiqin Yang",
        "Zhenbo Song",
        "Wei Xue",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "MemFly addresses the challenge of long-term memory in language models by using information bottleneck principles to create an adaptive memory structure with hybrid retrieval mechanisms for improved task performance. Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence , response fidelity , and accuracy.",
      "summary_en": "MemFly addresses the challenge of long-term memory in language models by using information bottleneck principles to create an adaptive memory structure with hybrid retrieval mechanisms for improved task performance.",
      "summary_zh": "MemFlyåˆ©ç”¨ä¿¡æ¯ç“¶é¢ˆåŸç†æ„å»ºå…·æœ‰æ··åˆæ£€ç´¢æœºåˆ¶çš„è‡ªé€‚åº”è®°å¿†ç»“æ„ï¼Œè§£å†³è¯­è¨€æ¨¡å‹çš„é•¿æœŸè®°å¿†æŒ‘æˆ˜å¹¶æå‡ä»»åŠ¡æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07885",
      "arxiv_url": "https://arxiv.org/abs/2602.07885",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07885",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:37:44.480214+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11761",
      "title": "MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling",
      "authors": [
        "MiniCPM Team",
        "Wenhao An",
        "Yingfa Chen",
        "Yewei Fang",
        "Jiayi Li",
        "Xin Li",
        "Yaohui Li",
        "Yishan Li",
        "Yuxuan Li",
        "Biyuan Lin",
        "Chuan Liu",
        "Hezi Liu",
        "Siyuan Liu",
        "Hongya Lyu",
        "Yinxu Pan",
        "Shixin Ren",
        "Xingyu Shen",
        "Zhou Su",
        "Haojun Sun",
        "Yangang Sun",
        "Zhen Leng Thai",
        "Xin Tian"
      ],
      "abstract": "MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs. The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture . While existing sparse and linear attention mechanisms attempt to mitigate these issues, they typically involve a trade-off between memory efficiency and model performance. This paper introduces MiniCPM-SALA, a 9B-parameter hybrid architecture that integrates the high-fidelity long-context modeling of sparse attention (InfLLM-V2) with the global efficiency of linear attention (Lightning Attention). By employing a layer selection algorithm to integrate these mechanisms in a 1:3 ratio and utilizing a hybrid positional encoding (HyPE), the model maintains efficiency and performance for long-context tasks. Furthermore, we introduce a cost-effective continual training framework that transforms pre-trained Transformer-based models into hybrid models, which reduces training costs by approximately 75% compared to training from scratch. Extensive experiments show that MiniCPM-SALA maintains general capabilities comparable to full-attention models while offering improved efficiency. On a single NVIDIA A6000D GPU, the model achieves up to 3.5x the inference speed of the full-attention model at the sequence length of 256K tokens and supports context lengths of up to 1M tokens, a scale where traditional full-attention 8B models fail because of memory constraints.",
      "summary_en": "MiniCPM-SALA combines sparse and linear attention mechanisms in a hybrid architecture to enable efficient processing of ultra-long contexts while maintaining model performance and reducing training costs.",
      "summary_zh": "MiniCPM-SALAé‡‡ç”¨æ··åˆæ¶æ„ç»“åˆç¨€ç–ä¸çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é«˜æ•ˆå¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11761",
      "arxiv_url": "https://arxiv.org/abs/2602.11761",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11761",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:38:37.120027+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.08194",
      "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
      "authors": [
        "Konstantinos Mitsides",
        "Maxence Faldor",
        "Antoine Cully"
      ],
      "abstract": "Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control. Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression . Empirically, DiCode enables agents to acquire long-horizon skills, achieving a 16% improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control , enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
      "summary_en": "Foundation models generate executable environment code to scaffold learning progress in open-ended worlds, enabling agents to acquire long-horizon skills through curriculum control.",
      "summary_zh": "åŸºç¡€æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œç¯å¢ƒä»£ç ï¼Œåœ¨å¼€æ”¾ä¸–ç•Œä¸­ä¸ºå­¦ä¹ è¿›ç¨‹æ­å»ºè„šæ‰‹æ¶ï¼Œä½¿æ™ºèƒ½ä½“é€šè¿‡è¯¾ç¨‹æ§åˆ¶ä¹ å¾—é•¿ç¨‹æŠ€èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08194",
      "arxiv_url": "https://arxiv.org/abs/2602.08194",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08194",
      "github_url": "https://github.com/konstantinosmitsides/dreaming-in-code",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:37:46.635284+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11337",
      "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
      "authors": [
        "Yejin Kim",
        "Wilbert Pumacay",
        "Omar Rayyan",
        "Max Argus",
        "Winson Han",
        "Eli VanderBilt",
        "Jordi Salvador",
        "Abhay Deshpande",
        "Rose Hendrix",
        "Snehal Jauhri",
        "Shuo Liu",
        "Nur Muhammad Mahi Shafiullah",
        "Maya Guru",
        "Ainaz Eftekhar",
        "Karen Farley",
        "Donovan Clay",
        "Jiafei Duan",
        "Arjun Guru",
        "Piper Wolters",
        "Alvaro Herrasti",
        "Ying-Chun Lee",
        "Georgia Chalvatzaki"
      ],
      "abstract": "MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators. Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies . MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks : static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, ho = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing , initial joint positions , and camera occlusion . Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
      "summary_en": "MolmoSpaces presents an open ecosystem with diverse indoor environments and annotated objects for large-scale robot policy benchmarking across multiple tasks and simulators.",
      "summary_zh": "MolmoSpaces æä¾›äº†ä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–å®¤å†…ç¯å¢ƒä¸æ ‡æ³¨ç‰©ä½“çš„å¼€æ”¾ç”Ÿæ€ç³»ç»Ÿï¼Œç”¨äºè·¨å¤šä»»åŠ¡å’Œæ¨¡æ‹Ÿå™¨çš„å¤§è§„æ¨¡æœºå™¨äººç­–ç•¥åŸºå‡†æµ‹è¯•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11337",
      "arxiv_url": "https://arxiv.org/abs/2602.11337",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11337",
      "github_url": "https://github.com/allenai/molmospaces",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:38:14.256041+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12164",
      "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
      "authors": [
        "Xiaohan He",
        "Shiyang Feng",
        "Songtao Huang",
        "Lei Bai",
        "Bin Wang",
        "Bo Zhang"
      ],
      "abstract": "Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness. Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning . In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier . In the second stage, we introduce a geometric reward mechanism that jointly considers consensus , reliability , and diversity , driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability , facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
      "summary_en": "Sci-CoE is a two-stage scientific co-evolving framework that enables large language models to self-evolve as both solver and verifier through sparse-to-unsupervised learning transitions, improving scientific reasoning capabilities and evaluation system robustness.",
      "summary_zh": "Sci-CoEæ˜¯ä¸€ç§ä¸¤é˜¶æ®µç§‘å­¦ååŒè¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä»ç¨€ç–åˆ°æ— ç›‘ç£å­¦ä¹ çš„è¿‡æ¸¡ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤ŸåŒæ—¶ä½œä¸ºæ±‚è§£å™¨å’ŒéªŒè¯å™¨è¿›è¡Œè‡ªæˆ‘è¿›åŒ–ï¼Œä»è€Œæå‡ç§‘å­¦æ¨ç†èƒ½åŠ›å’Œè¯„ä¼°ç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12164",
      "arxiv_url": "https://arxiv.org/abs/2602.12164",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12164",
      "github_url": "https://github.com/InternScience/Sci-CoE",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:38:59.575812+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12116",
      "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
      "authors": [
        "Pinyi Zhang",
        "Ting-En Lin",
        "Yuchuan Wu",
        "Jingyang Chen",
        "Zongqi Wang",
        "Hua Yang",
        "Ze Xu",
        "Fei Huang",
        "Kai Zhang",
        "Yongbin Li"
      ],
      "abstract": "Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy. Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning . A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling . P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset . Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
      "summary_en": "Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.",
      "summary_zh": "ä¸ªæ€§åŒ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹é€šè¿‡ç»“æ„åŒ–è¯„ä¼°é“¾ä¸åŒç²’åº¦ç¼©æ”¾æœºåˆ¶ï¼Œè§£å†³è¯­è¨€æ¨¡å‹å“åº”é€‚é…ä¸ªä½“ç”¨æˆ·åå¥½çš„æŒ‘æˆ˜ï¼Œä»¥æå‡æ³›åŒ–èƒ½åŠ›ä¸å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12116",
      "arxiv_url": "https://arxiv.org/abs/2602.12116",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12116",
      "github_url": "https://github.com/Tongyi-ConvAI/Qwen-Character",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:38:52.804208+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11543",
      "title": "Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm",
      "authors": [
        "Jinrui Zhang",
        "Chaodong Xiao",
        "Aoqi Wu",
        "Xindong Zhang",
        "Lei Zhang"
      ],
      "abstract": "A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies. Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization ; however, they still need to train the entire model on each node, remaining constrained by GPU memory limitations. In this work, we propose SParse Expert Synchronization (SPES), a memory-efficient decentralized framework for pretraining mixture-of-experts (MoE) LLMs. SPES trains only a subset of experts per node, substantially lowering the memory footprint. Each node updates its local experts and periodically synchronizes with other nodes, eliminating full-parameter transmission while ensuring efficient knowledge sharing. To accelerate convergence, we introduce an expert-merging warm-up strategy, where experts exchange knowledge early in training, to rapidly establish foundational capabilities. With SPES, we train a 2B-parameter MoE LLM using 16 standalone 48GB GPUs over internet connections, which achieves competitive performance with centrally trained LLMs under similar computational budgets. We further demonstrate scalability by training a 7B model from scratch and a 9B model upcycled from a dense checkpoint, both of which match prior centralized baselines. Our code is available at https://github.com/zjr2000/SPES.",
      "summary_en": "A memory-efficient decentralized framework for training mixture-of-experts language models using sparse expert synchronization and expert-merging warm-up strategies.",
      "summary_zh": "ä¸€ç§åŸºäºç¨€ç–ä¸“å®¶åŒæ­¥å’Œä¸“å®¶åˆå¹¶é¢„çƒ­ç­–ç•¥çš„å†…å­˜é«˜æ•ˆå»ä¸­å¿ƒåŒ–æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11543",
      "arxiv_url": "https://arxiv.org/abs/2602.11543",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11543",
      "github_url": "https://github.com/zjr2000/SPES",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:38:20.731998+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11509",
      "title": "Multimodal Fact-Level Attribution for Verifiable Reasoning",
      "authors": [
        "David Wan",
        "Han Wang",
        "Ziyang Wang",
        "Elias Stengel-Eskin",
        "Hyunji Lee",
        "Mohit Bansal"
      ],
      "abstract": "MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities. Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning . We introduce MuRGAt ( Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.",
      "summary_en": "MuRGAt is a benchmark for evaluating fact-level multimodal attribution in complex reasoning tasks, requiring models to provide precise citations for their answers across video, audio, and other modalities.",
      "summary_zh": "MuRGAtæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤æ‚æ¨ç†ä»»åŠ¡ä¸­äº‹å®çº§å¤šæ¨¡æ€å½’å› çš„åŸºå‡†ï¼Œè¦æ±‚æ¨¡å‹åœ¨è§†é¢‘ã€éŸ³é¢‘åŠå…¶ä»–æ¨¡æ€ä¸­ä¸ºç­”æ¡ˆæä¾›ç²¾ç¡®å¼•ç”¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11509",
      "arxiv_url": "https://arxiv.org/abs/2602.11509",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11509",
      "github_url": "https://github.com/meetdavidwan/murgat",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:38:16.087727+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.10575",
      "title": "MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning",
      "authors": [
        "Chenhao Zhang",
        "Yazhe Niu",
        "Hongsheng Li"
      ],
      "abstract": "MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks. Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks . Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench. Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmark s. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling , training data scaling , and the impact of different model architectures and training strategies , demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.",
      "summary_en": "MetaphorStar, an end-to-end visual reinforcement learning framework, significantly enhances metaphor comprehension in images through a specialized dataset, RL method, and benchmark, achieving state-of-the-art performance on multiple visual reasoning tasks.",
      "summary_zh": "MetaphorStaræ˜¯ä¸€ç§ç«¯åˆ°ç«¯è§†è§‰å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨çš„æ•°æ®é›†ã€RLæ–¹æ³•å’ŒåŸºå‡†ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒéšå–»ç†è§£èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10575",
      "arxiv_url": "https://arxiv.org/abs/2602.10575",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10575",
      "github_url": "https://github.com/MING-ZCH/MetaphorStar",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:38:02.120312+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.12203",
      "title": "ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images",
      "authors": [
        "Mathieu Sibue",
        "Andres MuÃ±oz Garza",
        "Samuel Mensah",
        "Pranav Shetty",
        "Zhiqiang Ma",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "abstract": "A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.",
      "summary_en": "A new benchmark dataset called ExStrucTiny is introduced for structured information extraction from document images, addressing limitations of existing datasets and evaluating vision-language models on diverse document types and flexible schemas.",
      "summary_zh": "ç ”ç©¶æå‡ºåä¸ºExStrucTinyçš„æ–°åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ–‡æ¡£å›¾åƒç»“æ„åŒ–ä¿¡æ¯æŠ½å–ï¼Œè§£å†³ç°æœ‰æ•°æ®é›†å±€é™ï¼Œå¹¶åœ¨å¤šæ ·åŒ–æ–‡æ¡£ç±»å‹ä¸çµæ´»schemaä¸Šè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12203",
      "arxiv_url": "https://arxiv.org/abs/2602.12203",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12203",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:39:04.210274+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11541",
      "title": "Budget-Constrained Agentic Large Language Models: Intention-Based Planning for Costly Tool Use",
      "authors": [
        "Hanbing Liu",
        "Chunhao Tian",
        "Nan An",
        "Ziyuan Wang",
        "Pinyan Lu",
        "Changyuan Yu",
        "Qi Qi"
      ],
      "abstract": "Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints. We study budget-constrained tool-augmented agents , where a large language model must solve multi-step tasks by invoking external tools under a strict monetary budget. We formalize this setting as sequential decision making in context space with priced and stochastic tool executions , making direct planning intractable due to massive state-action spaces, high variance of outcomes and prohibitive exploration cost. To address these challenges, we propose INTENT, an inference-time planning framework that leverages an intention-aware hierarchical world model to anticipate future tool usage, risk-calibrated cost , and guide decisions online. Across cost-augmented StableToolBench, INTENT strictly enforces hard budget feasibility while substantially improving task success over baselines, and remains robust under dynamic market shifts such as tool price changes and varying budgets.",
      "summary_en": "Budget-constrained tool-augmented agents use a hierarchical world model and intent-aware planning to optimize multi-step task completion under monetary constraints.",
      "summary_zh": "é¢„ç®—å—é™çš„å·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“åˆ©ç”¨åˆ†å±‚ä¸–ç•Œæ¨¡å‹å’Œæ„å›¾æ„ŸçŸ¥è§„åˆ’ï¼Œåœ¨èµ„é‡‘çº¦æŸä¸‹ä¼˜åŒ–å¤šæ­¥ä»»åŠ¡å®Œæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11541",
      "arxiv_url": "https://arxiv.org/abs/2602.11541",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11541",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:38:18.375183+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11792",
      "title": "Detecting RLVR Training Data via Structural Convergence of Reasoning",
      "authors": [
        "Hongbo Zhang",
        "Yue Yang",
        "Jianhao Yan",
        "Guangsheng Bao",
        "Yue Zhang"
      ],
      "abstract": "Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches. Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories , making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature : prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-kNN Distance , a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the k smallest nearest-neighbor edit distances. Min-kNN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-kNN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.",
      "summary_en": "Reinforcement learning with verifiable rewards induces behavioral signatures that can be detected using a black-box method based on prompt generation diversity, outperforming existing contamination detection approaches.",
      "summary_zh": "åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è¯±å¯¼å‡ºçš„è¡Œä¸ºç‰¹å¾å¯é€šè¿‡åŸºäºæç¤ºç”Ÿæˆå¤šæ ·æ€§çš„é»‘ç›’æ–¹æ³•æ£€æµ‹ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„æ±¡æŸ“æ£€æµ‹æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11792",
      "arxiv_url": "https://arxiv.org/abs/2602.11792",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11792",
      "github_url": "https://github.com/StevenZHB/Detect_RLVR_Data",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:38:39.520626+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11636",
      "title": "ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning",
      "authors": [
        "Changti Wu",
        "Jiahuai Mao",
        "Yuzhuo Miao",
        "Shijie Lian",
        "Bin Yu",
        "Xiaopeng Lin",
        "Cong Huang",
        "Lei Zhang",
        "Kai Chen"
      ],
      "abstract": "ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements. Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at https://github.com/ChangtiWu/ScalSelect{ScalSelect}.",
      "summary_en": "ScalSelect is a scalable training-free method for selecting representative multimodal data that achieves near-full-dataset performance with significantly reduced computational requirements.",
      "summary_zh": "ScalSelectæ˜¯ä¸€ç§å¯æ‰©å±•çš„å…è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºé€‰æ‹©ä»£è¡¨æ€§å¤šæ¨¡æ€æ•°æ®ï¼Œèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½è®¡ç®—éœ€æ±‚çš„åŒæ—¶è¾¾åˆ°æ¥è¿‘å®Œæ•´æ•°æ®é›†çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11636",
      "arxiv_url": "https://arxiv.org/abs/2602.11636",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11636",
      "github_url": "https://github.com/ChangtiWu/ScalSelect",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:38:25.552662+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.11598",
      "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
      "authors": [
        "Zedong Chu",
        "Shichao Xie",
        "Xiaolong Wu",
        "Yanfen Shen",
        "Minghua Luo",
        "Zhengbo Wang",
        "Fei Liu",
        "Xiaoxu Leng",
        "Junjun Hu",
        "Mingyang Yin",
        "Jia Lu",
        "Yingnan Guo",
        "Kai Yang",
        "Jiawei Han",
        "Xu Chen",
        "Yanqing Zhu",
        "Yuxiang Zhao",
        "Xin Liu",
        "Yirong Yang",
        "Ye He",
        "Jiahang Wang",
        "Yang Cai"
      ],
      "abstract": "A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks. Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation. To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 km^2). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory , enabling robust, long-horizon missions in dynamic real-world environments.",
      "summary_en": "A unified Vision-Language-Action model with a hierarchical architecture combining semantic reasoning and continuous trajectory generation achieves state-of-the-art performance across multiple embodied navigation tasks.",
      "summary_zh": "ä¸€ç§å…·æœ‰åˆ†å±‚æ¶æ„ã€ç»“åˆè¯­ä¹‰æ¨ç†ä¸è¿ç»­è½¨è¿¹ç”Ÿæˆçš„ç»Ÿä¸€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œåœ¨å¤šä¸ªå…·èº«å¯¼èˆªä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11598",
      "arxiv_url": "https://arxiv.org/abs/2602.11598",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11598",
      "github_url": "https://github.com/amap-cvlab/ABot-Navigation",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:38:23.036149+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.10585",
      "title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
      "authors": [
        "Guangzhi Xiong",
        "Sanchit Sinha",
        "Aidong Zhang"
      ],
      "abstract": "Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models. The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework , learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions . Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.",
      "summary_en": "Neural Additive Experts combines multiple specialized networks with a dynamic gating mechanism to balance predictive accuracy and feature interpretability in machine learning models.",
      "summary_zh": "ç¥ç»å¯åŠ ä¸“å®¶ç»“åˆå¤šä¸ªä¸“ç”¨ç½‘ç»œä¸åŠ¨æ€é—¨æ§æœºåˆ¶ï¼Œä»¥å¹³è¡¡æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„é¢„æµ‹å‡†ç¡®æ€§ä¸ç‰¹å¾å¯è§£é‡Šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10585",
      "arxiv_url": "https://arxiv.org/abs/2602.10585",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10585",
      "github_url": "https://github.com/Teddy-XiongGZ/NAE",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:38:03.905932+00:00"
    },
    {
      "date": "2026-02-13",
      "paper_id": "2602.09891",
      "title": "Stemphonic: All-at-once Flexible Multi-stem Music Generation",
      "authors": [
        "Shih-Lun Wu",
        "Ge Zhu",
        "Juan-Pablo Caceres",
        "Cheng-Zhi Anna Huang",
        "Nicholas J. Bryan"
      ],
      "abstract": "Stemphonic is a diffusion- and flow-based framework that generates variable sets of synchronized musical stems in single inference passes, improving both quality and efficiency over existing methods. Music stem generation , the task of producing musically-synchronized and isolated instrument audio clips, offers the potential of greater user control and better alignment with musician workflows compared to conventional text-to-music models. Existing stem generation approaches, however, either rely on fixed architectures that output a predefined set of stems in parallel, or generate only one stem at a time, resulting in slow inference despite flexibility in stem combination. We propose Stemphonic, a diffusion-/flow-based framework that overcomes this trade-off and generates a variable set of synchronized stems in one inference pass. During training, we treat each stem as a batch element, group synchronized stems in a batch, and apply a shared noise latent to each group. At inference-time, we use a shared initial noise latent and stem-specific text inputs to generate synchronized multi-stem outputs in one pass. We further expand our approach to enable one-pass conditional multi-stem generation and stem-wise activity controls to empower users to iteratively generate and orchestrate the temporal layering of a mix. We benchmark our results on multiple open-source stem evaluation sets and show that Stemphonic produces higher-quality outputs while accelerating the full mix generation process by 25 to 50%. Demos at: https://stemphonic-demo.vercel.app.",
      "summary_en": "Stemphonic is a diffusion- and flow-based framework that generates variable sets of synchronized musical stems in single inference passes, improving both quality and efficiency over existing methods.",
      "summary_zh": "Stemphonicæ˜¯ä¸€ç§åŸºäºæ‰©æ•£å’Œæµçš„æ¡†æ¶ï¼Œå¯åœ¨å•æ¬¡æ¨ç†ä¸­ç”Ÿæˆå¯å˜æ•°é‡çš„åŒæ­¥éŸ³ä¹åˆ†è½¨ï¼Œåœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09891",
      "arxiv_url": "https://arxiv.org/abs/2602.09891",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09891",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:37:57.397529+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10604",
      "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters",
      "authors": [
        "Ailin Huang",
        "Ang Li",
        "Aobo Kong",
        "Bin Wang",
        "Binxing Jiao",
        "Bo Dong",
        "Bojun Wang",
        "Boyu Chen",
        "Brian Li",
        "Buyun Ma",
        "Chang Su",
        "Changxin Miao",
        "Changyi Wan",
        "Chao Lou",
        "Chen Hu",
        "Chen Xu",
        "Chenfeng Yu",
        "Chengting Feng",
        "Chengyuan Yao",
        "Chunrui Han",
        "Dan Ma",
        "Dapeng Shi"
      ],
      "abstract": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks. We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/ full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback , while remaining stable under large-scale off-policy training , enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench , 86.4% on LiveCodeBench -v6 (2024.08-2025.05), 88.2% on tau2-Bench , 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.",
      "summary_en": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.",
      "summary_zh": "Step 3.5 Flash æ˜¯ä¸€ç§ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ï¼Œé€šè¿‡é«˜æ•ˆçš„å‚æ•°åˆ©ç”¨å’Œä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å‰æ²¿æ°´å¹³çš„æ™ºèƒ½ä½“æ™ºèƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10604",
      "arxiv_url": "https://arxiv.org/abs/2602.10604",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10604",
      "github_url": "https://github.com/stepfun-ai/Step-3.5-Flash",
      "upvotes": 176,
      "fetched_at": "2026-02-19T06:31:41.070741+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08099",
      "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval",
      "authors": [
        "Issar Tzachor",
        "Dvir Samuel",
        "Rami Ben-Ari"
      ],
      "abstract": "Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision. Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks .",
      "summary_en": "Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.",
      "summary_zh": "ç”Ÿæˆå¼å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é€šè¿‡ä¸­é—´å±‚åˆ†æä¸æ— è§†è§‰ç›‘ç£çš„æ–‡æœ¬å¯¹é½ï¼Œé€‚é…äºè§†é¢‘-æ–‡æœ¬åµŒå…¥ä¸æ£€ç´¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08099",
      "arxiv_url": "https://arxiv.org/abs/2602.08099",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08099",
      "github_url": "",
      "upvotes": 120,
      "fetched_at": "2026-02-19T06:31:01.647301+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11144",
      "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
      "authors": [
        "Ruichuan An",
        "Sihan Yang",
        "Ziyu Guo",
        "Wei Dai",
        "Zijun Shen",
        "Haodong Li",
        "Renrui Zhang",
        "Xinyu Wei",
        "Guopeng Li",
        "Wenshan Wu",
        "Wentao Zhang"
      ],
      "abstract": "GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability. Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy . Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.",
      "summary_en": "GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.",
      "summary_zh": "GENIUSé€šè¿‡æ¨¡å¼å½’çº³ã€çº¦æŸæ‰§è¡Œå’Œä¸Šä¸‹æ–‡é€‚åº”ä»»åŠ¡è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„ç”Ÿæˆæµä½“æ™ºèƒ½ï¼Œæ­ç¤ºäº†å…¶åœ¨ä¸Šä¸‹æ–‡ç†è§£è€Œéç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„ä¸è¶³ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11144",
      "arxiv_url": "https://arxiv.org/abs/2602.11144",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11144",
      "github_url": "",
      "upvotes": 53,
      "fetched_at": "2026-02-19T06:32:15.000282+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11124",
      "title": "PhyCritic: Multimodal Critic Models for Physical AI",
      "authors": [
        "Tianyi Xiong",
        "Shihao Wang",
        "Guilin Liu",
        "Yi Dong",
        "Ming Li",
        "Heng Huang",
        "Jan Kautz",
        "Zhiding Yu"
      ],
      "abstract": "PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities. With the rapid development of large multimodal models , reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception , causal reasoning , and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline : a physical skill warmup stage that enhances physically oriented perception and reasoning , followed by self-referential critic finetuning , where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model , further improves perception and reasoning in physically grounded tasks.",
      "summary_en": "PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.",
      "summary_zh": "PhyCriticæ˜¯ä¸€æ¬¾é¢å‘ç‰©ç†AIä»»åŠ¡çš„å¤šæ¨¡æ€criticæ¨¡å‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µRLVRæµç¨‹å¢å¼ºæ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11124",
      "arxiv_url": "https://arxiv.org/abs/2602.11124",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11124",
      "github_url": "",
      "upvotes": 50,
      "fetched_at": "2026-02-19T06:32:10.685816+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.04935",
      "title": "ASA: Training-Free Representation Engineering for Tool-Calling Agents",
      "authors": [
        "Youjin Wang",
        "Run Zhou",
        "Rong Fu",
        "Shuaishuai Cao",
        "Hongwei Zeng",
        "Jiaxuan Lu",
        "Sicheng Fan",
        "Jiaqiao Zhao",
        "Liangming Pan"
      ],
      "abstract": "A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors. Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers , while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations , yet the model remains conservative in entering tool mode, revealing a representation-behavior gap . We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B , ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.",
      "summary_en": "A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.",
      "summary_zh": "ä¸€ç§ç§°ä¸ºæ¿€æ´»å¼•å¯¼é€‚é…å™¨çš„å…è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ¢é’ˆä¸è·¯ç”±æ¡ä»¶å¼•å¯¼å‘é‡å¯¹ä¸­é—´å±‚æ¿€æ´»è¿›è¡Œå¹²é¢„ï¼Œçº æ­£è¯­è¨€æ¨¡å‹çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04935",
      "arxiv_url": "https://arxiv.org/abs/2602.04935",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04935",
      "github_url": "",
      "upvotes": 39,
      "fetched_at": "2026-02-19T06:30:42.396521+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10177",
      "title": "Towards Autonomous Mathematics Research",
      "authors": [
        "Tony Feng",
        "Trieu H. Trinh",
        "Garrett Bingham",
        "Dawsen Hwang",
        "Yuri Chervonyi",
        "Junehyuk Jung",
        "Joonkyung Lee",
        "Carlo Pagano",
        "Sang-hyun Kim",
        "Federico Pasqualotto",
        "Sergei Gukov",
        "Jonathan N. Lee",
        "Junsu Kim",
        "Kaiying Hou",
        "Golnaz Ghiasi",
        "Yi Tay",
        "YaGuang Li",
        "Chenkai Kuang",
        "Yuan Liu",
        "Hanzhao",
        "Lin",
        "Evan Zheran Liu"
      ],
      "abstract": "Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research. Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad . The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research . We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.",
      "summary_en": "Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.",
      "summary_zh": "Aletheiaæ˜¯ä¸€æ¬¾æ•°å­¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œé€šè¿‡ç«¯åˆ°ç«¯ç”Ÿæˆå¹¶éªŒè¯è‡ªç„¶è¯­è¨€è§£å†³æ–¹æ¡ˆå±•ç°å…ˆè¿›æ¨ç†èƒ½åŠ›ï¼Œå®ç°ä»å¥¥æ—åŒ¹å…‹ç«èµ›é¢˜ç›®åˆ°åšå£«çº§åˆ«ä¹ é¢˜çš„è‡ªä¸»ç ”ç©¶æˆæœï¼Œå¹¶ä¸ºAIè¾…åŠ©æ•°å­¦ç ”ç©¶åšå‡ºè´¡çŒ®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10177",
      "arxiv_url": "https://arxiv.org/abs/2602.10177",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10177",
      "github_url": "",
      "upvotes": 35,
      "fetched_at": "2026-02-19T06:31:25.332847+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10560",
      "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning",
      "authors": [
        "Leheng Sheng",
        "Yongtao Zhang",
        "Wenchang Ma",
        "Yaorui Shi",
        "Ting Huang",
        "Xiang Wang",
        "An Zhang",
        "Ke Shen",
        "Tat-Seng Chua"
      ],
      "abstract": "GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.",
      "summary_en": "GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.",
      "summary_zh": "GRU-Memé€šè¿‡å¼•å…¥æ–‡æœ¬æ§åˆ¶é—¨æ§å’Œå¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œè§£å†³å¤§è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†æŒ‘æˆ˜ï¼Œç¨³å®šè®°å¿†æ›´æ–°å¹¶æå‡è®¡ç®—æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10560",
      "arxiv_url": "https://arxiv.org/abs/2602.10560",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10560",
      "github_url": "",
      "upvotes": 28,
      "fetched_at": "2026-02-19T06:31:38.637503+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10622",
      "title": "How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning",
      "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Yang Chen",
        "Xiaotong Lin",
        "Wuliang Huang",
        "Ziyi Gao",
        "Xing Fu",
        "Yu Cheng",
        "Weiqiang Wang"
      ],
      "abstract": "Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis. Decoder-only large language models are increasingly used as behavioral encoders for user representation learning , yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention , we propose Gradient-Guided Soft Masking , a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning . Our code is available at https://github.com/JhCircle/Deepfind-GGSM.",
      "summary_en": "Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.",
      "summary_zh": "ç ”ç©¶æ¢è®¨äº†ä¸åŒæ³¨æ„åŠ›æ©ç ç­–ç•¥å¯¹ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ä¸­ç”¨æˆ·åµŒå…¥è´¨é‡çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ¢¯åº¦å¼•å¯¼çš„è½¯æ©ç æŠ€æœ¯ï¼Œä»¥æå‡ç”¨æˆ·è¡Œä¸ºåˆ†æä¸­çš„è®­ç»ƒç¨³å®šæ€§ä¸è¡¨å¾è´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10622",
      "arxiv_url": "https://arxiv.org/abs/2602.10622",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10622",
      "github_url": "https://github.com/JhCircle/Deepfind-GGSM",
      "upvotes": 26,
      "fetched_at": "2026-02-19T06:31:45.470281+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08711",
      "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions",
      "authors": [
        "Linli Yao",
        "Yuancheng Wei",
        "Yaojie Zhang",
        "Lei Li",
        "Xinlong Chen",
        "Feifan Song",
        "Ziyue Wang",
        "Kun Ouyang",
        "Yuanxin Liu",
        "Lingpeng Kong",
        "Qi Liu",
        "Pengfei Wan",
        "Kun Gai",
        "Yuanxing Zhang",
        "Xu Sun"
      ],
      "abstract": "Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.",
      "summary_en": "Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.",
      "summary_zh": "Omni Dense Captioning æå‡ºäº†ä¸€ç§å…­ç»´ç»“æ„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¸¦æ˜¾å¼æ—¶é—´æˆ³çš„æ—¶åºæ„ŸçŸ¥è§†å¬å™äº‹ï¼Œå¹¶æä¾›äº†ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡ä¸å¼ºåŸºçº¿æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08711",
      "arxiv_url": "https://arxiv.org/abs/2602.08711",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08711",
      "github_url": "https://github.com/yaolinli/TimeChat-Captioner",
      "upvotes": 26,
      "fetched_at": "2026-02-19T06:31:07.729957+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08253",
      "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
      "authors": [
        "Baoyun Zhao",
        "He Wang",
        "Liang Zeng"
      ],
      "abstract": "A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems. While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
      "summary_en": "A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.",
      "summary_zh": "ç”Ÿæˆå¼è¿›åŒ–æ¡†æ¶æ‰©å±•å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç»„åˆä¼˜åŒ–é—®é¢˜ä¸­å¤§é‚»åŸŸæœç´¢ç®—å­çš„è‡ªåŠ¨åŒ–è®¾è®¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08253",
      "arxiv_url": "https://arxiv.org/abs/2602.08253",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08253",
      "github_url": "https://github.com/ZBoyn/G-LNS",
      "upvotes": 25,
      "fetched_at": "2026-02-19T06:31:03.485409+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10224",
      "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
      "authors": [
        "Shiting Huang",
        "Zecheng Li",
        "Yu Zeng",
        "Qingnan Ren",
        "Zhen Fang",
        "Qisheng Su",
        "Kou Shi",
        "Lin Chen",
        "Zehui Chen",
        "Feng Zhao"
      ],
      "abstract": "Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience . Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory . Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience . The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood , which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.",
      "summary_en": "Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.",
      "summary_zh": "å…ƒç»éªŒå­¦ä¹ é€šè¿‡å¯¹æ¯”è½¨è¿¹åˆ†æä¸è¯­è¨€å»ºæ¨¡çš„å¥–åŠ±ä¿¡å·ï¼Œå°†è‡ªè’¸é¦é”™è¯¯è¡¨å¾çº³å…¥å‚æ•°åŒ–è®°å¿†ï¼Œä»è€Œå¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10224",
      "arxiv_url": "https://arxiv.org/abs/2602.10224",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10224",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T06:31:30.281131+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11089",
      "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
      "authors": [
        "Yicheng Chen",
        "Zerun Ma",
        "Xinchen Xie",
        "Yining Li",
        "Kai Chen"
      ],
      "abstract": "DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes. In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe , which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipe s remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.",
      "summary_en": "DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.",
      "summary_zh": "DataChef-32Bé€šè¿‡åŸºäºä»£ç†å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨åŒ–ç”Ÿæˆç”¨äºLLMé€‚é…çš„æ•°æ®é…æ–¹ï¼Œå–å¾—äº†ä¸äººå·¥è®¾è®¡é…æ–¹ç›¸å½“çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11089",
      "arxiv_url": "https://arxiv.org/abs/2602.11089",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11089",
      "github_url": "https://github.com/yichengchen24/DataChef",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:32:06.192487+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10975",
      "title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development",
      "authors": [
        "Qixing Zhou",
        "Jiacheng Zhang",
        "Haiyang Wang",
        "Rui Hao",
        "Jiahe Wang",
        "Minghao Han",
        "Yuxue Yang",
        "Shuzhe Wu",
        "Feiyang Pan",
        "Lue Fan",
        "Dandan Tu",
        "Zhaoxiang Zhang"
      ],
      "abstract": "FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories. Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development . FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph , our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding . Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage . The inherent verifiability of constructed environments also makes our method potentially valuable for agent training .",
      "summary_en": "FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.",
      "summary_zh": "FeatureBench é€šè¿‡åŸºäºæ‰§è¡Œçš„è¯„ä¼°å’Œä»ä»£ç ä»“åº“è‡ªåŠ¨æ´¾ç”Ÿä»»åŠ¡ï¼Œè¯„ä¼°é¢å‘å…¨é¢ç‰¹æ€§å¼€å‘çš„æ™ºèƒ½ä½“ç¼–ç æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10975",
      "arxiv_url": "https://arxiv.org/abs/2602.10975",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10975",
      "github_url": "https://github.com/LiberCoders/FeatureBench",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:31:59.714298+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11008",
      "title": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression",
      "authors": [
        "Ammar Ali",
        "Baher Mohammad",
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Stamatios Lefkimmiatis"
      ],
      "abstract": "ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization. We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem , selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning : using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\\% compression rates. Notably, it retains over 90\\% of the original model's performance at 30\\% compression without any fine-tuning . Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.",
      "summary_en": "ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.",
      "summary_zh": "ROCKETæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œå°†é€å±‚å‹ç¼©å½¢å¼åŒ–ä¸ºå¤šé€‰æ‹©èƒŒåŒ…é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ç¨€ç–çŸ©é˜µåˆ†è§£å®ç°é«˜æ•ˆçš„æƒé‡ç¨€ç–åŒ–ï¼Œæ— éœ€è¿­ä»£ä¼˜åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11008",
      "arxiv_url": "https://arxiv.org/abs/2602.11008",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11008",
      "github_url": "https://github.com/mts-ai/ROCKET",
      "upvotes": 17,
      "fetched_at": "2026-02-19T06:32:04.285141+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10609",
      "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
      "authors": [
        "Shuo He",
        "Lang Feng",
        "Xin Cheng",
        "Lei Feng",
        "Bo An"
      ],
      "abstract": "Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization. Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse . To address the issue, we propose Online Causal Kalman Filter ing for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.",
      "summary_en": "Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.",
      "summary_zh": "åœ¨çº¿å› æœå¡å°”æ›¼æ»¤æ³¢é€šè¿‡å°†é‡è¦æ€§é‡‡æ ·æ¯”ç‡å»ºæ¨¡ä¸ºæ¼”åŒ–çš„éšçŠ¶æ€ï¼Œå¹¶åˆ©ç”¨å¡å°”æ›¼æ»¤æ³¢å®ç°ç¨³å®šçš„ç­–ç•¥ä¼˜åŒ–ï¼Œè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­è¯å…ƒçº§é‡è¦æ€§é‡‡æ ·çš„é«˜æ–¹å·®é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10609",
      "arxiv_url": "https://arxiv.org/abs/2602.10609",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10609",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T06:31:42.927832+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11451",
      "title": "LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation",
      "authors": [
        "Ahmadreza Jeddi",
        "Marco Ciccone",
        "Babak Taati"
      ],
      "abstract": "LoopFormer is a looped Transformer architecture that enables adaptive computational depth through variable-length trajectory training and shortcut-consistency regularization, allowing flexible reasoning under different compute constraints. Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning . However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets . We introduce LoopFormer , a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning . Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling , opening a path toward controllable and budget-aware large language models.",
      "summary_en": "LoopFormer is a looped Transformer architecture that enables adaptive computational depth through variable-length trajectory training and shortcut-consistency regularization, allowing flexible reasoning under different compute constraints.",
      "summary_zh": "LoopFormeræ˜¯ä¸€ç§å¾ªç¯Transformeræ¶æ„ï¼Œé€šè¿‡å˜é•¿åº¦è½¨è¿¹è®­ç»ƒå’Œæ·å¾„ä¸€è‡´æ€§æ­£åˆ™åŒ–å®ç°è‡ªé€‚åº”è®¡ç®—æ·±åº¦ï¼Œä»è€Œåœ¨ä¸åŒè®¡ç®—çº¦æŸä¸‹å®ç°çµæ´»æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11451",
      "arxiv_url": "https://arxiv.org/abs/2602.11451",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11451",
      "github_url": "https://github.com/armenjeddi/loopformer",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:32:19.435513+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11103",
      "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development",
      "authors": [
        "Wayne Chi",
        "Yixiong Fang",
        "Arnav Yayavaram",
        "Siddharth Yayavaram",
        "Seth Karten",
        "Qiuhong Anna Wei",
        "Runkun Chen",
        "Alexander Wang",
        "Valerie Chen",
        "Ameet Talwalkar",
        "Chris Donahue"
      ],
      "abstract": "GameDevBench is introduced as the first benchmark for evaluating agents on game development tasks that combine software development complexity with deep multimodal understanding requirements. Despite rapid progress on coding agents , progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbed s that combine the complexity of software development with the need for deep multimodal understanding . Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench , the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials . Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development , with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity , with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development .",
      "summary_en": "GameDevBench is introduced as the first benchmark for evaluating agents on game development tasks that combine software development complexity with deep multimodal understanding requirements.",
      "summary_zh": "GameDevBenchæ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°æ™ºèƒ½ä½“æ¸¸æˆå¼€å‘ä»»åŠ¡çš„åŸºå‡†ï¼Œè¯¥ç±»ä»»åŠ¡ç»“åˆäº†è½¯ä»¶å¼€å‘å¤æ‚æ€§ä¸æ·±åº¦å¤šæ¨¡æ€ç†è§£éœ€æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11103",
      "arxiv_url": "https://arxiv.org/abs/2602.11103",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11103",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-19T06:32:08.772416+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.12108",
      "title": "The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context",
      "authors": [
        "Xiaoyuan Liu",
        "Tian Liang",
        "Dongyang Ma",
        "Deyu Zhou",
        "Haitao Mi",
        "Pinjia He",
        "Yan Wang"
      ],
      "abstract": "StateLM enables language models to actively manage their own memory and context through internal reasoning loops and memory tools, significantly improving performance on long-document tasks and chat memory challenges.",
      "summary_en": "StateLM enables language models to actively manage their own memory and context through internal reasoning loops and memory tools, significantly improving performance on long-document tasks and chat memory challenges.",
      "summary_zh": "StateLMä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å†…éƒ¨æ¨ç†å¾ªç¯å’Œè®°å¿†å·¥å…·ä¸»åŠ¨ç®¡ç†è‡ªèº«è®°å¿†ä¸ä¸Šä¸‹æ–‡ï¼Œæ˜¾è‘—æå‡åœ¨é•¿æ–‡æ¡£ä»»åŠ¡å’Œå¯¹è¯è®°å¿†æŒ‘æˆ˜ä¸Šçš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12108",
      "arxiv_url": "https://arxiv.org/abs/2602.12108",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12108",
      "github_url": "https://github.com/xyliu-cs/StateLM",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:32:21.993855+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10367",
      "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
      "authors": [
        "Zhiling Yan",
        "Dingjie Song",
        "Zhe Fang",
        "Yisheng Ji",
        "Xiang Li",
        "Quanzheng Li",
        "Lichao Sun"
      ],
      "abstract": "LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning. The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination , where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment , failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.",
      "summary_en": "LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.",
      "summary_zh": "LiveMedBench é€šè¿‡æä¾›æŒç»­æ›´æ–°ã€æ— æ±¡æŸ“çš„åŸºå‡†æµ‹è¯•ä»¥åŠåŸºäºè¯„åˆ†æ ‡å‡†ä¸”å¥‘åˆä¸“å®¶ä¸´åºŠæ¨ç†çš„è¯„ä¼°ï¼Œè§£å†³åŒ»ç–—LLMè¯„ä¼°çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10367",
      "arxiv_url": "https://arxiv.org/abs/2602.10367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10367",
      "github_url": "https://github.com/ZhilingYan/LiveMedBench",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:31:36.722748+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11149",
      "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
      "authors": [
        "Dawid J. Kopiczko",
        "Sagar Vaze",
        "Tijmen Blankevoort",
        "Yuki M. Asano"
      ],
      "abstract": "Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration. Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models . Standard machine learning intuition suggests that training with more unique training samples yields better generalization . Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME '24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting . We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization , a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization , as a new open problem for the community in understanding the training dynamics of large language models.",
      "summary_en": "Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.",
      "summary_zh": "åœ¨è¾ƒå°æ•°æ®é›†ä¸Šä½¿ç”¨é‡å¤æ ·æœ¬è®­ç»ƒæ¨ç†è¯­è¨€æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºåœ¨æ›´å¤§æ•°æ®é›†ä¸Šè¿›è¡Œå•è½®è®­ç»ƒï¼Œä¸” token å‡†ç¡®ç‡å¯ä½œä¸ºç¡®å®šæœ€ä¼˜è®­ç»ƒæ—¶é•¿çš„å¯é æŒ‡æ ‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11149",
      "arxiv_url": "https://arxiv.org/abs/2602.11149",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11149",
      "github_url": "https://github.com/dkopi/data-repetition",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:32:17.427661+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10231",
      "title": "Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards",
      "authors": [
        "Kirill Pavlenko",
        "Alexander Golubev",
        "Simon Karasik",
        "Boris Yangel"
      ],
      "abstract": "Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts. Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation , a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference , is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling . More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.",
      "summary_en": "Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.",
      "summary_zh": "å—çº§ä¼˜åŠ¿ä¼°è®¡é€šè¿‡ä¸ºä¸åŒæ–‡æœ¬å—åˆ†é…ç‹¬ç«‹ä¼˜åŠ¿ï¼Œå¹¶ä½¿ç”¨ç»“æœæ¡ä»¶åŸºçº¿æ¥é¿å…æ˜‚è´µçš„åµŒå¥—rolloutï¼Œä»è€Œè§£å†³ç»“æ„åŒ–ç”Ÿæˆä¸­çš„å¥–åŠ±å¹²æ‰°é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10231",
      "arxiv_url": "https://arxiv.org/abs/2602.10231",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10231",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:31:34.375100+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.02192",
      "title": "ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning",
      "authors": [
        "Jie Xiao",
        "Meng Chen",
        "Qingnan Ren",
        "Jingwei Song",
        "Jiaqi Huang",
        "Yangshen Deng",
        "Chris Tong",
        "Wanyi Chen",
        "Suli Wang",
        "Ziqian Bi",
        "Shuo Lu",
        "Yiqun Duan",
        "Xu Wang",
        "Rymon Yu",
        "Ween Yang",
        "Lynn Ai",
        "Eric Yang",
        "Bill Shi",
        "Song Jingwei"
      ],
      "abstract": "ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency. Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation , reward evaluation , and centralized learning . Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation , dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.",
      "summary_en": "ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency.",
      "summary_zh": "ECHO-2æ˜¯ä¸€ç§åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é‡å rolloutç”Ÿæˆã€åˆ†å‘ä¸è®­ç»ƒï¼Œå¹¶ç®¡ç†ç­–ç•¥é™ˆæ—§æ€§å’Œç½‘ç»œå»¶è¿Ÿï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆåè®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02192",
      "arxiv_url": "https://arxiv.org/abs/2602.02192",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02192",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:30:35.995286+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.07106",
      "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
      "authors": [
        "Haoyu Zhang",
        "Zhipeng Li",
        "Yiwen Guo",
        "Tianshu Yu"
      ],
      "abstract": "Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding. Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation . Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation , leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation . Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.",
      "summary_en": "Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.",
      "summary_zh": "Ex-Omniæ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œé€šè¿‡å°†è¯­ä¹‰æ¨ç†ä¸æ—¶åºç”Ÿæˆè§£è€¦ï¼Œå¹¶ä½¿ç”¨è¯­éŸ³å•å…ƒä½œä¸ºæ—¶åºæ”¯æ¶ï¼Œä¸ºå…¨æ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¢å¼ºä¼´éšè¯­éŸ³çš„3Dé¢éƒ¨åŠ¨ç”»èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07106",
      "arxiv_url": "https://arxiv.org/abs/2602.07106",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07106",
      "github_url": "",
      "upvotes": 11,
      "fetched_at": "2026-02-19T06:30:49.746964+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10999",
      "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion",
      "authors": [
        "Yusong Lin",
        "Haiyang Wang",
        "Shuzhe Wu",
        "Lue Fan",
        "Feiyang Pan",
        "Sanyuan Zhao",
        "Dandan Tu"
      ],
      "abstract": "CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.",
      "summary_en": "CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.",
      "summary_zh": "CLI-Gymé€šè¿‡æ¨¡æ‹Ÿå’Œæ¢ç´¢ç¯å¢ƒå†å²ï¼Œå®ç°äº†ç¯å¢ƒå¯†é›†å‹ä»»åŠ¡çš„å¯æ‰©å±•æ¨å¯¼ï¼›LiberCoderåˆ™é€šè¿‡åœ¨Terminal-Benchä¸Šè¿›è¡Œå¾®è°ƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10999",
      "arxiv_url": "https://arxiv.org/abs/2602.10999",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10999",
      "github_url": "https://github.com/LiberCoders/CLI-Gym",
      "upvotes": 10,
      "fetched_at": "2026-02-19T06:32:01.713328+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.09514",
      "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies",
      "authors": [
        "Xavier Hu",
        "Jinxiang Xia",
        "Shengze Xu",
        "Kangqi Song",
        "Yishuo Yuan",
        "Guibin Zhang",
        "JinCheng Ren",
        "Boyu Feng",
        "Li Lu",
        "Tieyong Zeng",
        "Jiaheng Liu",
        "Minghao Liu",
        "He Zhu",
        "Yuchen Eleanor Jiang",
        "Wei Wang",
        "Wangchunshu Zhou"
      ],
      "abstract": "EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation. Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents ; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics . We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies . EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity . Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.",
      "summary_en": "EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.",
      "summary_zh": "EcoGymæå‡ºäº†ä¸€ä¸ªå¯æ³›åŒ–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨å…·æœ‰æŒç»­åŠ¨æ€å’Œå¤šåœºæ™¯è¯„ä¼°çš„äº¤äº’å¼ç»æµç¯å¢ƒä¸­çš„é•¿ç¨‹è§„åˆ’èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09514",
      "arxiv_url": "https://arxiv.org/abs/2602.09514",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09514",
      "github_url": "https://github.com/OPPO-PersonalAI/EcoGym",
      "upvotes": 9,
      "fetched_at": "2026-02-19T06:31:19.034610+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.03773",
      "title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL",
      "authors": [
        "Ian Wu",
        "Yuxiao Qu",
        "Amrith Setlur",
        "Aviral Kumar"
      ],
      "abstract": "RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks. Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation . However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance , due to the improved summary-conditioned generation abilities learned through training.",
      "summary_en": "RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks.",
      "summary_zh": "RCæ˜¯ä¸€ç§è¿­ä»£è§£ç ç®—æ³•ï¼Œå®ƒé€šè¿‡æ„å»ºè·¨è¿­ä»£å¢å¼ºçš„æ¨ç†é“¾ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤–æ¨å¹¶åœ¨è®­ç»ƒé¢„ç®—ä¹‹å¤–æŒç»­æ”¹è¿›ï¼Œä»è€Œåœ¨é•¿ç¨‹ä»»åŠ¡ä¸Šå–å¾—æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03773",
      "arxiv_url": "https://arxiv.org/abs/2602.03773",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03773",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T06:30:40.554196+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.09713",
      "title": "Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models",
      "authors": [
        "Ruisi Zhao",
        "Haoren Zheng",
        "Zongxin Yang",
        "Hehe Fan",
        "Yi Yang"
      ],
      "abstract": "Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis. Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation , we employ the Skeletal Graph VAE ( Sk-VAE ) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT ( Sk-DiT ) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO , where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig : a dataset of textured and rigged meshes with captions, curated from Objaverse-XL . Additionally, we employ a preference optimization strategy, SKA-DPO , guided by a skeleton-mesh alignment score , to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.",
      "summary_en": "Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.",
      "summary_zh": "Stroke3Dé€šè¿‡ç»“åˆå¯æ§éª¨éª¼ç”Ÿæˆä¸å¢å¼ºç½‘æ ¼åˆæˆçš„ä¸¤é˜¶æ®µæµç¨‹ï¼Œä»äºŒç»´ç¬”ç”»å’Œæ–‡æœ¬æç¤ºç”Ÿæˆå¸¦éª¨éª¼ç»‘å®šçš„3Dç½‘æ ¼ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09713",
      "arxiv_url": "https://arxiv.org/abs/2602.09713",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09713",
      "github_url": "https://github.com/Whalesong-zrs/Stroke3D_project_page",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:31:21.373885+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10179",
      "title": "When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models",
      "authors": [
        "Jiacheng Hou",
        "Yining Sun",
        "Ruochong Jin",
        "Haochen Han",
        "Fangming Liu",
        "Wai Kin Victor Chan",
        "Alex Jinpeng Wang"
      ],
      "abstract": "Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms. Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench , a safety-oriented benchmark for image editing models . Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rate s of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning , which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models .",
      "summary_en": "Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.",
      "summary_zh": "è§†è§‰åˆ°è§†è§‰è¶Šç‹±æ”»å‡»é€šè¿‡æ¶æ„è§†è§‰è¾“å…¥ç ´åå›¾åƒç¼–è¾‘æ¨¡å‹ï¼ŒäºŸéœ€æ–°çš„å®‰å…¨åŸºå‡†ä¸é˜²å¾¡æœºåˆ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10179",
      "arxiv_url": "https://arxiv.org/abs/2602.10179",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10179",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:31:27.515195+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.09901",
      "title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
      "authors": [
        "Jianzhao Huang",
        "Xiaorui Huang",
        "Fei Zhao",
        "Yunpeng Liu",
        "Hui Zhang",
        "Fangcheng Shi",
        "Congfeng Li",
        "Zechen Sun",
        "Yi Wu",
        "Yao Hu",
        "Yunhan Bai",
        "Shaosheng Cao"
      ],
      "abstract": "A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance. Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning . Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking . Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.",
      "summary_en": "A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.",
      "summary_zh": "ä¸€ç§ç”¨äºç¤¾äº¤ç½‘ç»œæœç´¢æŸ¥è¯¢å¤„ç†çš„ç»Ÿä¸€ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ æå‡è¯­ä¹‰ç†è§£ï¼ŒåŒæ—¶å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09901",
      "arxiv_url": "https://arxiv.org/abs/2602.09901",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09901",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:31:23.269499+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10748",
      "title": "Benchmarking Large Language Models for Knowledge Graph Validation",
      "authors": [
        "Farzad Shami",
        "Stefano Marchesin",
        "Gianmaria Silvello"
      ],
      "abstract": "Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.",
      "summary_en": "Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±äº‹å®éªŒè¯æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç¼ºä¹ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œä¸”æ£€ç´¢å¢å¼ºç”Ÿæˆä¸å¤šæ¨¡å‹å…±è¯†æ–¹æ³•æ‰€å¸¦æ¥çš„æ”¹è¿›æ•ˆæœå‚å·®ä¸é½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10748",
      "arxiv_url": "https://arxiv.org/abs/2602.10748",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10748",
      "github_url": "https://github.com/FactCheck-AI/FactCheck",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:31:52.445467+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10229",
      "title": "Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens",
      "authors": [
        "Weihao Liu",
        "Dehai Min",
        "Lu Cheng"
      ],
      "abstract": "Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach. While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states , our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space . Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy .",
      "summary_en": "Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.",
      "summary_zh": "æ½œåœ¨æ€ç»´å¾®è°ƒæå‡ºäº†ä¸€ç§åœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†çš„æ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆä¸Šä¸‹æ–‡éšè—çŠ¶æ€ä¸é¢„æµ‹æ€§è¯­ä¹‰å¼•å¯¼ï¼Œå¹¶åˆ©ç”¨æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ æ–¹æ³•å®ç°é²æ£’æ¨æ–­ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10229",
      "arxiv_url": "https://arxiv.org/abs/2602.10229",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10229",
      "github_url": "https://github.com/NeosKnight233/Latent-Thoughts-Tuning",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:31:32.481759+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08489",
      "title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
      "authors": [
        "Hyunseok Lee",
        "Soheil Abbasloo",
        "Jihoon Tack",
        "Jinwoo Shin"
      ],
      "abstract": "Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities. Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning , but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy , and it reaches comparable performance in substantially fewer training steps. For example, on MATH500 , RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.",
      "summary_en": "Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.",
      "summary_zh": "å¯è¿ç§»å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLTRï¼‰é€šè¿‡å¯è¿ç§»å¥–åŠ±æµ‹è¯•è·¨æ¨¡å‹å¼•å¯¼èƒ½åŠ›ï¼Œç¡®ä¿æ¨ç†ç¨³å®šæ€§ä¸æ³›åŒ–æ€§ï¼Œä»è€Œå¢å¼º LLM æ¨ç†é²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08489",
      "arxiv_url": "https://arxiv.org/abs/2602.08489",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08489",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:31:05.855540+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08030",
      "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models",
      "authors": [
        "Yilun Zheng",
        "Dongyang Ma",
        "Tian Liang",
        "Jiahao Xu",
        "Xinting Huang",
        "Lihui Chen",
        "Haitao Mi",
        "Yan Wang"
      ],
      "abstract": "Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks. Reasoning models enhance problem-solving by scaling test-time compute , yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module , a plug-and-play LoRA adapter . By iteratively switching between reasoning and cleaning mode s, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state. Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale . Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.",
      "summary_en": "Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.",
      "summary_zh": "Free()LM é€šè¿‡ Free-Module å³æ’å³ç”¨ LoRA é€‚é…å™¨å¼•å…¥è‡ªé—å¿˜æœºåˆ¶ï¼Œè§£å†³æ¨ç†æ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡è·¨è§„æ¨¡åŠé•¿ç¨‹ä»»åŠ¡çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08030",
      "arxiv_url": "https://arxiv.org/abs/2602.08030",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08030",
      "github_url": "https://github.com/TemporaryLoRA/FreeLM",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:30:56.864706+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.07954",
      "title": "Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation",
      "authors": [
        "Krzysztof WrÃ³bel",
        "Jan Maria Kowalski",
        "Jerzy Surma",
        "Igor Ciuciura",
        "Maciej SzymaÅ„ski"
      ],
      "abstract": "Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.",
      "summary_en": "Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.",
      "summary_zh": "Bielik Guard æ˜¯ä¸€ä¸ªç´§å‡‘çš„æ³¢å…°è¯­å®‰å…¨åˆ†ç±»å™¨ç³»åˆ—ï¼ŒåŒ…å«ä¸¤ç§å˜ä½“ï¼Œå¯æœ‰æ•ˆå¯¹å†…å®¹è¿›è¡Œè·¨äº”ä¸ªå®‰å…¨é¢†åŸŸçš„åˆ†ç±»ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07954",
      "arxiv_url": "https://arxiv.org/abs/2602.07954",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07954",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:30:54.403375+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.07900",
      "title": "Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents",
      "authors": [
        "Zhi Chen",
        "Zhensu Sun",
        "Yuling Shi",
        "Chao Peng",
        "Xiaodong Gu",
        "David Lo",
        "Lingxiao Jiang"
      ],
      "abstract": "Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods. Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget. To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks . Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing . The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.",
      "summary_en": "Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.",
      "summary_zh": "å¯¹LLMä»£ç æ™ºèƒ½ä½“çš„å®è¯åˆ†æè¡¨æ˜ï¼Œæµ‹è¯•ç¼–å†™å¯¹é—®é¢˜è§£å†³çš„æå‡æœ‰é™ï¼Œä¸”å¸¸è¢«åŸºäºè§‚å¯Ÿçš„è°ƒè¯•æ–¹æ³•æ‰€å–ä»£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07900",
      "arxiv_url": "https://arxiv.org/abs/2602.07900",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07900",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:30:52.316520+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.06008",
      "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
      "authors": [
        "Xianyang Liu",
        "Shangding Gu",
        "Dawn Song"
      ],
      "abstract": "AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios. Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning , establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
      "summary_en": "AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.",
      "summary_zh": "AgenticPay æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ™ºèƒ½ä½“è¯­è¨€ä»‹å¯¼ç»æµäº¤äº’çš„åŸºå‡†æµ‹è¯•ä¸ä»¿çœŸæ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨å¤æ‚å¸‚åœºåœºæ™¯ä¸­çš„è°ˆåˆ¤è¡¨ç°ä¸ç­–ç•¥æ¨ç†æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06008",
      "arxiv_url": "https://arxiv.org/abs/2602.06008",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06008",
      "github_url": "https://github.com/SafeRL-Lab/AgenticPay",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:30:44.665405+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.09014",
      "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
      "authors": [
        "Zihan Yang",
        "Shuyuan Tu",
        "Licheng Zhang",
        "Qi Dai",
        "Yu-Gang Jiang",
        "Zuxuan Wu"
      ],
      "abstract": "ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training. Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps , motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes . This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory . To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.",
      "summary_en": "ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.",
      "summary_zh": "ArcFlowæ˜¯ä¸€ç§å°‘æ­¥è’¸é¦æ¡†æ¶ï¼Œåˆ©ç”¨éçº¿æ€§æµè½¨è¿¹é€¼è¿‘æ•™å¸ˆæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è½»é‡çº§é€‚é…å™¨è®­ç»ƒå®ç°å¿«é€Ÿæ¨ç†ä¸”è´¨é‡æŸå¤±æå°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09014",
      "arxiv_url": "https://arxiv.org/abs/2602.09014",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09014",
      "github_url": "https://github.com/pnotp/ArcFlow",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:31:16.351095+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.02459",
      "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
      "authors": [
        "Zhiyu Huang",
        "Yun Zhang",
        "Johnson Liu",
        "Rui Song",
        "Chen Tang",
        "Jiaqi Ma"
      ],
      "abstract": "Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training. Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control . Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning , aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/",
      "summary_en": "Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training.",
      "summary_zh": "æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹é€šè¿‡å»¶è¿Ÿæ„ŸçŸ¥æ¡†æ¶å¾—åˆ°å¢å¼ºï¼Œè¯¥æ¡†æ¶é€šè¿‡å»¶è¿Ÿè¯­ä¹‰æ§åˆ¶æ¥å£å’Œå»¶è¿Ÿä¸€è‡´æ€§è®­ç»ƒï¼Œåœ¨å®æ—¶åŠ¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹å»¶è¿Ÿçš„è¯­ä¹‰æ¨ç†è¿›è¡Œè¡¥å¿ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02459",
      "arxiv_url": "https://arxiv.org/abs/2602.02459",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02459",
      "github_url": "https://github.com/ucla-mobility/TIC-VLA",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:30:38.261690+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.11137",
      "title": "Weight Decay Improves Language Model Plasticity",
      "authors": [
        "Tessa Han",
        "Sebastian Bordt",
        "Hanlin Zhang",
        "Sham Kakade"
      ],
      "abstract": "Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting. The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity , that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning . We focus on the role of weight decay , a key regularization parameter during pretraining . Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning . Further investigation of weight decay 's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices , and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.",
      "summary_en": "Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.",
      "summary_zh": "ä½¿ç”¨æ›´å¤§çš„æƒé‡è¡°å‡å€¼è¿›è¡Œé¢„è®­ç»ƒå¯é€šè¿‡ä¿ƒè¿›çº¿æ€§å¯åˆ†çš„è¡¨å¾å¹¶å‡å°‘è¿‡æ‹Ÿåˆï¼Œæå‡æ¨¡å‹å¯å¡‘æ€§ä¸ä¸‹æ¸¸å¾®è°ƒæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11137",
      "arxiv_url": "https://arxiv.org/abs/2602.11137",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11137",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:32:13.042424+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10778",
      "title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation",
      "authors": [
        "Maximilian Thang",
        "Lichao Wu",
        "Sasha Behrouzi",
        "Mohamadreza Rostami",
        "Jona te Lintelo",
        "Stjepan Picek",
        "Ahmad-Reza Sadeghi"
      ],
      "abstract": "GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs. Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations , which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control. We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering , enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters , and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.",
      "summary_en": "GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.",
      "summary_zh": "GoodVibeæ˜¯ä¸€ç§ç¥ç»å…ƒçº§æ¡†æ¶ï¼Œé€šè¿‡å¯¹å®‰å…¨ç›¸å…³ç¥ç»å…ƒè¿›è¡Œé’ˆå¯¹æ€§å¾®è°ƒæ¥å¢å¼ºä»£ç è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ•ˆç”¨å¹¶æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10778",
      "arxiv_url": "https://arxiv.org/abs/2602.10778",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10778",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:31:55.008381+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10652",
      "title": "UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory",
      "authors": [
        "Yongshi Ye",
        "Hui Jiang",
        "Feihu Jiang",
        "Tian Lan",
        "Yichao Du",
        "Biao Fu",
        "Xiaodong Shi",
        "Qianghuai Jia",
        "Longyue Wang",
        "Weihua Luo"
      ],
      "abstract": "A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards. Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO . This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks . Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.",
      "summary_en": "A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.",
      "summary_zh": "ä¸€ç§é¢å‘LLMæ™ºèƒ½ä½“çš„è®°å¿†æå–ä¸ç®¡ç†ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰é‚»åŸŸå»ºæ¨¡å’Œè¾¹é™…æ•ˆç”¨å¥–åŠ±æå‡æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10652",
      "arxiv_url": "https://arxiv.org/abs/2602.10652",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10652",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:31:48.118574+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08995",
      "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
      "authors": [
        "Yuting Ning",
        "Jaylen Jones",
        "Zhehao Zhang",
        "Chentao Ye",
        "Weitong Ruan",
        "Junyi Li",
        "Rahul Gupta",
        "Huan Sun"
      ],
      "abstract": "Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution. Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection ) or from internal limitations (e.g., erroneous reasoning ). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions . We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback . DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
      "summary_en": "Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.",
      "summary_zh": "è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“é¢ä¸´ç”±å¤–éƒ¨æ”»å‡»æˆ–å†…éƒ¨é™åˆ¶å¯¼è‡´çš„æœªå¯¹é½è¡Œä¸ºçš„å®‰å…¨é£é™©ï¼Œå‚¬ç”Ÿäº†DeActionâ€”â€”ä¸€ç§å¯åœ¨æ‰§è¡Œå‰æ£€æµ‹å¹¶çº æ­£æ­¤ç±»è¡Œä¸ºçš„æŠ¤æ æœºåˆ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08995",
      "arxiv_url": "https://arxiv.org/abs/2602.08995",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08995",
      "github_url": "https://github.com/OSU-NLP-Group/Misaligned-Action-Detection",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:31:13.936066+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08741",
      "title": "Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing",
      "authors": [
        "Jona te Lintelo",
        "Lichao Wu",
        "Stjepan Picek"
      ],
      "abstract": "Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility. The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods . Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.",
      "summary_en": "Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.",
      "summary_zh": "æ”»å‡»æ–¹æ³•åˆ©ç”¨æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹çš„ä¸“å®¶è·¯ç”±åŠ¨æ€ï¼Œåœ¨ä¿æŒè¯­è¨€å®ç”¨æ€§çš„åŒæ—¶ç ´åå®‰å…¨å¯¹é½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08741",
      "arxiv_url": "https://arxiv.org/abs/2602.08741",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08741",
      "github_url": "https://github.com/jonatelintelo/LargeLanguageLobotomy",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:31:09.778443+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10870",
      "title": "FedPS: Federated data Preprocessing via aggregated Statistics",
      "authors": [
        "Xuefeng Xu",
        "Graham Cormode"
      ],
      "abstract": "FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets. Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics . FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling , encoding , discretization , and missing-value imputation , and extend preprocessing-related models such as k-Means , k-Nearest Neighbors , and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.",
      "summary_en": "FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.",
      "summary_zh": "FedPSæ˜¯ä¸€ç§è”é‚¦æ•°æ®é¢„å¤„ç†æ¡†æ¶ï¼Œåˆ©ç”¨èšåˆç»Ÿè®¡å’Œæ•°æ®è‰å›¾æŠ€æœ¯ï¼Œä¸ºè·¨åˆ†å¸ƒå¼æ•°æ®é›†çš„åä½œå¼æœºå™¨å­¦ä¹ æä¾›é«˜æ•ˆä¸”ä¿æŠ¤éšç§çš„æ•°æ®å‡†å¤‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10870",
      "arxiv_url": "https://arxiv.org/abs/2602.10870",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10870",
      "github_url": "https://github.com/xuefeng-xu/fedps",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:31:57.456392+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.10699",
      "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation",
      "authors": [
        "Jie Jiang",
        "Yangru Huang",
        "Zeyu Wang",
        "Changping Wang",
        "Yuling Xiong",
        "Jun Zhang",
        "Huan Yu"
      ],
      "abstract": "V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality. Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch . Conventional likelihood-dominated decoding (e.g., beam search ) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration , where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression , where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO , which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.",
      "summary_en": "V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.",
      "summary_zh": "V-STARç»“åˆä»·å€¼å¼•å¯¼è§£ç ä¸æ ‘ç»“æ„ä¼˜åŠ¿å¼ºåŒ–ï¼Œè§£å†³ç”Ÿæˆå¼æ¨èçš„å±€é™æ€§ï¼Œæå‡æ¢ç´¢ä¸å¥–åŠ±ä¿¡å·è´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10699",
      "arxiv_url": "https://arxiv.org/abs/2602.10699",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10699",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:31:50.275377+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08052",
      "title": "Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling",
      "authors": [
        "Bulent Soykan",
        "Sean Mondesire",
        "Ghaith Rabadi",
        "Grace Bochenek"
      ],
      "abstract": "A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time. The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.",
      "summary_en": "A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.",
      "summary_zh": "ä¸€ç§ç»“åˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä¸å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆå¹³è¡¡æ€»åŠ æƒæ‹–æœŸä¸æ€»å‡†å¤‡æ—¶é—´ï¼Œè§£å†³å¤šç›®æ ‡è°ƒåº¦æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08052",
      "arxiv_url": "https://arxiv.org/abs/2602.08052",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08052",
      "github_url": "https://github.com/bulentsoykan/GNN-DRL4UPMSP",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:30:59.288274+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.08934",
      "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
      "authors": [
        "Suraj Ranganath",
        "Atharv Ramesh"
      ],
      "abstract": "StealthRL uses reinforcement learning with LoRA adapters to create adversarial paraphrases that evade multiple AI text detectors while preserving meaning, demonstrating significant robustness gaps in current detection systems. AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B , optimizing a composite reward that balances detector evasion with semantic preservation . We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate . Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring , analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals . Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.",
      "summary_en": "StealthRL uses reinforcement learning with LoRA adapters to create adversarial paraphrases that evade multiple AI text detectors while preserving meaning, demonstrating significant robustness gaps in current detection systems.",
      "summary_zh": "StealthRLåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸LoRAé€‚é…å™¨ç”Ÿæˆå¯è§„é¿å¤šä¸ªAIæ–‡æœ¬æ£€æµ‹å™¨çš„å¯¹æŠ—æ€§é‡Šä¹‰ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ï¼Œæ˜¾ç¤ºå‡ºç°æœ‰æ£€æµ‹ç³»ç»Ÿå­˜åœ¨æ˜¾è‘—çš„é²æ£’æ€§ç¼ºé™·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08934",
      "arxiv_url": "https://arxiv.org/abs/2602.08934",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08934",
      "github_url": "https://github.com/suraj-ranganath/StealthRL",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:31:11.850793+00:00"
    },
    {
      "date": "2026-02-12",
      "paper_id": "2602.06841",
      "title": "From Features to Actions: Explainability in Traditional and Agentic AI Systems",
      "authors": [
        "Sindhuja Chaduvula",
        "Jessee Ho",
        "Kina Kim",
        "Aravind Narayanan",
        "Mahshid Alinoori",
        "Muskan Garg",
        "Dhanesh Ramachandram",
        "Shaina Raza"
      ],
      "abstract": "Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable. Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks ( TAU-bench Airline and AssistantBench ). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman Ï= 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories . In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7times more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour. Resources: https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework",
      "summary_en": "Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable.",
      "summary_zh": "é™æ€ä¸æ™ºèƒ½ä½“å¯è§£é‡Šæ€§æ–¹æ³•åœ¨è§£é‡Šæ¨¡å‹è¡Œä¸ºçš„èƒ½åŠ›ä¸Šå­˜åœ¨å·®å¼‚ï¼šå½’å› æ–¹æ³•å¯¹å•ä¸ªé¢„æµ‹æœ‰æ•ˆï¼Œä½†ä¸è¶³ä»¥è¯Šæ–­å¤šæ­¥å†³ç­–è¿‡ç¨‹ä¸­çš„å¤±è´¥ï¼Œè€ŒåŸºäºè½¨è¿¹çš„è¯Šæ–­æ–¹æ³•åˆ™æ›´ä¸ºå¯é ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06841",
      "arxiv_url": "https://arxiv.org/abs/2602.06841",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06841",
      "github_url": "https://github.com/VectorInstitute/unified-xai-evaluation-framework",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:30:46.941644+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05400",
      "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
      "authors": [
        "Shaobo Wang",
        "Xuan Ouyang",
        "Tianyi Xu",
        "Yuzheng Hu",
        "Jialin Liu",
        "Guo Chen",
        "Tianyu Zhang",
        "Junhao Zheng",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "abstract": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead. As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space . OPUS scores candidates by projecting their effective updates , shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia , OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "summary_en": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.",
      "summary_zh": "OPUSæ˜¯ä¸€ç§åŠ¨æ€æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç¨³å®šçš„ä»£ç†æ´¾ç”Ÿç›®æ ‡ç©ºé—´ä¸­åŸºäºä¼˜åŒ–å™¨è¯±å¯¼çš„æ›´æ–°æŠ•å½±å¯¹æ•°æ®å€™é€‰è¿›è¡Œè¯„åˆ†æ¥æå‡é¢„è®­ç»ƒæ•ˆç‡ï¼Œä»è€Œåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶å®ç°æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05400",
      "arxiv_url": "https://arxiv.org/abs/2602.05400",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05400",
      "github_url": "",
      "upvotes": 316,
      "fetched_at": "2026-02-19T06:20:41.061634+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09856",
      "title": "Code2World: A GUI World Model via Renderable Code Generation",
      "authors": [
        "Yuhao Zheng",
        "Li'an Zhong",
        "Yi Wang",
        "Rui Dai",
        "Kaikui Liu",
        "Xiangxiang Chu",
        "Linyuan Lv",
        "Philip Torr",
        "Kevin Qinghong Lin"
      ],
      "abstract": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.",
      "summary_en": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.",
      "summary_zh": "Code2Worldé€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆä½¿è‡ªä¸»GUIæ™ºèƒ½ä½“èƒ½å¤Ÿé¢„æµ‹ä¸‹ä¸€è§†è§‰çŠ¶æ€ï¼Œå®ç°é«˜è§†è§‰ä¿çœŸåº¦ä¸ç»“æ„å¯æ§æ€§ï¼ŒåŒæ—¶æå‡å¯¼èˆªæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09856",
      "arxiv_url": "https://arxiv.org/abs/2602.09856",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09856",
      "github_url": "https://github.com/AMAP-ML/Code2World",
      "upvotes": 189,
      "fetched_at": "2026-02-19T06:22:03.257257+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09082",
      "title": "UI-Venus-1.5 Technical Report",
      "authors": [
        "Veuns-Team",
        "Changlong Gao",
        "Zhangxuan Gu",
        "Yulin Liu",
        "Xinyu Qiu",
        "Shuheng Shen",
        "Yue Wen",
        "Tianyu Xia",
        "Zhenyu Xu",
        "Zhengwen Zeng",
        "Beitong Zhou",
        "Xingran Zhou",
        "Weizhi Chen",
        "Sunhao Dai",
        "Jingya Dou",
        "Yichen Gong",
        "Yuan Guo",
        "Zhenlin Guo",
        "Feng Li",
        "Qian Li",
        "Jinzhen Lin",
        "Yuqi Zhou"
      ],
      "abstract": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.",
      "summary_en": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.",
      "summary_zh": "UI-Venus-1.5æ˜¯ä¸€ä¸ªé€šè¿‡ä¸­æœŸè®­ç»ƒé˜¶æ®µã€åœ¨çº¿å¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹åˆå¹¶æŠ€æœ¯æå‡æ€§èƒ½çš„ç»Ÿä¸€GUIæ™ºèƒ½ä½“ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09082",
      "arxiv_url": "https://arxiv.org/abs/2602.09082",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09082",
      "github_url": "https://github.com/inclusionAI/UI-Venus",
      "upvotes": 149,
      "fetched_at": "2026-02-19T06:21:38.863993+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10063",
      "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
      "authors": [
        "Tianyi Jiang",
        "Arctanx An",
        "Hengyi Feng",
        "Naixin Zhai",
        "Haodong Li",
        "Xiaomin Yu",
        "Jiahui Liu",
        "Hanwen Du",
        "Shuo Zhang",
        "Zhi Yang",
        "Jie Huang",
        "Yuhua Li",
        "Yongxin Ni",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "abstract": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches. Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a com mon trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset ( CoM ), a training-free agentic framework that enables step-level adaptive mindset orchestration . CoM de com poses reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency . Our code is publicly available at https://github. com /QuantaAlpha/chain-of-mindset{https://github. com /QuantaAlpha/chain-of-mindset}.",
      "summary_en": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.",
      "summary_zh": "ä¸€ç§åä¸º Chain of Mindset çš„æ–°å‹å…è®­ç»ƒæ¡†æ¶é€šè¿‡æ•´åˆç©ºé—´ã€æ”¶æ•›ã€å‘æ•£å’Œç®—æ³•æ¨ç†æ–¹æ³•ï¼Œå®ç°äº†é¢å‘å¤§è¯­è¨€æ¨¡å‹çš„æ­¥éª¤çº§åˆ«è‡ªé€‚åº”æ€ç»´ç¼–æ’ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10063",
      "arxiv_url": "https://arxiv.org/abs/2602.10063",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10063",
      "github_url": "https://github.com/QuantaAlpha/chain-of-mindset",
      "upvotes": 70,
      "fetched_at": "2026-02-19T06:22:07.821598+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08234",
      "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
      "authors": [
        "Peng Xia",
        "Jianwen Chen",
        "Hanyang Wang",
        "Jiaqi Liu",
        "Kaide Zeng",
        "Yu Wang",
        "Siwei Han",
        "Yiyang Zhou",
        "Xujiang Zhao",
        "Haifeng Chen",
        "Zeyu Zheng",
        "Cihang Xie",
        "Huaxiu Yao"
      ],
      "abstract": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead. Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution . Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank , an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning . These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
      "summary_en": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.",
      "summary_zh": "SkillRLä½¿LLMæ™ºèƒ½ä½“é€šè¿‡å±‚æ¬¡åŒ–æŠ€èƒ½å‘ç°ä¸é€’å½’ç­–ç•¥æ¼”åŒ–å®ç°æ”¹è¿›ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸Šå–å¾—æ›´ä¼˜æ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—å¼€é”€ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08234",
      "arxiv_url": "https://arxiv.org/abs/2602.08234",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08234",
      "github_url": "https://github.com/aiming-lab/SkillRL",
      "upvotes": 65,
      "fetched_at": "2026-02-19T06:21:16.803051+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09443",
      "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
      "authors": [
        "Yun Luo",
        "Futing Wang",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haodi Lei",
        "Jianhao Yan",
        "Chenxi Li",
        "Jiacheng Chen",
        "Yufeng Zhao",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Wenxuan Zeng",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Yulun Wu",
        "Rui Huang"
      ],
      "abstract": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception. The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning . Our method harmonizes Curriculum Reinforcement Learning , which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation , enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro . Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
      "summary_en": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.",
      "summary_zh": "é¢å‘ç‰©ç†çš„è§†è§‰-è¯­è¨€æ¨¡å‹åˆ©ç”¨è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ å’Œæ™ºèƒ½ä½“å¢å¼ºï¼Œåœ¨é€šè¿‡å¤šæ¨¡æ€æ„ŸçŸ¥ä¿æŒç‰©ç†ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„ç§‘å­¦æ¨ç†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09443",
      "arxiv_url": "https://arxiv.org/abs/2602.09443",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09443",
      "github_url": "https://github.com/PRIME-RL/P1-VL",
      "upvotes": 57,
      "fetched_at": "2026-02-19T06:21:51.230976+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10090",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "authors": [
        "Zhaoyang Wang",
        "Canwen Xu",
        "Boyi Liu",
        "Yite Wang",
        "Siwei Han",
        "Zhewei Yao",
        "Huaxiu Yao",
        "Yuxiong He"
      ],
      "abstract": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches. Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents . Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization . The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
      "summary_en": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.",
      "summary_zh": "åœ¨åˆæˆç¯å¢ƒä¸­é€šè¿‡ä»£ç é©±åŠ¨æ¨¡æ‹Ÿå’Œæ•°æ®åº“æ”¯æŒçŠ¶æ€è½¬æ¢è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šåŸºå‡†çš„æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´ä¼˜çš„åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10090",
      "arxiv_url": "https://arxiv.org/abs/2602.10090",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10090",
      "github_url": "https://github.com/Snowflake-Labs/agent-world-model",
      "upvotes": 49,
      "fetched_at": "2026-02-19T06:22:10.439986+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08426",
      "title": "Prism: Spectral-Aware Block-Sparse Attention",
      "authors": [
        "Xinghao Wang",
        "Pengyu Wang",
        "Xiaoran Liu",
        "Fangxu Liu",
        "Jason Chu",
        "Kai Song",
        "Xipeng Qiu"
      ],
      "abstract": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration. Block-sparse attention is promising for accelerating long-context LLM pre-filling , yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings ( RoPE ). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration , Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.",
      "summary_en": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.",
      "summary_zh": "Prism é‡‡ç”¨é¢‘è°±æ„ŸçŸ¥æ–¹æ³•è§£å†³é•¿ä¸Šä¸‹æ–‡ LLM é¢„å¡«å……ä¸­å—ç¨€ç–æ³¨æ„åŠ›çš„ä½æ•ˆé—®é¢˜ï¼Œè¯¥æ–¹æ³•é€šè¿‡åŸºäºèƒ½é‡çš„æ¸©åº¦æ ¡å‡†æé«˜å—é€‰æ‹©å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08426",
      "arxiv_url": "https://arxiv.org/abs/2602.08426",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08426",
      "github_url": "https://github.com/xinghaow99/prism",
      "upvotes": 35,
      "fetched_at": "2026-02-19T06:21:23.446259+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07035",
      "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
      "authors": [
        "Jiahao Zhao",
        "Shaoxuan Xu",
        "Zhongxiang Sun",
        "Fengqi Zhu",
        "Jingyang Ou",
        "Yuling Shi",
        "Chongxuan Li",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm. Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge : the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm . Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge , we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct . P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C",
      "summary_en": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.",
      "summary_zh": "æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹é’ˆå¯¹æœç´¢æ™ºèƒ½ä½“ä¼˜åŒ–ï¼Œé€šè¿‡å¢å¼ºæ¨ç†èƒ½åŠ›å¹¶å€ŸåŠ©å¹¶è¡Œæ¨ç†èŒƒå¼é™ä½å»¶è¿Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07035",
      "arxiv_url": "https://arxiv.org/abs/2602.07035",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07035",
      "github_url": "https://github.com/bubble65/DLLM-Searcher",
      "upvotes": 30,
      "fetched_at": "2026-02-19T06:20:54.641223+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09084",
      "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
      "authors": [
        "Ruijie Ye",
        "Jiayi Zhang",
        "Zhuoxin Liu",
        "Zihao Zhu",
        "Siyuan Yang",
        "Li Li",
        "Tianfu Fu",
        "Franck Dernoncourt",
        "Yue Zhao",
        "Jiacheng Zhu",
        "Ryan Rossi",
        "Wenhao Chai",
        "Zhengzhong Tu"
      ],
      "abstract": "Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution. We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing . Agent Banana introduces two key mechanisms: (1) Context Folding , which compresses long interaction histories into structured memory for stable long-horizon control ; and (2) Image Layer Decomposition , which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs . To support rigorous evaluation, we build HDD-Bench , a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench , Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.",
      "summary_en": "Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.",
      "summary_zh": "Agent Banana é€šè¿‡å…·æœ‰ä¸Šä¸‹æ–‡æŠ˜å å’Œå›¾åƒå±‚åˆ†è§£çš„åˆ†å±‚æ¡†æ¶ï¼Œè§£å†³äº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä¸­çš„æŒ‘æˆ˜ï¼Œå®ç°äº†è¶…é«˜åˆ†è¾¨ç‡ä¸‹çš„é«˜ä¿çœŸå¤šè½®ç¼–è¾‘ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09084",
      "arxiv_url": "https://arxiv.org/abs/2602.09084",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09084",
      "github_url": "https://github.com/taco-group/agent-banana",
      "upvotes": 27,
      "fetched_at": "2026-02-19T06:21:40.767811+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10104",
      "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
      "authors": [
        "Yuxin Jiang",
        "Yuchao Gu",
        "Ivor W. Tsang",
        "Mike Zheng Shou"
      ],
      "abstract": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models. Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce SeqÎ”-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder . Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
      "summary_en": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.",
      "summary_zh": "åºåˆ—çº§æ§åˆ¶-æ•ˆæœå¯¹é½æ”¯æŒè§†é¢‘ä¸–ç•Œæ¨¡å‹å­¦ä¹ ç»“æ„åŒ–æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼Œå®ç°é›¶æ ·æœ¬åŠ¨ä½œè¿ç§»ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10104",
      "arxiv_url": "https://arxiv.org/abs/2602.10104",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10104",
      "github_url": "https://github.com/showlab/Olaf-World",
      "upvotes": 26,
      "fetched_at": "2026-02-19T06:22:20.128449+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08847",
      "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
      "authors": [
        "Lang Feng",
        "Longtao Zheng",
        "Shuo He",
        "Fuxiang Zhang",
        "Bo An"
      ],
      "abstract": "Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability. Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems . We show that under GRPO-style optimization , a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability . Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems . Dr. MAS uses an agent-wise remedy : normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems , supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.",
      "summary_en": "Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.",
      "summary_zh": "å¤šæ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ¨å¼ºåŒ–å­¦ä¹ ä¸­å› å…¨å±€å½’ä¸€åŒ–ä¸åŒ¹é…è€Œé¢ä¸´è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼ŒDr. MASé€šè¿‡æ™ºèƒ½ä½“ç‰¹å®šçš„ä¼˜åŠ¿å½’ä¸€åŒ–ä¸å¢å¼ºçš„è®­ç»ƒç¨³å®šæ€§è§£å†³è¯¥é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08847",
      "arxiv_url": "https://arxiv.org/abs/2602.08847",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08847",
      "github_url": "https://github.com/langfengQ/DrMAS",
      "upvotes": 24,
      "fetched_at": "2026-02-19T06:21:29.488458+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07422",
      "title": "Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model",
      "authors": [
        "Tianyi Wu",
        "Mingzhe Du",
        "Yue Liu",
        "Chengran Yang",
        "Terry Yue Zhuo",
        "Jiaheng Zhang",
        "See-Kiong Ng"
      ],
      "abstract": "SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling. Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation . SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision . Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.",
      "summary_en": "SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling.",
      "summary_zh": "SecCoderX åˆ©ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¯¹é½å¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¿ç•™åŠŸèƒ½æ€§çš„åŒæ—¶å®ç°å®‰å…¨ä»£ç ç”Ÿæˆï¼Œå¹¶é€šè¿‡é›†æˆæ¼æ´æ£€æµ‹ä¸å¥–åŠ±å»ºæ¨¡è§£å†³åŠŸèƒ½ä¸å®‰å…¨æ€§çš„æƒè¡¡é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07422",
      "arxiv_url": "https://arxiv.org/abs/2602.07422",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07422",
      "github_url": "https://github.com/AndrewWTY/SecCoderX",
      "upvotes": 21,
      "fetched_at": "2026-02-19T06:21:03.217258+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.00268",
      "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
      "authors": [
        "Ariel Shaulov",
        "Eitan Shaar",
        "Amit Edenzon",
        "Lior Wolf"
      ],
      "abstract": "Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency. Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift , where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation . Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space .",
      "summary_en": "Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.",
      "summary_zh": "è‡ªå›å½’è§†é¢‘ç”Ÿæˆå› æ½œåœ¨æ¡ä»¶tokençš„è¯¯å·®ç´¯ç§¯è€Œå­˜åœ¨æ—¶é—´æ¼‚ç§»ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¯†åˆ«å¹¶ç§»é™¤ä¸ç¨³å®štokenä»¥æå‡é•¿ç¨‹ä¸€è‡´æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00268",
      "arxiv_url": "https://arxiv.org/abs/2602.00268",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00268",
      "github_url": "https://github.com/arielshaulov/TokenTrim",
      "upvotes": 21,
      "fetched_at": "2026-02-19T06:20:19.280811+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07022",
      "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
      "authors": [
        "Yucheng Zhou",
        "Hao Li",
        "Jianbing Shen"
      ],
      "abstract": "Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory. Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion loss es. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss , highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss , demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition error s and leads to a stable condition distribution . Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution , effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.",
      "summary_en": "Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.",
      "summary_zh": "é‡‡ç”¨æ‰©æ•£æŸå¤±çš„è‡ªå›å½’æ¨¡å‹é€šè¿‡å—å»å™ªä¼˜åŒ–ä»¥åŠåŸºäºæœ€ä¼˜ä¼ è¾“ç†è®ºçš„æ¡ä»¶ç»†åŒ–ï¼Œæœ‰æ•ˆç¼“è§£æ¡ä»¶è¯¯å·®ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07022",
      "arxiv_url": "https://arxiv.org/abs/2602.07022",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07022",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T06:20:52.223477+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04208",
      "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
      "authors": [
        "Hyeonbeom Choi",
        "Daechul Ahn",
        "Youhan Lee",
        "Taewook Kang",
        "Seongwon Cho",
        "Jonghyun Choi"
      ],
      "abstract": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes. Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity , where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on ' self-uncertainty ', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
      "summary_en": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.",
      "summary_zh": "SCALEæ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ–°å‹æ¨ç†ç­–ç•¥ï¼Œå®ƒåŸºäºè‡ªä¸ç¡®å®šæ€§è”åˆè°ƒèŠ‚è§†è§‰æ„ŸçŸ¥ä¸åŠ¨ä½œï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤šæ¬¡å‰å‘ä¼ æ’­å³å¯æå‡é²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04208",
      "arxiv_url": "https://arxiv.org/abs/2602.04208",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04208",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T06:20:30.145387+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.00462",
      "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
      "authors": [
        "Benno Krojer",
        "Shravan Nayak",
        "Oscar MaÃ±as",
        "Vaibhav Adlakha",
        "Desmond Elliott",
        "Siva Reddy",
        "Marius Mosbach"
      ],
      "abstract": "LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought. Transforming a large language model ( LLM ) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM . Intriguingly, this mapping can be as simple as a shallow MLP transformation . To understand why LLM s can so readily process visual tokens , we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens . With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations .",
      "summary_en": "LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.",
      "summary_zh": "LatentLensé€šè¿‡å°†è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰tokenè¡¨ç¤ºä¸ä¸Šä¸‹æ–‡æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œå®ç°äº†å¯¹å…¶çš„è§£é‡Šï¼Œå¹¶æ­ç¤ºè§†è§‰tokenæ¯”ä¹‹å‰è®¤ä¸ºçš„æ›´å…·å¯è§£é‡Šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00462",
      "arxiv_url": "https://arxiv.org/abs/2602.00462",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00462",
      "github_url": "https://github.com/McGill-NLP/latentlens",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:20:21.376631+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10098",
      "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
      "authors": [
        "Jingwen Sun",
        "Wenyao Zhang",
        "Zekun Qi",
        "Shaojie Ren",
        "Zezhi Liu",
        "Hanxin Zhu",
        "Guangzhong Sun",
        "Xin Jin",
        "Zhibo Chen"
      ],
      "abstract": "VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks. Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions , making them vulnerable to appearance bias , nuisance motion , and information leakage . We introduce VLA- JEPA , a JEPA -style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames , while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA- JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes . This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA- JEPA achieves consistent gains in generalization and robustness over existing methods.",
      "summary_en": "VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.",
      "summary_zh": "VLA-JEPAæ˜¯ä¸€ç§JEPAé£æ ¼çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­çš„æ— æ³„æ¼çŠ¶æ€é¢„æµ‹æ”¹è¿›è§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥å­¦ä¹ ï¼Œæå‡æ“ä½œä»»åŠ¡çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10098",
      "arxiv_url": "https://arxiv.org/abs/2602.10098",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10098",
      "github_url": "https://github.com/ginwind/VLA-JEPA",
      "upvotes": 17,
      "fetched_at": "2026-02-19T06:22:12.746685+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09849",
      "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
      "authors": [
        "Yucheng Hu",
        "Jianke Zhang",
        "Yuanfei Luo",
        "Yanjiang Guo",
        "Xiaoyu Chen",
        "Xinshu Sun",
        "Kun Feng",
        "Qingzhou Lu",
        "Sheng Chen",
        "Yangang Zhang",
        "Wei Li",
        "Jianyu Chen"
      ],
      "abstract": "BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks. Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation , leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning , visual forecasting , and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning .",
      "summary_en": "BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.",
      "summary_zh": "BagelVLAæ˜¯ä¸€ç§ç»Ÿä¸€çš„Vision-Language-Actionæ¨¡å‹ï¼Œé€šè¿‡æ®‹å·®æµå¼•å¯¼æ•´åˆè¯­è¨€è§„åˆ’ã€è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆï¼Œä»¥æ”¹è¿›æ“ä½œä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09849",
      "arxiv_url": "https://arxiv.org/abs/2602.09849",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09849",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T06:22:00.781887+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09000",
      "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
      "authors": [
        "Ali Hatamizadeh",
        "Shrimai Prabhumoye",
        "Igor Gitman",
        "Ximing Lu",
        "Seungju Han",
        "Wei Ping",
        "Yejin Choi",
        "Jan Kautz"
      ],
      "abstract": "Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks. Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization . We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements , training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse . These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning .",
      "summary_en": "Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks.",
      "summary_zh": "è¿­ä»£å¼ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–é€šè¿‡ç»“åˆæ¢ç´¢æ€§èµ·è‰ä¸ç²¾ç»†åŒ–è¿­ä»£çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œå¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ç«äº‰æ€§åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€å…ˆè¿›çš„ç»“æœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09000",
      "arxiv_url": "https://arxiv.org/abs/2602.09000",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09000",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:21:31.787398+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.01244",
      "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments",
      "authors": [
        "Siwei Wu",
        "Yizhi Li",
        "Yuyang Song",
        "Wei Zhang",
        "Yang Wang",
        "Riza Batista-Navarro",
        "Xian Yang",
        "Mingjie Tang",
        "Bryan Dai",
        "Jian Yang",
        "Chenghua Lin"
      ],
      "abstract": "A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains. Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\emph{ Executability }, since each instance requires a suitable and often distinct Docker environment; and \\emph{ Verifiability }, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances , and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.",
      "summary_en": "A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.",
      "summary_zh": "åä¸º TerminalTraj çš„å¯æ‰©å±•æµç¨‹é€šè¿‡ç­›é€‰ä»£ç ä»“åº“ã€ç”Ÿæˆ Docker å¯¹é½çš„ä»»åŠ¡å®ä¾‹å¹¶åœ¨å¤šé¢†åŸŸåˆæˆå¯æ‰§è¡Œæ™ºèƒ½ä½“è½¨è¿¹ï¼Œè§£å†³äº†ä¸ºè®­ç»ƒæ™ºèƒ½ä½“æ¨¡å‹åˆ›å»ºé«˜è´¨é‡ç»ˆç«¯è½¨è¿¹çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01244",
      "arxiv_url": "https://arxiv.org/abs/2602.01244",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01244",
      "github_url": "https://github.com/multimodal-art-projection/TerminalTraj",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:20:23.586349+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10102",
      "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos",
      "authors": [
        "Zhongwei Ren",
        "Yunchao Wei",
        "Xiao Yu",
        "Guixun Luo",
        "Yao Zhao",
        "Bingyi Kang",
        "Jiashi Feng",
        "Xiaojie Jin"
      ],
      "abstract": "VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications. Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance : a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning . We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset , which substantially improves task performance on CALVIN . This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.",
      "summary_en": "VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.",
      "summary_zh": "VideoWorld 2 é€šè¿‡åŠ¨æ€å¢å¼ºçš„æ½œåœ¨åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå°†åŠ¨ä½œåŠ¨æ€ä¸è§†è§‰å¤–è§‚è§£è€¦ï¼Œå®ç°äº†ä»åŸå§‹è§†é¢‘ä¸­è¿›è¡Œå¯è¿ç§»çŸ¥è¯†å­¦ä¹ ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­æå‡äº†ä»»åŠ¡æ€§èƒ½ä¸é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10102",
      "arxiv_url": "https://arxiv.org/abs/2602.10102",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10102",
      "github_url": "https://github.com/ByteDance-Seed/VideoWorld",
      "upvotes": 14,
      "fetched_at": "2026-02-19T06:22:17.746149+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09439",
      "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
      "authors": [
        "Xu Ma",
        "Yitian Zhang",
        "Qihua Dong",
        "Yun Fu"
      ],
      "abstract": "A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles. High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning . Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment , or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning . Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment , visual fidelity , and prompt quality , with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning -level quality. Across a diverse set of pretrained diffusion and autoregressive models , fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.",
      "summary_en": "A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å®Œå…¨å¼€æºçš„æ–‡æœ¬åˆ°å›¾åƒå¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡600ä¸‡æ–‡æœ¬-å›¾åƒå¯¹ï¼Œå¹¶é’ˆå¯¹å¤šç§ä»»åŠ¡ç»„åˆå’Œè§†è§‰é£æ ¼è¿›è¡Œäº†ä¸¥æ ¼çš„å¯¹é½ä¸è´¨é‡è¿‡æ»¤ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09439",
      "arxiv_url": "https://arxiv.org/abs/2602.09439",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09439",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:21:49.147618+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.06820",
      "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
      "authors": [
        "Dunwei Tu",
        "Hongyan Hao",
        "Hansi Yang",
        "Yihao Chen",
        "Yi-Kai Zhang",
        "Zhikang Xia",
        "Yu Yang",
        "Yueqing Sun",
        "Xingchen Liu",
        "Furao Shen",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai"
      ],
      "abstract": "ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion. Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration . However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing , and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification . By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as Ï„^2-Bench and VitaBench , highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.",
      "summary_en": "ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.",
      "summary_zh": "ScaleEnvæ¡†æ¶ä»é›¶å¼€å§‹ç”Ÿæˆäº¤äº’å¼ç¯å¢ƒï¼Œé€šè¿‡å¤šæ ·åŒ–åŸŸç¼©æ”¾ä¸éªŒè¯ä»»åŠ¡å®Œæˆæ¥æå‡æ™ºèƒ½ä½“æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06820",
      "arxiv_url": "https://arxiv.org/abs/2602.06820",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06820",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:20:50.291355+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09017",
      "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
      "authors": [
        "Zichen Jeff Cui",
        "Omar Rayyan",
        "Haritheja Etukuru",
        "Bowen Tan",
        "Zavier Andrianarivo",
        "Zicheng Teng",
        "Yihang Zhou",
        "Krish Mehta",
        "Nicholas Wojno",
        "Kevin Yuanbo Wu",
        "Manan H Anjaria",
        "Ziyuan Wu",
        "Manrong Mao",
        "Guangxun Zhang",
        "Binit Shah",
        "Yejin Kim",
        "Soumith Chintala",
        "Lerrel Pinto",
        "Nur Muhammad Mahi Shafiullah"
      ],
      "abstract": "Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles. The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym , a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data , and outperforms large, state-of-the-art VLAs in zero-shot evaluation s by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/",
      "summary_en": "Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles.",
      "summary_zh": "æ¥è§¦é”šå®šç­–ç•¥ä»¥ç‰©ç†æ¥è§¦ç‚¹æ›¿ä»£è¯­è¨€æ¡ä»¶ï¼Œå¹¶é‡‡ç”¨æ¨¡å—åŒ–æ•ˆç”¨æ¨¡å‹å®ç°é²æ£’æ“æ§ï¼Œé€šè¿‡ real-to-sim è¿­ä»£å¾ªç¯ï¼Œä»¥æå°‘æ¼”ç¤ºæ•°æ®è¾¾æˆå“è¶Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09017",
      "arxiv_url": "https://arxiv.org/abs/2602.09017",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09017",
      "github_url": "https://github.com/jeffacce/cap-policy",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:21:34.406799+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09276",
      "title": "Effective Reasoning Chains Reduce Intrinsic Dimensionality",
      "authors": [
        "Archiki Prasad",
        "Mandar Joshi",
        "Kenton Lee",
        "Mohit Bansal",
        "Peter Shaw"
      ],
      "abstract": "Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds. Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies , we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.",
      "summary_en": "Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds.",
      "summary_zh": "æœ‰æ•ˆçš„æ€ç»´é“¾æ¨ç†ç­–ç•¥å¯é™ä½å†…åœ¨ç»´åº¦ï¼Œä»è€Œåœ¨è¾¾åˆ°ç»™å®šå‡†ç¡®ç‡é˜ˆå€¼æ—¶åªéœ€æ›´å°‘çš„æ¨¡å‹å‚æ•°ï¼Œå®ç°æ›´å¥½çš„æ³›åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09276",
      "arxiv_url": "https://arxiv.org/abs/2602.09276",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09276",
      "github_url": "",
      "upvotes": 11,
      "fetched_at": "2026-02-19T06:21:46.907823+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08382",
      "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
      "authors": [
        "Zhuoen Chen",
        "Dongfang Li",
        "Meishan Zhang",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches. Large Language Models (LLMs) face significant challenges in long-context processing , including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall , rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor . A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning , while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA , extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent .",
      "summary_en": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.",
      "summary_zh": "ä¸€ç§å—è®¤çŸ¥å¯å‘çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡æ¡†æ¶é‡‡ç”¨åˆ†å—å‹ç¼©å’Œé€‰æ‹©æ€§è®°å¿†å¬å›ï¼Œä»¥æå‡æ•ˆç‡å’Œæ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08382",
      "arxiv_url": "https://arxiv.org/abs/2602.08382",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08382",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T06:21:21.015506+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07276",
      "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
      "authors": [
        "Pengrui Han",
        "Xueqiang Xu",
        "Keyang Xuan",
        "Peiyang Song",
        "Siru Ouyang",
        "Runchu Tian",
        "Yuqing Jiang",
        "Cheng Qian",
        "Pengcheng Jiang",
        "Jiashuo Sun",
        "Junxia Cui",
        "Ming Zhong",
        "Ge Liu",
        "Jiawei Han",
        "Jiaxuan You"
      ],
      "abstract": "STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.",
      "summary_en": "STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.",
      "summary_zh": "STEER2ADAPTæ˜¯ä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œé€šè¿‡ä»å¯å¤ç”¨çš„è¯­ä¹‰å…ˆéªŒå­ç©ºé—´ä¸­ç»„åˆå¼•å¯¼å‘é‡æ¥é€‚é…å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå€ŸåŠ©åŸºå‘é‡çš„çº¿æ€§ç»„åˆå®ç°é«˜æ•ˆä¸”çµæ´»çš„ä»»åŠ¡é€‚é…ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07276",
      "arxiv_url": "https://arxiv.org/abs/2602.07276",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07276",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T06:20:59.095957+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08025",
      "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models",
      "authors": [
        "Yixuan Ye",
        "Xuanyu Lu",
        "Yuxin Jiang",
        "Yuchao Gu",
        "Rui Zhao",
        "Qiwei Liang",
        "Jiachun Pan",
        "Fengda Zhang",
        "Weijia Wu",
        "Alex Jinpeng Wang"
      ],
      "abstract": "MIND is introduced as the first open-domain closed-loop benchmark for evaluating memory consistency and action control in world models, featuring high-quality videos and diverse action spaces to assess temporal stability and contextual coherence. World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models . MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control , capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline . Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models , including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/",
      "summary_en": "MIND is introduced as the first open-domain closed-loop benchmark for evaluating memory consistency and action control in world models, featuring high-quality videos and diverse action spaces to assess temporal stability and contextual coherence.",
      "summary_zh": "MINDæ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°ä¸–ç•Œæ¨¡å‹è®°å¿†ä¸€è‡´æ€§ä¸åŠ¨ä½œæ§åˆ¶çš„å¼€æ”¾åŸŸé—­ç¯åŸºå‡†ï¼Œå…·æœ‰é«˜è´¨é‡è§†é¢‘ä¸å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´ï¼Œç”¨äºè¯„ä¼°æ—¶é—´ç¨³å®šæ€§ä¸ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08025",
      "arxiv_url": "https://arxiv.org/abs/2602.08025",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08025",
      "github_url": "https://github.com/CSU-JPG/MIND",
      "upvotes": 9,
      "fetched_at": "2026-02-19T06:21:14.828419+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09823",
      "title": "Covo-Audio Technical Report",
      "authors": [
        "Wenfu Wang",
        "Chenxing Li",
        "Liqiang Zhang",
        "Yiyang Zhao",
        "Yuxiang Zou",
        "Hanzhao Li",
        "Mingyu Cui",
        "Hao Zhang",
        "Kun Wei",
        "Le Xu",
        "Zikang Huang",
        "Jiajun Xu",
        "Jiliang Hu",
        "Xiang He",
        "Zeyu Xie",
        "Jiawen Kang",
        "Youjun Chen",
        "Meng Yu",
        "Dong Yu",
        "Rilin Chen",
        "Linlin Di",
        "Shulin Feng"
      ],
      "abstract": "Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques. In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training , Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling , spoken dialogue , speech understanding , audio understanding , and full-duplex voice interaction . Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant , demonstrates strong spoken conversational abilities , including understanding, contextual reasoning , instruction following , and generating contextually appropriate and empathetic responses , validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model , achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALM s for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering , enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning , and suggest a scalable path toward more capable and versatile LALMs.",
      "summary_en": "Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques.",
      "summary_zh": "Covo-Audioæ˜¯ä¸€ä¸ª7Bå‚æ•°çš„ç«¯åˆ°ç«¯å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¯å¤„ç†è¿ç»­éŸ³é¢‘è¾“å…¥å¹¶ç”ŸæˆéŸ³é¢‘è¾“å‡ºï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œåè®­ç»ƒæŠ€æœ¯ï¼Œåœ¨è¯­éŸ³-æ–‡æœ¬å»ºæ¨¡ã€å£è¯­å¯¹è¯å’Œå…¨åŒå·¥è¯­éŸ³äº¤äº’ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09823",
      "arxiv_url": "https://arxiv.org/abs/2602.09823",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09823",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:21:58.759959+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09268",
      "title": "Rethinking Global Text Conditioning in Diffusion Transformers",
      "authors": [
        "Nikita Starodubcev",
        "Daniil Pakhomov",
        "Zongze Wu",
        "Ilya Drobyshevskiy",
        "Yuchen Liu",
        "Zhonghao Wang",
        "Yuqian Zhou",
        "Zhe Lin",
        "Dmitry Baranchuk"
      ],
      "abstract": "Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms. Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding . Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing .",
      "summary_en": "Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.",
      "summary_zh": "æ‰©æ•£Transformerä¸­ï¼ŒåŸºäºè°ƒåˆ¶çš„æ–‡æœ¬æ¡ä»¶åŒ–åœ¨ä½œä¸ºå¯æ§ç”Ÿæˆå¼•å¯¼è€Œéä»…ä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨æ—¶å…·æœ‰æ€§èƒ½ä¼˜åŠ¿ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09268",
      "arxiv_url": "https://arxiv.org/abs/2602.09268",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09268",
      "github_url": "https://github.com/quickjkee/modulation-guidance",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:21:44.885507+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10116",
      "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
      "authors": [
        "Hongchi Xia",
        "Xuan Li",
        "Zhaoshuo Li",
        "Qianli Ma",
        "Jiashu Xu",
        "Ming-Yu Liu",
        "Yin Cui",
        "Tsung-Yi Lin",
        "Wei-Chiu Ma",
        "Shenlong Wang",
        "Shuran Song",
        "Fangyin Wei"
      ],
      "abstract": "SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability. Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility , visual realism , and physical stability . Through iterative reasoning and adaptive tool selection , it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training . Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI . Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
      "summary_en": "SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.",
      "summary_zh": "SAGEæ˜¯ä¸€ç§æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¸ƒå±€ä¸ç‰©ä½“ç»„åˆç”Ÿæˆå™¨ä»¥åŠç”¨äºè¯­ä¹‰åˆç†æ€§å’Œç‰©ç†ç¨³å®šæ€§çš„è¯„ä¼°è¯„åˆ¤å™¨ï¼Œä¸ºå…·èº«æ™ºèƒ½è‡ªåŠ¨ç”Ÿæˆå¯ç›´æ¥ç”¨äºä»¿çœŸçš„3Dç¯å¢ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10116",
      "arxiv_url": "https://arxiv.org/abs/2602.10116",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10116",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:22:22.475854+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09662",
      "title": "TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution",
      "authors": [
        "Deyang Jiang",
        "Jing Huang",
        "Xuanle Zhao",
        "Lei Chen",
        "Liming Zheng",
        "Fanfan Liu",
        "Haibo Qiu",
        "Peng Shi",
        "Zhixiong Zeng"
      ],
      "abstract": "TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification. Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning , which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning . In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (i.e., trajectory difficulty) and breadth (i.e., trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.",
      "summary_en": "TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.",
      "summary_zh": "TreeCUAé€šè¿‡æ ‘çŠ¶ç»“æ„è½¨è¿¹ç»„ç»‡ä¸å¤šæ™ºèƒ½ä½“åä½œå®ç°GUIè‡ªåŠ¨åŒ–çš„é«˜æ•ˆæ‰©å±•ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”æ¢ç´¢ä¸è½¨è¿¹éªŒè¯æå‡GUIè§„åˆ’èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09662",
      "arxiv_url": "https://arxiv.org/abs/2602.09662",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09662",
      "github_url": "https://github.com/UITron-hub/TreeCUA",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:21:56.177494+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07839",
      "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
      "authors": [
        "Jiaxi Liu",
        "Yanzuo Jiang",
        "Guibin Zhang",
        "Zihan Zhang",
        "Heng Chang",
        "Zhenfei Yin",
        "Qibing Ren",
        "Junchi Yan"
      ],
      "abstract": "TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization. Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory , a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory , we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization ( IGPO ), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.",
      "summary_en": "TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.",
      "summary_zh": "TodoEvolveé€šè¿‡æ¨¡å—åŒ–è®¾è®¡ç©ºé—´ä¸å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œå®ç°ä»»åŠ¡ç‰¹å®šè§„åˆ’æ¶æ„çš„è‡ªä¸»åˆæˆä¸ä¿®è®¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07839",
      "arxiv_url": "https://arxiv.org/abs/2602.07839",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07839",
      "github_url": "https://github.com/EcthelionLiu/TodoEvolve",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:21:09.846009+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09153",
      "title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes",
      "authors": [
        "Nicholas Pfaff",
        "Thomas Cohn",
        "Sergey Zakharov",
        "Rick Cory",
        "Russ Tedrake"
      ],
      "abstract": "SceneSmith is a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts through multiple stages involving VLM agents and integrated asset generation techniques. Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation . We introduce SceneSmith , a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts . SceneSmith constructs scenes through successive stagesx2013from architectural layout to furniture placement to small object populationx2013each implemented as an interaction among VLM agents : designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation . SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.",
      "summary_en": "SceneSmith is a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts through multiple stages involving VLM agents and integrated asset generation techniques.",
      "summary_zh": "SceneSmithæ˜¯ä¸€ç§åˆ†å±‚æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡åŒ…å«VLMæ™ºèƒ½ä½“ä¸é›†æˆèµ„äº§ç”ŸæˆæŠ€æœ¯çš„å¤šé˜¶æ®µæµç¨‹ï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå¯ç›´æ¥ç”¨äºä»¿çœŸçš„å®¤å†…ç¯å¢ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09153",
      "arxiv_url": "https://arxiv.org/abs/2602.09153",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09153",
      "github_url": "https://github.com/nepfaff/scenesmith",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:43.069531+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09024",
      "title": "Autoregressive Image Generation with Masked Bit Modeling",
      "authors": [
        "Qihang Yu",
        "Qihao Liu",
        "Ju He",
        "Xinyang Zhang",
        "Yang Liu",
        "Liang-Chieh Chen",
        "Xi Chen"
      ],
      "abstract": "Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs. This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook size s. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256 , outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/",
      "summary_en": "Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.",
      "summary_zh": "ç¦»æ•£tokenizeråœ¨é€‚å½“æ‰©å±•æ—¶å¯åŒ¹æ•Œç”šè‡³è¶…è¶Šè¿ç»­æ–¹æ³•ï¼Œä¸”ä¸€ç§æ–°çš„æ©ç Bitè‡ªå›å½’å»ºæ¨¡æ–¹æ³•ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬è¾¾åˆ°äº†æœ€ä¼˜ç»“æœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09024",
      "arxiv_url": "https://arxiv.org/abs/2602.09024",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09024",
      "github_url": "https://github.com/amazon-far/BAR",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:36.512548+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08344",
      "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
      "authors": [
        "Qi Guo",
        "Jianing Wang",
        "Deyang Kong",
        "Xiangyu Xi",
        "Jianfei Zhang",
        "Yi Lu",
        "Jingang Wang",
        "Wei Wang",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "abstract": "Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery. Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking , aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase , with limited attention to the path exploration stage . In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards ( RLVR ) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.",
      "summary_en": "Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.",
      "summary_zh": "å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ é€šè¿‡å¤§çº²å¼•å¯¼çš„è·¯å¾„æ¢ç´¢æ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„å¹¶è¡Œæ€è€ƒï¼Œä»è€Œå‡å°‘ä¿¡æ¯å†—ä½™å¹¶æå‡è§£çš„å‘ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08344",
      "arxiv_url": "https://arxiv.org/abs/2602.08344",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08344",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:18.746955+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07755",
      "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs",
      "authors": [
        "Yiming Xiong",
        "Shengran Hu",
        "Jeff Clune"
      ],
      "abstract": "ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains. The statelessness of foundation models bottlenecks agentic systems ' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems ), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms . Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.",
      "summary_en": "ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.",
      "summary_zh": "ALMAæ˜¯ä¸€ä¸ªåˆ©ç”¨å…ƒå­¦ä¹ è‡ªåŠ¨å‘ç°æ™ºèƒ½ä½“ç³»ç»Ÿè®°å¿†è®¾è®¡çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸå®ç°æ— éœ€äººå·¥å·¥ç¨‹çš„æŒç»­å­¦ä¹ ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07755",
      "arxiv_url": "https://arxiv.org/abs/2602.07755",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07755",
      "github_url": "https://github.com/zksha/alma",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:07.803056+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07153",
      "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
      "authors": [
        "Jinbiao Wei",
        "Yilun Zhao",
        "Kangqi Ni",
        "Arman Cohan"
      ],
      "abstract": "A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants. End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations . Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
      "summary_en": "A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.",
      "summary_zh": "åä¸ºAnchorçš„è½¨è¿¹æ‰©å±•æ¡†æ¶é€šè¿‡è¯†åˆ«åˆ†æ”¯ç‚¹å¹¶ç”ŸæˆåŸºäºçŠ¶æ€çš„ä»»åŠ¡å˜ä½“æ–°è½¨è¿¹ï¼Œä»ç§å­æ¼”ç¤ºä¸­å¼•å¯¼å¯æ‰©å±•çš„æ¡Œé¢ç›‘ç£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07153",
      "arxiv_url": "https://arxiv.org/abs/2602.07153",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07153",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:20:56.641233+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05085",
      "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories",
      "authors": [
        "Sidi Lu",
        "Zhenwen Liang",
        "Dongyang Ma",
        "Yan Wang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Locas, a locally-supported parametric memory mechanism, enables flexible integration with transformer models for continual learning while minimizing catastrophic forgetting through principled initialization techniques. In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformer s, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning . We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning . Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation . Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.",
      "summary_en": "Locas, a locally-supported parametric memory mechanism, enables flexible integration with transformer models for continual learning while minimizing catastrophic forgetting through principled initialization techniques.",
      "summary_zh": "Locasæ˜¯ä¸€ç§å±€éƒ¨æ”¯æŒçš„å‚æ•°åŒ–è®°å¿†æœºåˆ¶ï¼Œå¯ä¸Transformeræ¨¡å‹çµæ´»é›†æˆä»¥è¿›è¡ŒæŒç»­å­¦ä¹ ï¼ŒåŒæ—¶é€šè¿‡åŸåˆ™æ€§åˆå§‹åŒ–æŠ€æœ¯æœ€å°åŒ–ç¾éš¾æ€§é—å¿˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05085",
      "arxiv_url": "https://arxiv.org/abs/2602.05085",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05085",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:20:38.575779+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09591",
      "title": "On the Optimal Reasoning Length for RL-Trained Language Models",
      "authors": [
        "Daisuke Nohara",
        "Taishi Nakamura",
        "Rio Yokota"
      ],
      "abstract": "Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors. Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance . In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition , while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.",
      "summary_en": "Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.",
      "summary_zh": "å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­çš„é•¿åº¦æ§åˆ¶æ–¹æ³•ä¼šå½±å“æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œæœ€ä¼˜è¾“å‡ºé•¿åº¦èƒ½å¤Ÿå¹³è¡¡è¿™äº›å› ç´ ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09591",
      "arxiv_url": "https://arxiv.org/abs/2602.09591",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09591",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:21:53.570598+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08503",
      "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
      "authors": [
        "Yi Ding",
        "Ziliang Qiu",
        "Bolian Li",
        "Ruqi Zhang"
      ],
      "abstract": "Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies. Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.",
      "summary_en": "Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.",
      "summary_zh": "Octopus æ˜¯ä¸€ç§ RL rollout å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡åˆæˆæ ·æœ¬ç”Ÿæˆä¸å“åº”æ©ç ç­–ç•¥ï¼Œå®ç°äº†è§†è§‰-è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆè‡ªæ ¡æ­£å­¦ä¹ ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08503",
      "arxiv_url": "https://arxiv.org/abs/2602.08503",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08503",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:21:25.252998+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.06161",
      "title": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding",
      "authors": [
        "Yanzheng Xiang",
        "Lan Wei",
        "Yizhen Yao",
        "Qinglin Zhu",
        "Hanqi Yan",
        "Chen Jin",
        "Philip Alexander Teare",
        "Dandan Zhang",
        "Lin Gui",
        "Amrutha Saseendran",
        "Yulan He"
      ],
      "abstract": "COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction. Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations , where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override : selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift , and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.",
      "summary_en": "COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.",
      "summary_zh": "COVER é€šè¿‡å®ç°ç¼“å­˜è¦†ç›–éªŒè¯ï¼Œä½¿æ‰©æ•£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆå¹¶è¡Œè§£ç ï¼Œå‡å°‘ä¸å¿…è¦çš„ä¿®æ­£ï¼Œå¹¶é€šè¿‡ç¨³å®šçš„èµ·è‰ä¸æ³¨æ„åŠ›è§†å›¾æ„å»ºä¿æŒè¾“å‡ºè´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06161",
      "arxiv_url": "https://arxiv.org/abs/2602.06161",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06161",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:47.889711+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05892",
      "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
      "authors": [
        "Han Li",
        "Letian Zhu",
        "Bohan Zhang",
        "Rili Feng",
        "Jiaming Wang",
        "Yue Pan",
        "Earl T. Barr",
        "Sarro Federica",
        "Zhaoyang Chu",
        "He Ye"
      ],
      "abstract": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information. LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents . ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts . We further implement an automated evaluation framework that tracks agent trajectories and measures context recall , precision, and efficiency throughout issue resolution . Using ContextBench, we evaluate four frontier LLMs and five coding agents . Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents ), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.",
      "summary_en": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information.",
      "summary_zh": "ContextBench é€šè¿‡è¯¦ç»†çš„è¿‡ç¨‹åˆ†æè¯„ä¼° coding agents çš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œæ­ç¤ºå…ˆè¿›çš„ agent è®¾è®¡åœ¨ä¸Šä¸‹æ–‡ä½¿ç”¨æ–¹é¢çš„æå‡æœ‰é™ï¼ŒåŒæ—¶çªæ˜¾äº†å·²æ¢ç´¢ä¿¡æ¯ä¸å·²åˆ©ç”¨ä¿¡æ¯ä¹‹é—´çš„å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05892",
      "arxiv_url": "https://arxiv.org/abs/2602.05892",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05892",
      "github_url": "https://github.com/EuniAI/ContextBench",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:45.438477+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05435",
      "title": "Stable Velocity: A Variance Perspective on Flow Matching",
      "authors": [
        "Donglin Yang",
        "Yongxing Zhang",
        "Xin Yu",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Xiaojuan Qi",
        "Renjie Liao"
      ],
      "abstract": "Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed. While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
      "summary_en": "Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.",
      "summary_zh": "Stable Velocity æ¡†æ¶é€šè¿‡è¯†åˆ«ä½æ–¹å·®åŒºåŸŸå¹¶æå‡ºæ–¹å·®ç¼©å‡æŠ€æœ¯ï¼Œè§£å†³æµåŒ¹é…ä¸­çš„é«˜æ–¹å·®è®­ç»ƒé—®é¢˜ï¼Œä»è€Œæå‡è®­ç»ƒæ•ˆç‡å’Œé‡‡æ ·é€Ÿåº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05435",
      "arxiv_url": "https://arxiv.org/abs/2602.05435",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05435",
      "github_url": "https://github.com/linYDTHU/StableVelocity",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:43.254130+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.02464",
      "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
      "authors": [
        "Or Shafran",
        "Shaked Ronen",
        "Omri Fahn",
        "Shauli Ravfogel",
        "Atticus Geiger",
        "Mor Geva"
      ],
      "abstract": "Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties. Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space . Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure . MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space , and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space . Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders . Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control , accounting for complex structures that isolated directions fail to capture.",
      "summary_en": "Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.",
      "summary_zh": "å› å­åˆ†æå™¨æ··åˆæ¨¡å‹ï¼ˆMFAï¼‰æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ— ç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡é«˜æ–¯åŒºåŸŸåŠå…¶åæ–¹å·®ç‰¹æ€§å¯¹å±€éƒ¨å‡ ä½•ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå‘ç°è¯­è¨€æ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„å¤æ‚éçº¿æ€§ç»“æ„ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02464",
      "arxiv_url": "https://arxiv.org/abs/2602.02464",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02464",
      "github_url": "https://github.com/ordavid-s/decomposing-activations-local-geometry",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:27.956194+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10099",
      "title": "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders",
      "authors": [
        "Amandeep Kumar",
        "Vishal M. Patel"
      ],
      "abstract": "Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling. Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders , rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation , RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF",
      "summary_en": "Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.",
      "summary_zh": "æ ‡å‡†æ‰©æ•£Transformerä¸­çš„å‡ ä½•å¹²æ‰°é˜»ç¢äº†è¡¨ç¤ºç¼–ç å™¨çš„æ”¶æ•›ï¼Œé€šè¿‡ç»“åˆJacobiæ­£åˆ™åŒ–çš„é»æ›¼æµåŒ¹é…å¯è§£å†³è¯¥é—®é¢˜ï¼Œä»è€Œåœ¨æ— éœ€å®½åº¦ç¼©æ”¾çš„æƒ…å†µä¸‹å®ç°æœ‰æ•ˆè®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10099",
      "arxiv_url": "https://arxiv.org/abs/2602.10099",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10099",
      "github_url": "https://github.com/amandpkr/RJF",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:22:15.296928+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07398",
      "title": "AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management",
      "authors": [
        "Ruoyao Wen",
        "Hao Li",
        "Chaowei Xiao",
        "Ning Zhang"
      ],
      "abstract": "AgentSys defends against indirect prompt injection in LLM agents through hierarchical memory isolation and controlled data flow, significantly reducing attack success rates while maintaining performance. Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window , which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack. We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls , each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing . Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length. On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.",
      "summary_en": "AgentSys defends against indirect prompt injection in LLM agents through hierarchical memory isolation and controlled data flow, significantly reducing attack success rates while maintaining performance.",
      "summary_zh": "AgentSysé€šè¿‡åˆ†å±‚å†…å­˜éš”ç¦»å’Œå—æ§æ•°æ®æµé˜²å¾¡LLMæ™ºèƒ½ä½“ä¸­çš„é—´æ¥æç¤ºæ³¨å…¥ï¼Œæ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07398",
      "arxiv_url": "https://arxiv.org/abs/2602.07398",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07398",
      "github_url": "https://github.com/ruoyaow/agentsys-memory",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:21:01.099791+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09924",
      "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations",
      "authors": [
        "William Lugoloobi",
        "Thomas Foster",
        "William Bankes",
        "Chris Russell"
      ],
      "abstract": "LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance. Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks , substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC , which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning . Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty",
      "summary_en": "LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.",
      "summary_zh": "LLMçš„å†…éƒ¨è¡¨å¾å¯ä»¥é¢„æµ‹é—®é¢˜éš¾åº¦ï¼Œå¹¶å®ç°é«˜æ•ˆæ¨ç†è·¯ç”±ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½æˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09924",
      "arxiv_url": "https://arxiv.org/abs/2602.09924",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09924",
      "github_url": "https://github.com/KabakaWilliam/llms_know_difficulty",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:22:05.328936+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08519",
      "title": "Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering",
      "authors": [
        "Yunhui Liu",
        "Pengyu Qiu",
        "Yu Xing",
        "Yongchao Liu",
        "Peng Du",
        "Chuntao Hong",
        "Jiajun Zheng",
        "Tao Zheng",
        "Tieke He"
      ],
      "abstract": "PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols. Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data . Despite its significance in industrial applications such as fraud detection and user segmentation , a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high- homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily . Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).",
      "summary_en": "PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.",
      "summary_zh": "PyAGC æå‡ºäº†ä¸€ä¸ªé¢å‘å±æ€§å›¾èšç±»çš„ç”Ÿäº§å°±ç»ªåŸºå‡†ä¸åº“ï¼Œé€šè¿‡å¯æ‰©å±•ã€å†…å­˜é«˜æ•ˆçš„å®ç°å’Œå…¨é¢çš„è¯„ä¼°åè®®ï¼Œè§£å†³äº†å½“å‰ç ”ç©¶çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08519",
      "arxiv_url": "https://arxiv.org/abs/2602.08519",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08519",
      "github_url": "https://github.com/Cloudy1225/PyAGC",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:21:27.360331+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07918",
      "title": "CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution",
      "authors": [
        "Minbeom Kim",
        "Mihir Parmar",
        "Phillip Wallis",
        "Lesly Miculicich",
        "Kyomin Jung",
        "Krishnamurthy Dj Dvijotham",
        "Long T. Le",
        "Tomas Pfister"
      ],
      "abstract": "CausalArmor is a selective defense framework for AI agents that uses causal ablation to detect and mitigate Indirect Prompt Injection attacks by identifying dominant untrusted segments and applying targeted sanitization. AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation -based attribution s at privileged decision points , and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.",
      "summary_en": "CausalArmor is a selective defense framework for AI agents that uses causal ablation to detect and mitigate Indirect Prompt Injection attacks by identifying dominant untrusted segments and applying targeted sanitization.",
      "summary_zh": "CausalArmoræ˜¯ä¸€ç§é¢å‘AIæ™ºèƒ½ä½“çš„é€‰æ‹©æ€§é˜²å¾¡æ¡†æ¶ï¼Œåˆ©ç”¨å› æœæ¶ˆèæŠ€æœ¯æ£€æµ‹å’Œç¼“è§£é—´æ¥æç¤ºæ³¨å…¥æ”»å‡»ï¼Œé€šè¿‡è¯†åˆ«ä¸»å¯¼æ€§ä¸å¯ä¿¡ç‰‡æ®µå¹¶å®æ–½å®šå‘å‡€åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07918",
      "arxiv_url": "https://arxiv.org/abs/2602.07918",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07918",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:21:12.327747+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07670",
      "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation",
      "authors": [
        "Jarrod Barnes"
      ],
      "abstract": "Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.",
      "summary_en": "Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.",
      "summary_zh": "æµ‹è¯•æ—¶è®­ç»ƒå› è¿‡åº¦é”åŒ–åœ¨åŸºäºéªŒè¯çš„ä»»åŠ¡ä¸­å¤±æ•ˆï¼Œè€ŒæƒŠå¥‡å€¼å¼•å¯¼çš„é€‰æ‹©é€šè¿‡åå¥½å¤šæ ·åŒ–ä½ç½®ä¿¡æ ·æœ¬æå‡æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07670",
      "arxiv_url": "https://arxiv.org/abs/2602.07670",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07670",
      "github_url": "https://github.com/jbarnes850/test-time-training",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:21:05.658756+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04908",
      "title": "Temporal Pair Consistency for Variance-Reduced Flow Matching",
      "authors": [
        "Chika Maduabuchi",
        "Jindong Wang"
      ],
      "abstract": "Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures. Continuous-time generative models , such as diffusion models , flow matching , and rectified flow , learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective . Instantiated within flow matching , TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow .",
      "summary_en": "Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.",
      "summary_zh": "æ—¶åºé…å¯¹ä¸€è‡´æ€§é€šè¿‡åœ¨é…å¯¹æ—¶é—´æ­¥è€¦åˆé€Ÿåº¦é¢„æµ‹æ¥é™ä½è¿ç»­æ—¶é—´ç”Ÿæˆæ¨¡å‹çš„æ–¹å·®ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–è®­ç»ƒæµç¨‹çš„æƒ…å†µä¸‹æå‡æ ·æœ¬è´¨é‡ä¸æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04908",
      "arxiv_url": "https://arxiv.org/abs/2602.04908",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04908",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:36.725890+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04802",
      "title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
      "authors": [
        "Qing'an Liu",
        "Juntong Feng",
        "Yuhao Wang",
        "Xinzhe Han",
        "Yujie Cheng",
        "Yue Zhu",
        "Haiwen Diao",
        "Yunzhi Zhuge",
        "Huchuan Lu"
      ],
      "abstract": "VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations. Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries . In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception , reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap : models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text . This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.",
      "summary_en": "VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.",
      "summary_zh": "VISTA-Benchè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£å¯è§†åŒ–æ–‡æœ¬ä¸çº¯æ–‡æœ¬æŸ¥è¯¢çš„èƒ½åŠ›ï¼Œæ­ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½å·®è·ä»¥åŠå¯¹æ¸²æŸ“å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04802",
      "arxiv_url": "https://arxiv.org/abs/2602.04802",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04802",
      "github_url": "https://github.com/QingAnLiu/VISTA-Bench",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:34.778179+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04521",
      "title": "C-Î”Î˜: Circuit-Restricted Weight Arithmetic for Selective Refusal",
      "authors": [
        "Aditya Kasliwal",
        "Pratinav Seth",
        "Vinay Kumar Sankarapu"
      ],
      "abstract": "Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance. Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Î”Î¸: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update Î”Î¸C supported only on that circuit (typically <5% of parameters). Applying Î”Î¸C yields a drop-in edited checkpoint with no inference-time hooks , shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.",
      "summary_en": "Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹çš„ç¦»çº¿é€‰æ‹©æ€§æ‹’ç»é€šè¿‡ç”µè·¯å—é™æƒé‡æ›´æ–°å®ç°ï¼Œåœ¨æ¶ˆé™¤è¿è¡Œæ—¶å¹²é¢„æˆæœ¬çš„åŒæ—¶ä¿æŒæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04521",
      "arxiv_url": "https://arxiv.org/abs/2602.04521",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04521",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:32.675564+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.01725",
      "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
      "authors": [
        "Yurun Chen",
        "Zeyi Liao",
        "Ping Yin",
        "Taotao Xie",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "abstract": "SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments. With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach , with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction : by using safety policies as the basis for risk prediction , SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization : translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning . Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
      "summary_en": "SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.",
      "summary_zh": "SafePredæ˜¯ä¸€ç§é¢å‘è®¡ç®—æœºä½¿ç”¨æ™ºèƒ½ä½“çš„é¢„æµ‹æ€§æŠ¤æ æ¡†æ¶ï¼Œåˆ©ç”¨é£é™©é¢„æµ‹å’Œå†³ç­–ä¼˜åŒ–æ¥é˜²æ­¢å¤æ‚ç¯å¢ƒä¸­çš„å³æ—¶å’Œå»¶è¿Ÿé«˜é£é™©åæœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01725",
      "arxiv_url": "https://arxiv.org/abs/2602.01725",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01725",
      "github_url": "https://github.com/YurunChen/SafePred",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:25.691666+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2601.21235",
      "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
      "authors": [
        "Alok Abhishek",
        "Tushar Bandopadhyay",
        "Lisa Erickson"
      ],
      "abstract": "Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics. Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior . This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias , fairness , ethics , and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk . The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility . Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
      "summary_en": "Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„ç¤¾ä¼šé£é™©ï¼Œå…¶æœ€åæƒ…å†µè¡Œä¸ºå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œè€Œä¼ ç»Ÿæ ‡é‡è¯„ä¼°æŒ‡æ ‡æ— æ³•æ•æ‰è¿™äº›å·®å¼‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21235",
      "arxiv_url": "https://arxiv.org/abs/2601.21235",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21235",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:17.074995+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08222",
      "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
      "authors": [
        "Zehao Chen",
        "Gongxun Li",
        "Tianxiang Ai",
        "Yifei Li",
        "Zixuan Huang",
        "Wang Zhou",
        "Fuzhen Zhuang",
        "Xianglong Liu",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
      ],
      "abstract": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models. As post-training optimization becomes central to improving large language models , we observe a persistent saturation bottleneck : once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning , WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
      "summary_en": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.",
      "summary_zh": "WMSSæ˜¯ä¸€ç§åè®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å¼±æ¨¡å‹æ£€æŸ¥ç‚¹è¯†åˆ«å¹¶å¡«è¡¥å­¦ä¹ ç¼ºå£ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨è¶…è¶Šä¼ ç»Ÿé¥±å’Œç‚¹åä»æŒç»­æ”¹è¿›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08222",
      "arxiv_url": "https://arxiv.org/abs/2602.08222",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08222",
      "github_url": "https://github.com/chenzehao82/Weak-Driven-Learning",
      "upvotes": 256,
      "fetched_at": "2026-02-19T06:03:14.229880+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07274",
      "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents",
      "authors": [
        "Kaijie Zhu",
        "Yuzhou Nie",
        "Yijiang Li",
        "Yiming Huang",
        "Jialian Wu",
        "Jiang Liu",
        "Ximeng Sun",
        "Zhenfei Yin",
        "Lun Wang",
        "Zicheng Liu",
        "Emad Barsoum",
        "William Yang Wang",
        "Wenbo Guo"
      ],
      "abstract": "TermiGen introduces a pipeline for generating verifiable terminal environments and resilient trajectories to improve open-weight LLMs' ability to execute complex tasks and recover from runtime errors. Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch , leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories . Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop . Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles . Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench . This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.",
      "summary_en": "TermiGen introduces a pipeline for generating verifiable terminal environments and resilient trajectories to improve open-weight LLMs' ability to execute complex tasks and recover from runtime errors.",
      "summary_zh": "TermiGenæå‡ºäº†ä¸€ç§ç”Ÿæˆå¯éªŒè¯ç»ˆç«¯ç¯å¢ƒä¸å¼¹æ€§è½¨è¿¹çš„æµç¨‹ï¼Œæ—¨åœ¨æå‡å¼€æ”¾æƒé‡LLMæ‰§è¡Œå¤æ‚ä»»åŠ¡åŠä»è¿è¡Œæ—¶é”™è¯¯ä¸­æ¢å¤çš„èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07274",
      "arxiv_url": "https://arxiv.org/abs/2602.07274",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07274",
      "github_url": "https://github.com/ucsb-mlsec/terminal-bench-env",
      "upvotes": 197,
      "fetched_at": "2026-02-19T06:02:47.360137+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07085",
      "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
      "authors": [
        "Jun Han",
        "Shuo Zhang",
        "Wei Li",
        "Zhi Yang",
        "Yifan Dong",
        "Tu Hu",
        "Jialuo Yuan",
        "Xiaomin Yu",
        "Yumo Zhu",
        "Fangqi Lou",
        "Xin Guo",
        "Zhaowei Liu",
        "Tianyi Jiang",
        "Ruichuan An",
        "Jingping Liu",
        "Biao Wu",
        "Rongze Chen",
        "Kunyi Wang",
        "Yifan Wang",
        "Sen Hu",
        "Xinbing Kong",
        "Liwen Zhang"
      ],
      "abstract": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
      "summary_en": "Financial markets are noisy and non-stationary, making alpha mining sensitive to backtesting noise and regime shifts, while existing agentic frameworks lack controllable multi-round search and reliable reuse of validated experience. QuantaAlpha addresses these challenges through an evolutionary framework that treats each mining run as a trajectory, improving factors via trajectory-level mutation and crossover operations that localize suboptimal steps for targeted revision and recombine complementary high-reward segments to reuse effective patterns. The framework enforces semantic consistency across hypothesis, factor expression, and executable code while constraining complexity and redundancy to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains, with GPT-5.2 achieving an Information Coefficient (IC) of 0.1501, Annualized Rate of Return (ARR) of 27.75%, and Maximum Drawdown (MDD) of 7.98%, and factors transferring effectively to the China Securities Index 500 (CSI 500) and Standard & Poor's 500 Index (S&P 500) to deliver 160% and 137% cumulative excess return over four years, respectively.",
      "summary_zh": "é‡‘èå¸‚åœºå…·æœ‰å˜ˆæ‚ä¸”éå¹³ç¨³çš„ç‰¹æ€§ï¼Œä½¿å¾—é˜¿å°”æ³•æŒ–æ˜å¯¹å›æµ‹å™ªå£°å’Œæœºåˆ¶è½¬æ¢æ•æ„Ÿï¼Œè€Œç°æœ‰çš„æ™ºèƒ½ä½“æ¡†æ¶ç¼ºä¹å¯æ§çš„å¤šè½®æœç´¢å’Œå·²éªŒè¯ç»éªŒçš„å¯é å¤ç”¨ã€‚QuantaAlphaé€šè¿‡è¿›åŒ–æ¡†æ¶åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå°†æ¯æ¬¡æŒ–æ˜è¿è¡Œè§†ä¸ºä¸€æ¡è½¨è¿¹ï¼Œé€šè¿‡è½¨è¿¹çº§å˜å¼‚å’Œäº¤å‰æ“ä½œæ¥æ”¹è¿›å› å­ï¼šå‰è€…å®šä½æ¬¡ä¼˜æ­¥éª¤ä»¥è¿›è¡Œé’ˆå¯¹æ€§ä¿®æ­£ï¼Œåè€…é‡ç»„äº’è¡¥çš„é«˜æ”¶ç›Šç‰‡æ®µä»¥å¤ç”¨æœ‰æ•ˆæ¨¡å¼ã€‚è¯¥æ¡†æ¶åœ¨å‡è®¾ã€å› å­è¡¨è¾¾å¼å’Œå¯æ‰§è¡Œä»£ç ä¹‹é—´å¼ºåˆ¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ï¼ŒåŒæ—¶çº¦æŸå¤æ‚åº¦å’Œå†—ä½™ä»¥ç¼“è§£æ‹¥æŒ¤ã€‚åœ¨æ²ªæ·±300æŒ‡æ•°ï¼ˆCSI 300ï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜æ€§èƒ½æŒç»­æå‡ï¼Œå…¶ä¸­GPT-5.2å®ç°äº†0.1501çš„ä¿¡æ¯ç³»æ•°ï¼ˆICï¼‰ã€27.75%çš„å¹´åŒ–æ”¶ç›Šç‡ï¼ˆARRï¼‰å’Œ7.98%çš„æœ€å¤§å›æ’¤ï¼ˆMDDï¼‰ï¼›å› å­æœ‰æ•ˆè¿ç§»è‡³ä¸­è¯500æŒ‡æ•°ï¼ˆCSI 500ï¼‰å’Œæ ‡æ™®500æŒ‡æ•°ï¼ˆS&P 500ï¼‰ï¼Œå››å¹´å†…åˆ†åˆ«å®ç°160%å’Œ137%çš„ç´¯è®¡è¶…é¢æ”¶ç›Šã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07085",
      "arxiv_url": "https://arxiv.org/abs/2602.07085",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07085",
      "github_url": "https://github.com/QuantaAlpha/QuantaAlpha",
      "upvotes": 181,
      "fetched_at": "2026-02-19T06:02:36.702701+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang"
      ],
      "abstract": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks. Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content , including realistic lip-synced speech , environment-aware sound effects , and content-aligned music . MOVA employs a Mixture-of-Experts ( MoE ) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference , LoRA fine-tuning , and prompt enhancement .",
      "summary_en": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.",
      "summary_zh": "MOVAæ˜¯ä¸€æ¬¾é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMixture-of-Expertsï¼‰æ¶æ„ã€æ‹¥æœ‰320äº¿å‚æ•°çš„å¼€æºæ¨¡å‹ï¼Œå¯ç”ŸæˆåŒæ­¥çš„éŸ³è§†é¢‘å†…å®¹ï¼Œæ”¯æŒå›¾æ–‡åˆ°éŸ³è§†é¢‘ç”Ÿæˆä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08794",
      "arxiv_url": "https://arxiv.org/abs/2602.08794",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08794",
      "github_url": "https://github.com/OpenMOSS/MOVA",
      "upvotes": 151,
      "fetched_at": "2026-02-19T06:03:32.240922+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07026",
      "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
      "authors": [
        "Xiaomin Yu",
        "Yi Xin",
        "Wenjie Zhang",
        "Chonghan Liu",
        "Hanzhen Zhao",
        "Xiaoxing Hu",
        "Xinlei Yu",
        "Ziyue Qiao",
        "Hao Tang",
        "Xue Yang",
        "Xiaobin Hu",
        "Chengwei Qin",
        "Hui Xiong",
        "Yu Qiao",
        "Shuicheng Yan"
      ],
      "abstract": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data. Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly , the Modality Gap , remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions , hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory , which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign , a training-free modality alignment strategy. Utilizing statistics from massive unpaired data , ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment , thereby explicitly rectifying geometric misalignment. Building on ReAlign , we propose ReVision , a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage , enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning , without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
      "summary_en": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.",
      "summary_zh": "ç ”ç©¶äººå‘˜æå‡ºäº†å›ºå®šå¸§ç†è®ºå’Œå…è®­ç»ƒå¯¹é½æ–¹æ³•ï¼Œä»¥è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ¨¡æ€å·®è·ï¼Œå®ç°äº†åˆ©ç”¨éé…å¯¹æ•°æ®é«˜æ•ˆæ‰©å±•å¤šæ¨¡æ€æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07026",
      "arxiv_url": "https://arxiv.org/abs/2602.07026",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07026",
      "github_url": "https://github.com/Yu-xm/ReVision",
      "upvotes": 133,
      "fetched_at": "2026-02-19T06:02:23.335532+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06855",
      "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
      "authors": [
        "Alisia Lupidi",
        "Bhavul Gauri",
        "Thomas Simon Foster",
        "Bassel Al Omari",
        "Despoina Magka",
        "Alberto Pepe",
        "Alexis Audran-Reiss",
        "Muna Aghamelu",
        "Nicolas Baldwin",
        "Lucia Cipolina-Kun",
        "Jean-Christophe Gagnon-Audet",
        "Chee Hau Leow",
        "Sandra Lefdal",
        "Hossam Mossalam",
        "Abhinav Moudgil",
        "Saba Nazir",
        "Emanuel Tewolde",
        "Isabel Urrego",
        "Jordi Armengol Estape",
        "Amar Budhiraja",
        "Gaurav Chaurasia",
        "Abhishek Charnalia"
      ],
      "abstract": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancement. LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark ), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds . Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
      "summary_en": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancement.",
      "summary_zh": "AIRS-Benchæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°è·¨å¤šä¸ªç§‘å­¦é¢†åŸŸçš„LLMæ™ºèƒ½ä½“ï¼Œæ­ç¤ºå½“å‰å±€é™æ€§å¹¶æä¾›å¼€æºèµ„æºä»¥æ¨åŠ¨å‘å±•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06855",
      "arxiv_url": "https://arxiv.org/abs/2602.06855",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06855",
      "github_url": "https://github.com/facebookresearch/airs-bench",
      "upvotes": 70,
      "fetched_at": "2026-02-19T06:02:18.646878+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08990",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "authors": [
        "Shiyang Feng",
        "Runmin Ma",
        "Xiangchao Yan",
        "Yue Fan",
        "Yusong Hu",
        "Songtao Huang",
        "Shuaiyu Zhang",
        "Zongsheng Cao",
        "Tianshuo Peng",
        "Jiakang Yuan",
        "Zijie Guo",
        "Zhijie Zhong",
        "Shangheng Du",
        "Weida Wang",
        "Jinxin Shi",
        "Yuhao Zhou",
        "Xiaohan He",
        "Zhiyin Yu",
        "Fangchen Yu",
        "Qihao Zheng",
        "Jiamin Wu",
        "Mianxin Liu"
      ],
      "abstract": "InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution. We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research , solution optimization , and long horizon memory . The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system . We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery .",
      "summary_en": "InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.",
      "summary_zh": "InternAgent-1.5æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªä¸»ç§‘å­¦å‘ç°ç³»ç»Ÿï¼Œé€šè¿‡åè°ƒçš„ç”Ÿæˆã€éªŒè¯ä¸æ¼”åŒ–å­ç³»ç»Ÿé›†æˆè®¡ç®—å»ºæ¨¡ä¸å®éªŒç ”ç©¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08990",
      "arxiv_url": "https://arxiv.org/abs/2602.08990",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08990",
      "github_url": "https://github.com/InternScience/InternAgent",
      "upvotes": 69,
      "fetched_at": "2026-02-19T06:03:43.577684+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07845",
      "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
      "authors": [
        "Yalcin Tur",
        "Jalal Naghiyev",
        "Haoquan Fang",
        "Wei-Chuan Tsai",
        "Jiafei Duan",
        "Dieter Fox",
        "Ranjay Krishna"
      ],
      "abstract": "RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates. Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence . Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
      "summary_en": "RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.",
      "summary_zh": "RD-VLA ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å¼•å…¥äº†ä¸€ç§å¾ªç¯æ¶æ„ï¼Œé€šè¿‡æ½œåœ¨è¿­ä»£ç»†åŒ–è‡ªé€‚åº”è°ƒæ•´è®¡ç®—æ·±åº¦ï¼Œå®ç°äº†æ’å®šå†…å­˜å ç”¨å¹¶æå‡äº†ä»»åŠ¡æˆåŠŸç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07845",
      "arxiv_url": "https://arxiv.org/abs/2602.07845",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07845",
      "github_url": "https://github.com/rd-vla/rd-vla",
      "upvotes": 68,
      "fetched_at": "2026-02-19T06:03:00.977675+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08676",
      "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
      "authors": [
        "Tiwei Bie",
        "Maosong Cao",
        "Xiang Cao",
        "Bingsen Chen",
        "Fuyuan Chen",
        "Kun Chen",
        "Lun Du",
        "Daozhuo Feng",
        "Haibo Feng",
        "Mingliang Gong",
        "Zhuocheng Gong",
        "Yanmei Gu",
        "Jian Guan",
        "Kaiyuan Guan",
        "Hongliang He",
        "Zenan Huang",
        "Juyong Jiang",
        "Zhonghui Jiang",
        "Zhenzhong Lan",
        "Chengxi Li",
        "Jianguo Li",
        "Zehuan Li"
      ],
      "abstract": "LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models. While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme . This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation . This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed . Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+ , 801 TPS on BigCodeBench , and 663 TPS on LiveCodeBench .",
      "summary_en": "LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.",
      "summary_zh": "LLaDA2.1æå‡ºäº†ä¸€ç§æ–°é¢–çš„token-to-tokenç¼–è¾‘æ–¹æ³•ï¼Œå…·å¤‡é€Ÿåº¦å’Œè´¨é‡æ¨¡å¼ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºï¼Œä»¥æå‡å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹çš„æ¨ç†ä¸æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08676",
      "arxiv_url": "https://arxiv.org/abs/2602.08676",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08676",
      "github_url": "https://github.com/inclusionAI/LLaDA2.X",
      "upvotes": 66,
      "fetched_at": "2026-02-19T06:03:30.274297+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07837",
      "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
      "authors": [
        "Hongzhi Zang",
        "Shu'ang Yu",
        "Hao Lin",
        "Tianxing Zhou",
        "Zefang Huang",
        "Zhen Guo",
        "Xin Xu",
        "Jiakai Zhou",
        "Yuze Sheng",
        "Shizhe Zhang",
        "Feng Gao",
        "Wenhao Tang",
        "Yufeng Yue",
        "Quanlu Zhang",
        "Xinlei Chen",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "USER is a unified systems framework that enables scalable, asynchronous online policy learning in physical robots by treating them as first-class hardware resources and supporting diverse learning paradigms including VLA models. Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence . Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning . USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer , enabling automatic discovery, management, and scheduling of heterogeneous robots . To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking , distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer , enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination , heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning .",
      "summary_en": "USER is a unified systems framework that enables scalable, asynchronous online policy learning in physical robots by treating them as first-class hardware resources and supporting diverse learning paradigms including VLA models.",
      "summary_zh": "USERæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿæ¡†æ¶ï¼Œé€šè¿‡å°†ç‰©ç†æœºå™¨äººè§†ä¸ºä¸€ç­‰ç¡¬ä»¶èµ„æºå¹¶æ”¯æŒåŒ…æ‹¬VLAæ¨¡å‹åœ¨å†…çš„å¤šç§å­¦ä¹ èŒƒå¼ï¼Œå®ç°å¯æ‰©å±•çš„å¼‚æ­¥åœ¨çº¿ç­–ç•¥å­¦ä¹ ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07837",
      "arxiv_url": "https://arxiv.org/abs/2602.07837",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07837",
      "github_url": "",
      "upvotes": 53,
      "fetched_at": "2026-02-19T06:02:58.626380+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.00169",
      "title": "Towards Agentic Intelligence for Materials Science",
      "authors": [
        "Huan Zhang",
        "Yizhan Li",
        "Wenhao Huang",
        "Ziyu Hou",
        "Yu Song",
        "Xuye Liu",
        "Farshid Effaty",
        "Jinya Jiang",
        "Sifan Wu",
        "Qianggang Ding",
        "Izumi Takahara",
        "Leonard R. MacGillivray",
        "Teruyasu Mizoguchi",
        "Tianshu Yu",
        "Lizi Liao",
        "Yuyu Luo",
        "Yu Rong",
        "Jia Li",
        "Ying Diao",
        "Heng Ji",
        "Bang Liu"
      ],
      "abstract": "AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit. The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning , to goal-conditioned agents interfacing with simulation and experimental platforms . Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment . To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science . We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition , predictive analytics , and natural language processing for literature mining , materials characterization , and property prediction ; from the materials science perspective, it highlights applications in materials design , process optimization , and the acceleration of computational workflows via integration with external tools (e.g., DFT , robotic labs ). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.",
      "summary_en": "AI-driven materials science integrates large language models across discovery pipelines from data curation to agent-based experimentation, emphasizing system-level optimization and autonomous goal pursuit.",
      "summary_zh": "AIé©±åŠ¨çš„ææ–™ç§‘å­¦å°†å¤§è¯­è¨€æ¨¡å‹æ•´åˆåˆ°ä»æ•°æ®æ•´ç†åˆ°åŸºäºæ™ºèƒ½ä½“çš„å®éªŒçš„å‘ç°æµç¨‹ä¸­ï¼Œå¼ºè°ƒç³»ç»Ÿçº§ä¼˜åŒ–å’Œè‡ªä¸»ç›®æ ‡è¿½æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00169",
      "arxiv_url": "https://arxiv.org/abs/2602.00169",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00169",
      "github_url": "",
      "upvotes": 46,
      "fetched_at": "2026-02-19T06:01:47.182578+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06422",
      "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
      "authors": [
        "Yunze Tong",
        "Mushui Liu",
        "Canyu Zhao",
        "Wanggui He",
        "Shiyi Zhang",
        "Hongwei Zhang",
        "Peng Zhang",
        "Jinlong Liu",
        "Ju Huang",
        "Jiamang Wang",
        "Hao Jiang",
        "Pipei Huang"
      ],
      "abstract": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories. Deploying GRPO on Flow Matching models has proven effective for text-to-image generation . However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint- GRPO (TP- GRPO ), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory . TP- GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards , providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points -steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact . Turning points are detected solely via sign changes in incremental rewards , making TP- GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP- GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint- GRPO .",
      "summary_en": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.",
      "summary_zh": "TP-GRPOé€šè¿‡å¼•å…¥æ­¥éª¤çº§å¢é‡å¥–åŠ±å¹¶è¯†åˆ«è½¬æŠ˜ç‚¹ï¼Œè§£å†³æµåŒ¹é…æ¨¡å‹ä¸­çš„å¥–åŠ±ç¨€ç–æ€§é—®é¢˜ï¼Œä»è€Œæ•æ‰å»å™ªè½¨è¿¹ä¸­çš„é•¿æœŸæ•ˆåº”ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06422",
      "arxiv_url": "https://arxiv.org/abs/2602.06422",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06422",
      "github_url": "https://github.com/YunzeTong/TurningPoint-GRPO",
      "upvotes": 42,
      "fetched_at": "2026-02-19T06:02:05.419662+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08321",
      "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models",
      "authors": [
        "Zijie Chen",
        "Zhenghao Lin",
        "Xiao Liu",
        "Zhenzhong Lan",
        "Yeyun Gong",
        "Peng Cheng"
      ],
      "abstract": "A large-scale scientific question dataset and post-training pipeline are developed to improve open-ended science question answering through enhanced data processing and reinforcement learning with rubric-guided evaluation. Solving open-ended science questions remains challenging for large language models , particula rl y due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset , which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT , which broadens the model's reasoning pattern coverage prior to RL ; (ii) Dynamic Difficulty Curriculum , which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL , which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general , consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning , especially in open-ended settings.",
      "summary_en": "A large-scale scientific question dataset and post-training pipeline are developed to improve open-ended science question answering through enhanced data processing and reinforcement learning with rubric-guided evaluation.",
      "summary_zh": "å¼€å‘äº†å¤§è§„æ¨¡ç§‘å­¦é—®ç­”æ•°æ®é›†ä¸åè®­ç»ƒæµç¨‹ï¼Œé€šè¿‡å¢å¼ºçš„æ•°æ®å¤„ç†åŠåŸºäºè¯„åˆ†æ ‡å‡†è¯„ä¼°çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ”¹è¿›å¼€æ”¾å¼ç§‘å­¦é—®ç­”ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08321",
      "arxiv_url": "https://arxiv.org/abs/2602.08321",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08321",
      "github_url": "",
      "upvotes": 40,
      "fetched_at": "2026-02-19T06:03:18.766224+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.09007",
      "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
      "authors": [
        "Haodong Li",
        "Jingwei Wu",
        "Quan Sun",
        "Guopeng Li",
        "Juanxi Tian",
        "Huanyu Zhang",
        "Yanlin Lai",
        "Ruichuan An",
        "Hongbo Peng",
        "Yuhong Dai",
        "Chenxi Li",
        "Chunmei Qing",
        "Jia Wang",
        "Ziyang Meng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "abstract": "A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences. Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity , leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench , a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation . GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score , a novel five-dimensional metric that assesses Goal Achievement , Interaction Logic , Content Consistency , UI Plausibility , and Visual Quality . Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/ GEBench .",
      "summary_en": "A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.",
      "summary_zh": "ç ”ç©¶è€…å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°GUIç”Ÿæˆæ¨¡å‹çš„æ—¶é—´è¿è´¯æ€§ä¸åŠ¨æ€äº¤äº’ï¼Œæ­ç¤ºäº†åœ¨æ‰©å±•äº¤äº’åºåˆ—ä¸­ä¿æŒä¸€è‡´æ€§æ–¹é¢å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09007",
      "arxiv_url": "https://arxiv.org/abs/2602.09007",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09007",
      "github_url": "https://github.com/stepfun-ai/GEBench",
      "upvotes": 38,
      "fetched_at": "2026-02-19T06:03:47.845871+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08439",
      "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
      "authors": [
        "Yuhao Dong",
        "Shulin Tian",
        "Shuai Liu",
        "Shuangrui Ding",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Ziwei Liu"
      ],
      "abstract": "Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization. Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning , a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench , a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization , jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench , demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
      "summary_en": "Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.",
      "summary_zh": "ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€é¡¹ç”¨äºè¯„ä¼°æ¨¡å‹ä»å°‘æ ·æœ¬ç¤ºä¾‹ä¸­å­¦ä¹ èƒ½åŠ›çš„è§†é¢‘ç†è§£ä»»åŠ¡ä¸åŸºå‡†ï¼Œä»¥åŠä¸€ç§é‡‡ç”¨è§†é¢‘ç›‘ç£ä¸åå¥½ä¼˜åŒ–ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•çš„ä¸“ç”¨MLLMæ¶æ„ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08439",
      "arxiv_url": "https://arxiv.org/abs/2602.08439",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08439",
      "github_url": "https://github.com/dongyh20/Demo-ICL",
      "upvotes": 28,
      "fetched_at": "2026-02-19T06:03:20.893592+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06025",
      "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
      "authors": [
        "Haozhen Zhang",
        "Haodong Yue",
        "Tao Feng",
        "Quanyu Long",
        "Jianzhu Bao",
        "Bowen Jin",
        "Weizhi Zhang",
        "Xiao Li",
        "Jiaxuan You",
        "Chengwei Qin",
        "Wenya Wang"
      ],
      "abstract": "BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage. Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control . BudgetMem structures memory processing as a set of memory modules , each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning . Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers : implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo , LongMemEval , and HotpotQA , BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
      "summary_en": "BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.",
      "summary_zh": "BudgetMemæ˜¯ä¸€ç§é¢å‘LLMæ™ºèƒ½ä½“çš„è¿è¡Œæ—¶å†…å­˜æ¡†æ¶ï¼Œé‡‡ç”¨åŒ…å«ä¸‰çº§é¢„ç®—å±‚çº§çš„æ¨¡å—åŒ–ç»„ä»¶å’Œç¥ç»ç­–ç•¥è·¯ç”±å™¨ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ä¸­çš„æ€§èƒ½ä¸æˆæœ¬æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06025",
      "arxiv_url": "https://arxiv.org/abs/2602.06025",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06025",
      "github_url": "https://github.com/ViktorAxelsen/BudgetMem",
      "upvotes": 27,
      "fetched_at": "2026-02-19T06:02:03.049606+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08543",
      "title": "GISA: A Benchmark for General Information-Seeking Assistant",
      "authors": [
        "Yutao Zhu",
        "Xingshuo Zhang",
        "Maosen Zhang",
        "Jiajie Jin",
        "Liancheng Zhang",
        "Xiaoshuai Song",
        "Kangzhi Zhao",
        "Wencong Zeng",
        "Ruiming Tang",
        "Han Li",
        "Ji-Rong Wen",
        "Zhicheng Dou"
      ],
      "abstract": "A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization. The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score , with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
      "summary_en": "A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.",
      "summary_zh": "æ–°åŸºå‡† GISA ç”¨äºè¯„ä¼°ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ï¼Œå…·æœ‰äººå·¥ç¼–å†™çš„æŸ¥è¯¢ã€ç»“æ„åŒ–ç­”æ¡ˆæ ¼å¼å’Œå®æ—¶æ›´æ–°æœºåˆ¶ï¼Œä»¥é˜²æ­¢è®°å¿†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08543",
      "arxiv_url": "https://arxiv.org/abs/2602.08543",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08543",
      "github_url": "https://github.com/RUC-NLPIR/GISA",
      "upvotes": 26,
      "fetched_at": "2026-02-19T06:03:22.996714+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07962",
      "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
      "authors": [
        "Weihao Zeng",
        "Yuzhen Huang",
        "Junxian He"
      ],
      "abstract": "LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management. Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \" context rot \". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies . While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/ LOCA-bench",
      "summary_en": "LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.",
      "summary_zh": "LOCA-bench è¢«æå‡ºä½œä¸ºè¯„ä¼°è¯­è¨€æ™ºèƒ½ä½“çš„åŸºå‡†ï¼Œé’ˆå¯¹å…·å¤‡å—æ§ç¯å¢ƒçŠ¶æ€ç®¡ç†çš„é•¿ä¸Šä¸‹æ–‡æ™ºèƒ½ä½“åœºæ™¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07962",
      "arxiv_url": "https://arxiv.org/abs/2602.07962",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07962",
      "github_url": "https://github.com/hkust-nlp/LOCA-bench",
      "upvotes": 24,
      "fetched_at": "2026-02-19T06:03:05.810265+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07055",
      "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
      "authors": [
        "Pingyue Zhang",
        "Zihan Huang",
        "Yue Wang",
        "Jieyu Zhang",
        "Letian Xue",
        "Zihan Wang",
        "Qineng Wang",
        "Keshigeyan Chandrasegaran",
        "Ruohan Zhang",
        "Yejin Choi",
        "Ranjay Krishna",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Manling Li"
      ],
      "abstract": "Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge. Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing , which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap , where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia , where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models . Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial belief s during active exploration .",
      "summary_en": "Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.",
      "summary_zh": "å½“å‰å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ä¸»åŠ¨æ¢ç´¢è¿‡ç¨‹ä¸­ç»´æŒè¿è´¯ç©ºé—´ä¿¡å¿µæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè¡¨ç°å‡ºä¸»åŠ¨ä¸è¢«åŠ¨æ€§èƒ½å·®è·ã€æ¢ç´¢ç­–ç•¥ä½æ•ˆä»¥åŠæ›´æ–°è¿‡æ—¶ç©ºé—´çŸ¥è¯†å›°éš¾ç­‰é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07055",
      "arxiv_url": "https://arxiv.org/abs/2602.07055",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07055",
      "github_url": "https://github.com/mll-lab-nu/Theory-of-Space",
      "upvotes": 22,
      "fetched_at": "2026-02-19T06:02:29.766955+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06540",
      "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
      "authors": [
        "Yishan Li",
        "Wentong Chen",
        "Yukun Yan",
        "Mingwei Li",
        "Sen Mei",
        "Xiaorong Wang",
        "Kunpeng Liu",
        "Xin Cong",
        "Shuo Wang",
        "Zhong Zhang",
        "Yaxi Lu",
        "Zhenghao Liu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "",
      "summary_en": "AgentCPM-Reportæ˜¯ç”± THUNLP ã€ä¸­å›½äººæ°‘å¤§å­¦ RUCBM å’Œ ModelBest è”åˆå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ã€‚å®ƒåŸºäº MiniCPM4.1 80äº¿å‚æ•°åŸºåº§æ¨¡å‹ï¼Œæ¥å—ç”¨æˆ·æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œè‡ªä¸»ç”Ÿæˆé•¿ç¯‡æŠ¥å‘Šã€‚å…¶æœ‰ä»¥ä¸‹äº®ç‚¹ï¼š",
      "summary_zh": "AgentCPM-Reportæ˜¯ç”±THUNLPã€ä¸­å›½äººæ°‘å¤§å­¦RUCBMå’ŒModelBestè”åˆå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ã€‚å®ƒåŸºäºMiniCPM4.1 80äº¿å‚æ•°åŸºåº§æ¨¡å‹ï¼Œæ¥å—ç”¨æˆ·æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œè‡ªä¸»ç”Ÿæˆé•¿ç¯‡æŠ¥å‘Šã€‚å…¶æœ‰ä»¥ä¸‹äº®ç‚¹ï¼š",
      "hf_url": "https://huggingface.co/papers/2602.06540",
      "arxiv_url": "https://arxiv.org/abs/2602.06540",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06540",
      "github_url": "https://github.com/OpenBMB/AgentCPM",
      "upvotes": 21,
      "fetched_at": "2026-02-19T06:02:12.686518+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.09022",
      "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
      "authors": [
        "Zehan Wang",
        "Tengfei Wang",
        "Haiyu Zhang",
        "Xuhui Zuo",
        "Junta Wu",
        "Haoyuan Wang",
        "Wenqiang Sun",
        "Zhenwei Wang",
        "Chenjie Cao",
        "Hengshuang Zhao",
        "Chunchao Guo",
        "Zhou Zhao"
      ],
      "abstract": "WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms. This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models , enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy : We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions : We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
      "summary_en": "WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.",
      "summary_zh": "WorldCompass é€šè¿‡é‡‡ç”¨ç‰‡æ®µçº§ rolloutã€äº’è¡¥å¥–åŠ±å’Œé«˜æ•ˆ RL ç®—æ³•çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œå¢å¼ºåŸºäºè§†é¢‘çš„é•¿ç¨‹ä¸–ç•Œæ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09022",
      "arxiv_url": "https://arxiv.org/abs/2602.09022",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09022",
      "github_url": "",
      "upvotes": 20,
      "fetched_at": "2026-02-19T06:03:50.195696+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07075",
      "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
      "authors": [
        "Xinwu Ye",
        "Yicheng Mao",
        "Jia Zhang",
        "Yimeng Liu",
        "Li Hao",
        "Fang Wu",
        "Zhiwei Li",
        "Yuxuan Liao",
        "Zehong Wang",
        "Zhiyuan Liu",
        "Zhenfei Yin",
        "Li Yuan",
        "Philip Torr",
        "Huan Sun",
        "Xiangxiang Zeng",
        "Mengdi Wang",
        "Le Cong",
        "Shenghua Gao",
        "Xiangru Tang"
      ],
      "abstract": "LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches. Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation , enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench , while delivering a 10.84times average inference speedup . Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
      "summary_en": "LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.",
      "summary_zh": "LatentChemé€šè¿‡è¿ç»­æ½œåœ¨ç©ºé—´è®¡ç®—è€Œéç¦»æ•£æ–‡æœ¬tokenå®ç°åŒ–å­¦æ¨ç†ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ€ç»´é“¾æ–¹æ³•å–å¾—äº†æ›´ä¼˜çš„æ€§èƒ½ä¸æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07075",
      "arxiv_url": "https://arxiv.org/abs/2602.07075",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07075",
      "github_url": "https://github.com/xinwuye/LatentChem",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:02:32.477251+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06694",
      "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
      "authors": [
        "Hyochan Chong",
        "Dongkyu Kim",
        "Changdong Kim",
        "Minseop Choi"
      ],
      "abstract": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment. Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization , achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
      "summary_en": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.",
      "summary_zh": "NanoQuanté€šè¿‡ä½ç§©äºŒå€¼åˆ†è§£å’ŒADMMä¼˜åŒ–ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆåè®­ç»ƒé‡åŒ–è‡³äºŒå€¼åŠäºš1æ¯”ç‰¹çº§åˆ«ï¼Œåœ¨è¾¾åˆ°æœ€å…ˆè¿›ç²¾åº¦çš„åŒæ—¶é™ä½æ¶ˆè´¹çº§ç¡¬ä»¶éƒ¨ç½²çš„å†…å­˜éœ€æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06694",
      "arxiv_url": "https://arxiv.org/abs/2602.06694",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06694",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:02:16.508724+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.03784",
      "title": "Context Compression via Explicit Information Transmission",
      "authors": [
        "Jiangnan Ye",
        "Hanqi Yan",
        "Zhenyi Shen",
        "Heng Chang",
        "Ye Mao",
        "Yulan He"
      ],
      "abstract": "ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms. Long-context inference with Large Language Models ( LLMs ) is costly due to quadratic attention and growing key-value caches , motivating context compression. In this work, we study soft context compression , where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission ), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states . This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors , mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
      "summary_en": "ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.",
      "summary_zh": "ComprExIT æå‡ºäº†ä¸€ç§ç”¨äºå¤§è¯­è¨€æ¨¡å‹é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å†»ç»“éšçŠ¶æ€ä¸Šçš„æ˜¾å¼ä¿¡æ¯ä¼ è¾“ï¼Œå¹¶å€ŸåŠ©æ·±åº¦ä¸å®½åº¦æ–¹å‘çš„ä¼ è¾“æœºåˆ¶æ¥æå‡å‹ç¼©æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03784",
      "arxiv_url": "https://arxiv.org/abs/2602.03784",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03784",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-19T06:01:53.843519+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08658",
      "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
      "authors": [
        "Mingzi Cao",
        "Xingwei Tan",
        "Mahmud Akhter",
        "Marco Valentino",
        "Maria Liakata",
        "Xi Wang",
        "Nikolaos Aletras"
      ],
      "abstract": "Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks. Deduction, induction, and abduction are fundamental reasoning paradigms , core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning , and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts . We comprehensively evaluate induced models on realistic out-of-domain tasks , that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks.",
      "summary_en": "Research investigates how fundamental reasoning paradigms influence large language model generalization through targeted training approaches and evaluation on real-world tasks.",
      "summary_zh": "ç ”ç©¶é€šè¿‡é’ˆå¯¹æ€§è®­ç»ƒæ–¹æ³•å’Œå¯¹çœŸå®ä¸–ç•Œä»»åŠ¡çš„è¯„ä¼°ï¼Œæ¢è®¨åŸºç¡€æ¨ç†èŒƒå¼å¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08658",
      "arxiv_url": "https://arxiv.org/abs/2602.08658",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08658",
      "github_url": "https://github.com/voalmciaf/FR-OOD",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:03:27.640100+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06454",
      "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
      "authors": [
        "Jiwon Song",
        "Yoongon Kim",
        "Jae-Joon Kim"
      ],
      "abstract": "RelayGen is a training-free framework that dynamically switches between large and small reasoning models during inference based on segment-level difficulty estimation, achieving faster execution with minimal accuracy loss. Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories , but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins , we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding , RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
      "summary_en": "RelayGen is a training-free framework that dynamically switches between large and small reasoning models during inference based on segment-level difficulty estimation, achieving faster execution with minimal accuracy loss.",
      "summary_zh": "RelayGenæ˜¯ä¸€ç§å…è®­ç»ƒæ¡†æ¶ï¼Œå®ƒåŸºäºç‰‡æ®µçº§éš¾åº¦ä¼°è®¡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ‡æ¢å¤§å‹ä¸å°å‹æ¨ç†æ¨¡å‹ï¼Œä»¥æå°çš„ç²¾åº¦æŸå¤±å®ç°æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06454",
      "arxiv_url": "https://arxiv.org/abs/2602.06454",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06454",
      "github_url": "https://github.com/jiwonsong-dev/RelayGen",
      "upvotes": 11,
      "fetched_at": "2026-02-19T06:02:10.509721+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08236",
      "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
      "authors": [
        "Shoubin Yu",
        "Yue Zhang",
        "Zun Wang",
        "Jaehong Yoon",
        "Huaxiu Yao",
        "Mingyu Ding",
        "Mohit Bansal"
      ],
      "abstract": "Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary. Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination , but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination . Across spatial reasoning benchmarks ( SAT , MMSI ) and an embodied navigation benchmark ( R2R ), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
      "summary_en": "Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.",
      "summary_zh": "ç»“åˆä¸–ç•Œæ¨¡å‹çš„è‡ªé€‚åº”æµ‹è¯•æ—¶æ¡†æ¶æ”¯æŒé€‰æ‹©æ€§è§†è§‰æƒ³è±¡ä»¥è¿›è¡Œç©ºé—´æ¨ç†ï¼Œé€šè¿‡åˆ¤æ–­ä½•æ—¶éœ€è¦æƒ³è±¡æ¥æé«˜æ•ˆç‡ä¸å¯é æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08236",
      "arxiv_url": "https://arxiv.org/abs/2602.08236",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08236",
      "github_url": "https://github.com/Yui010206/Adaptive-Visual-Imagination-Control",
      "upvotes": 9,
      "fetched_at": "2026-02-19T06:03:16.329480+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08808",
      "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
      "authors": [
        "Yapei Chang",
        "Kyle Lo",
        "Mohit Iyyer",
        "Luca Soldaini"
      ],
      "abstract": "A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality. Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation . Our framework includes How2Mine , which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench , a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score , an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining . Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
      "summary_en": "A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.",
      "summary_zh": "ä¸€ç§å¯æ‰©å±•æ¡†æ¶ï¼Œé€šè¿‡å¤§è§„æ¨¡ç½‘ç»œæŒ–æ˜ã€è‡ªåŠ¨è¯„åˆ†å’Œå¼ºåŒ–å­¦ä¹ æ¥è¯„ä¼°ä¸æ”¹è¿›ç›®æ ‡æ¡ä»¶åŒ–ç¨‹åºç”Ÿæˆï¼Œä»¥æå‡åˆ†æ­¥æŒ‡ä»¤è´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08808",
      "arxiv_url": "https://arxiv.org/abs/2602.08808",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08808",
      "github_url": "https://github.com/lilakk/how2everything",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:03:34.360855+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08145",
      "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
      "authors": [
        "Xinyu Yang",
        "Junlin Han",
        "Rishi Bommasani",
        "Jinqi Luo",
        "Wenjie Qu",
        "Wangchunshu Zhou",
        "Adel Bibi",
        "Xiyao Wang",
        "Jaehong Yoon",
        "Elias Stengel-Eskin",
        "Shengbang Tong",
        "Lingfeng Shen",
        "Rafael Rafailov",
        "Runjia Li",
        "Zhaoyang Wang",
        "Yiyang Zhou",
        "Chenhang Cui",
        "Yu Wang",
        "Wenhao Zheng",
        "Huichi Zhou",
        "Jindong Gu",
        "Zhaorun Chen"
      ],
      "abstract": "Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.",
      "summary_en": "Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.",
      "summary_zh": "åŒ…æ‹¬LLMsã€MLLMså’Œç”Ÿæˆæ¨¡å‹åœ¨å†…çš„åŸºç¡€æ¨¡å‹éœ€è¦å¯é ä¸”è´Ÿè´£ä»»çš„å¼€å‘ï¼Œè§£å†³åè§ã€å®‰å…¨æ€§ã€å¯è§£é‡Šæ€§åŠå…¶ä»–å…³é”®é—®é¢˜ï¼Œä»¥å®ç°è·¨å¤šä¸ªé¢†åŸŸçš„å¯ä¿¡éƒ¨ç½²ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08145",
      "arxiv_url": "https://arxiv.org/abs/2602.08145",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08145",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:03:11.757019+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07796",
      "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
      "authors": [
        "Jiatong Li",
        "Changdae Oh",
        "Hyeong Kyu Choi",
        "Jindong Wang",
        "Sharon Li"
      ],
      "abstract": "Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results. Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.",
      "summary_en": "Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.",
      "summary_zh": "åœ¨ç”¨æˆ·å‚ä¸åœºæ™¯ä¸­ï¼ŒLLMæ™ºèƒ½ä½“çš„æ˜¾å¼æ¨ç†å¯èƒ½å› å‡å°‘ä¿¡æ¯æŠ«éœ²å¹¶å‰Šå¼±æ™ºèƒ½ä½“ä¸ç”¨æˆ·çš„æ²Ÿé€šè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œé€æ˜æ€§æ„ŸçŸ¥æç¤ºåˆ™èƒ½å–å¾—æ›´å¥½æ•ˆæœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07796",
      "arxiv_url": "https://arxiv.org/abs/2602.07796",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07796",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:02:54.402668+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07775",
      "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
      "authors": [
        "Haodong Li",
        "Shaoteng Liu",
        "Zhe Lin",
        "Manmohan Chandraker"
      ],
      "abstract": "Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis. Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing , which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance . These insights lead to Rolling Sink . Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
      "summary_en": "Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.",
      "summary_zh": "è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶å­˜åœ¨è®­ç»ƒ-æµ‹è¯•å·®è·ï¼Œä½†ä¸€ç§åä¸ºRolling Sinkçš„å…è®­ç»ƒæ–¹æ³•é€šè¿‡ç»´æŠ¤ARç¼“å­˜å¹¶æ”¯æŒè¶…é•¿è§†é¢‘åˆæˆè§£å†³äº†è¯¥é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07775",
      "arxiv_url": "https://arxiv.org/abs/2602.07775",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07775",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:02:52.242091+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07080",
      "title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs",
      "authors": [
        "Yicheng He",
        "Zheng Zhao",
        "Zhou Kaiyu",
        "Bryan Dai",
        "Jie Fu",
        "Yonghui Yang"
      ],
      "abstract": "LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits. Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability , we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs . By decomposing complex residual flows , we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.",
      "summary_en": "LLM code verification can be achieved through internal neural dynamics analysis, identifying structural signatures that distinguish correct reasoning from logical failures in computational circuits.",
      "summary_zh": "LLMä»£ç éªŒè¯å¯é€šè¿‡å†…éƒ¨ç¥ç»åŠ¨åŠ›å­¦åˆ†æå®ç°ï¼Œè¯†åˆ«åŒºåˆ†è®¡ç®—å›è·¯ä¸­æ­£ç¡®æ¨ç†ä¸é€»è¾‘é”™è¯¯çš„ç»“æ„ç‰¹å¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07080",
      "arxiv_url": "https://arxiv.org/abs/2602.07080",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07080",
      "github_url": "https://github.com/bruno686/CodeCircuit",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:02:34.578137+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.09003",
      "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
      "authors": [
        "Yudong Wang",
        "Zixuan Fu",
        "Hengyu Zhao",
        "Chen Zhao",
        "Chuyue Zhou",
        "Xinle Lin",
        "Hongya Lyu",
        "Shuaikang Xue",
        "Yi Yi",
        "Yingjiao Wang",
        "Zhi Zheng",
        "Yuzhou Zhang",
        "Jie Zhou",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development. The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency . In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training , mid-training , and alignment . The framework balances data quality , acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management . We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.",
      "summary_en": "Large language models are increasingly guiding data management processes through a tiered framework that optimizes data quality, cost, and training efficiency across different stages of model development.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹æ­£æ—¥ç›Šé€šè¿‡åˆ†å±‚æ¡†æ¶æŒ‡å¯¼æ•°æ®ç®¡ç†æµç¨‹ï¼Œåœ¨æ¨¡å‹å¼€å‘çš„ä¸åŒé˜¶æ®µä¼˜åŒ–æ•°æ®è´¨é‡ã€æˆæœ¬å’Œè®­ç»ƒæ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09003",
      "arxiv_url": "https://arxiv.org/abs/2602.09003",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09003",
      "github_url": "https://github.com/UltraData-OpenBMB/UltraData-Math",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:03:45.538931+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07803",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "authors": [
        "Jiale Qian",
        "Hao Meng",
        "Tian Zheng",
        "Pengcheng Zhu",
        "Haopeng Lin",
        "Yuhang Dai",
        "Hanke Xie",
        "Wenxiao Cao",
        "Ruixuan Shang",
        "Jun Wu",
        "Hongmei Liu",
        "Hanlin Wen",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen",
        "Shunshun Yin",
        "Ming Tao",
        "Jianguo Wei",
        "Lei Xie",
        "Xinsheng Wang"
      ],
      "abstract": "A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.",
      "summary_en": "A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.",
      "summary_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¼€æºæ­Œå£°åˆæˆç³»ç»Ÿï¼Œæ”¯æŒå¤šè¯­è¨€å’Œå¯æ§ç”Ÿæˆï¼ŒåŒæ—¶æä¾›äº†ä¸€ä¸ªç”¨äºè¯„ä¼°é›¶æ ·æœ¬æ€§èƒ½çš„ä¸“ç”¨åŸºå‡†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07803",
      "arxiv_url": "https://arxiv.org/abs/2602.07803",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07803",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:02:56.541917+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2601.21363",
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "authors": [
        "Weidong Huang",
        "Zhehan Li",
        "Hangxin Liu",
        "Biao Hou",
        "Yao Su",
        "Jingwen Zhang"
      ],
      "abstract": "Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models. Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency , the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model . This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
      "summary_en": "Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.",
      "summary_zh": "ç¦»ç­–ç•¥Soft Actor-Criticé€šè¿‡å¤§æ‰¹é‡æ›´æ–°å®ç°é«˜æ•ˆçš„äººå½¢æœºå™¨äººè¿åŠ¨ç­–ç•¥é¢„è®­ç»ƒï¼Œè€ŒåŸºäºæ¨¡å‹çš„æ–¹æ³•åˆ™é€šè¿‡ç¡®å®šæ€§æ•°æ®æ”¶é›†ä¸ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹ä¸­çš„éšæœºæ¢ç´¢æ¥ä¿ƒè¿›å®‰å…¨é€‚åº”ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21363",
      "arxiv_url": "https://arxiv.org/abs/2601.21363",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21363",
      "github_url": "https://github.com/bigai-ai/LIFT-humanoid",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:01:44.620575+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.09782",
      "title": "Flexible Entropy Control in RLVR with Gradient-Preserving Perspective",
      "authors": [
        "Kun Chen",
        "Peng Shi",
        "Fanfan Liu",
        "Haibo Qiu",
        "Zhixiong Zeng",
        "Siqi Yang",
        "Wenji Mao"
      ],
      "abstract": "Reinforcement learning with verifiable rewards faces entropy collapse issues due to gradient-preserving clipping, which this paper addresses through dynamic entropy control mechanisms. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse , characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control . This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping . We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies , including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.",
      "summary_en": "Reinforcement learning with verifiable rewards faces entropy collapse issues due to gradient-preserving clipping, which this paper addresses through dynamic entropy control mechanisms.",
      "summary_zh": "åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ å› æ¢¯åº¦ä¿æŒè£å‰ªè€Œé¢ä¸´ç†µåç¼©é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡åŠ¨æ€ç†µæ§åˆ¶æœºåˆ¶è§£å†³è¯¥é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09782",
      "arxiv_url": "https://arxiv.org/abs/2602.09782",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09782",
      "github_url": "https://github.com/Kwen-Chen/Flexible-Entropy-Control",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:03:52.308339+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08961",
      "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
      "authors": [
        "Ruijie Zhu",
        "Jiahao Lu",
        "Wenbo Hu",
        "Xiaoguang Han",
        "Jianfei Cai",
        "Ying Shan",
        "Chuanxia Zheng"
      ],
      "abstract": "MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture. We introduce MotionCrafter, a video diffusion -based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system , and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents -despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
      "summary_en": "MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.",
      "summary_zh": "MotionCrafteræ˜¯ä¸€ç§è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œåˆ©ç”¨æ–°é¢–çš„è”åˆè¡¨ç¤ºä¸4D VAEæ¶æ„ï¼Œè”åˆé‡å»º4Då‡ ä½•å¹¶ä¼°è®¡ç¨ å¯†è¿åŠ¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08961",
      "arxiv_url": "https://arxiv.org/abs/2602.08961",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08961",
      "github_url": "https://github.com/TencentARC/MotionCrafter",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:03:40.866859+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08829",
      "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
      "authors": [
        "Hao Peng",
        "Yunjia Qi",
        "Xiaozhi Wang",
        "Zijun Yao",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity. Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions ? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models , with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models . Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
      "summary_en": "WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.",
      "summary_zh": "WildReward è¡¨æ˜ï¼Œåˆ©ç”¨åºæ•°å›å½’åŸºäºé‡å¤–ç”¨æˆ·äº¤äº’å¯æœ‰æ•ˆè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œåœ¨è¾¾åˆ°ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“æ€§èƒ½çš„åŒæ—¶å—ç›Šäºç”¨æˆ·å¤šæ ·æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08829",
      "arxiv_url": "https://arxiv.org/abs/2602.08829",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08829",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:03:38.633455+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08004",
      "title": "Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality",
      "authors": [
        "George Ling",
        "Shanshan Zhong",
        "Richard Huang"
      ],
      "abstract": "",
      "summary_en": "",
      "summary_zh": "",
      "hf_url": "https://huggingface.co/papers/2602.08004",
      "arxiv_url": "https://arxiv.org/abs/2602.08004",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08004",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:03:09.896374+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06942",
      "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
      "authors": [
        "Duygu Altinok"
      ],
      "abstract": "A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics. Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics , and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization ; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets ( WordPiece , morphology level , and character baselines), and evaluates across semantic ( NLI , STS , sentiment analysis , NER ), syntactic ( POS , dependency parsing ), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under- segmentation indices , character/word edit distances ( CER / WER ), continuation rates , and affix-type coverage and token-level atomicity . Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.",
      "summary_en": "A comprehensive study of Turkish subword tokenization systematically investigates the relationship between vocabulary size, training corpus, and tokenizer performance across multiple linguistic tasks and diagnostics.",
      "summary_zh": "ä¸€é¡¹é’ˆå¯¹åœŸè€³å…¶è¯­å­è¯åˆ†è¯çš„å…¨é¢ç ”ç©¶ç³»ç»Ÿæ€§åœ°æ¢ç©¶äº†è¯æ±‡é‡ã€è®­ç»ƒè¯­æ–™åº“ä¸åˆ†è¯å™¨æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œæ¶µç›–å¤šä¸ªè¯­è¨€ä»»åŠ¡ä¸è¯Šæ–­æµ‹è¯•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06942",
      "arxiv_url": "https://arxiv.org/abs/2602.06942",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06942",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:02:21.125671+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06445",
      "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
      "authors": [
        "Weidong Huang",
        "Jingwen Zhang",
        "Jiongye Li",
        "Shibowen Zhang",
        "Jiayang Wu",
        "Jiayi Wang",
        "Hangxin Liu",
        "Yaodong Yang",
        "Yao Su"
      ],
      "abstract": "Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning. Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO ( Energy-Constrained Optimization ), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method , to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfer s on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
      "summary_en": "Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.",
      "summary_zh": "èƒ½é‡çº¦æŸä¼˜åŒ–æ¡†æ¶ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥æ–¹æ³•å°†èƒ½é‡æŒ‡æ ‡ä¸å¥–åŠ±åˆ†ç¦»ï¼Œå®ç°ç¨³å®šã€èŠ‚èƒ½çš„äººå½¢æœºå™¨äººè¿åŠ¨ï¼Œå¹¶å‡å°‘è¶…å‚æ•°è°ƒä¼˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06445",
      "arxiv_url": "https://arxiv.org/abs/2602.06445",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06445",
      "github_url": "https://github.com/bigai-ai/ECO-humanoid",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:02:07.963400+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08818",
      "title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models",
      "authors": [
        "Annemette Brok Pirchert",
        "Jacob Nielsen",
        "Mogens Henrik From",
        "Lukas Galke Poech",
        "Peter Schneider-Kamp"
      ],
      "abstract": "FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters. Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts , which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 2^0 to 2^{14} resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available.",
      "summary_en": "FlexMoRE demonstrates that low-rank adapters can replace full-sized experts in mixture-of-experts architectures, achieving better performance with significantly fewer parameters.",
      "summary_zh": "FlexMoRE è¡¨æ˜ï¼Œä½ç§©é€‚é…å™¨å¯ä»¥æ›¿ä»£æ··åˆä¸“å®¶æ¶æ„ä¸­çš„å…¨å°ºå¯¸ä¸“å®¶ï¼Œä»¥æ˜¾è‘—æ›´å°‘çš„å‚æ•°å®ç°æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08818",
      "arxiv_url": "https://arxiv.org/abs/2602.08818",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08818",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:03:36.202095+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07150",
      "title": "On Randomness in Agentic Evals",
      "authors": [
        "Bjarni Haukur Bjarnason",
        "AndrÃ© Silva",
        "Martin Monperrus"
      ],
      "abstract": "Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.",
      "summary_en": "Analysis of agentic system evaluation reveals significant variance in single-run performance estimates, necessitating multiple runs and advanced metrics for reliable assessment.",
      "summary_zh": "å¯¹æ™ºèƒ½ä½“ç³»ç»Ÿè¯„ä¼°çš„åˆ†æè¡¨æ˜ï¼Œå•æ¬¡è¿è¡Œçš„æ€§èƒ½ä¼°è®¡æ–¹å·®æ˜¾è‘—ï¼Œéœ€è¦è¿›è¡Œå¤šæ¬¡è¿è¡Œå¹¶é‡‡ç”¨é«˜çº§æŒ‡æ ‡ä»¥å®ç°å¯é è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07150",
      "arxiv_url": "https://arxiv.org/abs/2602.07150",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07150",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:02:45.134118+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07040",
      "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
      "authors": [
        "Emmett Bicker"
      ],
      "abstract": "Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements. We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performance s. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs. We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute. Aster is accessible via a web interface and API at asterlab.ai.",
      "summary_en": "Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.",
      "summary_zh": "Aster æ˜¯ä¸€ç§ AI æ™ºèƒ½ä½“ï¼Œé€šè¿‡è¿­ä»£æ”¹è¿›ç¨‹åºåŠ é€Ÿç§‘å­¦å‘ç°ï¼Œåœ¨æ•°å­¦ã€ç”Ÿç‰©å­¦å’Œæœºå™¨å­¦ä¹ ç­‰å¤šä¸ªé¢†åŸŸå–å¾—æœ€å…ˆè¿›æˆæœï¼Œä¸”è®¡ç®—éœ€æ±‚å¤§å¹…é™ä½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07040",
      "arxiv_url": "https://arxiv.org/abs/2602.07040",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07040",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:02:25.326140+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.06600",
      "title": "Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning",
      "authors": [
        "Zhuoyuan Hao",
        "Zhuo Li",
        "Wu Li",
        "Fangming Liu",
        "Min Zhang",
        "Jing Li"
      ],
      "abstract": "Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques. Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking , adding generic `` thinking tokens '' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the Echo of Prompt (EOP), as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap Î”L as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (ED-SFT) to instill an ``echo-then-reason'' pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.",
      "summary_en": "Large reasoning models exhibit spontaneous question repetition patterns that can be formalized and leveraged to improve computational efficiency and accuracy through echo-aware training and prompting techniques.",
      "summary_zh": "å¤§å‹æ¨ç†æ¨¡å‹è¡¨ç°å‡ºè‡ªå‘çš„é—®é¢˜é‡å¤æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯è¢«å½¢å¼åŒ–å¹¶åŠ ä»¥åˆ©ç”¨ï¼Œé€šè¿‡å›å£°æ„ŸçŸ¥è®­ç»ƒä¸æç¤ºæŠ€æœ¯æå‡è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06600",
      "arxiv_url": "https://arxiv.org/abs/2602.06600",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06600",
      "github_url": "https://github.com/hhh2210/echoes-as-anchors",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:02:14.688561+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.05929",
      "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
      "authors": [
        "Jian Chen",
        "Zhuoran Wang",
        "Jiayu Qin",
        "Ming Li",
        "Meng Wang",
        "Changyou Chen",
        "Yin Chen",
        "Qizhen Weng",
        "Yirui Liu"
      ],
      "abstract": "KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains. Large language models rely on kv-cache s to avoid redundant computation during autoregressive decoding , but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth . Recent work has explored KV-cache compression , yet most approaches neglect the data-dependent nature of kv-cache s and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-cache s. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation . Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
      "summary_en": "KV-CoRE method evaluates kv-cache compressibility through SVD-based low-rank approximation, revealing patterns linking compressibility to model architecture and training data across multiple languages and domains.",
      "summary_zh": "KV-CoREæ–¹æ³•é€šè¿‡åŸºäºSVDçš„ä½ç§©è¿‘ä¼¼è¯„ä¼°KVç¼“å­˜å¯å‹ç¼©æ€§ï¼Œæ­ç¤ºå…¶ä¸æ¨¡å‹æ¶æ„åŠè®­ç»ƒæ•°æ®ä¹‹é—´çš„å…³è”è§„å¾‹ï¼Œæ¶µç›–å¤šç§è¯­è¨€å’Œé¢†åŸŸã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05929",
      "arxiv_url": "https://arxiv.org/abs/2602.05929",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05929",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:01:58.158978+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.02827",
      "title": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval",
      "authors": [
        "Roi Pony",
        "Adi Raz",
        "Oshri Naparstek",
        "Idan Friedman",
        "Udi Barzelay"
      ],
      "abstract": "Col-Bandit reduces computational costs in multi-vector late-interaction retrieval by adaptively pruning token-level interactions during query processing while maintaining ranking accuracy. Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-K identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5times, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.",
      "summary_en": "Col-Bandit reduces computational costs in multi-vector late-interaction retrieval by adaptively pruning token-level interactions during query processing while maintaining ranking accuracy.",
      "summary_zh": "Col-Bandité€šè¿‡åœ¨æŸ¥è¯¢å¤„ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°å‰ªæè¯å…ƒçº§äº¤äº’ï¼Œåœ¨å¤šå‘é‡åæœŸäº¤äº’æ£€ç´¢ä¸­é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ’åºå‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02827",
      "arxiv_url": "https://arxiv.org/abs/2602.02827",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02827",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:01:51.857309+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.08629",
      "title": "CauScale: Neural Causal Discovery at Scale",
      "authors": [
        "Bo Peng",
        "Sirui Chen",
        "Jiaguo Tian",
        "Yu Qiao",
        "Chaochao Lu"
      ],
      "abstract": "CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods. Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention map s. To keep high causal discovery accuracy, CauScale adopts a two-stream design : a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals . CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.",
      "summary_en": "CauScale is a neural architecture that enables efficient causal discovery on large graphs through compressed embeddings and tied attention weights, achieving high accuracy and significant speedups over previous methods.",
      "summary_zh": "CauScaleæ˜¯ä¸€ç§ç¥ç»æ¶æ„ï¼Œé€šè¿‡å‹ç¼©åµŒå…¥å’Œå…±äº«æ³¨æ„åŠ›æƒé‡ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡å›¾ä¸Šé«˜æ•ˆè¿›è¡Œå› æœå‘ç°ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡å¹¶æ˜¾è‘—å¿«äºå…ˆå‰æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08629",
      "arxiv_url": "https://arxiv.org/abs/2602.08629",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08629",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:03:25.391962+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07970",
      "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
      "authors": [
        "Zheyuan Hu",
        "Weitao Chen",
        "Cengiz Ã–ztireli",
        "Chenliang Zhou",
        "Fangcheng Zhong"
      ],
      "abstract": "Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems. Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers , and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings , together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
      "summary_en": "Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.",
      "summary_zh": "ç ”ç©¶æ¢ç´¢äº†åå¾®åˆ†æ–¹ç¨‹æ±‚è§£å™¨ï¼ˆåŒ…æ‹¬é¢å‘ç§‘å­¦æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæ¡†æ¶ï¼‰ï¼Œè€ƒå¯Ÿäº†å¤šå˜é‡ä¸éçº¿æ€§ç³»ç»Ÿä¸­çš„æ­£é—®é¢˜ã€é€†é—®é¢˜åŠæ–¹ç¨‹å‘ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07970",
      "arxiv_url": "https://arxiv.org/abs/2602.07970",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07970",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:03:07.891150+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07491",
      "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design",
      "authors": [
        "Isabella A. Stewart",
        "Tarjei Paule Hage",
        "Yu-Chuan Hsu",
        "Markus J. Buehler"
      ],
      "abstract": "A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives. Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science , where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition , evidence retrieval , design parameter extraction, and graph traversal , uncovering latent connections across distinct knowledge pockets to support hypothesis generation . Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance , thermal stability , chemical resistance , and biocompatibility . This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.",
      "summary_en": "A multi-agent framework guided by knowledge graphs addresses materials science challenges by integrating specialized agents for problem decomposition, evidence retrieval, and graph traversal to discover sustainable PFAS alternatives.",
      "summary_zh": "ç”±çŸ¥è¯†å›¾è°±å¼•å¯¼çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶é€šè¿‡æ•´åˆä¸“é—¨ç”¨äºé—®é¢˜åˆ†è§£ã€è¯æ®æ£€ç´¢å’Œå›¾éå†çš„æ™ºèƒ½ä½“æ¥å‘ç°å¯æŒç»­PFASæ›¿ä»£å“ï¼Œä»è€Œåº”å¯¹ææ–™ç§‘å­¦æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07491",
      "arxiv_url": "https://arxiv.org/abs/2602.07491",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07491",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:02:49.748291+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07120",
      "title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model",
      "authors": [
        "Jacqueline He",
        "Jonathan Hayase",
        "Wen-tau Yih",
        "Sewoong Oh",
        "Luke Zettlemoyer",
        "Pang Wei Koh"
      ],
      "abstract": "Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility. Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding , a plug-and-play inference-time method for suppressing verbatim copying : it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM . Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model ( TinyComma 1.8B ), as well as Anchored_{Byte} Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored_{Byte} Decoding define a new Pareto frontier , preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.",
      "summary_en": "Anchor decoding suppresses verbatim copying in language models while maintaining fluency and factual accuracy through constrained generation that balances risk and utility.",
      "summary_zh": "é”šç‚¹è§£ç é€šè¿‡å¹³è¡¡é£é™©ä¸æ•ˆç”¨çš„çº¦æŸç”Ÿæˆï¼Œåœ¨æŠ‘åˆ¶è¯­è¨€æ¨¡å‹é€å­—å¤åˆ¶çš„åŒæ—¶ä¿æŒæµç•…æ€§å’Œäº‹å®å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07120",
      "arxiv_url": "https://arxiv.org/abs/2602.07120",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07120",
      "github_url": "https://github.com/jacqueline-he/anchored-decoding",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:02:41.146969+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07090",
      "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
      "authors": [
        "Yu-Che Tsai",
        "Hsiang Hsiao",
        "Kuan-Yu Chen",
        "Shou-De Lin"
      ],
      "abstract": "SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration. Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks , which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings . SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
      "summary_en": "SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.",
      "summary_zh": "SPARSEæ˜¯ä¸€ç§ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œé€šè¿‡å¯å¾®æ©ç å’Œé©¬æ°è·ç¦»å™ªå£°æ ¡å‡†é€‰æ‹©æ€§æ‰°åŠ¨æ•æ„Ÿç»´åº¦ï¼Œä»è€Œä¿æŠ¤æ–‡æœ¬åµŒå…¥å…å—éšç§æ³„éœ²ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07090",
      "arxiv_url": "https://arxiv.org/abs/2602.07090",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07090",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:02:39.204499+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07054",
      "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization",
      "authors": [
        "Ashutosh Chaubey",
        "Jiacheng Pang",
        "Maksim Siniukov",
        "Mohammad Soleymani"
      ],
      "abstract": "A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues. Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations , hallucinations and modality agreement . We then propose AVEm-DPO , a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries . Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations , and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors , thereby mitigating modality-specific cue hallucinations . Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.",
      "summary_en": "A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åŸºå‡†æµ‹è¯•ä¸ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡è§£å†³è§†å¬çº¿ç´¢ä¸­çš„è™šå‡å…³è”å’Œå¹»è§‰é—®é¢˜ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07054",
      "arxiv_url": "https://arxiv.org/abs/2602.07054",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07054",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:02:27.816090+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.05708",
      "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
      "authors": [
        "Chuangtao Ma",
        "Zeyu Zhang",
        "Arijit Khan",
        "Sebastian Schelter",
        "Paul Groth"
      ],
      "abstract": "CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality. Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching . To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching , focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration .",
      "summary_en": "CE-RAG4EM reduces computational overhead in large-scale entity matching by implementing blocking-based batch retrieval and generation while maintaining competitive matching quality.",
      "summary_zh": "CE-RAG4EMé€šè¿‡å®ç°åŸºäºåˆ†å—çš„æ‰¹é‡æ£€ç´¢ä¸ç”Ÿæˆï¼Œåœ¨ä¿æŒæœ‰ç«äº‰åŠ›åŒ¹é…è´¨é‡çš„åŒæ—¶ï¼Œé™ä½å¤§è§„æ¨¡å®ä½“åŒ¹é…çš„è®¡ç®—å¼€é”€ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05708",
      "arxiv_url": "https://arxiv.org/abs/2602.05708",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05708",
      "github_url": "https://github.com/machuangtao/CE-RAG4EM",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:01:55.820634+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07948",
      "title": "dewi-kadita: A Python Library for Idealized Fish Schooling Simulation with Entropy-Based Diagnostics",
      "authors": [
        "Sandy H. S. Herho",
        "Iwan P. Anwar",
        "Faruq Khadami",
        "Alfita P. Handayani",
        "Karina A. Sujatmiko",
        "Kamaluddin Kasim",
        "Rusmawan Suwarman",
        "Dasapta E. Irawan"
      ],
      "abstract": "Collective motion in fish schools exemplifies emergent self-organization in active matter systems, yet computational tools for simulating and analyzing these dynamics remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with comprehensive entropy diagnostics tailored for marine collective behavior research. The library introduces seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that characterize distinct organizational features inaccessible to classical order parameters. These metrics combine into an Oceanic Schooling Index (OSI) providing a single scalar measure of collective disorder. Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors: the swarm maintains disorder with polarization P < 0.1 and OSI approx 0.71, while the highly parallel state achieves P = 0.998 with OSI = 0.24 and velocity correlation entropy vanishing to zero. The entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes through different organizational mechanisms. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by 10--100times, enabling simulations of 150--250 agents over 1000--2000 time steps within five minutes on standard workstation hardware. NetCDF4 output ensures interoperability with oceanographic analysis tools. The library addresses the need for standardized, reproducible infrastructure in collective behavior modeling analogous to established molecular dynamics codes.",
      "summary_en": "Computational tools for simulating collective motion in fish schools remain fragmented across research groups. We present dewi-kadita, an open-source Python library implementing the three-dimensional Couzin zone-based model with seven information-theoretic metrics -- school cohesion entropy, polarization entropy, depth stratification entropy, angular momentum entropy, nearest-neighbor entropy, velocity correlation entropy, and school shape entropy -- that combine into an Oceanic Schooling Index (OSI). Validation across four canonical configurations (swarm, torus, dynamic parallel, highly parallel) confirms correct reproduction of known phase behaviors, with the swarm maintaining disorder (polarization P < 0.1 and OSI approx 0.71) and the highly parallel state achieving P = 0.998 with OSI = 0.24, while the entropy framework successfully discriminates the torus and dynamic parallel configurations that exhibit comparable order parameter magnitudes. Numba just-in-time (JIT) compilation accelerates pairwise interaction calculations by 10--100times, enabling simulations of 150--250 agents over 1000--2000 time steps within five minutes on standard workstation hardware, and NetCDF4 output ensures interoperability with oceanographic analysis tools.",
      "summary_zh": "ç”¨äºæ¨¡æ‹Ÿé±¼ç¾¤é›†ä½“è¿åŠ¨çš„è®¡ç®—å·¥å…·ç›®å‰ä»åˆ†æ•£äºå„ç ”ç©¶ç»„ã€‚æˆ‘ä»¬æ¨å‡ºdewi-kaditaï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºPythonåº“ï¼Œå®ç°äº†ä¸‰ç»´CouzinåŒºåŸŸæ¨¡å‹ï¼Œå¹¶åŒ…å«ä¸ƒä¸ªä¿¡æ¯è®ºæŒ‡æ ‡â€”â€”ç¾¤ä½“å‡èšç†µã€æåŒ–ç†µã€æ·±åº¦åˆ†å±‚ç†µã€è§’åŠ¨é‡ç†µã€æœ€è¿‘é‚»ç†µã€é€Ÿåº¦ç›¸å…³ç†µå’Œç¾¤ä½“å½¢çŠ¶ç†µâ€”â€”è¿™äº›æŒ‡æ ‡ç»„åˆæˆæµ·æ´‹é›†ç¾¤æŒ‡æ•°(OSI)ã€‚åœ¨å››ç§å…¸å‹æ„å‹ï¼ˆç¾¤é›†ã€ç¯é¢ã€åŠ¨æ€å¹³è¡Œã€é«˜åº¦å¹³è¡Œï¼‰ä¸Šçš„éªŒè¯ç¡®è®¤äº†å·²çŸ¥ç›¸è¡Œä¸ºçš„æ­£ç¡®å¤ç°ï¼Œå…¶ä¸­ç¾¤é›†æ€ä¿æŒæ— åºï¼ˆæåŒ–åº¦P < 0.1ä¸”OSIçº¦0.71ï¼‰ï¼Œé«˜åº¦å¹³è¡Œæ€è¾¾åˆ°P = 0.998ä¸”OSI = 0.24ï¼ŒåŒæ—¶ç†µæ¡†æ¶æˆåŠŸåŒºåˆ†äº†ç¯é¢å’ŒåŠ¨æ€å¹³è¡Œæ„å‹ï¼ŒäºŒè€…å…·æœ‰å¯æ¯”çš„åºå‚é‡å¹…åº¦ã€‚Numbaå³æ—¶(JIT)ç¼–è¯‘å°†æˆå¯¹ç›¸äº’ä½œç”¨è®¡ç®—åŠ é€Ÿ10-100å€ï¼Œä½¿å¾—åœ¨æ ‡å‡†å·¥ä½œç«™ç¡¬ä»¶ä¸Šå¯åœ¨äº”åˆ†é’Ÿå†…å®Œæˆ150-250ä¸ªä¸ªä½“è·¨è¶Š1000-2000ä¸ªæ—¶é—´æ­¥çš„æ¨¡æ‹Ÿï¼Œä¸”NetCDF4è¾“å‡ºç¡®ä¿äº†ä¸æµ·æ´‹å­¦åˆ†æå·¥å…·çš„äº’æ“ä½œæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07948",
      "arxiv_url": "https://arxiv.org/abs/2602.07948",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07948",
      "github_url": "https://github.com/sandyherho/dewi-kadita",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:03:03.789141+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.07125",
      "title": "Reasoning-Augmented Representations for Multimodal Retrieval",
      "authors": [
        "Jianrui Zhang",
        "Anirudh Sundara Rajan",
        "Brandon Han",
        "Soochahn Lee",
        "Sukanta Ganguly",
        "Yong Jae Lee"
      ],
      "abstract": "UMR systems face challenges with latent reasoning tasks, which the proposed framework addresses by decoupling reasoning from retrieval through enhanced visual and textual representations. Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints ). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints . Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.",
      "summary_en": "UMR systems face challenges with latent reasoning tasks, which the proposed framework addresses by decoupling reasoning from retrieval through enhanced visual and textual representations.",
      "summary_zh": "UMRç³»ç»Ÿåœ¨å¤„ç†éšå¼æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæ‰€ææ¡†æ¶é€šè¿‡å¢å¼ºè§†è§‰ä¸æ–‡æœ¬è¡¨å¾æ¥è§£è€¦æ¨ç†ä¸æ£€ç´¢ï¼Œä»è€Œè§£å†³è¯¥é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07125",
      "arxiv_url": "https://arxiv.org/abs/2602.07125",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07125",
      "github_url": "https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:02:42.950672+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.05946",
      "title": "f-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
      "authors": [
        "Rajdeep Haldar",
        "Lantao Mei",
        "Guang Lin",
        "Yue Xing",
        "Qifan Song"
      ],
      "abstract": "Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation. Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards ( RLVR ), where only environmental rewards are available. Within this unified framework, we propose f-Group Relative Policy Optimization (f-GRPO), a class of on-policy reinforcement learning , and f-Hybrid Alignment Loss (f-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of f-divergence s. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR ( Math Reasoning ) and PA tasks ( Safety Alignment ), demonstrating superior performance and flexibility compared to current methods.",
      "summary_en": "Preference alignment objectives are extended to general alignment settings using f-divergence variational representations, introducing novel on-policy and hybrid policy optimization methods for LLM alignment with theoretical and empirical validation.",
      "summary_zh": "åˆ©ç”¨ f-æ•£åº¦å˜åˆ†è¡¨ç¤ºï¼Œå°†åå¥½å¯¹é½ç›®æ ‡æ‰©å±•è‡³é€šç”¨å¯¹é½è®¾ç½®ï¼Œå¼•å…¥é¢å‘ LLM å¯¹é½çš„æ–°å‹åŒç­–ç•¥ä¸æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶ç»è¿‡ç†è®ºä¸å®è¯éªŒè¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05946",
      "arxiv_url": "https://arxiv.org/abs/2602.05946",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05946",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:02:00.595569+00:00"
    },
    {
      "date": "2026-02-10",
      "paper_id": "2602.02285",
      "title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch",
      "authors": [
        "Yuanhe Zhang",
        "Jason D. Lee",
        "Fanghui Liu"
      ],
      "abstract": "A comprehensive formalization of statistical learning theory in Lean 4 addresses gaps in mathematical libraries and demonstrates human-AI collaboration for verified machine learning theory foundations. We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory . Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library , including a complete development of Gaussian Lipschitz concentration , the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes , and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow , in which humans design proof strategies and AI agents execute tactical proof construction , leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory",
      "summary_en": "A comprehensive formalization of statistical learning theory in Lean 4 addresses gaps in mathematical libraries and demonstrates human-AI collaboration for verified machine learning theory foundations.",
      "summary_zh": "åœ¨ Lean 4 ä¸­å¯¹ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„å…¨é¢å½¢å¼åŒ–å¡«è¡¥äº†æ•°å­¦åº“çš„ç©ºç™½ï¼Œå¹¶å±•ç¤ºäº†äººæœºåä½œåœ¨æ„å»ºç»è¿‡éªŒè¯çš„æœºå™¨å­¦ä¹ ç†è®ºåŸºç¡€æ–¹é¢çš„åº”ç”¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02285",
      "arxiv_url": "https://arxiv.org/abs/2602.02285",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02285",
      "github_url": "https://github.com/YuanheZ/lean-stat-learning-theory",
      "upvotes": 0,
      "fetched_at": "2026-02-19T06:01:49.429850+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06717",
      "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
      "authors": [
        "Daniil Plyusov",
        "Alexey Gorbatovski",
        "Boris Shaposhnikov",
        "Viacheslav Sinii",
        "Alexey Malakhov",
        "Daniil Gavrilov"
      ],
      "abstract": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost. Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates . In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss , that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO , DAPO , and CISPO . On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 ( GRPO ), 69.3 rightarrow 72.5 ( DAPO ), and 73.2 rightarrow 76.8 ( CISPO ), while preserving or improving pass@1, without increasing group size or computational cost.",
      "summary_en": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.",
      "summary_zh": "ä½¿ç”¨ç»„é‡‡æ ·çš„RLVRæ–¹æ³•å­˜åœ¨åå‘é«˜æ¦‚ç‡è½¨è¿¹è€Œé—æ¼ç¨€æœ‰æ­£ç¡®è½¨è¿¹çš„é—®é¢˜ï¼›éš¾åº¦æ„ŸçŸ¥ä¼˜åŠ¿ç¼©æ”¾æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„å‰æä¸‹æå‡åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06717",
      "arxiv_url": "https://arxiv.org/abs/2602.06717",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06717",
      "github_url": "",
      "upvotes": 71,
      "fetched_at": "2026-02-19T05:58:16.710161+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06570",
      "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
      "authors": [
        "Baichuan-M3 Team",
        "Chengfeng Dou",
        "Fan Yang",
        "Fei Li",
        "Jiyuan Jia",
        "Qiang Ju",
        "Shuai Wang",
        "Tianpeng Li",
        "Xiangrong Zeng",
        "Yijie Zhou",
        "Hongda Zhang",
        "Jinyang Tai",
        "Linzhuang Sun",
        "Peidong Guo",
        "Yichuan Mo",
        "Xiaochuan Wang",
        "Hengfu Cui",
        "Zhishou Zhang"
      ],
      "abstract": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression. We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench , the newly introduced HealthBench-Hallu and ScanBench , significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.",
      "summary_en": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.",
      "summary_zh": "Baichuan-M3æ˜¯ä¸€æ¬¾é¢å‘ä¸´åºŠå†³ç­–æ”¯æŒçš„åŒ»å­¦å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡ä¸»åŠ¨ä¿¡æ¯æ”¶é›†ã€é•¿ç¨‹æ¨ç†å’Œå¹»è§‰æŠ‘åˆ¶èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06570",
      "arxiv_url": "https://arxiv.org/abs/2602.06570",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06570",
      "github_url": "https://github.com/baichuan-inc/Baichuan-M3-235B",
      "upvotes": 59,
      "fetched_at": "2026-02-19T05:58:09.945123+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05027",
      "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
      "authors": [
        "Georgii Aparin",
        "Tasnima Sadekova",
        "Alexey Rukhovich",
        "Assel Yermekova",
        "Laida Kushnareva",
        "Vadim Popov",
        "Kristian Kuznetsov",
        "Irina Piontkovskaya"
      ],
      "abstract": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity. Sparse Autoencoders (SAEs) are po wer ful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT , provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whisper ing) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper 's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception , indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.",
      "summary_en": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.",
      "summary_zh": "åœ¨ Whisper å’Œ HuBERT æ¨¡å‹ä¸Šè®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨å±•ç°å‡ºç¨³å®šçš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œæœ‰æ•ˆè§£è€¦å£°å­¦ä¿¡æ¯ä¸è¯­ä¹‰ä¿¡æ¯ï¼Œåœ¨éŸ³é¢‘å¤„ç†ä¸­å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œå¹¶ä¸äººç±»ç¥ç»æ´»åŠ¨å­˜åœ¨ç›¸å…³æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05027",
      "arxiv_url": "https://arxiv.org/abs/2602.05027",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05027",
      "github_url": "https://github.com/audiosae/audiosae_demo",
      "upvotes": 59,
      "fetched_at": "2026-02-19T05:43:58.197237+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05843",
      "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
      "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Qiushi Sun",
        "Jinyang Wu",
        "Zixian Huang",
        "Muye Huang",
        "Jingyang Gong",
        "Zichen Ding",
        "Kanzhi Cheng",
        "Yian Wang",
        "Xinyu Che",
        "Zeyi Sun",
        "Jian Zhang",
        "Zhangyue Yin",
        "Haoran Luo",
        "Xuanjing Huang",
        "Ben Kao",
        "Jun Liu",
        "Qika Lin"
      ],
      "abstract": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws. The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena , which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
      "summary_en": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.",
      "summary_zh": "OdysseyArenaæå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿ç¨‹å½’çº³å¼æ™ºèƒ½ä½“ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¿™ç±»ä»»åŠ¡å¼ºè°ƒå¯¹ç¯å¢ƒè½¬ç§»è§„å¾‹çš„è‡ªä¸»å‘ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05843",
      "arxiv_url": "https://arxiv.org/abs/2602.05843",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05843",
      "github_url": "https://github.com/xufangzhi/Odyssey-Arena",
      "upvotes": 57,
      "fetched_at": "2026-02-19T05:44:04.128535+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03392",
      "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
      "authors": [
        "Shumin Wang",
        "Yuexiang Xie",
        "Wenhao Zhang",
        "Yuchang Sun",
        "Yanxi Chen",
        "Yaliang Li",
        "Yanyong Zhang"
      ],
      "abstract": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis. Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning ( RFT ), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update . This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization ( GRPO ). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy -based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
      "summary_en": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.",
      "summary_zh": "æœ¬æ–‡å»ºç«‹äº†å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å¾®è°ƒä¸­ç†µåŠ¨æ€åˆ†æçš„ç†è®ºæ¡†æ¶ï¼Œæ¨å¯¼äº†ç†µå˜åŒ–çš„è¡¨è¾¾å¼ï¼Œå¹¶åŸºäºåˆ¤åˆ«åˆ†ææå‡ºäº†ç†µæ§åˆ¶æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03392",
      "arxiv_url": "https://arxiv.org/abs/2602.03392",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03392",
      "github_url": "https://github.com/agentscope-ai/Trinity-RFT",
      "upvotes": 53,
      "fetched_at": "2026-02-19T05:43:48.268160+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2601.18415",
      "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
      "authors": [
        "Ivan Bondarenko",
        "Daniil Grebenkin",
        "Oleg Sedukhin",
        "Mikhail Klementev",
        "Roman Derunets",
        "Lyudmila Budneva"
      ],
      "abstract": "A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian speech recognition. This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2 , false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper . The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality . The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to Whisper X and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
      "summary_en": "A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian speech recognition.",
      "summary_zh": "ä¸€ç§ä¸‰ç»„ä»¶è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿç»“åˆWav2Vec2ã€ASTå’ŒWhisperæ¨¡å‹ï¼Œé€šè¿‡è¯¾ç¨‹å­¦ä¹ ä¸ä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œæå‡ä¿„è¯­è¯­éŸ³è¯†åˆ«çš„è½¬å½•å‡†ç¡®ç‡å¹¶å‡å°‘å¹»è§‰ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.18415",
      "arxiv_url": "https://arxiv.org/abs/2601.18415",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.18415",
      "github_url": "https://github.com/bond005/pisets",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:43:39.785703+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.01734",
      "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
      "authors": [
        "Lianhai Ren",
        "Yucheng Ding",
        "Xiao Liu",
        "Qianxiao Li",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "abstract": "Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions. Training instability remains a critical challenge in large language model (LLM) pretraining , often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via Î¼P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm ), and (2) increasing alignment between adjacent layer Jacobian s. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
      "summary_en": "Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸ç¨³å®šæ€§ä¸æƒé‡çŸ©é˜µç¨³å®šç§©ä¸‹é™åŠ Jacobian å¯¹é½ç›¸å…³ï¼ŒMSign é€šè¿‡çŸ©é˜µç¬¦å·è¿ç®—è§£å†³è¯¥é—®é¢˜ä»¥é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01734",
      "arxiv_url": "https://arxiv.org/abs/2602.01734",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01734",
      "github_url": "",
      "upvotes": 32,
      "fetched_at": "2026-02-19T05:43:44.153517+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06949",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "authors": [
        "Shenyuan Gao",
        "William Liang",
        "Kaiyuan Zheng",
        "Ayaan Malik",
        "Seonghyeon Ye",
        "Sihyun Yu",
        "Wei-Cheng Tseng",
        "Yuzhu Dong",
        "Kaichun Mo",
        "Chen-Hsuan Lin",
        "Qianli Ma",
        "Seungjun Nah",
        "Loic Magne",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Ruijie Zheng",
        "Dantong Niu",
        "You Liang Tan",
        "K. R. Zentner",
        "George Kurian",
        "Suneel Indupuru",
        "Pooya Jannaty"
      ],
      "abstract": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation. Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels . As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels , we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world model s, including live teleoperation , policy evaluation , and model-based planning . Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world model s.",
      "summary_en": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.",
      "summary_zh": "DreamDojoæ˜¯ä¸€ç§åœ¨44kå°æ—¶ç¬¬ä¸€äººç§°äººç±»è§†é¢‘ä¸Šè®­ç»ƒçš„åŸºç¡€ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡è¿ç»­æ½œåœ¨åŠ¨ä½œå’Œå®æ—¶è’¸é¦ï¼Œå®ç°çµå·§æœºå™¨äººä»»åŠ¡çš„é«˜æ•ˆæ¨¡æ‹Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06949",
      "arxiv_url": "https://arxiv.org/abs/2602.06949",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06949",
      "github_url": "",
      "upvotes": 30,
      "fetched_at": "2026-02-19T05:58:28.023980+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06130",
      "title": "Self-Improving World Modelling with Latent Actions",
      "authors": [
        "Yifu Qiu",
        "Zheng Zhao",
        "Waylon Li",
        "Yftah Ziser",
        "Anna Korhonen",
        "Shay B. Cohen",
        "Edoardo M. Ponti"
      ],
      "abstract": "SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks. Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_Î¸(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_Ï†(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation , which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation , which updates the IDM to explain observed transitions, effectively performing coordinate ascent . Both models are trained with reinforcement learning (specifically, GRPO ) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.",
      "summary_en": "SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.",
      "summary_zh": "SWIRLæ˜¯ä¸€ç§è‡ªæ”¹è¿›æ¡†æ¶ï¼Œé€šè¿‡åœ¨å‰å‘ä¸é€†å‘åŠ¨åŠ›å­¦å»ºæ¨¡ä¹‹é—´äº¤æ›¿ï¼Œå¹¶é‡‡ç”¨å˜åˆ†ä¿¡æ¯æœ€å¤§åŒ–ä¸ELBOæœ€å¤§åŒ–ï¼Œä»ä»…çŠ¶æ€åºåˆ—ä¸­å­¦ä¹ ä¸–ç•Œæ¨¡å‹ï¼Œåœ¨å¤šç§æ¨ç†ä¸è§„åˆ’åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06130",
      "arxiv_url": "https://arxiv.org/abs/2602.06130",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06130",
      "github_url": "",
      "upvotes": 29,
      "fetched_at": "2026-02-19T05:44:13.066392+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06291",
      "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math",
      "authors": [
        "Guijin Son",
        "Donghun Yang",
        "Hitesh Laxmichand Patel",
        "Hyunwoo Ko",
        "Amit Agarwal",
        "Sunghee Ahn",
        "Kyong-Ha Lee",
        "Youngjae Yu"
      ],
      "abstract": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation. Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models , generative reward models , and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap , maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.",
      "summary_en": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.",
      "summary_zh": "åŸºäºåæœçš„æ•ˆç”¨é€šè¿‡æµ‹è¯•æ•°å­¦è§£å†³æ–¹æ¡ˆä½œä¸ºç›¸å…³é—®é¢˜èŒƒä¾‹çš„æœ‰æ•ˆæ€§æ¥è¯„ä¼°è¿™äº›æ–¹æ¡ˆï¼Œåœ¨æ’åºè´¨é‡å’Œå¯¹é”™åŒºåˆ†æ–¹é¢ä¼˜äºå¥–åŠ±æ¨¡å‹å’ŒLLMè¯„åˆ¤å™¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06291",
      "arxiv_url": "https://arxiv.org/abs/2602.06291",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06291",
      "github_url": "",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:44:18.793795+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06079",
      "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers",
      "authors": [
        "Liangyu Wang",
        "Siqi Zhang",
        "Junjie Wang",
        "Yiming Dong",
        "Bo Zheng",
        "Zihan Qiu",
        "Shengkun Tang",
        "Di Wang",
        "Rui Men",
        "Dayiheng Liu"
      ],
      "abstract": "Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.",
      "summary_en": "Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.",
      "summary_zh": "Canzonaæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¼‚æ­¥æ¡†æ¶ï¼Œè§£å†³äº†åŸºäºçŸ©é˜µçš„ä¼˜åŒ–å™¨ä¸åˆ†å¸ƒå¼å¼ é‡ç¢ç‰‡åŒ–åœ¨LLMè®­ç»ƒä¸­çš„å†²çªï¼Œæå‡äº†æ•ˆç‡å¹¶é™ä½äº†å»¶è¿Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06079",
      "arxiv_url": "https://arxiv.org/abs/2602.06079",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06079",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:44:09.888939+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05940",
      "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
      "authors": [
        "Junxiao Liu",
        "Zhijun Wang",
        "Yixiao Li",
        "Zhejian Lai",
        "Liqian Huang",
        "Xin Huang",
        "Xue Han",
        "Junlan Feng",
        "Shujian Huang"
      ],
      "abstract": "TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages. Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning . To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning . Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH , our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200 .",
      "summary_en": "TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.",
      "summary_zh": "TRITæ¡†æ¶é€šè¿‡è”åˆè®­ç»ƒç¿»è¯‘å’Œæ¨ç†ç»„ä»¶ï¼Œæå‡å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¢å¼ºè·¨è¯­è¨€çš„é—®é¢˜ç†è§£ä¸å›ç­”ç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05940",
      "arxiv_url": "https://arxiv.org/abs/2602.05940",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05940",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:44:07.148907+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06391",
      "title": "POINTS-GUI-G: GUI-Grounding Journey",
      "authors": [
        "Zhongyin Zhao",
        "Yuan Liu",
        "Yikun Liu",
        "Haicheng Wang",
        "Le Tian",
        "Xiao Zhou",
        "Yangxiu You",
        "Zilin Yu",
        "Yang Yu",
        "Jie Zhou"
      ],
      "abstract": "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.",
      "summary_en": "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.",
      "summary_zh": "ç”¨äºè‡ªåŠ¨åŒ–æ•°å­—ä»»åŠ¡çš„GUIæ™ºèƒ½ä½“ä¾èµ–å…·å¤‡å¢å¼ºgroundingèƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›èƒ½åŠ›é€šè¿‡ç²¾ç»†åŒ–çš„æ•°æ®å·¥ç¨‹ã€æ”¹è¿›çš„è®­ç»ƒç­–ç•¥ä»¥åŠå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æ¥å®ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06391",
      "arxiv_url": "https://arxiv.org/abs/2602.06391",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06391",
      "github_url": "https://github.com/Tencent/POINTS-GUI",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:44:20.305138+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05281",
      "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
      "authors": [
        "Pengyi Li",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
      ],
      "abstract": "A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation , our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks . Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse . Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32 , highlighting its superior capability in generating diverse correct reasoning paths.",
      "summary_en": "A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.",
      "summary_zh": "ä¸€ç§åä¸ºARMçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šè¿‡åŠ¨æ€å¥–åŠ±å¡‘é€ å‡è¡¡å„æ­£ç¡®å›ç­”çš„ç½®ä¿¡åº¦æ°´å¹³ï¼Œä»¥è§£å†³LLMæ¨ç†ä¸­çš„ç†µåç¼©é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05281",
      "arxiv_url": "https://arxiv.org/abs/2602.05281",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05281",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:43:59.679234+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06075",
      "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
      "authors": [
        "Guangyi Liu",
        "Pengxiang Zhao",
        "Yaozhen Liang",
        "Qinyi Luo",
        "Shunye Tang",
        "Yuxiang Chai",
        "Weifeng Lin",
        "Han Xiao",
        "WenHao Wang",
        "Siheng Chen",
        "Zhengxi Lu",
        "Gao Wu",
        "Hao Wang",
        "Liang Liu",
        "Yong Liu"
      ],
      "abstract": "A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights. Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation . Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention ; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics ; and (4) RQ-driven assessment of 11 state-of-the-art agents . Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes , and synthesize 5 actionable design implications . All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
      "summary_en": "A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.",
      "summary_zh": "ä¸€é¡¹å…¨é¢çš„é¢å‘ç§»åŠ¨GUIæ™ºèƒ½ä½“çš„è®°å¿†åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ˜¾è‘—çš„è®°å¿†èƒ½åŠ›å·®è·ï¼Œå¹¶æä¾›äº†ç³»ç»Ÿæ€§è¯„ä¼°æ–¹æ³•å’Œè®¾è®¡è§è§£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06075",
      "arxiv_url": "https://arxiv.org/abs/2602.06075",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06075",
      "github_url": "https://github.com/lgy0404/MemGUI-Bench",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:44:08.554813+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06960",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "authors": [
        "Yuchen Yan",
        "Liang Jiang",
        "Jin Jiang",
        "Shuaicheng Li",
        "Zujie Wen",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Jian Shao",
        "Yueting Zhuang",
        "Yongliang Shen"
      ],
      "abstract": "InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models. Large reasoning models achieve strong performance by scaling inference-time chain-of-thought , but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization . InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning , enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "summary_en": "InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.",
      "summary_zh": "InftyThink+é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06960",
      "arxiv_url": "https://arxiv.org/abs/2602.06960",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06960",
      "github_url": "https://github.com/ZJU-REAL/InftyThink-Plus",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:58:29.908523+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06139",
      "title": "EgoAVU: Egocentric Audio-Visual Understanding",
      "authors": [
        "Ashish Seth",
        "Xinhao Mei",
        "Changsheng Zhao",
        "Varun Nagaraja",
        "Ernie Chang",
        "Gregory P. Meyer",
        "Gael Le Lan",
        "Yunyang Xiong",
        "Vikas Chandra",
        "Yangyang Shi",
        "Dinesh Manocha",
        "Zhipeng Cai"
      ],
      "abstract": "Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning. Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations , questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling . Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct , a large-scale training dataset of 3M samples, and EgoAVU-Bench , a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench . Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion , achieving up to 28% relative performance gain. Code will be released to the community.",
      "summary_en": "Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.",
      "summary_zh": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹éš¾ä»¥è”åˆç†è§£ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ä¸­çš„éŸ³é¢‘å’Œè§†è§‰ä¿¡å·ï¼Œä½†ä¸€ç§æ–°çš„å¯æ‰©å±•æ•°æ®å¼•æ“å’Œæ•°æ®é›†é€šè¿‡é’ˆå¯¹æ€§å¾®è°ƒæ˜¾è‘—æå‡äº†å…¶æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06139",
      "arxiv_url": "https://arxiv.org/abs/2602.06139",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06139",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:44:14.728820+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05847",
      "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
      "authors": [
        "Zhangquan Chen",
        "Jiale Tao",
        "Ruihuang Li",
        "Yihao Hu",
        "Ruitao Chen",
        "Zhantao Yang",
        "Xinlei Yu",
        "Haodong Jing",
        "Manyuan Zhang",
        "Shuai Shao",
        "Biao Wang",
        "Qinglin Lu",
        "Ruqi Huang"
      ],
      "abstract": "OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning. While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning . OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
      "summary_en": "OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.",
      "summary_zh": "OmniVideo-R1é€šè¿‡èåˆè‡ªç›‘ç£ä¸å¯¹æ¯”å­¦ä¹ çš„å¼ºåŒ–æ¡†æ¶ï¼Œå¢å¼ºéŸ³è§†é¢‘ç†è§£ä»¥å®ç°å¤šæ¨¡æ€æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05847",
      "arxiv_url": "https://arxiv.org/abs/2602.05847",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05847",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:44:05.557235+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04649",
      "title": "Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models",
      "authors": [
        "Binghai Wang",
        "Yantao Liu",
        "Yuxuan Liu",
        "Tianyi Tang",
        "Shenzhi Wang",
        "Chang Gao",
        "Chujie Zheng",
        "Yichang Zhang",
        "Le Yu",
        "Shixuan Liu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Bowen Yu",
        "Fei Huang",
        "Junyang Lin"
      ],
      "abstract": "Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF. Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy , which undermines their ability to generalize during RLHF . We introduce Rationale Consistency , a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment , while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF , our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks . Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.",
      "summary_en": "Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF.",
      "summary_zh": "ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹å› ä¼˜å…ˆä¼˜åŒ–ç»“æœå‡†ç¡®æ€§è€Œé­å—æ¬ºéª—æ€§å¯¹é½ï¼Œä½†ç†ç”±ä¸€è‡´æ€§æŒ‡æ ‡ä¸æ··åˆè®­ç»ƒä¿¡å·å¯æ”¹å–„RLHFä¸­çš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04649",
      "arxiv_url": "https://arxiv.org/abs/2602.04649",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04649",
      "github_url": "https://github.com/QwenLM/RationaleRM",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:43:54.074362+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06176",
      "title": "Large Language Model Reasoning Failures",
      "authors": [
        "Peiyang Song",
        "Pengrui Han",
        "Noah Goodman"
      ],
      "abstract": "Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies. Large Language Models (LLMs) have exhibited remarkable reasoning capabilities , achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities . We additionally release a comprehensive collection of research works on LLM reasoning failures , as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.",
      "summary_en": "Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ¨ç†å¤±è´¥ï¼Œå¯åˆ†ä¸ºå…·èº«ä¸éå…·èº«ç±»å‹ï¼Œå¹¶åŒ…å«åŸºç¡€å‹ã€åº”ç”¨ç‰¹å®šå‹åŠé²æ£’æ€§ç›¸å…³å‹ç­‰å­ç±»å‹ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„åˆ†æä¸ç¼“è§£ç­–ç•¥ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06176",
      "arxiv_url": "https://arxiv.org/abs/2602.06176",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06176",
      "github_url": "https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures",
      "upvotes": 11,
      "fetched_at": "2026-02-19T05:44:15.943778+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05711",
      "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
      "authors": [
        "Jingze Shi",
        "Zhangyang Peng",
        "Yizhang Zhu",
        "Yifan Wu",
        "Guang Liu",
        "Yuyu Luo"
      ],
      "abstract": "OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms. Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts , enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access . To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
      "summary_en": "OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.",
      "summary_zh": "OmniMoEæå‡ºäº†ä¸€ç§ç³»ç»Ÿ-ç®—æ³•ååŒè®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡å‘é‡çº§åŸå­ä¸“å®¶ä»¥åŠä¼˜åŒ–çš„è·¯ç”±ä¸è°ƒåº¦æœºåˆ¶ï¼Œåœ¨æ··åˆä¸“å®¶æ¶æ„ä¸­å®ç°äº†ç»†ç²’åº¦çš„ä¸“å®¶ç‰¹åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05711",
      "arxiv_url": "https://arxiv.org/abs/2602.05711",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05711",
      "github_url": "https://github.com/flash-algo/omni-moe",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:44:02.652511+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.02581",
      "title": "QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals",
      "authors": [
        "Nan Zhang",
        "Eugene Kwek",
        "Yusen Zhang",
        "Muyu Pan",
        "Suhang Wang",
        "Prasenjit Mitra",
        "Rui Zhang"
      ],
      "abstract": "QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation. Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning , we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term \"protecting both ends\". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning ) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning , which greatly enhances its applicability.",
      "summary_en": "QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.",
      "summary_zh": "QuantLRMåˆ©ç”¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„æƒé‡æ›´æ–°å¹…åº¦ä¿¡å·æ¥æ”¹è¿›å¤§å‹æ¨ç†æ¨¡å‹çš„é‡åŒ–ï¼Œé€šè¿‡é€šé“é‡è¦æ€§ä¼°è®¡å®ç°äº†ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02581",
      "arxiv_url": "https://arxiv.org/abs/2602.02581",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02581",
      "github_url": "https://github.com/psunlpgroup/QuantLRM",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:43:45.416376+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04837",
      "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
      "authors": [
        "Zhaotian Weng",
        "Antonis Antoniades",
        "Deepak Nathani",
        "Zhen Zhang",
        "Xiao Pu",
        "Xin Eric Wang"
      ],
      "abstract": "Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness. Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit , enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks , where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified , 88.3% vs. 68.3% on Polyglot ) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods .",
      "summary_en": "Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.",
      "summary_zh": "ç¾¤ä½“è¿›åŒ–æ™ºèƒ½ä½“å°†æ™ºèƒ½ä½“ç¾¤ä½“è§†ä¸ºè¿›åŒ–å•å…ƒï¼Œé€šè¿‡é«˜æ•ˆçš„ç»éªŒå…±äº«ä¸å¤ç”¨æå‡ä»£ç æ€§èƒ½ä¸é²æ£’æ€§ï¼Œå®ç°å¼€æ”¾å¼è‡ªæˆ‘æ”¹è¿›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04837",
      "arxiv_url": "https://arxiv.org/abs/2602.04837",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04837",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:43:56.641430+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06669",
      "title": "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data",
      "authors": [
        "Lucie Termignon",
        "Simonas Zilinskas",
        "Hadrien PÃ©lissier",
        "AurÃ©lien Barrot",
        "Nicolas Chesnais",
        "Elie Gavoty"
      ],
      "abstract": "Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.",
      "summary_en": "Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.",
      "summary_zh": "Compar:IA æ˜¯ä¸€ä¸ªå¼€æºå¹³å°ï¼Œæ”¶é›†ç”¨äºå¤šè¯­è¨€è¯­è¨€æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°çš„å¤§è§„æ¨¡äººç±»åå¥½æ•°æ®ï¼Œå…·å¤‡ç›²æµ‹æˆå¯¹æ¯”è¾ƒç•Œé¢ï¼Œå¹¶ä»¥å¼€æ”¾è®¸å¯å‘å¸ƒä¸‰ä¸ªæ•°æ®é›†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06669",
      "arxiv_url": "https://arxiv.org/abs/2602.06669",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06669",
      "github_url": "https://github.com/betagouv/ComparIA",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:58:14.493864+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05367",
      "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
      "authors": [
        "Youngcheon You",
        "Banseok Lee",
        "Minseop Choi",
        "Seonyoung Kim",
        "Hyochan Chong",
        "Changdong Kim",
        "Youngmin Kim",
        "Dongkyu Kim"
      ],
      "abstract": "Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs. Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation : during quantization-aware training ( QAT ), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT , a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy . Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization ( VQ ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090 .",
      "summary_en": "Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.",
      "summary_zh": "æ®‹å·®äºŒå€¼åŒ–æ¡†æ¶RaBiTé€šè¿‡åˆ†å±‚è·¯å¾„æ¨å¯¼ä¸é²æ£’åˆå§‹åŒ–è§£å†³é‡åŒ–LLMä¸­çš„ç‰¹å¾ååŒé€‚åº”é—®é¢˜ï¼Œå®ç°äº†æ›´ä¼˜çš„ç²¾åº¦-æ•ˆç‡æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05367",
      "arxiv_url": "https://arxiv.org/abs/2602.05367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05367",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:44:01.187355+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06869",
      "title": "Uncovering Cross-Objective Interference in Multi-Objective Alignment",
      "authors": [
        "Yining Lu",
        "Meng Jiang"
      ],
      "abstract": "Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals. We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms , showing that interference is pervasive and exhibits strong model dependence. To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference . Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Åojasiewicz condition , establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.",
      "summary_en": "Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡å¯¹é½å­˜åœ¨è·¨ç›®æ ‡å¹²æ‰°ï¼Œå³æå‡æŸäº›ç›®æ ‡æ€§èƒ½ä¼šé™ä½å…¶ä»–ç›®æ ‡ï¼›è¯¥ç ”ç©¶é‡‡ç”¨åæ–¹å·®åˆ†æï¼Œå¹¶æå‡ºä¸€ç§ä¿æŒå¥–åŠ±ä¸è®­ç»ƒä¿¡å·æ­£ç›¸å…³æ€§çš„æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06869",
      "arxiv_url": "https://arxiv.org/abs/2602.06869",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06869",
      "github_url": "https://github.com/yining610/ctwa",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:58:23.230832+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06854",
      "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
      "authors": [
        "Mingqian Feng",
        "Xiaodong Liu",
        "Weiwei Yang",
        "Jialin Song",
        "Xuekai Zhu",
        "Chenliang Xu",
        "Jianfeng Gao"
      ],
      "abstract": "A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets. Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models , and jailbreak judges , our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT ( Supervised Fine-Tuning ) and DPO ( Direct Preference Optimization ) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
      "summary_en": "A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets.",
      "summary_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º SEMA çš„æ–°æ¡†æ¶ï¼Œå¯åœ¨ä¸ä¾èµ–ç°æœ‰ç­–ç•¥æˆ–å¤–éƒ¨æ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆè®­ç»ƒé¢å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè½®æ”»å‡»å™¨ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ï¼Œä¸”å…·å¤‡ç´§å‡‘ã€å¯å¤ç°ã€å¯è·¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†è¿ç§»çš„ç‰¹æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06854",
      "arxiv_url": "https://arxiv.org/abs/2602.06854",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06854",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:58:20.732702+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03075",
      "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
      "authors": [
        "Junjie Huang",
        "Jiarui Qin",
        "Di Yin",
        "Weiwen Liu",
        "Yong Yu",
        "Xing Sun",
        "Weinan Zhang"
      ],
      "abstract": "ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop. Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training . However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training , utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT ( Reinforcement Learning -Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase , prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop , enabling continuous and self-reinforcing evolution of LLMs.",
      "summary_en": "ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.",
      "summary_zh": "ReMiTæå‡ºäº†ä¸€ç§åŒå‘è®­ç»ƒæ–¹æ³•ï¼Œå…¶ä¸­å¼ºåŒ–å­¦ä¹ å¼•å¯¼çš„è®­ç»ƒä¸­æœŸtokené‡åŠ æƒé€šè¿‡è¿­ä»£åé¦ˆå¾ªç¯æå‡å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒä¸åè®­ç»ƒæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03075",
      "arxiv_url": "https://arxiv.org/abs/2602.03075",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03075",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:43:46.853806+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06663",
      "title": "PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks",
      "authors": [
        "Junxian Li",
        "Kai Liu",
        "Leyang Chen",
        "Weida Wang",
        "Zhixin Wang",
        "Jiaqi Xu",
        "Fan Li",
        "Renjing Pei",
        "Linghe Kong",
        "Yulun Zhang"
      ],
      "abstract": "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.",
      "summary_en": "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.",
      "summary_zh": "PlanVizåŸºå‡†æµ‹è¯•é€šè¿‡è·¯å¾„è§„åˆ’ã€å·¥ä½œå›¾è¡¨ç»˜åˆ¶å’Œç½‘é¡µä¸UIå±•ç¤ºå­ä»»åŠ¡ï¼Œé‡‡ç”¨ä»»åŠ¡è‡ªé€‚åº”è¯„åˆ†ç³»ç»Ÿï¼Œè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨è®¡ç®—æœºä½¿ç”¨è§„åˆ’ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06663",
      "arxiv_url": "https://arxiv.org/abs/2602.06663",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06663",
      "github_url": "https://github.com/lijunxian111/PlanViz",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:58:11.830421+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06554",
      "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
      "authors": [
        "Tianyi Hu",
        "Qingxu Fu",
        "Yanxi Chen",
        "Zhaoyang Liu",
        "Bolin Ding"
      ],
      "abstract": "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates. Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies. In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/ multi-turn scenarios . We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO 's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios . To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems . Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction . Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.",
      "summary_en": "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.",
      "summary_zh": "SeeUPOæ˜¯ä¸€ç§æ— criticçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å°†åºåˆ—å†³ç­–å»ºæ¨¡ä¸ºå¤šæ™ºèƒ½ä½“bandité—®é¢˜å¹¶ä½¿ç”¨é€†å‘å½’çº³è¿›è¡Œç­–ç•¥æ›´æ–°ï¼Œç¡®ä¿å¤šè½®æ™ºèƒ½ä½“äº¤äº’çš„æ”¶æ•›æ€§ä¿è¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06554",
      "arxiv_url": "https://arxiv.org/abs/2602.06554",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06554",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:44:22.936836+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06883",
      "title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
      "authors": [
        "Ambroise Odonnat",
        "Laetitia Chapel",
        "Romain Tavenard",
        "Ievgen Redko"
      ],
      "abstract": "Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial. The smoothness of the transformer architecture has been extensively studied in the context of generalization , training stability , and adversarial robustness . However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity . Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness . We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance . Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit- plasticity .",
      "summary_en": "Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.",
      "summary_zh": "Vision Transformerç»„ä»¶è¡¨ç°å‡ºä¸å¾®è°ƒæ€§èƒ½ç›¸å…³çš„ä¸åŒå¯å¡‘æ€§æ°´å¹³ï¼ŒæŒ‘æˆ˜äº†å¹³æ»‘æ€§æ€»æ˜¯æœ‰ç›Šçš„è¿™ä¸€å‡è®¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06883",
      "arxiv_url": "https://arxiv.org/abs/2602.06883",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06883",
      "github_url": "https://github.com/ambroiseodt/vit-plasticity",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:58:25.427610+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06471",
      "title": "Revisiting the Shape Convention of Transformer Language Models",
      "authors": [
        "Feng-Ting Liao",
        "Meng-Hsi Chen",
        "Guan-Ting Yi",
        "Da-shan Shiu"
      ],
      "abstract": "Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling. Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP , allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLP s offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer , challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub- MLP s connected by residual pathways . We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.",
      "summary_en": "Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.",
      "summary_zh": "åœ¨Transformerä¸­ç”¨æ²™æ¼å½¢MLPæ›¿æ¢ä¼ ç»Ÿå‰é¦ˆç½‘ç»œï¼Œé€šè¿‡å®ç°æ›´ä¼˜çš„å‚æ•°åˆ©ç”¨å’Œå…·æœ‰ç«äº‰åŠ›çš„æ‰©å±•æ€§ï¼Œæå‡äº†æ¨¡å‹æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06471",
      "arxiv_url": "https://arxiv.org/abs/2602.06471",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06471",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:44:21.655739+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03548",
      "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue",
      "authors": [
        "Yuqin Dai",
        "Ning Gao",
        "Wei Zhang",
        "Jie Wang",
        "Zichen Luo",
        "Jinpeng Wang",
        "Yujie Wang",
        "Ruiyuan Wu",
        "Chaozheng Wang"
      ],
      "abstract": "SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models. Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues , as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.",
      "summary_en": "SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.",
      "summary_zh": "SEADæ¡†æ¶ä½¿æœåŠ¡å¯¹è¯æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªè¿›åŒ–ç”¨æˆ·å»ºæ¨¡ç»„ä»¶å­¦ä¹ æœ‰æ•ˆç­–ç•¥ï¼Œåœ¨ä»»åŠ¡å®Œæˆå’Œå¯¹è¯æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºç¡€æ¨¡å‹å’Œå•†ä¸šæ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03548",
      "arxiv_url": "https://arxiv.org/abs/2602.03548",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03548",
      "github_url": "https://github.com/Da1yuqin/SEAD",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:43:49.821404+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06566",
      "title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
      "authors": [
        "Niccolo Avogaro",
        "Nayanika Debnath",
        "Li Mi",
        "Thomas Frick",
        "Junling Wang",
        "Zexue He",
        "Hang Hua",
        "Konrad Schindler",
        "Mattia Rigotti"
      ],
      "abstract": "SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks. Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning , leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning . Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks , SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the V^* VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200times lower token budget .",
      "summary_en": "SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks.",
      "summary_zh": "SPARCæ˜¯ä¸€ç§æ¨¡å—åŒ–æ¡†æ¶ï¼Œå®ƒå°†è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†åœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­è§£è€¦ï¼Œé€šè¿‡é’ˆå¯¹æ€§çš„è®¡ç®—åˆ†é…å®ç°é«˜æ•ˆçš„æµ‹è¯•æ—¶æ‰©å±•ï¼Œå¹¶æå‡è§†è§‰æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06566",
      "arxiv_url": "https://arxiv.org/abs/2602.06566",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06566",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:58:07.346370+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06964",
      "title": "Learning a Generative Meta-Model of LLM Activations",
      "authors": [
        "Grace Luo",
        "Jiahai Feng",
        "Trevor Darrell",
        "Alec Radford",
        "Jacob Steinhardt"
      ],
      "abstract": "Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions. Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity . We explore this direction by training diffusion models on one billion residual stream activations , creating \" meta-models \" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.",
      "summary_en": "Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions.",
      "summary_zh": "åœ¨ç¥ç»ç½‘ç»œæ¿€æ´»ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹å¯æ„å»ºå­¦ä¹ å†…éƒ¨çŠ¶æ€åˆ†å¸ƒçš„å…ƒæ¨¡å‹ï¼Œæ— éœ€é™åˆ¶æ€§ç»“æ„å‡è®¾å³å¯æå‡å¹²é¢„ä¿çœŸåº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06964",
      "arxiv_url": "https://arxiv.org/abs/2602.06964",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06964",
      "github_url": "https://github.com/g-luo/generative_latent_prior",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:58:32.272125+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06724",
      "title": "Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion",
      "authors": [
        "Tian Lan",
        "Felix Henry",
        "Bin Zhu",
        "Qianghuai Jia",
        "Junyang Ren",
        "Qihang Pu",
        "Haijun Li",
        "Longyue Wang",
        "Zhao Xu",
        "Weihua Luo"
      ],
      "abstract": "Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management. Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states , including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states : filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search , Wide Search , and the challenging DeepWide Search . Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.",
      "summary_en": "Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.",
      "summary_zh": "Table-as-Searchæ¡†æ¶å°†ä¿¡æ¯æ£€ç´¢ä»»åŠ¡é‡æ–°è¡¨è¿°ä¸ºè¡¨æ ¼è¡¥å…¨é—®é¢˜ï¼Œé€šè¿‡ç»“æ„åŒ–çŠ¶æ€ç®¡ç†æå‡é•¿ç¨‹æœç´¢é²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06724",
      "arxiv_url": "https://arxiv.org/abs/2602.06724",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06724",
      "github_url": "https://github.com/AIDC-AI/Marco-DeepResearch",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:58:18.882352+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04811",
      "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
      "authors": [
        "Jiarui Yuan",
        "Tailin Jin",
        "Weize Chen",
        "Zeyuan Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches. True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox , where training with reference documentation inhibits retention, requiring \" Closed-Book Training \" to force knowledge compression into weights; (2) the RL Gap , where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients ; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT , but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization . Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
      "summary_en": "SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.",
      "summary_zh": "SE-Bench æ„å»ºäº†ä¸€ä¸ªéšè— NumPy API çš„è¯Šæ–­ç¯å¢ƒï¼Œç”¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨æ— å¤–éƒ¨æ–‡æ¡£æ—¶å†…éƒ¨å­˜å‚¨å’Œåˆ©ç”¨æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†ä¸åŒè®­ç»ƒæ–¹æ³•åœ¨çŸ¥è¯†ä¿æŒä¸å†…åŒ–æ–¹é¢å­˜åœ¨çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04811",
      "arxiv_url": "https://arxiv.org/abs/2602.04811",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04811",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:43:55.279809+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.01064",
      "title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs",
      "authors": [
        "Ruihan Jin",
        "Pengpeng Shao",
        "Zhengqi Wen",
        "Jinyang Wu",
        "Mingkuan Feng",
        "Shuo Yang",
        "Chu Yuan Zhang",
        "Jianhua Tao"
      ],
      "abstract": "Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes. Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models . In this paper, we introduce the concept of Knowledge Purification , which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification , we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts . Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.",
      "summary_en": "Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.",
      "summary_zh": "çŸ¥è¯†å‡€åŒ–æŠ€æœ¯æ•´åˆæ¥è‡ªå¤šä¸ªæ•™å¸ˆLLMçš„æ¨ç†ä¾æ®ï¼Œä»¥å‡å°‘è’¸é¦è¿‡ç¨‹ä¸­çš„å†²çªå¹¶æé«˜æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01064",
      "arxiv_url": "https://arxiv.org/abs/2602.01064",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01064",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:43:42.724279+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06181",
      "title": "Uncertainty Drives Social Bias Changes in Quantized Large Language Models",
      "authors": [
        "Stanley Z. Hua",
        "Sanae Lotfi",
        "Irene Y. Chen"
      ],
      "abstract": "Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels. Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping , in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty , where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups , where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes . Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.",
      "summary_en": "Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåé‡åŒ–ä¼šå¯¼è‡´èšåˆæŒ‡æ ‡æ— æ³•æ£€æµ‹åˆ°çš„ç¤¾ä¼šåè§æ˜¾è‘—å˜åŒ–ï¼Œä¸”é‡åŒ–å¼•å‘çš„æ©è”½åè§ç¿»è½¬åœ¨ä¸ç¡®å®šçš„å›å¤å’Œæ›´å¼ºçš„é‡åŒ–çº§åˆ«ä¸­å‡ºç°å¾—æ›´é¢‘ç¹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06181",
      "arxiv_url": "https://arxiv.org/abs/2602.06181",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06181",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:44:17.227679+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06129",
      "title": "Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction",
      "authors": [
        "Olaf Yunus Laitinen Imanov",
        "Derya Umut Kulali",
        "Taner Yilmaz"
      ],
      "abstract": "A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications. Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer ; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation ; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.",
      "summary_en": "A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.",
      "summary_zh": "ä¸€ç§diffusion-transformeræ¡†æ¶æ•´åˆæ—¶ç©ºåŸå¸‚æ•°æ®ä»¥é¢„æµ‹å»ºç­‘çº§æ°”å€™é£é™©ï¼ŒåŒæ—¶èåˆäº¤é€šç½‘ç»œç»“æ„ä»¥æ”¯æŒåº”æ€¥å“åº”åº”ç”¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06129",
      "arxiv_url": "https://arxiv.org/abs/2602.06129",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06129",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:44:11.315087+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04454",
      "title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search",
      "authors": [
        "Tianming Liang",
        "Qirui Du",
        "Jian-Fang Hu",
        "Haichao Jiang",
        "Zicheng Lin",
        "Wei-Shi Zheng"
      ],
      "abstract": "Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks. Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search , Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation . Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.",
      "summary_en": "Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.",
      "summary_zh": "Seg-ReSearch æå‡ºäº†ä¸€ç§ç»“åˆäº¤é”™æ¨ç†ä¸å¤–éƒ¨æœç´¢çš„æ–°é¢–åˆ†å‰²æ–¹æ³•ï¼Œä»¥å…‹æœå†»ç»“ MLLM çŸ¥è¯†çš„å±€é™æ€§ï¼Œé‡‡ç”¨åˆ†å±‚å¥–åŠ±è®¾è®¡è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨è§†é¢‘ç›®æ ‡åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04454",
      "arxiv_url": "https://arxiv.org/abs/2602.04454",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04454",
      "github_url": "https://github.com/iSEE-Laboratory/Seg-ReSearch",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:43:52.748358+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03998",
      "title": "AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology",
      "authors": [
        "Ahmed Alagha",
        "Christopher Leclerc",
        "Yousef Kotp",
        "Omar Metwally",
        "Calvin Moras",
        "Peter Rentopoulos",
        "Ghodsiyeh Rostami",
        "Bich Ngoc Nguyen",
        "Jumanah Baig",
        "Abdelhakim Khellaf",
        "Vincent Quoc-Huy Trinh",
        "Rabeb Mizouni",
        "Hadi Otrok",
        "Jamal Bentahar",
        "Mahdi S. Hosseini"
      ],
      "abstract": "AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead. Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction , is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection , or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model . The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning , matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.",
      "summary_en": "AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead.",
      "summary_zh": "AtlasPatch æ˜¯ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„å…¨åˆ‡ç‰‡å›¾åƒé¢„å¤„ç†æ¡†æ¶ï¼Œä½¿ç”¨å¾®è°ƒåçš„ Segment-Anything æ¨¡å‹å®ç°ç²¾ç¡®çš„ç»„ç»‡æ£€æµ‹å’Œé«˜é€šé‡ patch æå–ï¼ŒåŒæ—¶é™ä½è®¡ç®—å¼€é”€ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03998",
      "arxiv_url": "https://arxiv.org/abs/2602.03998",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03998",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:43:51.410332+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2601.23039",
      "title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
      "authors": [
        "Yizhi Liu"
      ],
      "abstract": "Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training. Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing Îµto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse . By analyzing the non-normal dynamics of the Sinkhorn fixed-point map , we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/Îµ). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.",
      "summary_en": "Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.",
      "summary_zh": "ç ”ç©¶äººå‘˜é€šè¿‡é˜²æ­¢å¤§è§„æ¨¡è®­ç»ƒæ¢¯åº¦çˆ†ç‚¸çš„è‡ªé€‚åº”ç¨³å®šæ€§æ§åˆ¶ç®—æ³•ï¼Œè¯†åˆ«å¹¶è§£å†³äº†åŸºäºæœ€ä¼˜ä¼ è¾“çš„ç»“æ„é¢„æµ‹æ¨¡å‹ä¸­çš„è¿‡æ—©æ¨¡å¼å´©æºƒé—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23039",
      "arxiv_url": "https://arxiv.org/abs/2601.23039",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23039",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:43:41.193780+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2601.22027",
      "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty",
      "authors": [
        "Johannes Kirmayr",
        "Lukas Stappen",
        "Elisabeth AndrÃ©"
      ],
      "abstract": "Current LLM agent benchmarks fail to evaluate reliability in real-world scenarios with uncertain user inputs, prompting the creation of CAR-bench to test consistency, uncertainty management, and capability awareness in in-car assistant applications. Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence . We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks , underscoring the need for more reliable and self-aware LLM agents in real-world settings.",
      "summary_en": "Current LLM agent benchmarks fail to evaluate reliability in real-world scenarios with uncertain user inputs, prompting the creation of CAR-bench to test consistency, uncertainty management, and capability awareness in in-car assistant applications.",
      "summary_zh": "ç°æœ‰LLM agentåŸºå‡†æµ‹è¯•éš¾ä»¥è¯„ä¼°çœŸå®åœºæ™¯ä¸‹ä¸ç¡®å®šç”¨æˆ·è¾“å…¥çš„å¯é æ€§ï¼Œå› è€Œå‚¬ç”Ÿäº†CAR-benchï¼Œç”¨äºæµ‹è¯•è½¦è½½åŠ©æ‰‹åº”ç”¨çš„ä¸€è‡´æ€§ã€ä¸ç¡®å®šæ€§ç®¡ç†å’Œèƒ½åŠ›è®¤çŸ¥ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22027",
      "arxiv_url": "https://arxiv.org/abs/2601.22027",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22027",
      "github_url": "https://github.com/CAR-bench/car-bench",
      "upvotes": 81,
      "fetched_at": "2026-02-19T05:42:30.246901+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05386",
      "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
      "authors": [
        "Zhenxiong Yu",
        "Zhi Yang",
        "Zhiheng Jin",
        "Shuhe Wang",
        "Heng Zhang",
        "Yanlin Fei",
        "Lingfeng Zeng",
        "Fangqi Lou",
        "Shuo Zhang",
        "Tu Hu",
        "Jingping Liu",
        "Rongze Chen",
        "Xingyu Zhu",
        "Kunyi Wang",
        "Chaofa Yuan",
        "Xin Guo",
        "Zhaowei Liu",
        "Feipeng Zhang",
        "Jie Huang",
        "Huacan Wang",
        "Ronghao Chen",
        "Liwen Zhang"
      ],
      "abstract": "Spider-Sense framework provides intrinsic and selective agent security through event-driven defense with intrinsic risk sensing, achieving low attack success and false positive rates with minimal latency overhead. As large language models (LLMs) evolve into autonomous agents , their real-world applicability has expanded significantly, accompanied by new security challenges . Most existing agent defense mechanisms adopt a mandatory checking paradigm , in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning , thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
      "summary_en": "Spider-Sense framework provides intrinsic and selective agent security through event-driven defense with intrinsic risk sensing, achieving low attack success and false positive rates with minimal latency overhead.",
      "summary_zh": "Spider-Senseæ¡†æ¶é€šè¿‡äº‹ä»¶é©±åŠ¨é˜²å¾¡ä¸å†…åœ¨é£é™©æ„ŸçŸ¥ï¼Œæä¾›å†…åœ¨ä¸”é€‰æ‹©æ€§çš„æ™ºèƒ½ä½“å®‰å…¨ï¼Œåœ¨æä½å»¶è¿Ÿå¼€é”€ä¸‹å®ç°ä½æ”»å‡»æˆåŠŸç‡ä¸è¯¯æŠ¥ç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05386",
      "arxiv_url": "https://arxiv.org/abs/2602.05386",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05386",
      "github_url": "https://github.com/aifinlab/Spider-Sense",
      "upvotes": 69,
      "fetched_at": "2026-02-19T05:43:08.010462+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.02474",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
      ],
      "abstract": "MemSkill introduces a learnable and evolvable memory system for LLM agents that dynamically selects and refines memory operations through controller-executor-designer components. Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present MemSkill, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces . Inspired by the design philosophy of agent skills , MemSkill employs a controller that learns to select a small set of relevant skills, paired with an LLM-based executor that produces skill-guided memories. Beyond learning skill selection , MemSkill introduces a designer that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "summary_en": "MemSkill introduces a learnable and evolvable memory system for LLM agents that dynamically selects and refines memory operations through controller-executor-designer components.",
      "summary_zh": "MemSkillä¸ºLLMæ™ºèƒ½ä½“å¼•å…¥äº†ä¸€ç§å¯å­¦ä¹ ä¸”å¯æ¼”åŒ–çš„è®°å¿†ç³»ç»Ÿï¼Œé€šè¿‡controller-executor-designerç»„ä»¶åŠ¨æ€é€‰æ‹©å’Œä¼˜åŒ–è®°å¿†æ“ä½œã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02474",
      "arxiv_url": "https://arxiv.org/abs/2602.02474",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02474",
      "github_url": "https://github.com/ViktorAxelsen/MemSkill",
      "upvotes": 55,
      "fetched_at": "2026-02-19T05:42:41.753491+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05261",
      "title": "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR",
      "authors": [
        "Fanfan Liu",
        "Youyang Yin",
        "Peng Shi",
        "Siqi Yang",
        "Zhixiong Zeng",
        "Haibo Qiu"
      ],
      "abstract": "Research analyzes RLVR algorithms' impact on response length in LLMs and VLMs, proposing LUSPO to eliminate length bias and improve reasoning performance. Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models ( LLMs ) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.",
      "summary_en": "Research analyzes RLVR algorithms' impact on response length in LLMs and VLMs, proposing LUSPO to eliminate length bias and improve reasoning performance.",
      "summary_zh": "ç ”ç©¶åˆ†æäº†RLVRç®—æ³•å¯¹LLMså’ŒVLMså›å¤é•¿åº¦çš„å½±å“ï¼Œæå‡ºLUSPOä»¥æ¶ˆé™¤é•¿åº¦åç½®å¹¶æå‡æ¨ç†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05261",
      "arxiv_url": "https://arxiv.org/abs/2602.05261",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05261",
      "github_url": "https://github.com/murphy4122/LUSPO",
      "upvotes": 49,
      "fetched_at": "2026-02-19T05:43:03.826230+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.06036",
      "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
      "authors": [
        "Jian Chen",
        "Yesheng Liang",
        "Zhijian Liu"
      ],
      "abstract": "DFlash is a speculative decoding framework that uses a lightweight block diffusion model for parallel token drafting, achieving significant speedup over existing autoregressive methods while maintaining high-quality outputs. Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation , but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates . Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3 .",
      "summary_en": "DFlash is a speculative decoding framework that uses a lightweight block diffusion model for parallel token drafting, achieving significant speedup over existing autoregressive methods while maintaining high-quality outputs.",
      "summary_zh": "DFlashæ˜¯ä¸€ç§æŠ•æœºè§£ç æ¡†æ¶ï¼Œä½¿ç”¨è½»é‡çº§å—æ‰©æ•£æ¨¡å‹è¿›è¡Œå¹¶è¡Œtokenèµ·è‰ï¼Œåœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶ï¼Œç›¸è¾ƒç°æœ‰è‡ªå›å½’æ–¹æ³•å®ç°äº†æ˜¾è‘—åŠ é€Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06036",
      "arxiv_url": "https://arxiv.org/abs/2602.06036",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06036",
      "github_url": "https://github.com/z-lab/dflash",
      "upvotes": 41,
      "fetched_at": "2026-02-19T05:43:31.661470+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.06028",
      "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
      "authors": [
        "Shuo Chen",
        "Cong Wei",
        "Sun Sun",
        "Ping Nie",
        "Kai Zhou",
        "Ge Zhang",
        "Ming-Hsuan Yang",
        "Wenhu Chen"
      ],
      "abstract": "Context Forcing addresses student-teacher mismatch in long video generation by using a long-context teacher to guide long-rollout students through a Slow-Fast Memory architecture that extends context length beyond 20 seconds.",
      "summary_en": "Context Forcing addresses student-teacher mismatch in long video generation by using a long-context teacher to guide long-rollout students through a Slow-Fast Memory architecture that extends context length beyond 20 seconds.",
      "summary_zh": "Context Forcing é€šè¿‡ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡æ•™å¸ˆæŒ‡å¯¼é•¿ rollout å­¦ç”Ÿï¼Œå¹¶é‡‡ç”¨ Slow-Fast Memory æ¶æ„å°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•è‡³20ç§’ä»¥ä¸Šï¼Œè§£å†³é•¿è§†é¢‘ç”Ÿæˆä¸­çš„å¸ˆç”Ÿä¸åŒ¹é…é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06028",
      "arxiv_url": "https://arxiv.org/abs/2602.06028",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06028",
      "github_url": "https://github.com/TIGER-AI-Lab/Context-Forcing",
      "upvotes": 35,
      "fetched_at": "2026-02-19T05:43:25.964003+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05885",
      "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
      "authors": [
        "Wei Liu",
        "Jiawei Xu",
        "Yingru Li",
        "Longtao Zheng",
        "Tianjian Li",
        "Qian Liu",
        "Junxian He"
      ],
      "abstract": "Reinforcement learning approach for kernel generation addresses reward hacking and optimization issues through specialized environment and unbiased policy gradient methods, achieving competitive performance with state-of-the-art models. High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation . We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO . To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL . To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
      "summary_en": "Reinforcement learning approach for kernel generation addresses reward hacking and optimization issues through specialized environment and unbiased policy gradient methods, achieving competitive performance with state-of-the-art models.",
      "summary_zh": "é’ˆå¯¹å†…æ ¸ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šè¿‡ä¸“ç”¨ç¯å¢ƒå’Œæ— åç­–ç•¥æ¢¯åº¦æ–¹æ³•è§£å†³å¥–åŠ±é»‘å®¢ä¸ä¼˜åŒ–é—®é¢˜ï¼Œå–å¾—äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05885",
      "arxiv_url": "https://arxiv.org/abs/2602.05885",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05885",
      "github_url": "https://github.com/hkust-nlp/KernelGYM",
      "upvotes": 28,
      "fetched_at": "2026-02-19T05:43:19.712091+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04884",
      "title": "Reinforced Attention Learning",
      "authors": [
        "Bangzheng Li",
        "Jianmo Ni",
        "Chen Qu",
        "Ian Miao",
        "Liu Yang",
        "Xingyu Fu",
        "Muhao Chen",
        "Derek Zhiyuan Cheng"
      ],
      "abstract": "Reinforced Attention Learning optimizes internal attention distributions in multimodal language models, improving information allocation and cross-modal alignment through policy-gradient methods. Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling . However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance. We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation , demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation . Our results position attention policies as a principled and general alternative for multimodal post-training.",
      "summary_en": "Reinforced Attention Learning optimizes internal attention distributions in multimodal language models, improving information allocation and cross-modal alignment through policy-gradient methods.",
      "summary_zh": "å¼ºåŒ–æ³¨æ„åŠ›å­¦ä¹ é€šè¿‡ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¼˜åŒ–å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­çš„å†…éƒ¨æ³¨æ„åŠ›åˆ†å¸ƒï¼Œæ”¹è¿›ä¿¡æ¯åˆ†é…ä¸è·¨æ¨¡æ€å¯¹é½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04884",
      "arxiv_url": "https://arxiv.org/abs/2602.04884",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04884",
      "github_url": "",
      "upvotes": 28,
      "fetched_at": "2026-02-19T05:42:52.293790+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05842",
      "title": "Reinforcement World Model Learning for LLM-based Agents",
      "authors": [
        "Xiao Yu",
        "Baolin Peng",
        "Ruize Xu",
        "Yelong Shen",
        "Pengcheng He",
        "Suman Nath",
        "Nikhil Singh",
        "Jiangfeng Gao",
        "Zhou Yu"
      ],
      "abstract": "Reinforcement World Model Learning enables LLM-based agents to better anticipate action consequences and adapt to environment dynamics through self-supervised training that aligns simulated and real-world state transitions in embedding space. Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards . Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space . Unlike next-state token prediction , which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and Ï„^2 Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards , our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and Ï„^2 Bench respectively, while matching the performance of expert-data training.",
      "summary_en": "Reinforcement World Model Learning enables LLM-based agents to better anticipate action consequences and adapt to environment dynamics through self-supervised training that aligns simulated and real-world state transitions in embedding space.",
      "summary_zh": "å¼ºåŒ–ä¸–ç•Œæ¨¡å‹å­¦ä¹ é€šè¿‡åœ¨åµŒå…¥ç©ºé—´ä¸­å¯¹é½æ¨¡æ‹Ÿä¸çœŸå®ä¸–ç•ŒçŠ¶æ€è½¬ç§»çš„è‡ªç›‘ç£è®­ç»ƒï¼Œä½¿åŸºäºLLMçš„æ™ºèƒ½ä½“èƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹åŠ¨ä½œåæœå¹¶é€‚åº”ç¯å¢ƒåŠ¨æ€ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05842",
      "arxiv_url": "https://arxiv.org/abs/2602.05842",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05842",
      "github_url": "",
      "upvotes": 27,
      "fetched_at": "2026-02-19T05:43:15.196514+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05986",
      "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
      "authors": [
        "Mingxin Liu",
        "Shuran Ma",
        "Shibei Meng",
        "Xiangyu Zhao",
        "Zicheng Zhang",
        "Shaofeng Zhang",
        "Zhihang Zhong",
        "Peixian Chen",
        "Haoyu Cao",
        "Xing Sun",
        "Haodong Duan",
        "Xue Yang"
      ],
      "abstract": "RISE-Video presents a novel benchmark for evaluating text-image-to-video synthesis models based on cognitive reasoning rather than visual fidelity, using a multi-dimensional metric system and automated LMM-based evaluation. While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: Reasoning Alignment , Temporal Consistency , Physical Rationality , and Visual Quality . To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
      "summary_en": "RISE-Video presents a novel benchmark for evaluating text-image-to-video synthesis models based on cognitive reasoning rather than visual fidelity, using a multi-dimensional metric system and automated LMM-based evaluation.",
      "summary_zh": "RISE-Videoæå‡ºäº†ä¸€ä¸ªåŸºäºè®¤çŸ¥æ¨ç†è€Œéè§†è§‰ä¿çœŸåº¦è¯„ä¼°æ–‡æœ¬-å›¾åƒåˆ°è§†é¢‘åˆæˆæ¨¡å‹çš„æ–°åŸºå‡†ï¼Œé‡‡ç”¨å¤šç»´æŒ‡æ ‡ç³»ç»Ÿå’ŒåŸºäºLMMçš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05986",
      "arxiv_url": "https://arxiv.org/abs/2602.05986",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05986",
      "github_url": "https://github.com/VisionXLab/Rise-Video",
      "upvotes": 26,
      "fetched_at": "2026-02-19T05:43:24.376799+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.03338",
      "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention",
      "authors": [
        "Rakshith Vasudev",
        "Melisa Russak",
        "Dan Bikel",
        "Waseem Alshikh"
      ],
      "abstract": "LLM critic models with high offline accuracy can cause variable performance impacts at deployment, necessitating pre-deployment testing to determine intervention safety and effectiveness. Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy ( AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe. We identify a disruption-recovery tradeoff : interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.",
      "summary_en": "LLM critic models with high offline accuracy can cause variable performance impacts at deployment, necessitating pre-deployment testing to determine intervention safety and effectiveness.",
      "summary_zh": "å…·æœ‰é«˜ç¦»çº¿å‡†ç¡®ç‡çš„LLM criticæ¨¡å‹åœ¨éƒ¨ç½²æ—¶å¯èƒ½é€ æˆå¤šå˜çš„æ€§èƒ½å½±å“ï¼Œå› æ­¤éœ€è¦è¿›è¡Œéƒ¨ç½²å‰æµ‹è¯•ä»¥ç¡®å®šå¹²é¢„çš„å®‰å…¨æ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03338",
      "arxiv_url": "https://arxiv.org/abs/2602.03338",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03338",
      "github_url": "",
      "upvotes": 26,
      "fetched_at": "2026-02-19T05:42:44.852414+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05327",
      "title": "ProAct: Agentic Lookahead in Interactive Environments",
      "authors": [
        "Yangbin Yu",
        "Mingyu Yang",
        "Junyou Li",
        "Yiming Gao",
        "Feiyu Liu",
        "Yijun Yang",
        "Zichuan Lin",
        "Jiafei Lyu",
        "Yicheng Liu",
        "Zhicong Lu",
        "Deheng Ye",
        "Jie Jiang"
      ],
      "abstract": "ProAct enhances LLM agents' long-horizon planning by combining supervised fine-tuning with search-derived trajectories and a Monte-Carlo critic for improved policy optimization.",
      "summary_en": "ProAct enhances LLM agents' long-horizon planning by combining supervised fine-tuning with search-derived trajectories and a Monte-Carlo critic for improved policy optimization.",
      "summary_zh": "ProActç»“åˆç›‘ç£å¾®è°ƒä¸æœç´¢æ´¾ç”Ÿè½¨è¿¹ï¼Œå¹¶åˆ©ç”¨è’™ç‰¹å¡æ´›criticæ”¹è¿›ç­–ç•¥ä¼˜åŒ–ï¼Œä»è€Œå¢å¼ºLLMæ™ºèƒ½ä½“çš„é•¿ç¨‹è§„åˆ’èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05327",
      "arxiv_url": "https://arxiv.org/abs/2602.05327",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05327",
      "github_url": "https://github.com/GreatX3/ProAct",
      "upvotes": 25,
      "fetched_at": "2026-02-19T05:43:06.660969+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04942",
      "title": "Privileged Information Distillation for Language Models",
      "authors": [
        "Emiliano Penaloza",
        "Dheeraj Vattikonda",
        "Nicolas Gontier",
        "Alexandre Lacoste",
        "Laurent Charlin",
        "Massimo Caccia"
      ],
      "abstract": "Training methods that utilize privileged information for language model distillation in multi-turn environments outperform standard supervised fine-tuning followed by reinforcement learning approaches. Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce Ï€-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that Ï€-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on Ï€-Distill and characterizing when OPSD is competitive.",
      "summary_en": "Training methods that utilize privileged information for language model distillation in multi-turn environments outperform standard supervised fine-tuning followed by reinforcement learning approaches.",
      "summary_zh": "åœ¨å¤šè½®ç¯å¢ƒä¸­åˆ©ç”¨ç‰¹æƒä¿¡æ¯è¿›è¡Œè¯­è¨€æ¨¡å‹è’¸é¦çš„è®­ç»ƒæ–¹æ³•ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒåæ¥å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04942",
      "arxiv_url": "https://arxiv.org/abs/2602.04942",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04942",
      "github_url": "",
      "upvotes": 25,
      "fetched_at": "2026-02-19T05:42:53.735142+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.06035",
      "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
      "authors": [
        "Sirui Xu",
        "Samuel Schulter",
        "Morteza Ziyadi",
        "Xialin He",
        "Xiaohan Fei",
        "Yu-Xiong Wang",
        "Liangyan Gui"
      ],
      "abstract": "A scalable framework called InterPrior learns a unified generative controller through imitation learning and reinforcement learning to enable humanoids to generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination . To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning . InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations , and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.",
      "summary_en": "A scalable framework called InterPrior learns a unified generative controller through imitation learning and reinforcement learning to enable humanoids to generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination.",
      "summary_zh": "åä¸º InterPrior çš„å¯æ‰©å±•æ¡†æ¶é€šè¿‡æ¨¡ä»¿å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ è®­ç»ƒç»Ÿä¸€çš„ç”Ÿæˆå¼æ§åˆ¶å™¨ï¼Œä½¿äººå½¢æœºå™¨äººèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–åœºæ™¯ä¸­æ³›åŒ–ç§»åŠ¨æ“ä½œæŠ€èƒ½ï¼ŒåŒæ—¶ä¿æŒç‰©ç†ä¸€è‡´æ€§çš„å…¨èº«åè°ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06035",
      "arxiv_url": "https://arxiv.org/abs/2602.06035",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06035",
      "github_url": "",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:43:30.224065+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05216",
      "title": "Semantic Search over 9 Million Mathematical Theorems",
      "authors": [
        "Luke Alexander",
        "Eric Leonen",
        "Sophie Szeto",
        "Artemii Remizov",
        "Ignacio Tejeda",
        "Giovanni Inchiostro",
        "Vasily Ilin"
      ],
      "abstract": "Large-scale semantic theorem retrieval system demonstrates superior performance over existing baselines using a 9.2 million theorem corpus with systematic analysis of representation context, language model choice, and embedding strategies. Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of 9.2 million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice , embedding model , and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at https://huggingface.co/spaces/uw-math-ai/theorem-search{this link}, and the dataset is available at https://huggingface.co/datasets/uw-math-ai/TheoremSearch{this link}.",
      "summary_en": "Large-scale semantic theorem retrieval system demonstrates superior performance over existing baselines using a 9.2 million theorem corpus with systematic analysis of representation context, language model choice, and embedding strategies.",
      "summary_zh": "å¤§è§„æ¨¡è¯­ä¹‰å®šç†æ£€ç´¢ç³»ç»ŸåŸºäº920ä¸‡å®šç†è¯­æ–™åº“ï¼Œé€šè¿‡å¯¹è¡¨ç¤ºä¸Šä¸‹æ–‡ã€è¯­è¨€æ¨¡å‹é€‰æ‹©åŠåµŒå…¥ç­–ç•¥çš„ç³»ç»Ÿåˆ†æï¼Œå±•ç°å‡ºä¼˜äºç°æœ‰åŸºçº¿çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05216",
      "arxiv_url": "https://arxiv.org/abs/2602.05216",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05216",
      "github_url": "",
      "upvotes": 20,
      "fetched_at": "2026-02-19T05:43:00.782393+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2601.21937",
      "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities",
      "authors": [
        "Shuangshuang Ying",
        "Zheyu Wang",
        "Yunjian Peng",
        "Jin Chen",
        "Yuhao Wu",
        "Hongbin Lin",
        "Dingyu He",
        "Siyi Liu",
        "Gengchen Yu",
        "YinZhu Piao",
        "Yuchen Wu",
        "Xin Gui",
        "Zhongyuan Peng",
        "Xin Li",
        "Xeron Du",
        "Libo Qin",
        "YiXin Cao",
        "Ge Zhang",
        "Stephen Huang"
      ],
      "abstract": "DeR2 presents a controlled evaluation framework for assessing language models' document-grounded reasoning capabilities by isolating reasoning from retrieval and toolchain decisions. Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility . We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search : multi-step synthesis , denoising , and evidence-based conclusion making . DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution . To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability . To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales . Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.",
      "summary_en": "DeR2 presents a controlled evaluation framework for assessing language models' document-grounded reasoning capabilities by isolating reasoning from retrieval and toolchain decisions.",
      "summary_zh": "DeR2 æå‡ºäº†ä¸€ä¸ªå—æ§è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å°†æ¨ç†ä¸æ£€ç´¢åŠå·¥å…·é“¾å†³ç­–åˆ†ç¦»ï¼Œæ¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„åŸºäºæ–‡æ¡£çš„æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21937",
      "arxiv_url": "https://arxiv.org/abs/2601.21937",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21937",
      "github_url": "https://github.com/Retrieval-Infused-Reasoning-Sandbox/Retrieval-Infused-Reasoning-Sandbox",
      "upvotes": 19,
      "fetched_at": "2026-02-19T05:42:28.931788+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2601.21296",
      "title": "Grounding and Enhancing Informativeness and Utility in Dataset Distillation",
      "authors": [
        "Shaobo Wang",
        "Yantai Yang",
        "Guo Chen",
        "Peiru Li",
        "Kaixin Li",
        "Yufa Zhou",
        "Zhaorun Chen",
        "Linfeng Zhang"
      ],
      "abstract": "Dataset distillation method that balances informativeness and utility through game-theoretic and gradient-based optimization techniques, achieving improved performance on ImageNet-1K. Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation -based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility , capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm . These components ensure that the distilled dataset is both informative and utility -optimized. Experiments demonstrate that our method achieves a 6.1\\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.",
      "summary_en": "Dataset distillation method that balances informativeness and utility through game-theoretic and gradient-based optimization techniques, achieving improved performance on ImageNet-1K.",
      "summary_zh": "é€šè¿‡åšå¼ˆè®ºå’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æŠ€æœ¯å¹³è¡¡ä¿¡æ¯é‡ä¸æ•ˆç”¨çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ï¼Œåœ¨ImageNet-1Kä¸Šå–å¾—æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21296",
      "arxiv_url": "https://arxiv.org/abs/2601.21296",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21296",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T05:42:27.594541+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05115",
      "title": "SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers",
      "authors": [
        "Keyang Xuan",
        "Pengda Wang",
        "Chongrui Ye",
        "Haofei Yu",
        "Tal August",
        "Jiaxuan You"
      ],
      "abstract": "SocialVeil presents a social learning environment that simulates communication barriers in LLM interactions, demonstrating significant performance degradation under realistic conditions and limited effectiveness of adaptation strategies. Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence . However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present SocialVeil, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers . Grounded in a systematic literature review of communication challenges in human interaction, SocialVeil introduces three representative types of such disruption, semantic vagueness , sociocultural mismatch , and emotional interference . We also introduce two barrier-aware evaluation metrics, unresolved confusion and mutual understanding , to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\\% on average, and confusion elevated by nearly 50\\%. Human evaluations validate the fidelity of these simulated barriers (ICCapprox0.78, Pearson rapprox0.80). We further demonstrate that adaptation strategies ( Repair Instruction and Interactive learning ) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.",
      "summary_en": "SocialVeil presents a social learning environment that simulates communication barriers in LLM interactions, demonstrating significant performance degradation under realistic conditions and limited effectiveness of adaptation strategies.",
      "summary_zh": "SocialVeil æ„å»ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿ LLM äº¤äº’ä¸­é€šä¿¡éšœç¢çš„ç¤¾äº¤å­¦ä¹ ç¯å¢ƒï¼Œè¡¨æ˜åœ¨ç°å®æ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸”é€‚åº”ç­–ç•¥çš„æœ‰æ•ˆæ€§æœ‰é™ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05115",
      "arxiv_url": "https://arxiv.org/abs/2602.05115",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05115",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:42:59.481809+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04210",
      "title": "Steering LLMs via Scalable Interactive Oversight",
      "authors": [
        "Enyu Zhou",
        "Zhiheng Xi",
        "Long Ma",
        "Zhihao Zhang",
        "Shihan Dou",
        "Zhikai Lei",
        "Guoteng Wang",
        "Rui Zheng",
        "Hang Yan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Scalable Interactive Oversight framework decomposes complex tasks into manageable decision trees to enhance human supervision and alignment in AI systems. As Large Language Models increasingly automate complex, long-horizon tasks such as vibe coding , a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight : enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight , a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\\% improvement in alignment . Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback , offering a practical pathway for maintaining human control as AI scales.",
      "summary_en": "Scalable Interactive Oversight framework decomposes complex tasks into manageable decision trees to enhance human supervision and alignment in AI systems.",
      "summary_zh": "å¯æ‰©å±•äº¤äº’å¼ç›‘ç£æ¡†æ¶å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯ç®¡ç†çš„å†³ç­–æ ‘ï¼Œä»¥å¢å¼ºAIç³»ç»Ÿä¸­çš„äººç±»ç›‘ç£å’Œå¯¹é½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04210",
      "arxiv_url": "https://arxiv.org/abs/2602.04210",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04210",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:42:46.572486+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.02393",
      "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
      "authors": [
        "Ruiqi Wu",
        "Xuanhua He",
        "Meng Cheng",
        "Tianyu Yang",
        "Yong Zhang",
        "Zhuoliang Kang",
        "Xunliang Cai",
        "Xiaoming Wei",
        "Chunle Guo",
        "Chongyi Li",
        "Ming-Ming Cheng"
      ],
      "abstract": "Infinite-World is a robust interactive world model that maintains coherent visual memory over 1000+ frames through hierarchical pose-free memory compression, uncertainty-aware action labeling, and revisit-dense fine-tuning strategies. We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world model s can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone , HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic . This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities . Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.",
      "summary_en": "Infinite-World is a robust interactive world model that maintains coherent visual memory over 1000+ frames through hierarchical pose-free memory compression, uncertainty-aware action labeling, and revisit-dense fine-tuning strategies.",
      "summary_zh": "Infinite-Worldæ˜¯ä¸€ç§é²æ£’çš„äº¤äº’å¼ä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡åˆ†å±‚æ— å§¿æ€è®°å¿†å‹ç¼©ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ¨ä½œæ ‡æ³¨å’Œé‡è®¿å¯†é›†å¾®è°ƒç­–ç•¥ï¼Œåœ¨1000+å¸§ä¸Šä¿æŒè¿è´¯çš„è§†è§‰è®°å¿†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02393",
      "arxiv_url": "https://arxiv.org/abs/2602.02393",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02393",
      "github_url": "https://github.com/MeiGen-AI/Infinite-World",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:42:40.305059+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2601.21037",
      "title": "Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning",
      "authors": [
        "Chengzu Li",
        "Zanyi Wang",
        "Jiaang Li",
        "Yi Xu",
        "Han Zhou",
        "Huanyu Zhang",
        "Ruichuan An",
        "Dengyang Jiang",
        "Zhaochong An",
        "Ivan VuliÄ‡",
        "Serge Belongie",
        "Anna Korhonen"
      ],
      "abstract": "Video generation models demonstrate robust zero-shot generalization for visual reasoning tasks through explicit visual context utilization and test-time scaling capabilities. Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning . In this work, we formulate visual reasoning by means of video generation models , positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization : In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context : The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling : We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning .",
      "summary_en": "Video generation models demonstrate robust zero-shot generalization for visual reasoning tasks through explicit visual context utilization and test-time scaling capabilities.",
      "summary_zh": "è§†é¢‘ç”Ÿæˆæ¨¡å‹é€šè¿‡æ˜¾å¼åˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡å’Œæµ‹è¯•æ—¶ç¼©æ”¾èƒ½åŠ›ï¼Œåœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21037",
      "arxiv_url": "https://arxiv.org/abs/2601.21037",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21037",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:42:26.025965+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.03036",
      "title": "LatentMem: Customizing Latent Memory for Multi-Agent Systems",
      "authors": [
        "Muxin Fu",
        "Guibin Zhang",
        "Xiangyuan Xue",
        "Yafu Li",
        "Zefeng He",
        "Siyuan Huang",
        "Xiaoye Qu",
        "Yu Cheng",
        "Yang Yang"
      ],
      "abstract": "LatentMem is a learnable multi-agent memory framework that customizes agent-specific memories through latent representations, improving performance in multi-agent systems without modifying underlying frameworks. Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts . Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to 19.36% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.",
      "summary_en": "LatentMem is a learnable multi-agent memory framework that customizes agent-specific memories through latent representations, improving performance in multi-agent systems without modifying underlying frameworks.",
      "summary_zh": "LatentMemæ˜¯ä¸€ç§å¯å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“è®°å¿†æ¡†æ¶ï¼Œé€šè¿‡æ½œåœ¨è¡¨ç¤ºå®šåˆ¶ç‰¹å®šäºæ™ºèƒ½ä½“çš„è®°å¿†ï¼Œåœ¨æ— éœ€ä¿®æ”¹åº•å±‚æ¡†æ¶çš„æƒ…å†µä¸‹æå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03036",
      "arxiv_url": "https://arxiv.org/abs/2602.03036",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03036",
      "github_url": "https://github.com/KANABOON1/LatentMem",
      "upvotes": 14,
      "fetched_at": "2026-02-19T05:42:43.156527+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05975",
      "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
      "authors": [
        "Tiansheng Hu",
        "Yilun Zhao",
        "Canyu Zhang",
        "Arman Cohan",
        "Chen Zhao"
      ],
      "abstract": "LLM-based retrievers show limited effectiveness in deep research agent workflows, with traditional BM25 performing better, though corpus-level test-time scaling can improve retrieval performance. Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct ) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.",
      "summary_en": "LLM-based retrievers show limited effectiveness in deep research agent workflows, with traditional BM25 performing better, though corpus-level test-time scaling can improve retrieval performance.",
      "summary_zh": "åŸºäºLLMçš„æ£€ç´¢å™¨åœ¨æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“å·¥ä½œæµä¸­æ•ˆæœæœ‰é™ï¼Œä¼ ç»ŸBM25è¡¨ç°æ›´ä¼˜ï¼Œè€Œè¯­æ–™åº“çº§æµ‹è¯•æ—¶æ‰©å±•å¯æ”¹å–„æ£€ç´¢æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05975",
      "arxiv_url": "https://arxiv.org/abs/2602.05975",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05975",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:43:22.868202+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05547",
      "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks",
      "authors": [
        "Shyam Sundhar Ramesh",
        "Xiaotong Ji",
        "Matthieu Zimmer",
        "Sangwoong Yoon",
        "Zhiyong Wang",
        "Haitham Bou Ammar",
        "Aurelien Lucchi",
        "Ilija Bogunovic"
      ],
      "abstract": "Multi-Task GRPO algorithm improves balanced performance across diverse reasoning tasks by dynamically adapting task weights and using a ratio-preserving sampler to ensure equitable optimization. RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal . To address these issues, we propose a novel Multi-Task GRPO (MT- GRPO ) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT- GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT- GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT- GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.",
      "summary_en": "Multi-Task GRPO algorithm improves balanced performance across diverse reasoning tasks by dynamically adapting task weights and using a ratio-preserving sampler to ensure equitable optimization.",
      "summary_zh": "å¤šä»»åŠ¡GRPOç®—æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡å¹¶ä½¿ç”¨æ¯”ä¾‹ä¿æŒé‡‡æ ·å™¨ç¡®ä¿å…¬å¹³ä¼˜åŒ–ï¼Œä»è€Œåœ¨å„ç±»æ¨ç†ä»»åŠ¡ä¸­æå‡å‡è¡¡æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05547",
      "arxiv_url": "https://arxiv.org/abs/2602.05547",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05547",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:43:12.216963+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.02016",
      "title": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers",
      "authors": [
        "Ionut-Vlad Modoranu",
        "Philip Zmushko",
        "Erik Schultheis",
        "Mher Safaryan",
        "Dan Alistarh"
      ],
      "abstract": "Distributed Accelerated SHampoo (DASH) improves upon Shampoo optimization through efficient 3D tensor operations and faster inverse matrix root computations, achieving faster convergence and better performance. Shampoo is one of the leading approximate second-order optimizers : a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \\method (for Distributed Accelerated SHampoo ), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization ; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo . Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to 4.83times faster optimizer steps compared to the well-optimized Distributed Shampoo , while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.",
      "summary_en": "Distributed Accelerated SHampoo (DASH) improves upon Shampoo optimization through efficient 3D tensor operations and faster inverse matrix root computations, achieving faster convergence and better performance.",
      "summary_zh": "åˆ†å¸ƒå¼åŠ é€Ÿ SHampooï¼ˆDASHï¼‰é€šè¿‡é«˜æ•ˆçš„ä¸‰ç»´å¼ é‡è¿ç®—å’Œæ›´å¿«çš„é€†çŸ©é˜µæ ¹è®¡ç®—æ”¹è¿›äº† Shampoo ä¼˜åŒ–ï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¥½çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02016",
      "arxiv_url": "https://arxiv.org/abs/2602.02016",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02016",
      "github_url": "https://github.com/IST-DASLab/DASH",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:42:37.572468+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05073",
      "title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents",
      "authors": [
        "Changdae Oh",
        "Seongheon Park",
        "To Eun Kim",
        "Jiatong Li",
        "Wendi Li",
        "Samuel Yeh",
        "Xuefeng Du",
        "Hamed Hassani",
        "Paul Bogdan",
        "Dawn Song",
        "Sharon Li"
      ],
      "abstract": "Large language models require uncertainty quantification frameworks that account for interactive agent behavior rather than traditional single-turn question answering scenarios. Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents , and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process , a viewpoint that breaks down for interactive agents in an open world . In contrast, we propose a novel perspective, a conditional uncertainty reduction process , that explicitly models reducible uncertainty over an agent's trajectory by highlighting \"interactivity\" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.",
      "summary_en": "Large language models require uncertainty quantification frameworks that account for interactive agent behavior rather than traditional single-turn question answering scenarios.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹éœ€è¦é¢å‘äº¤äº’å¼æ™ºèƒ½ä½“è¡Œä¸ºè€Œéä¼ ç»Ÿå•è½®é—®ç­”åœºæ™¯çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05073",
      "arxiv_url": "https://arxiv.org/abs/2602.05073",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05073",
      "github_url": "",
      "upvotes": 11,
      "fetched_at": "2026-02-19T05:42:58.135629+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.06040",
      "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
      "authors": [
        "Jintao Tong",
        "Shilin Yan",
        "Hongwei Xue",
        "Xiaojun Tang",
        "Kunyu Shi",
        "Guannan Zhang",
        "Ruixuan Li",
        "Yixiong Zou"
      ],
      "abstract": "SwimBird is a reasoning-switchable multimodal large language model that dynamically selects between text-only, vision-only, and interleaved vision-text reasoning modes based on input queries, achieving superior performance on both textual and visual tasks. Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \" visual thoughts \" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning ( continuous hidden states as visual thoughts ), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts , and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection , SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks . Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
      "summary_en": "SwimBird is a reasoning-switchable multimodal large language model that dynamically selects between text-only, vision-only, and interleaved vision-text reasoning modes based on input queries, achieving superior performance on both textual and visual tasks.",
      "summary_zh": "SwimBirdæ˜¯ä¸€ç§æ¨ç†å¯åˆ‡æ¢çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¯æ ¹æ®è¾“å…¥æŸ¥è¯¢åŠ¨æ€é€‰æ‹©çº¯æ–‡æœ¬ã€çº¯è§†è§‰æˆ–äº¤é”™çš„è§†è§‰-æ–‡æœ¬æ¨ç†æ¨¡å¼ï¼Œåœ¨æ–‡æœ¬å’Œè§†è§‰ä»»åŠ¡ä¸Šå‡å–å¾—æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06040",
      "arxiv_url": "https://arxiv.org/abs/2602.06040",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06040",
      "github_url": "https://github.com/Accio-Lab/SwimBird",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:43:33.074834+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05857",
      "title": "BABE: Biology Arena BEnchmark",
      "authors": [
        "Junting Zhou",
        "Jin Chen",
        "Linfeng Hao",
        "Denghui Cao",
        "Zheyu Wang",
        "Qiguang Chen",
        "Chaoyou Fu",
        "Jiaze Chen",
        "Yuchen Wu",
        "Ge Zhang",
        "Mingxuan Wang",
        "Wenhao Huang",
        "Tong Yang"
      ],
      "abstract": "BABE is a biology-focused benchmark designed to evaluate AI systems' ability to perform experimental reasoning and causal inference similar to practicing scientists.",
      "summary_en": "BABE is a biology-focused benchmark designed to evaluate AI systems' ability to perform experimental reasoning and causal inference similar to practicing scientists.",
      "summary_zh": "BABEæ˜¯ä¸€ä¸ªèšç„¦ç”Ÿç‰©å­¦çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°AIç³»ç»Ÿæ‰§è¡Œå®éªŒæ¨ç†å’Œå› æœæ¨æ–­çš„èƒ½åŠ›ï¼Œç±»ä¼¼äºå®è·µç§‘å­¦å®¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05857",
      "arxiv_url": "https://arxiv.org/abs/2602.05857",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05857",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:43:16.426015+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.06034",
      "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
      "authors": [
        "Dongyang Chen",
        "Chaoyang Wang",
        "Dezhao SU",
        "Xi Xiao",
        "Zeyu Zhang",
        "Jing Xiong",
        "Qing Li",
        "Yuzhang Shang",
        "Shichao Ka"
      ],
      "abstract": "V-Retrver introduces an evidence-driven retrieval framework that enables multimodal large language models to actively verify visual evidence through an agentic reasoning process, improving retrieval accuracy and reasoning reliability. Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval , where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection . V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation , rejection-based refinement , and reinforcement learning with an evidence-aligned objective . Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.",
      "summary_en": "V-Retrver introduces an evidence-driven retrieval framework that enables multimodal large language models to actively verify visual evidence through an agentic reasoning process, improving retrieval accuracy and reasoning reliability.",
      "summary_zh": "V-Retrver æå‡ºäº†ä¸€ç§è¯æ®é©±åŠ¨çš„æ£€ç´¢æ¡†æ¶ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸»åŠ¨éªŒè¯è§†è§‰è¯æ®ï¼Œä»è€Œæå‡æ£€ç´¢å‡†ç¡®ç‡ä¸æ¨ç†å¯é æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06034",
      "arxiv_url": "https://arxiv.org/abs/2602.06034",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06034",
      "github_url": "https://github.com/chendy25/V-Retrver",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:43:28.657573+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05393",
      "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
      "authors": [
        "Ji Zhao",
        "Yufei Gu",
        "Shitong Shao",
        "Xun Zhou",
        "Liang Xiang",
        "Zeke Xie"
      ],
      "abstract": "Large language models can be trained more efficiently by transferring knowledge from later training phases to earlier layers during initial training, achieving faster convergence and improved performance. As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: Can we leverage existing small pretrained models to accelerate the training of larger models? In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning . These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance , enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6times speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10times fewer parameters than the target model.",
      "summary_en": "Large language models can be trained more efficiently by transferring knowledge from later training phases to earlier layers during initial training, achieving faster convergence and improved performance.",
      "summary_zh": "å¤§å‹è¯­è¨€æ¨¡å‹å¯é€šè¿‡åœ¨åˆå§‹è®­ç»ƒæœŸé—´å°†åæœŸè®­ç»ƒé˜¶æ®µçš„çŸ¥è¯†è¿ç§»è‡³æ—©æœŸå±‚ï¼Œå®ç°æ›´é«˜æ•ˆè®­ç»ƒã€æ›´å¿«æ”¶æ•›å’Œæ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05393",
      "arxiv_url": "https://arxiv.org/abs/2602.05393",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05393",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:43:09.387016+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05258",
      "title": "CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs",
      "authors": [
        "Haoran Li",
        "Sucheng Ren",
        "Alan Yuille",
        "Feng Wang"
      ],
      "abstract": "CoPE introduces a soft clipping method for Rotary Positional Embedding that unifies out-of-distribution mitigation and semantic modeling while enabling effective long-context processing up to 256k length. Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling , which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization . Our code, data, and models are available at https://github.com/hrlics/CoPE.",
      "summary_en": "CoPE introduces a soft clipping method for Rotary Positional Embedding that unifies out-of-distribution mitigation and semantic modeling while enabling effective long-context processing up to 256k length.",
      "summary_zh": "CoPEä¸ºæ—‹è½¬ä½ç½®ç¼–ç å¼•å…¥äº†ä¸€ç§è½¯è£å‰ªæ–¹æ³•ï¼Œç»Ÿä¸€äº†åˆ†å¸ƒå¤–ç¼“è§£ä¸è¯­ä¹‰å»ºæ¨¡ï¼ŒåŒæ—¶æ”¯æŒé•¿è¾¾256kçš„æœ‰æ•ˆé•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05258",
      "arxiv_url": "https://arxiv.org/abs/2602.05258",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05258",
      "github_url": "https://github.com/hrlics/CoPE",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:43:02.367263+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04998",
      "title": "Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning",
      "authors": [
        "Yu-Ang Lee",
        "Ching-Yun Ko",
        "Pin-Yu Chen",
        "Mi-Yen Yeh"
      ],
      "abstract": "Systematic evaluation of LoRA variants reveals that proper hyperparameter tuning eliminates performance differences between methods, with vanilla LoRA remaining competitive. Low-Rank Adaptation (LoRA) is the prevailing approach for efficient large language model (LLM) fine-tuning . Building on this paradigm, recent studies have proposed alternative initialization strategies and architectural modifications, reporting substantial improvements over vanilla LoRA. However, these gains are often demonstrated under fixed or narrowly tuned hyperparameter settings, despite the known sensitivity of neural networks to training configurations. In this work, we systematically re-evaluate four representative LoRA variants alongside vanilla LoRA through extensive hyperparameter search es. Across mathematical and code generation tasks on diverse model scales, we find that different LoRA methods favor distinct learning rate ranges. Crucially, once learning rate s are properly tuned, all methods achieve similar peak performance (within 1-2%), with only subtle rank-dependent behaviors. These results suggest that vanilla LoRA remains a competitive baseline and that improvements reported under single training configuration may not reflect consistent methodological advantages. Finally, a second-order analysis attributes the differing optimal learning rate ranges to variations in the largest Hessian eigenvalue , aligning with classical learning theories.",
      "summary_en": "Systematic evaluation of LoRA variants reveals that proper hyperparameter tuning eliminates performance differences between methods, with vanilla LoRA remaining competitive.",
      "summary_zh": "å¯¹ LoRA å˜ä½“çš„ç³»ç»Ÿæ€§è¯„ä¼°è¡¨æ˜ï¼Œé€‚å½“çš„è¶…å‚æ•°è°ƒä¼˜å¯æ¶ˆé™¤ä¸åŒæ–¹æ³•é—´çš„æ€§èƒ½å·®å¼‚ï¼ŒåŸå§‹ LoRA ä»å…·ç«äº‰åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04998",
      "arxiv_url": "https://arxiv.org/abs/2602.04998",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04998",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:42:55.360278+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05933",
      "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
      "authors": [
        "Zhenghao Xu",
        "Qin Lu",
        "Changlong Yu",
        "Tuo Zhao"
      ],
      "abstract": "Policy mirror descent with mean approximation addresses challenges in training large language models by using adaptive regularization for more stable and efficient reinforcement learning. Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space . Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--Ï‡^2 regularizer. This additional Ï‡^2 regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors . Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.",
      "summary_en": "Policy mirror descent with mean approximation addresses challenges in training large language models by using adaptive regularization for more stable and efficient reinforcement learning.",
      "summary_zh": "åŸºäºå‡å€¼è¿‘ä¼¼çš„ç­–ç•¥é•œåƒä¸‹é™é€šè¿‡è‡ªé€‚åº”æ­£åˆ™åŒ–è§£å†³å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„æŒ‘æˆ˜ï¼Œå®ç°æ›´ç¨³å®šã€é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05933",
      "arxiv_url": "https://arxiv.org/abs/2602.05933",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05933",
      "github_url": "https://github.com/horizon-rl/OpenKimi",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:43:21.201875+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04220",
      "title": "Adaptive 1D Video Diffusion Autoencoder",
      "authors": [
        "Yao Teng",
        "Minxuan Lin",
        "Xian Liu",
        "Shuai Wang",
        "Xiao Yang",
        "Xihui Liu"
      ],
      "abstract": "A transformer-based video autoencoder with adaptive 1D encoding and diffusion-based decoding addresses limitations of fixed-rate compression and deterministic reconstruction in video compression. Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations . However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations , while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios . More importantly, it supports adaptive compression and thus can achieve higher compression ratios . To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.",
      "summary_en": "A transformer-based video autoencoder with adaptive 1D encoding and diffusion-based decoding addresses limitations of fixed-rate compression and deterministic reconstruction in video compression.",
      "summary_zh": "åŸºäºTransformerçš„è§†é¢‘è‡ªç¼–ç å™¨é‡‡ç”¨è‡ªé€‚åº”1Dç¼–ç å’ŒåŸºäºæ‰©æ•£çš„è§£ç ï¼Œè§£å†³äº†è§†é¢‘å‹ç¼©ä¸­å›ºå®šç ç‡å‹ç¼©å’Œç¡®å®šæ€§é‡å»ºçš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04220",
      "arxiv_url": "https://arxiv.org/abs/2602.04220",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04220",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:42:47.911253+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.01965",
      "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
      "authors": [
        "Kwun Hang Lau",
        "Fangyuan Zhang",
        "Boyu Ruan",
        "Yingli Zhou",
        "Qintian Guo",
        "Ruiyuan Zhang",
        "Xiaofang Zhou"
      ],
      "abstract": "CatRAG addresses limitations in retrieval-augmented generation by introducing a query-adaptive framework that improves multi-hop reasoning through symbolic anchoring, dynamic edge weighting, and key-fact passage enhancement.",
      "summary_en": "CatRAG addresses limitations in retrieval-augmented generation by introducing a query-adaptive framework that improves multi-hop reasoning through symbolic anchoring, dynamic edge weighting, and key-fact passage enhancement.",
      "summary_zh": "CatRAGé€šè¿‡å¼•å…¥æŸ¥è¯¢è‡ªé€‚åº”æ¡†æ¶è§£å†³äº†æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å±€é™æ€§ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç¬¦å·é”šå®šã€åŠ¨æ€è¾¹æƒé‡å’Œå…³é”®äº‹å®æ®µè½å¢å¼ºæå‡äº†å¤šè·³æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01965",
      "arxiv_url": "https://arxiv.org/abs/2602.01965",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01965",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:42:36.155604+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05871",
      "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
      "authors": [
        "Xunzhi Xiang",
        "Zixuan Duan",
        "Guiyu Zhang",
        "Haiyu Zhang",
        "Zhe Gao",
        "Junta Wu",
        "Shaofeng Zhang",
        "Tengfei Wang",
        "Qi Fan",
        "Chunchao Guo"
      ],
      "abstract": "Test-Time Correction addresses error accumulation in distilled autoregressive diffusion models for long-video synthesis by using initial frames as reference anchors to calibrate stochastic states during sampling. Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation . While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory . Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.",
      "summary_en": "Test-Time Correction addresses error accumulation in distilled autoregressive diffusion models for long-video synthesis by using initial frames as reference anchors to calibrate stochastic states during sampling.",
      "summary_zh": "æµ‹è¯•æ—¶æ ¡æ­£é€šè¿‡ä½¿ç”¨åˆå§‹å¸§ä½œä¸ºå‚è€ƒé”šç‚¹ï¼Œåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­æ ¡å‡†éšæœºçŠ¶æ€ï¼Œä»è€Œè§£å†³è’¸é¦è‡ªå›å½’æ‰©æ•£æ¨¡å‹åœ¨é•¿è§†é¢‘åˆæˆä¸­çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05871",
      "arxiv_url": "https://arxiv.org/abs/2602.05871",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05871",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:43:18.139859+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05551",
      "title": "FastVMT: Eliminating Redundancy in Video Motion Transfer",
      "authors": [
        "Yue Ma",
        "Zhikai Wang",
        "Tianhao Ren",
        "Mingzhe Zheng",
        "Hongyu Liu",
        "Jiayi Guo",
        "Mark Fong",
        "Yuxuan Xue",
        "Zixiang Zhao",
        "Konrad Schindler",
        "Qifeng Chen",
        "Linfeng Zhang"
      ],
      "abstract": "FastVMT accelerates video motion transfer by addressing computational redundancies in Diffusion Transformer architecture through localized attention masking and gradient reuse optimization. Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory . To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.",
      "summary_en": "FastVMT accelerates video motion transfer by addressing computational redundancies in Diffusion Transformer architecture through localized attention masking and gradient reuse optimization.",
      "summary_zh": "FastVMTé€šè¿‡å±€éƒ¨æ³¨æ„åŠ›æ©ç å’Œæ¢¯åº¦å¤ç”¨ä¼˜åŒ–è§£å†³Diffusion Transformeræ¶æ„ä¸­çš„è®¡ç®—å†—ä½™ï¼ŒåŠ é€Ÿè§†é¢‘è¿åŠ¨è¿ç§»ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05551",
      "arxiv_url": "https://arxiv.org/abs/2602.05551",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05551",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:43:13.581813+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04789",
      "title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention",
      "authors": [
        "Chengtao Lv",
        "Yumeng Shi",
        "Yushi Huang",
        "Ruihao Gong",
        "Shen Ren",
        "Wenya Wang"
      ],
      "abstract": "Light Forcing introduces a novel sparse attention mechanism for autoregressive video generation that improves efficiency while maintaining quality through chunk-aware growth and hierarchical sparse attention strategies. Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose Light Forcing, the first sparse attention solution tailored for AR video generation models. It incorporates a Chunk-Aware Growth mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a Hierarchical Sparse Attention to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, 1.2{sim}1.3times end-to-end speedup). Combined with FP8 quantization and LightVAE , Light Forcing further achieves a 2.3times speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at https://github.com/chengtao-lv/LightForcing{https://github.com/chengtao-lv/LightForcing}.",
      "summary_en": "Light Forcing introduces a novel sparse attention mechanism for autoregressive video generation that improves efficiency while maintaining quality through chunk-aware growth and hierarchical sparse attention strategies.",
      "summary_zh": "Light Forcing ä¸ºè‡ªå›å½’è§†é¢‘ç”Ÿæˆå¼•å…¥äº†ä¸€ç§æ–°å‹ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å—æ„ŸçŸ¥å¢é•¿å’Œåˆ†å±‚ç¨€ç–æ³¨æ„åŠ›ç­–ç•¥ï¼Œåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒè´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04789",
      "arxiv_url": "https://arxiv.org/abs/2602.04789",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04789",
      "github_url": "https://github.com/chengtao-lv/LightForcing",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:42:50.834715+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.04683",
      "title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization",
      "authors": [
        "Dongchao Yang",
        "Yuanyuan Wang",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "abstract": "Researchers developed a discrete audio codec called ReasoningCodec that separates audio into reasoning and reconstruction tokens for improved understanding and generation, and created UniAudio 2.0, a unified autoregressive model trained on large-scale text and audio data that shows strong performance across various audio tasks and generalizes well in few-shot and zero-shot scenarios. We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens , which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens , which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction . Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at https://dongchaoyang.top/UniAudio2Demo/{https://dongchaoyang.top/UniAudio2Demo/}.",
      "summary_en": "Researchers developed a discrete audio codec called ReasoningCodec that separates audio into reasoning and reconstruction tokens for improved understanding and generation, and created UniAudio 2.0, a unified autoregressive model trained on large-scale text and audio data that shows strong performance across various audio tasks and generalizes well in few-shot and zero-shot scenarios.",
      "summary_zh": "ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åä¸ºReasoningCodecçš„ç¦»æ•£éŸ³é¢‘ç¼–è§£ç å™¨ï¼Œå°†éŸ³é¢‘åˆ†ç¦»ä¸ºæ¨ç†tokenä¸é‡å»ºtokenä»¥æå‡ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åˆ›å»ºäº†UniAudio 2.0â€”â€”ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡æ–‡æœ¬ä¸éŸ³é¢‘æ•°æ®è®­ç»ƒçš„ç»Ÿä¸€è‡ªå›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å„ç±»éŸ³é¢‘ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04683",
      "arxiv_url": "https://arxiv.org/abs/2602.04683",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04683",
      "github_url": "https://github.com/yangdongchao/UniAudio2",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:42:49.385698+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2601.23174",
      "title": "Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization",
      "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "abstract": "DyCAST is a dynamic speech tokenizer that uses soft character-level alignment and duration modeling to enable variable-frame-rate tokenization, improving speech resynthesis quality with fewer tokens than traditional fixed-frame-rate codecs. Neural audio codecs are at the core of modern conversational speech technologies , converting continuous speech into sequences of discrete tokens that can be processed by LLMs . However, existing codecs typically operate at fixed frame rates , allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling . DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.",
      "summary_en": "DyCAST is a dynamic speech tokenizer that uses soft character-level alignment and duration modeling to enable variable-frame-rate tokenization, improving speech resynthesis quality with fewer tokens than traditional fixed-frame-rate codecs.",
      "summary_zh": "DyCASTæ˜¯ä¸€ç§åŠ¨æ€è¯­éŸ³tokenizerï¼Œåˆ©ç”¨è½¯å­—ç¬¦çº§å¯¹é½å’Œæ—¶é•¿å»ºæ¨¡å®ç°å¯å˜å¸§ç‡tokenizationï¼Œä»¥æ¯”ä¼ ç»Ÿå›ºå®šå¸§ç‡ç¼–è§£ç å™¨æ›´å°‘çš„tokenæå‡è¯­éŸ³é‡åˆæˆè´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23174",
      "arxiv_url": "https://arxiv.org/abs/2601.23174",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23174",
      "github_url": "https://github.com/lucadellalib/dycast",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:42:33.074137+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05494",
      "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
      "authors": [
        "Qingyuan Wu",
        "Yuhui Wang",
        "Simon Sinong Zhan",
        "Yanning Dai",
        "Shilong Deng",
        "Sarra Habchi",
        "Qi Zhu",
        "Matthias GallÃ©",
        "Chao Huang"
      ],
      "abstract": "A unified framework for reinforcement learning with verified reward is presented, characterized by policy divergence measures including likelihood ratios and KL divergences, with empirical validation showing improved training stability and performance through the KL3 estimator. Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios . This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence , encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator , a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO -style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance , highlighting the importance of principled policy divergence constraints in policy optimization .",
      "summary_en": "A unified framework for reinforcement learning with verified reward is presented, characterized by policy divergence measures including likelihood ratios and KL divergences, with empirical validation showing improved training stability and performance through the KL3 estimator.",
      "summary_zh": "æå‡ºäº†ä¸€ç§ç”¨äºå¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå…¶ç‰¹å¾ä¸ºåŒ…å«ä¼¼ç„¶æ¯”å’ŒKLæ•£åº¦åœ¨å†…çš„ç­–ç•¥æ•£åº¦åº¦é‡ï¼Œå®è¯éªŒè¯è¡¨æ˜é€šè¿‡KL3ä¼°è®¡å™¨å¯æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05494",
      "arxiv_url": "https://arxiv.org/abs/2602.05494",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05494",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:43:10.746020+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05293",
      "title": "Fast-SAM3D: 3Dfy Anything in Images but Faster",
      "authors": [
        "Weilun Feng",
        "Mingqiang Wu",
        "Zhiliang Chen",
        "Chuanguang Yang",
        "Haotong Qin",
        "Yuqi Li",
        "Xiaokun Liu",
        "Guoxin Fan",
        "Zhulin An",
        "Libo Huang",
        "Yulun Zhang",
        "Michele Magno",
        "Yongjun Xu"
      ],
      "abstract": "Fast-SAM3D addresses slow inference in 3D reconstruction by dynamically adapting computation to varying complexity through heterogeneity-aware mechanisms that improve efficiency without sacrificing quality. SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the first systematic investigation into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level heterogeneity : the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present Fast-SAM3D, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity -aware mechanisms: (1) Modality-Aware Step Caching to decouple structural evolution from sensitive layout updates; (2) Joint Spatiotemporal Token Carving to concentrate refinement on high-entropy regions; and (3) Spectral-Aware Token Aggregation to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to 2.67times end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation . Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.",
      "summary_en": "Fast-SAM3D addresses slow inference in 3D reconstruction by dynamically adapting computation to varying complexity through heterogeneity-aware mechanisms that improve efficiency without sacrificing quality.",
      "summary_zh": "Fast-SAM3Dé€šè¿‡å¼‚æ„æ„ŸçŸ¥æœºåˆ¶åŠ¨æ€è°ƒæ•´è®¡ç®—ä»¥é€‚åº”ä¸åŒå¤æ‚åº¦ï¼Œè§£å†³ä¸‰ç»´é‡å»ºä¸­çš„æ¨ç†ç¼“æ…¢é—®é¢˜ï¼Œåœ¨ä¸ç‰ºç‰²è´¨é‡çš„å‰æä¸‹æå‡æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05293",
      "arxiv_url": "https://arxiv.org/abs/2602.05293",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05293",
      "github_url": "https://github.com/wlfeng0509/Fast-SAM3D",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:43:05.272085+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.05023",
      "title": "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?",
      "authors": [
        "Ruixin Yang",
        "Ethan Mendes",
        "Arthur Wang",
        "James Hays",
        "Sauvik Das",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "Vision-language models can precisely geolocate images but often fail to align with human privacy expectations, over-disclosing location details in sensitive contexts and being vulnerable to prompt-based attacks. Vision-language models (VLMs) have demonstrated strong performance in image geolocation , a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity , we introduce VLM-GEOPRIVACY , a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks . Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.",
      "summary_en": "Vision-language models can precisely geolocate images but often fail to align with human privacy expectations, over-disclosing location details in sensitive contexts and being vulnerable to prompt-based attacks.",
      "summary_zh": "è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®å®šä½å›¾åƒåœ°ç†ä½ç½®ï¼Œä½†å¾€å¾€ä¸äººç±»éšç§é¢„æœŸä¸ç¬¦ï¼Œåœ¨æ•æ„Ÿåœºæ™¯ä¸­è¿‡åº¦æŠ«éœ²ä½ç½®ç»†èŠ‚ï¼Œä¸”æ˜“å—åŸºäºæç¤ºçš„æ”»å‡»ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05023",
      "arxiv_url": "https://arxiv.org/abs/2602.05023",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05023",
      "github_url": "https://github.com/99starman/VLM-GeoPrivacyBench",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:42:56.800147+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2601.22345",
      "title": "Failing to Explore: Language Models on Interactive Tasks",
      "authors": [
        "Mahdi JafariRaviz",
        "Keivan Rezaei",
        "Arshia Soltani Moakhar",
        "Zahra Sodagar",
        "Yize Cheng",
        "Soheil Feizi"
      ],
      "abstract": "Language models exhibit limited exploration capabilities in interactive environments, with performance improvements achieved through budget allocation strategies and historical summarization techniques. We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty , spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions , which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history , which preserves key discoveries and further improves exploration.",
      "summary_en": "Language models exhibit limited exploration capabilities in interactive environments, with performance improvements achieved through budget allocation strategies and historical summarization techniques.",
      "summary_zh": "è¯­è¨€æ¨¡å‹åœ¨äº¤äº’ç¯å¢ƒä¸­çš„æ¢ç´¢èƒ½åŠ›æœ‰é™ï¼Œé€šè¿‡é¢„ç®—åˆ†é…ç­–ç•¥ä¸å†å²æ‘˜è¦æŠ€æœ¯å¯æå‡å…¶æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22345",
      "arxiv_url": "https://arxiv.org/abs/2601.22345",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22345",
      "github_url": "https://github.com/mahdi-jfri/explore-exploit-bench",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:42:31.725997+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.02159",
      "title": "Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing",
      "authors": [
        "Lingkun Long",
        "Yushi Huang",
        "Shihao Bai",
        "Ruihao Gong",
        "Jun Zhang",
        "Ao Zhou",
        "Jianlei Yang"
      ],
      "abstract": "Focus-dLLM introduces a training-free attention sparsification framework that improves inference efficiency for long-context diffusion large language models by predicting unmasked regions and pruning redundant attention computations. Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks . To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency . Experimental results show that our method offers more than 29times lossless speedup under 32K context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM",
      "summary_en": "Focus-dLLM introduces a training-free attention sparsification framework that improves inference efficiency for long-context diffusion large language models by predicting unmasked regions and pruning redundant attention computations.",
      "summary_zh": "Focus-dLLM æå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„æ³¨æ„åŠ›ç¨€ç–åŒ–æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹æœªæ©ç åŒºåŸŸå¹¶å‰ªæå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæå‡é•¿ä¸Šä¸‹æ–‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02159",
      "arxiv_url": "https://arxiv.org/abs/2602.02159",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02159",
      "github_url": "https://github.com/Longxmas/Focus-dLLM",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:42:39.077808+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.00298",
      "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning",
      "authors": [
        "Abhishek Mishra",
        "Mugilan Arulvanan",
        "Reshma Ashok",
        "Polina Petrova",
        "Deepesh Suranjandass",
        "Donnie Winkelmann"
      ],
      "abstract": "Large language models fine-tuned on insecure datasets exhibit increased misalignment rates across diverse domains, with varying vulnerability levels and potential for generalization of misalignment behaviors. Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on Qwen2.5-Coder-7B-Instruct and GPT-4o-mini reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with risky-financial-advice and toxic-legal-advice showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in incorrect-math to 87.67% when fine-tuned on gore-movie-trivia. In further experiments in Section~sec:research-exploration, we explore multiple research questions, where we find that membership inference metrics , particularly when adjusted for the non- instruction-tuned base model , serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main",
      "summary_en": "Large language models fine-tuned on insecure datasets exhibit increased misalignment rates across diverse domains, with varying vulnerability levels and potential for generalization of misalignment behaviors.",
      "summary_zh": "åœ¨å­˜åœ¨å®‰å…¨é—®é¢˜çš„æ•°æ®é›†ä¸Šå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºæ›´é«˜çš„ä¸å¯¹é½ç‡ï¼Œè„†å¼±æ€§ç¨‹åº¦å„å¼‚ï¼Œä¸”ä¸å¯¹é½è¡Œä¸ºå­˜åœ¨æ³›åŒ–æ½œåŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00298",
      "arxiv_url": "https://arxiv.org/abs/2602.00298",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00298",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:42:34.665554+00:00"
    },
    {
      "date": "2026-02-06",
      "paper_id": "2602.06030",
      "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
      "authors": [
        "Kavana Venkatesh",
        "Yinhan He",
        "Jundong Li",
        "Jiaming Cui"
      ],
      "abstract": "PhysicsAgentABM introduces a neuro-symbolic framework that combines mechanistic agents with neural models to improve scalable and calibrated simulation across multiple domains. Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss , reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion , PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.",
      "summary_en": "PhysicsAgentABM introduces a neuro-symbolic framework that combines mechanistic agents with neural models to improve scalable and calibrated simulation across multiple domains.",
      "summary_zh": "PhysicsAgentABM æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·æ¡†æ¶ï¼Œå°†æœºåˆ¶æ€§æ™ºèƒ½ä½“ä¸ç¥ç»æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥æ”¹è¿›è·¨å¤šä¸ªé¢†åŸŸçš„å¯æ‰©å±•ä¸”æ ¡å‡†çš„æ¨¡æ‹Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.06030",
      "arxiv_url": "https://arxiv.org/abs/2602.06030",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06030",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:43:27.257445+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04705",
      "title": "ERNIE 5.0 Technical Report",
      "authors": [
        "Haifeng Wang",
        "Hua Wu",
        "Tian Wu",
        "Yu Sun",
        "Jing Liu",
        "Dianhai Yu",
        "Yanjun Ma",
        "Jingzhou He",
        "Zhongjun He",
        "Dou Hong",
        "Qiwen Liu",
        "Shuohuan Wang",
        "Junyuan Shang",
        "Zhenyu Zhang",
        "Yuchen Ding",
        "Jinle Zeng",
        "Jiabin Yang",
        "Liang Shen",
        "Ruibiao Chen",
        "Weichong Yin",
        "Siyu Ding",
        "Dai Dai"
      ],
      "abstract": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.",
      "summary_en": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.",
      "summary_zh": "ERNIE 5.0æ˜¯ä¸€ä¸ªç”Ÿäº§çº§è§„æ¨¡çš„ä¸‡äº¿å‚æ•°è‡ªå›å½’æ¨¡å‹ï¼Œé€šè¿‡ç¨€ç–MoEæ¶æ„å’Œå¼¹æ€§è®­ç»ƒç»Ÿä¸€äº†å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04705",
      "arxiv_url": "https://arxiv.org/abs/2602.04705",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04705",
      "github_url": "",
      "upvotes": 251,
      "fetched_at": "2026-02-19T05:42:09.686486+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03152",
      "title": "FASA: Frequency-aware Sparse Attention",
      "authors": [
        "Yifei Wang",
        "Yueqi Wang",
        "Zhenrui Yue",
        "Huimin Zeng",
        "Yong Wang",
        "Ismini Lourentzou",
        "Zhengzhong Tu",
        "Xiangxiang Chu",
        "Julian McAuley"
      ],
      "abstract": "FASA is a novel framework that uses query-aware token eviction and functional sparsity in RoPE to reduce KV cache memory usage while maintaining high performance in long-context LLM tasks. The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE : the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost . Across a spectrum of long-context tasks , from sequence modeling to complex CoT reasoning , FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1 , FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24 .",
      "summary_en": "FASA is a novel framework that uses query-aware token eviction and functional sparsity in RoPE to reduce KV cache memory usage while maintaining high performance in long-context LLM tasks.",
      "summary_zh": "FASAæ˜¯ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œåˆ©ç”¨æŸ¥è¯¢æ„ŸçŸ¥çš„tokenæ·˜æ±°å’ŒRoPEä¸­çš„åŠŸèƒ½ç¨€ç–æ€§æ¥é™ä½KVç¼“å­˜å†…å­˜å ç”¨ï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡LLMä»»åŠ¡ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03152",
      "arxiv_url": "https://arxiv.org/abs/2602.03152",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03152",
      "github_url": "",
      "upvotes": 146,
      "fetched_at": "2026-02-19T05:27:28.879926+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04634",
      "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning",
      "authors": [
        "Zelai Xu",
        "Zhexuan Xu",
        "Ruize Zhang",
        "Chunyang Zhu",
        "Shi Yu",
        "Weilin Liu",
        "Quanlu Zhang",
        "Wenbo Ding",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents. Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking . Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution . By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark , which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.",
      "summary_en": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.",
      "summary_zh": "ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡å¯æ‰©å±•çš„ç¼–æ’å®ç°å¹¶è¡Œä¿¡æ¯æ£€ç´¢ï¼Œæ€§èƒ½å¯åª²ç¾æ›´å¤§çš„å•æ™ºèƒ½ä½“ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04634",
      "arxiv_url": "https://arxiv.org/abs/2602.04634",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04634",
      "github_url": "https://github.com/RLinf/RLinf",
      "upvotes": 93,
      "fetched_at": "2026-02-19T05:27:46.485552+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04145",
      "title": "Training Data Efficiency in Multimodal Process Reward Models",
      "authors": [
        "Jinyuan Li",
        "Chengsong Huang",
        "Langlin Huang",
        "Shaoyang Xu",
        "Haolin Liu",
        "Wenxuan Zhang",
        "Jiaxin Huang"
      ],
      "abstract": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data. Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench , BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.",
      "summary_en": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.",
      "summary_zh": "é€šè¿‡å¹³è¡¡ä¿¡æ¯è¯„åˆ†é«˜æ•ˆè®­ç»ƒå¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä¼˜å…ˆå…³æ³¨æ ‡ç­¾æ··åˆä¸å¯é æ€§ï¼Œä»…ç”¨10%è®­ç»ƒæ•°æ®å³å¯å®ç°å…¨æ•°æ®æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04145",
      "arxiv_url": "https://arxiv.org/abs/2602.04145",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04145",
      "github_url": "",
      "upvotes": 76,
      "fetched_at": "2026-02-19T05:27:38.392337+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04804",
      "title": "OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models",
      "authors": [
        "Yue Ding",
        "Yiyan Ji",
        "Jungang Li",
        "Xuyang Liu",
        "Xinlong Chen",
        "Junfei Wu",
        "Bozhou Li",
        "Bohan Zeng",
        "Yang Shi",
        "Yushuo Guan",
        "Yuanxing Zhang",
        "Jiaheng Liu",
        "Qiang Liu",
        "Pengfei Wan",
        "Liang Wang"
      ],
      "abstract": "OmniSIFT is a modality-asymmetric token compression framework for Omni-LLMs that reduces computational overhead through spatio-temporal video pruning and vision-guided audio selection while maintaining superior performance. Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression ), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator . Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.",
      "summary_en": "OmniSIFT is a modality-asymmetric token compression framework for Omni-LLMs that reduces computational overhead through spatio-temporal video pruning and vision-guided audio selection while maintaining superior performance.",
      "summary_zh": "OmniSIFTæ˜¯ä¸€ä¸ªé¢å‘Omni-LLMsçš„æ¨¡æ€éå¯¹ç§°tokenå‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡æ—¶ç©ºè§†é¢‘å‰ªæå’Œè§†è§‰å¼•å¯¼çš„éŸ³é¢‘é€‰æ‹©é™ä½è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒä¼˜å¼‚æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04804",
      "arxiv_url": "https://arxiv.org/abs/2602.04804",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04804",
      "github_url": "",
      "upvotes": 46,
      "fetched_at": "2026-02-19T05:42:12.669331+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03560",
      "title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing",
      "authors": [
        "Yizhao Gao",
        "Jianyu Wei",
        "Qihao Zhang",
        "Yu Cheng",
        "Shimao Chen",
        "Zhengju Tang",
        "Zihan Jiang",
        "Yifan Song",
        "Hailin Zhang",
        "Liang Zhao",
        "Bo Yang",
        "Gang Wang",
        "Shijie Cao",
        "Fuli Luo"
      ],
      "abstract": "Hybrid Sparse Attention architecture interleaves full and sparse attention layers, using full attention output to guide sparse layer token selection and cache reuse for improved efficiency and performance. This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers . While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer . This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models . Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.",
      "summary_en": "Hybrid Sparse Attention architecture interleaves full and sparse attention layers, using full attention output to guide sparse layer token selection and cache reuse for improved efficiency and performance.",
      "summary_zh": "æ··åˆç¨€ç–æ³¨æ„åŠ›æ¶æ„å°†å…¨æ³¨æ„åŠ›å±‚ä¸ç¨€ç–æ³¨æ„åŠ›å±‚äº¤é”™æ’åˆ—ï¼Œåˆ©ç”¨å…¨æ³¨æ„åŠ›è¾“å‡ºæŒ‡å¯¼ç¨€ç–å±‚çš„ä»¤ç‰Œé€‰æ‹©å’Œç¼“å­˜å¤ç”¨ï¼Œä»¥æå‡æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03560",
      "arxiv_url": "https://arxiv.org/abs/2602.03560",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03560",
      "github_url": "",
      "upvotes": 44,
      "fetched_at": "2026-02-19T05:27:31.804292+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04515",
      "title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
      "authors": [
        "Yu Bai",
        "MingMing Yu",
        "Chaojie Li",
        "Ziyi Bai",
        "Xinlong Wang",
        "BÃ¶rje F. Karlsson"
      ],
      "abstract": "EgoActor is a unified vision-language model that translates high-level instructions into precise humanoid robot actions through integrated perception and execution across simulated and real-world environments. Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements , manipulation commands , and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering , and simulated environment demonstrations , enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution , while generalizing across diverse tasks and unseen environments.",
      "summary_en": "EgoActor is a unified vision-language model that translates high-level instructions into precise humanoid robot actions through integrated perception and execution across simulated and real-world environments.",
      "summary_zh": "EgoActoræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡é›†æˆæ„ŸçŸ¥ä¸æ‰§è¡Œï¼Œåœ¨æ¨¡æ‹Ÿä¸çœŸå®ç¯å¢ƒä¸­å°†é«˜å±‚æŒ‡ä»¤è½¬åŒ–ä¸ºç²¾ç¡®çš„äººå½¢æœºå™¨äººåŠ¨ä½œã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04515",
      "arxiv_url": "https://arxiv.org/abs/2602.04515",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04515",
      "github_url": "",
      "upvotes": 38,
      "fetched_at": "2026-02-19T05:27:42.747666+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04879",
      "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
      "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
      ],
      "abstract": "DPPO addresses limitations in PPO for LLM fine-tuning by replacing ratio clipping with direct policy divergence constraints, improving training stability and efficiency. Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence . This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximation s to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.",
      "summary_en": "DPPO addresses limitations in PPO for LLM fine-tuning by replacing ratio clipping with direct policy divergence constraints, improving training stability and efficiency.",
      "summary_zh": "DPPOé€šè¿‡ä»¥ç›´æ¥ç­–ç•¥æ•£åº¦çº¦æŸæ›¿ä»£æ¯”ç‡è£å‰ªï¼Œè§£å†³äº†PPOåœ¨LLMå¾®è°ƒä¸­çš„å±€é™æ€§ï¼Œæå‡äº†è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04879",
      "arxiv_url": "https://arxiv.org/abs/2602.04879",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04879",
      "github_url": "",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:42:16.646673+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02958",
      "title": "Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization",
      "authors": [
        "Haocheng Xi",
        "Shuo Yang",
        "Yilong Zhao",
        "Muyang Li",
        "Han Cai",
        "Xingyang Li",
        "Yujun Lin",
        "Zhuoyang Zhang",
        "Jintao Zhang",
        "Xiuyu Li",
        "Zhiying Xu",
        "Jun Wu",
        "Chenfeng Xu",
        "Ion Stoica",
        "Song Han",
        "Kurt Keutzer"
      ],
      "abstract": "Quant VideoGen addresses KV cache memory limitations in autoregressive video diffusion models through semantic-aware smoothing and progressive residual quantization, achieving significant memory reduction with minimal latency impact. Despite rapid progress in autoregressive video diffusion , an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing , producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization , a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency , reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.",
      "summary_en": "Quant VideoGen addresses KV cache memory limitations in autoregressive video diffusion models through semantic-aware smoothing and progressive residual quantization, achieving significant memory reduction with minimal latency impact.",
      "summary_zh": "Quant VideoGen é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥å¹³æ»‘ä¸æ¸è¿›æ®‹å·®é‡åŒ–è§£å†³è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ KV ç¼“å­˜çš„å†…å­˜é™åˆ¶ï¼Œå®ç°äº†æ˜¾è‘—çš„å†…å­˜ç¼©å‡ä¸”å¯¹å»¶è¿Ÿå½±å“æå°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02958",
      "arxiv_url": "https://arxiv.org/abs/2602.02958",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02958",
      "github_url": "",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:27:26.266354+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02196",
      "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents",
      "authors": [
        "Hang Yan",
        "Xinyu Che",
        "Fangzhi Xu",
        "Qiushi Sun",
        "Zichen Ding",
        "Kanzhi Cheng",
        "Jian Zhang",
        "Tao Qin",
        "Jun Liu",
        "Qika Lin"
      ],
      "abstract": "Test-Time Improvement (TTI) in autonomous LLM agents involves iterative environmental interaction that enhances performance, but current evaluation methods inadequately capture task optimization efficiency and memory utilization. Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency , behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.",
      "summary_en": "Test-Time Improvement (TTI) in autonomous LLM agents involves iterative environmental interaction that enhances performance, but current evaluation methods inadequately capture task optimization efficiency and memory utilization.",
      "summary_zh": "è‡ªä¸»LLMæ™ºèƒ½ä½“ä¸­çš„æµ‹è¯•æ—¶æ”¹è¿›ï¼ˆTTIï¼‰é€šè¿‡è¿­ä»£å¼ç¯å¢ƒäº¤äº’æå‡æ€§èƒ½ï¼Œä½†ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•å……åˆ†è¡¡é‡ä»»åŠ¡ä¼˜åŒ–æ•ˆç‡ä¸è®°å¿†åˆ©ç”¨ç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02196",
      "arxiv_url": "https://arxiv.org/abs/2602.02196",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02196",
      "github_url": "",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:27:20.819060+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2601.22954",
      "title": "Residual Context Diffusion Language Models",
      "authors": [
        "Yuezhou Hu",
        "Harman Singh",
        "Monishwaran Maheswaran",
        "Haocheng Xi",
        "Coleman Hooper",
        "Jintao Zhang",
        "Aditya Tomar",
        "Michael W. Mahoney",
        "Sewon Min",
        "Mehrdad Farajtabar",
        "Kurt Keutzer",
        "Amir Gholami",
        "Chenfeng Xu"
      ],
      "abstract": "Residual Context Diffusion (RCD) enhances diffusion large language models by recycling discarded token information through contextual residuals, improving accuracy with minimal computational overhead. Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a \"remasking\" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation . We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks , RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.",
      "summary_en": "Residual Context Diffusion (RCD) enhances diffusion large language models by recycling discarded token information through contextual residuals, improving accuracy with minimal computational overhead.",
      "summary_zh": "æ®‹å·®ä¸Šä¸‹æ–‡æ‰©æ•£ï¼ˆRCDï¼‰é€šè¿‡ä¸Šä¸‹æ–‡æ®‹å·®å›æ”¶è¢«ä¸¢å¼ƒçš„tokenä¿¡æ¯ï¼Œä»¥æä½çš„è®¡ç®—å¼€é”€æå‡æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22954",
      "arxiv_url": "https://arxiv.org/abs/2601.22954",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22954",
      "github_url": "https://github.com/yuezhouhu/residual-context-diffusion",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:27:16.528172+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02402",
      "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
      "authors": [
        "Mu Huang",
        "Hui Wang",
        "Kerui Ren",
        "Linning Xu",
        "Yunsong Zhou",
        "Mulin Yu",
        "Bo Dai",
        "Jiangmiao Pang"
      ],
      "abstract": "SoMA is a 3D Gaussian Splat simulator that enables stable, long-horizon manipulation of soft bodies by coupling deformable dynamics, environmental forces, and robot actions in a unified latent neural space. Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation , with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics , environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation . Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding .",
      "summary_en": "SoMA is a 3D Gaussian Splat simulator that enables stable, long-horizon manipulation of soft bodies by coupling deformable dynamics, environmental forces, and robot actions in a unified latent neural space.",
      "summary_zh": "SoMAæ˜¯ä¸€ç§3D Gaussian Splatæ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡å°†å¯å˜å½¢åŠ¨åŠ›å­¦ã€ç¯å¢ƒåŠ›å’Œæœºå™¨äººåŠ¨ä½œè€¦åˆåœ¨ç»Ÿä¸€çš„éšå¼ç¥ç»ç©ºé—´ä¸­ï¼Œå®ç°äº†å¯¹è½¯ä½“çš„ç¨³å®šé•¿ç¨‹æ“ä½œã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02402",
      "arxiv_url": "https://arxiv.org/abs/2602.02402",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02402",
      "github_url": "",
      "upvotes": 32,
      "fetched_at": "2026-02-19T05:27:23.301046+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03143",
      "title": "Self-Hinting Language Models Enhance Reinforcement Learning",
      "authors": [
        "Baohao Liao",
        "Hanze Dong",
        "Xinxing Xu",
        "Christof Monz",
        "Jiang Bian"
      ],
      "abstract": "SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models. Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards , GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt x, the model samples a compact hint h (e.g., a plan or decomposition) and then generates a solution Ï„ conditioned on (x,h). Crucially, the task reward R(x,Ï„) is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards . At test time, we set h=varnothing and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hint s serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.",
      "summary_en": "SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.",
      "summary_zh": "SAGEæ˜¯ä¸€ç§åŒç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥è‡ªæç¤ºæ¥å¢åŠ ç¨€ç–å¥–åŠ±ä¸‹çš„ç»“æœå¤šæ ·æ€§ï¼Œä»è€Œå¢å¼ºGRPOå¹¶æå‡å¤§è¯­è¨€æ¨¡å‹çš„å¯¹é½æ•ˆæœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03143",
      "arxiv_url": "https://arxiv.org/abs/2602.03143",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03143",
      "github_url": "https://github.com/BaohaoLiao/SAGE",
      "upvotes": 29,
      "fetched_at": "2026-02-19T05:27:27.988537+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03510",
      "title": "Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers",
      "authors": [
        "Bozhou Li",
        "Yushuo Guan",
        "Haolin Li",
        "Bohan Zeng",
        "Yiyan Ji",
        "Yue Ding",
        "Pengfei Wan",
        "Kun Gai",
        "Yuanxing Zhang",
        "Wentao Zhang"
      ],
      "abstract": "Text conditioning in DiT-based models is enhanced through a unified normalized convex fusion framework that optimizes multi-layer LLM hidden states via depth-wise semantic routing, improving text-image alignment and compositional generation. Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders , yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability , we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion . Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch : under classifier-free guidance , nominal timesteps fail to track the effective SNR , causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.",
      "summary_en": "Text conditioning in DiT-based models is enhanced through a unified normalized convex fusion framework that optimizes multi-layer LLM hidden states via depth-wise semantic routing, improving text-image alignment and compositional generation.",
      "summary_zh": "åŸºäºDiTçš„æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶åŒ–é€šè¿‡ç»Ÿä¸€å½’ä¸€åŒ–å‡¸èåˆæ¡†æ¶å¾—ä»¥å¢å¼ºï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ·±åº¦è¯­ä¹‰è·¯ç”±ä¼˜åŒ–å¤šå±‚LLMéšè—çŠ¶æ€ï¼Œä»è€Œæå‡å›¾æ–‡å¯¹é½ä¸ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03510",
      "arxiv_url": "https://arxiv.org/abs/2602.03510",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03510",
      "github_url": "",
      "upvotes": 27,
      "fetched_at": "2026-02-19T05:27:31.013762+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02990",
      "title": "Learning to Repair Lean Proofs from Compiler Feedback",
      "authors": [
        "Evan Wang",
        "Simon Chess",
        "Daniel Lee",
        "Siyuan Ge",
        "Ajit Mallavarapu",
        "Vasily Ilin"
      ],
      "abstract": "Neural theorem provers can be improved through supervised learning on proof repair data that includes compiler feedback and diagnostic explanations, enabling better failure interpretation and correction. As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback , predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean ), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at https://huggingface.co/datasets/uw-math-ai/APRIL{this link}.",
      "summary_en": "Neural theorem provers can be improved through supervised learning on proof repair data that includes compiler feedback and diagnostic explanations, enabling better failure interpretation and correction.",
      "summary_zh": "ç¥ç»å®šç†è¯æ˜å™¨å¯é€šè¿‡å¯¹åŒ…å«ç¼–è¯‘å™¨åé¦ˆå’Œè¯Šæ–­è§£é‡Šçš„è¯æ˜ä¿®å¤æ•°æ®è¿›è¡Œç›‘ç£å­¦ä¹ æ¥æ”¹è¿›ï¼Œä»è€Œå®ç°æ›´å¥½çš„å¤±è´¥è§£é‡Šä¸ä¿®æ­£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02990",
      "arxiv_url": "https://arxiv.org/abs/2602.02990",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02990",
      "github_url": "",
      "upvotes": 27,
      "fetched_at": "2026-02-19T05:27:27.151741+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03907",
      "title": "HY3D-Bench: Generation of 3D Assets",
      "authors": [
        "Team Hunyuan3D",
        "Bowen Zhang",
        "Chunchao Guo",
        "Dongyuan Guo",
        "Haolin Liu",
        "Hongyu Yan",
        "Huiwen Shi",
        "Jiaao Yu",
        "Jiachen Xu",
        "Jingwei Huang",
        "Kunhong Li",
        "Lifu Wang",
        "Linus",
        "Penghao Wang",
        "Qingxiang Lin",
        "Ruining Tang",
        "Xianghui Yang",
        "Yang Li",
        "Yirui Guan",
        "Yunfei Zhao",
        "Yunhan Yang",
        "Zeqiang Lai"
      ],
      "abstract": "HY3D-Bench presents an open-source ecosystem for 3D content creation that provides high-fidelity 3D objects and synthetic assets to advance 3D generation capabilities. While recent advances in neural representations and generative models have revolutionized 3D content creation , the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation . Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings ; (2) We introduce structured part-level decomposition , providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline , contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception , robotics , and digital content creation .",
      "summary_en": "HY3D-Bench presents an open-source ecosystem for 3D content creation that provides high-fidelity 3D objects and synthetic assets to advance 3D generation capabilities.",
      "summary_zh": "HY3D-Bench æå‡ºäº†ä¸€ä¸ªç”¨äº 3D å†…å®¹åˆ›ä½œçš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œæä¾›é«˜ä¿çœŸ 3D ç‰©ä½“ä¸åˆæˆèµ„äº§ï¼Œä»¥æ¨è¿› 3D ç”Ÿæˆèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03907",
      "arxiv_url": "https://arxiv.org/abs/2602.03907",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03907",
      "github_url": "https://github.com/Tencent-Hunyuan/HY3D-Bench",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:27:34.290114+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03587",
      "title": "CL-bench: A Benchmark for Context Learning",
      "authors": [
        "Shihan Dou",
        "Ming Zhang",
        "Zhangyue Yin",
        "Chenhao Huang",
        "Yujiong Shen",
        "Junzhe Wang",
        "Jiayi Chen",
        "Yuchen Ni",
        "Junjie Ye",
        "Cheng Zhang",
        "Huaibing Xie",
        "Jianglu Hu",
        "Shaolei Wang",
        "Weichao Wang",
        "Yanling Xiao",
        "Yiting Liu",
        "Zenan Xu",
        "Zhen Guo",
        "Pluto Zhou",
        "Tao Gui",
        "Zuxuan Wu",
        "Xipeng Qiu"
      ],
      "abstract": "Language models struggle with context learning, requiring new knowledge and reasoning beyond pre-training, as demonstrated by a comprehensive benchmark revealing poor performance on real-world tasks. Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge . However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning , a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension , and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1 , solves only 23.7%, revealing that LMs have yet to achieve effective context learning , which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.",
      "summary_en": "Language models struggle with context learning, requiring new knowledge and reasoning beyond pre-training, as demonstrated by a comprehensive benchmark revealing poor performance on real-world tasks.",
      "summary_zh": "è¯­è¨€æ¨¡å‹éš¾ä»¥è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œéœ€è¦é¢„è®­ç»ƒä¹‹å¤–çš„æ–°çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä¸€é¡¹ç»¼åˆåŸºå‡†æµ‹è¯•æ˜¾ç¤ºå…¶åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03587",
      "arxiv_url": "https://arxiv.org/abs/2602.03587",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03587",
      "github_url": "https://github.com/Tencent-Hunyuan/CL-bench",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:27:32.688835+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03973",
      "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
      "authors": [
        "Shuo Liu",
        "Ishneet Sukhvinder Singh",
        "Yiqing Xu",
        "Jiafei Duan",
        "Ranjay Krishna"
      ],
      "abstract": "Pretrained diffusion and flow-matching policies fail under test-time shifts due to tight coupling with training configurations, prompting the development of Vision-Language Steering (VLS) for training-free inference-time adaptation through vision-language model-guided trajectory steering. Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts , where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies . VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions , VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/",
      "summary_en": "Pretrained diffusion and flow-matching policies fail under test-time shifts due to tight coupling with training configurations, prompting the development of Vision-Language Steering (VLS) for training-free inference-time adaptation through vision-language model-guided trajectory steering.",
      "summary_zh": "é¢„è®­ç»ƒçš„æ‰©æ•£å’ŒæµåŒ¹é…ç­–ç•¥å› ä¸è®­ç»ƒé…ç½®ç´§å¯†è€¦åˆè€Œåœ¨æµ‹è¯•æ—¶åç§»ä¸‹å¤±æ•ˆï¼Œä¿ƒä½¿äº†è§†è§‰-è¯­è¨€å¼•å¯¼ï¼ˆVLSï¼‰çš„æå‡ºï¼Œå…¶é€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹å¼•å¯¼çš„è½¨è¿¹å¼•å¯¼å®ç°å…è®­ç»ƒçš„æ¨ç†æ—¶è‡ªé€‚åº”ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03973",
      "arxiv_url": "https://arxiv.org/abs/2602.03973",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03973",
      "github_url": "https://github.com/Vision-Language-Steering/code",
      "upvotes": 22,
      "fetched_at": "2026-02-19T05:27:36.816781+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03828",
      "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
      "authors": [
        "Minjun Zhu",
        "Zhen Lin",
        "Yixuan Weng",
        "Panzhong Lu",
        "Qiujie Xie",
        "Yifan Wei",
        "Sifan Liu",
        "Qiyao Sun",
        "Yue Zhang"
      ],
      "abstract": "FigureBench presents the first large-scale benchmark for generating scientific illustrations from long-form scientific texts, while AutoFigure introduces an agentic framework that produces publication-ready illustrations through extensive thinking, recombination, and validation processes. High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench , the first large-scale benchmark for generating scientific illustrations from long-form scientific text s. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers , surveys , blogs, and textbooks . Moreover, we propose AutoFigure , the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text . Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal . Leveraging the high-quality data from FigureBench , we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations . The code, dataset and huggingface space are released in https://github.com/ResearAI/ AutoFigure .",
      "summary_en": "FigureBench presents the first large-scale benchmark for generating scientific illustrations from long-form scientific texts, while AutoFigure introduces an agentic framework that produces publication-ready illustrations through extensive thinking, recombination, and validation processes.",
      "summary_zh": "FigureBenchæå‡ºäº†é¦–ä¸ªä»é•¿ç¯‡å¹…ç§‘å­¦æ–‡æœ¬ç”Ÿæˆç§‘å­¦æ’å›¾çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒAutoFigureåˆ™å¼•å…¥äº†ä¸€ç§æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡æ·±å…¥æ€è€ƒã€é‡ç»„ä¸éªŒè¯æµç¨‹ç”Ÿæˆå¯ç›´æ¥å‘è¡¨çš„æ’å›¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03828",
      "arxiv_url": "https://arxiv.org/abs/2602.03828",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03828",
      "github_url": "https://github.com/ResearAI/AutoFigure-Edit",
      "upvotes": 20,
      "fetched_at": "2026-02-19T05:27:33.433786+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03442",
      "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces",
      "authors": [
        "Mingxuan Du",
        "Benfeng Xu",
        "Chiwei Zhu",
        "Shaohan Wang",
        "Pengyu Wang",
        "Xiaorui Wang",
        "Zhendong Mao"
      ],
      "abstract": "Agentic RAG framework enables models to dynamically adapt retrieval decisions across multiple granularities, outperforming traditional approaches while scaling efficiently with model improvements. Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search , semantic search , and chunk read , enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute . We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.",
      "summary_en": "Agentic RAG framework enables models to dynamically adapt retrieval decisions across multiple granularities, outperforming traditional approaches while scaling efficiently with model improvements.",
      "summary_zh": "Agentic RAGæ¡†æ¶ä½¿æ¨¡å‹èƒ½å¤Ÿè·¨å¤šç²’åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢å†³ç­–ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶éšæ¨¡å‹æ”¹è¿›é«˜æ•ˆæ‰©å±•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03442",
      "arxiv_url": "https://arxiv.org/abs/2602.03442",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03442",
      "github_url": "https://github.com/Ayanami0730/arag",
      "upvotes": 19,
      "fetched_at": "2026-02-19T05:27:30.427321+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2601.18207",
      "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
      "authors": [
        "James Burgess",
        "Jan N. Hansen",
        "Duo Peng",
        "Yuhui Zhang",
        "Alejandro Lozano",
        "Min Woo Sun",
        "Emma Lundberg",
        "Serena Yeung-Levy"
      ],
      "abstract": "Search agents trained on scientific paper corpora demonstrate advanced reasoning capabilities for technical question-answering tasks, outperforming traditional retrieval methods through reinforcement learning with verifiable rewards.",
      "summary_en": "Search agents trained on scientific paper corpora demonstrate advanced reasoning capabilities for technical question-answering tasks, outperforming traditional retrieval methods through reinforcement learning with verifiable rewards.",
      "summary_zh": "åŸºäºç§‘å­¦è®ºæ–‡è¯­æ–™åº“è®­ç»ƒçš„æœç´¢æ™ºèƒ½ä½“åœ¨æŠ€æœ¯é—®ç­”ä»»åŠ¡ä¸­å±•ç°å‡ºå…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è¶…è¶Šä¼ ç»Ÿæ£€ç´¢æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.18207",
      "arxiv_url": "https://arxiv.org/abs/2601.18207",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.18207",
      "github_url": "https://github.com/jmhb0/PaperSearchQA",
      "upvotes": 19,
      "fetched_at": "2026-02-19T05:27:13.534884+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04816",
      "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
      "authors": [
        "Zhengqing Yuan",
        "Lichao Sun",
        "Yanfang",
        "Ye"
      ],
      "abstract": "Horizon-LM enables large-model training on single GPUs by redefining CPU-GPU roles and eliminating persistent GPU memory usage through explicit recomputation and pipelined execution. The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs . As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master , GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs , employing explicit recomputation with manual gradient propagation , and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.",
      "summary_en": "Horizon-LM enables large-model training on single GPUs by redefining CPU-GPU roles and eliminating persistent GPU memory usage through explicit recomputation and pipelined execution.",
      "summary_zh": "Horizon-LM é€šè¿‡é‡æ–°å®šä¹‰ CPU-GPU è§’è‰²ï¼Œå¹¶å€ŸåŠ©æ˜¾å¼é‡è®¡ç®—å’Œæµæ°´çº¿æ‰§è¡Œæ¶ˆé™¤æŒä¹…æ€§ GPU å†…å­˜å ç”¨ï¼Œä»è€Œåœ¨å• GPU ä¸Šå®ç°å¤§æ¨¡å‹è®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04816",
      "arxiv_url": "https://arxiv.org/abs/2602.04816",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04816",
      "github_url": "https://github.com/DLYuanGod/Horizon-LM",
      "upvotes": 17,
      "fetched_at": "2026-02-19T05:42:15.312300+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04575",
      "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
      "authors": [
        "Jiaheng Liu",
        "Yuanxing Zhang",
        "Shihao Li",
        "Xinping Lei"
      ],
      "abstract": "Vibe AIGC introduces a new generative AI paradigm where users provide high-level aesthetic and functional preferences, which are then orchestrated through multi-agent workflows to bridge the gap between human intent and machine execution. For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding , we introduce the Vibe AIGC , a new paradigm for content generation via agentic orchestration , which represents the autonomous synthesis of hierarchical multi-agent workflows . Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration , Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.",
      "summary_en": "Vibe AIGC introduces a new generative AI paradigm where users provide high-level aesthetic and functional preferences, which are then orchestrated through multi-agent workflows to bridge the gap between human intent and machine execution.",
      "summary_zh": "Vibe AIGCæå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¼AIèŒƒå¼ï¼Œç”¨æˆ·æä¾›é«˜å±‚æ¬¡çš„å®¡ç¾ä¸åŠŸèƒ½åå¥½ï¼Œè¿™äº›åå¥½é€šè¿‡å¤šæ™ºèƒ½ä½“å·¥ä½œæµè¿›è¡Œç¼–æ’ï¼Œä»è€Œå¼¥åˆäººç±»æ„å›¾ä¸æœºå™¨æ‰§è¡Œä¹‹é—´çš„å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04575",
      "arxiv_url": "https://arxiv.org/abs/2602.04575",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04575",
      "github_url": "",
      "upvotes": 17,
      "fetched_at": "2026-02-19T05:27:44.326326+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2601.22859",
      "title": "MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering",
      "authors": [
        "Chuanzhe Guo",
        "Jingjing Wu",
        "Sijun He",
        "Yang Chen",
        "Zhaoqi Kuang",
        "Shilong Fan",
        "Bingjin Chen",
        "Siqi Bao",
        "Jing Liu",
        "Hua Wu",
        "Qingfu Zhu",
        "Wanxiang Che",
        "Haifeng Wang"
      ],
      "abstract": "MEnvAgent is a multi-language framework that automates environment construction for software engineering tasks using a planning-execution-verification architecture and environment reuse mechanism, achieving improved performance on a new benchmark and creating the largest open-source polyglot dataset of verifiable Docker environments. The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets , a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench , a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE , the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.",
      "summary_en": "MEnvAgent is a multi-language framework that automates environment construction for software engineering tasks using a planning-execution-verification architecture and environment reuse mechanism, achieving improved performance on a new benchmark and creating the largest open-source polyglot dataset of verifiable Docker environments.",
      "summary_zh": "MEnvAgent æ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ¡†æ¶ï¼Œé‡‡ç”¨è§„åˆ’-æ‰§è¡Œ-éªŒè¯æ¶æ„å’Œç¯å¢ƒå¤ç”¨æœºåˆ¶ï¼Œä¸ºè½¯ä»¶å·¥ç¨‹ä»»åŠ¡è‡ªåŠ¨åŒ–æ„å»ºç¯å¢ƒï¼Œåœ¨æ–°åŸºå‡†ä¸Šå–å¾—æ€§èƒ½æå‡ï¼Œå¹¶åˆ›å»ºäº†æœ€å¤§è§„æ¨¡çš„å¼€æºå¤šè¯­è¨€å¯éªŒè¯ Docker ç¯å¢ƒæ•°æ®é›†ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22859",
      "arxiv_url": "https://arxiv.org/abs/2601.22859",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22859",
      "github_url": "https://github.com/ernie-research/MEnvAgent",
      "upvotes": 17,
      "fetched_at": "2026-02-19T05:27:15.874235+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04735",
      "title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
      "authors": [
        "Mengru Wang",
        "Zhenqian Xu",
        "Junfeng Fang",
        "Yunzhi Yao",
        "Shumin Deng",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Data2Behavior predicts unintended model behaviors before training using MDF, a lightweight method that analyzes data features to reveal potential biases without parameter updates. Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior , a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities .",
      "summary_en": "Data2Behavior predicts unintended model behaviors before training using MDF, a lightweight method that analyzes data features to reveal potential biases without parameter updates.",
      "summary_zh": "Data2Behavioråˆ©ç”¨MDFè¿™ä¸€è½»é‡çº§æ–¹æ³•ï¼Œåœ¨è®­ç»ƒå‰åˆ†ææ•°æ®ç‰¹å¾ä»¥é¢„æµ‹éé¢„æœŸçš„æ¨¡å‹è¡Œä¸ºï¼Œæ— éœ€å‚æ•°æ›´æ–°å³å¯æ­ç¤ºæ½œåœ¨åå·®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04735",
      "arxiv_url": "https://arxiv.org/abs/2602.04735",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04735",
      "github_url": "https://github.com/zjunlp/Data2Behavior",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:42:11.168927+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02160",
      "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use",
      "authors": [
        "Bowen Xu",
        "Shaoyu Wu",
        "Hao Jiang",
        "Kai Liu",
        "Xin Chen",
        "Lulu Hu",
        "Bin Yang"
      ],
      "abstract": "A two-stage training framework called D-CORE is proposed to improve large reasoning models' ability to decompose complex tasks and compose reasoning processes, achieving superior performance in tool-use benchmarks. Effective tool use and reasoning are essential capabilities for large reasoning models ~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning . To address this, we propose a two-stage training framework D-CORE~(\\textbf{D}ecomposing tasks and \\textbf{Co}mposing \\textbf{Re}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation , followed by diversity-aware reinforcement learning ~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5times smaller. The source code is available at https://github.com/alibaba/EfficientAI.",
      "summary_en": "A two-stage training framework called D-CORE is proposed to improve large reasoning models' ability to decompose complex tasks and compose reasoning processes, achieving superior performance in tool-use benchmarks.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åä¸ºD-COREçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œä»¥æå‡å¤§å‹æ¨ç†æ¨¡å‹åˆ†è§£å¤æ‚ä»»åŠ¡å¹¶ç»„åˆæ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œåœ¨å·¥å…·ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02160",
      "arxiv_url": "https://arxiv.org/abs/2602.02160",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02160",
      "github_url": "https://github.com/alibaba/EfficientAI",
      "upvotes": 14,
      "fetched_at": "2026-02-19T05:27:20.127432+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04284",
      "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning",
      "authors": [
        "Yansong Ning",
        "Jun Fang",
        "Naiqiang Tan",
        "Hao Liu"
      ],
      "abstract": "Agent-Omit is a training framework that enables LLM agents to adaptively omit redundant thoughts and observations during multi-turn interactions, achieving superior effectiveness-efficiency trade-offs compared to existing methods. Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data , including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence . Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.",
      "summary_en": "Agent-Omit is a training framework that enables LLM agents to adaptively omit redundant thoughts and observations during multi-turn interactions, achieving superior effectiveness-efficiency trade-offs compared to existing methods.",
      "summary_zh": "Agent-Omitæ˜¯ä¸€ç§è®­ç»ƒæ¡†æ¶ï¼Œä½¿LLMæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¤šè½®äº¤äº’ä¸­è‡ªé€‚åº”åœ°çœç•¥å†—ä½™çš„æ€è€ƒå’Œè§‚å¯Ÿï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å®ç°äº†æ›´ä¼˜çš„æ•ˆæœ-æ•ˆç‡æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04284",
      "arxiv_url": "https://arxiv.org/abs/2602.04284",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04284",
      "github_url": "https://github.com/usail-hkust/Agent-Omit",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:27:39.936607+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02140",
      "title": "Quantifying the Gap between Understanding and Generation within Unified Multimodal Models",
      "authors": [
        "Chenlong Wang",
        "Yuhang Chen",
        "Zhihan Hu",
        "Dongping Chen",
        "Wenhu Chen",
        "Sarah Wiegreffe",
        "Tianyi Zhou"
      ],
      "abstract": "Unified multimodal models exhibit a persistent gap between understanding and generation capabilities, indicating only surface-level integration rather than deep cognitive convergence. Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency . Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.",
      "summary_en": "Unified multimodal models exhibit a persistent gap between understanding and generation capabilities, indicating only surface-level integration rather than deep cognitive convergence.",
      "summary_zh": "ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ä¹‹é—´å­˜åœ¨æŒç»­å·®è·ï¼Œè¡¨æ˜å…¶ä»…ä¸ºè¡¨å±‚æ•´åˆè€Œéæ·±åº¦è®¤çŸ¥èåˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02140",
      "arxiv_url": "https://arxiv.org/abs/2602.02140",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02140",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:27:19.394008+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03916",
      "title": "SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?",
      "authors": [
        "Azmine Toushik Wasi",
        "Wahid Faisal",
        "Abdur Rahman",
        "Mahfuz Ahmed Anik",
        "Munem Shahriar",
        "Mohsin Mahmud Topu",
        "Sadia Tasnim Meem",
        "Rahatun Nesa Priti",
        "Sabrina Afroz Mitu",
        "Md. Iqramul Hoque",
        "Shahriyar Zaman Ridoy",
        "Mohammed Eunus Ali",
        "Majd Hawasly",
        "Mohammad Raza",
        "Md Rizwan Parvez"
      ],
      "abstract": "SpatiaLab presents a comprehensive benchmark for evaluating vision-language models' spatial reasoning capabilities across realistic, diverse scenarios, revealing significant gaps compared to human performance.",
      "summary_en": "SpatiaLab presents a comprehensive benchmark for evaluating vision-language models' spatial reasoning capabilities across realistic, diverse scenarios, revealing significant gaps compared to human performance.",
      "summary_zh": "SpatiaLabæ„å»ºäº†ä¸€ä¸ªç»¼åˆæ€§åŸºå‡†ï¼Œè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨çœŸå®å¤šæ ·åœºæ™¯ä¸­çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œæ­ç¤ºäº†å…¶ç›¸è¾ƒäºäººç±»è¡¨ç°çš„æ˜¾è‘—å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03916",
      "arxiv_url": "https://arxiv.org/abs/2602.03916",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03916",
      "github_url": "https://github.com/SpatiaLab-Reasoning/SpatiaLab",
      "upvotes": 11,
      "fetched_at": "2026-02-19T05:27:35.071403+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03359",
      "title": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
      "authors": [
        "Ning Ding",
        "Fangcheng Liu",
        "Kyungrae Kim",
        "Linji Hao",
        "Kyeng-Hun Lee",
        "Hyeonmok Ko",
        "Yehui Tang"
      ],
      "abstract": "MeKi enables efficient large language model deployment on edge devices by injecting pre-stored semantic knowledge through token-level memory experts and re-parameterization techniques. Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table . By offloading the knowledge to ROM , MeKi decouples model capacity f rom computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.",
      "summary_en": "MeKi enables efficient large language model deployment on edge devices by injecting pre-stored semantic knowledge through token-level memory experts and re-parameterization techniques.",
      "summary_zh": "MeKié€šè¿‡tokençº§è®°å¿†ä¸“å®¶å’Œé‡å‚æ•°åŒ–æŠ€æœ¯æ³¨å…¥é¢„å­˜å‚¨çš„è¯­ä¹‰çŸ¥è¯†ï¼Œå®ç°äº†å¤§è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03359",
      "arxiv_url": "https://arxiv.org/abs/2602.03359",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03359",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:27:29.680824+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03979",
      "title": "Likelihood-Based Reward Designs for General LLM Reasoning",
      "authors": [
        "Ariel Kwiatkowski",
        "Natasha Butt",
        "Ismail Labiad",
        "Julia Kempe",
        "Yann Ollivier"
      ],
      "abstract": "Log-probability rewards derived from the reference answer's likelihood outperform binary rewards in chain-of-thought fine-tuning across both verifiable and non-verifiable reasoning benchmarks. Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function , often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards . Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks , and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining . In verifiable settings , log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards , and yield much better perplexity. In non-verifiable settings , they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning , bridging the short, verifiable and long, non-verifiable answer settings.",
      "summary_en": "Log-probability rewards derived from the reference answer's likelihood outperform binary rewards in chain-of-thought fine-tuning across both verifiable and non-verifiable reasoning benchmarks.",
      "summary_zh": "åœ¨æ€ç»´é“¾å¾®è°ƒä¸­ï¼ŒåŸºäºå‚è€ƒç­”æ¡ˆä¼¼ç„¶çš„å¯¹æ•°æ¦‚ç‡å¥–åŠ±åœ¨å¯éªŒè¯å’Œä¸å¯éªŒè¯æ¨ç†åŸºå‡†ä¸Šå‡ä¼˜äºäºŒå…ƒå¥–åŠ±ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03979",
      "arxiv_url": "https://arxiv.org/abs/2602.03979",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03979",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:27:37.589545+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.03955",
      "title": "AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent",
      "authors": [
        "Yinyi Luo",
        "Yiqiao Jin",
        "Weichen Yu",
        "Mengqi Zhang",
        "Srijan Kumar",
        "Xiaoxiao Li",
        "Weijie Xu",
        "Xin Chen",
        "Jindong Wang"
      ],
      "abstract": "AgentArk distills multi-agent reasoning dynamics into a single model through hierarchical distillation strategies, enabling efficient yet powerful reasoning capabilities. While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning ; trajectory-based augmentation ; and process-aware distillation . By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.",
      "summary_en": "AgentArk distills multi-agent reasoning dynamics into a single model through hierarchical distillation strategies, enabling efficient yet powerful reasoning capabilities.",
      "summary_zh": "AgentArk é€šè¿‡åˆ†å±‚è’¸é¦ç­–ç•¥å°†å¤šæ™ºèƒ½ä½“æ¨ç†åŠ¨æ€è’¸é¦è‡³å•ä¸€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆä¸”å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03955",
      "arxiv_url": "https://arxiv.org/abs/2602.03955",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03955",
      "github_url": "https://github.com/AIFrontierLab/AgentArk",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:27:35.932498+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02554",
      "title": "BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation",
      "authors": [
        "Jingwen Xu",
        "Yiyang Lu",
        "Zisu Huang",
        "Changze Lv",
        "Xiaohua Wang",
        "Shizheng Li",
        "Zhibo Xu",
        "Zhengkang Guo",
        "Zhengyuan Wang",
        "Muzhao Tian",
        "Xuanjing Huang",
        "Xiaoqing Zheng"
      ],
      "abstract": "BatCoder is a self-supervised reinforcement learning framework that jointly optimizes code and documentation generation through back-translation, achieving superior performance on code-related benchmarks. Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production . BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward , enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1 , outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity .",
      "summary_en": "BatCoder is a self-supervised reinforcement learning framework that jointly optimizes code and documentation generation through back-translation, achieving superior performance on code-related benchmarks.",
      "summary_zh": "BatCoderæ˜¯ä¸€ç§è‡ªç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å›è¯‘è”åˆä¼˜åŒ–ä»£ç å’Œæ–‡æ¡£ç”Ÿæˆï¼Œåœ¨ä»£ç ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­å–å¾—æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02554",
      "arxiv_url": "https://arxiv.org/abs/2602.02554",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02554",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:27:24.826432+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.01640",
      "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain",
      "authors": [
        "Shuai Zhang",
        "Jiayu Hu",
        "Zijie Chen",
        "Zeyuan Ding",
        "Yi Zhang",
        "Yingji Zhang",
        "Ziyi Zhou",
        "Junwei Liao",
        "Shengjie Zhou",
        "Yong Dai",
        "Zhenzhong Lan",
        "Xiaozhu Ju"
      ],
      "abstract": "Agentic automatic evaluation framework automates embodied vision-language model assessment through collaborative agents that reduce evaluation costs and improve ranking accuracy. Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite , while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suite s by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking bias es, improves human alignment to Spearman's rho =0.85, and maintains high ranking fidelity ( Kendall's tau =0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.",
      "summary_en": "Agentic automatic evaluation framework automates embodied vision-language model assessment through collaborative agents that reduce evaluation costs and improve ranking accuracy.",
      "summary_zh": "æ™ºèƒ½ä½“è‡ªåŠ¨è¯„ä¼°æ¡†æ¶é€šè¿‡åä½œæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–å…·èº«è§†è§‰-è¯­è¨€æ¨¡å‹è¯„ä¼°ï¼Œé™ä½è¯„ä¼°æˆæœ¬å¹¶æé«˜æ’åºå‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01640",
      "arxiv_url": "https://arxiv.org/abs/2602.01640",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01640",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:27:17.901089+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2601.20499",
      "title": "Efficient Autoregressive Video Diffusion with Dummy Head",
      "authors": [
        "Hang Guo",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Yuanhao Cai",
        "Jiangshan Wang",
        "Yawei Li",
        "Yan Lu"
      ],
      "abstract": "Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality. The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising . In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression . Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.",
      "summary_en": "Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.",
      "summary_zh": "è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶æ•ˆç‡ä½ä¸‹ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å†å²å¸§ï¼Œè€ŒDummy Forcingè¿™ä¸€æ–°æ–¹æ³•é€šè¿‡å¼‚æ„å†…å­˜åˆ†é…ä¸åŠ¨æ€å¤´ç¼–ç¨‹ï¼Œåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶æå‡äº†æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.20499",
      "arxiv_url": "https://arxiv.org/abs/2601.20499",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.20499",
      "github_url": "https://github.com/csguoh/DummyForcing",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:27:14.384525+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04805",
      "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
      "authors": [
        "Jia-peng Zhang",
        "Cheng-Feng Pu",
        "Meng-Hao Guo",
        "Yan-Pei Cao",
        "Shi-Min Hu"
      ],
      "abstract": "Generative 3D models face challenges in animation rigging, which this work addresses by introducing SkinTokensâ€”a learned discrete representation for skinning weightsâ€”and TokenRig, a unified autoregressive framework that models skeletons and skin deformations together, improving rigging accuracy through reinforcement learning. The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights . By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.",
      "summary_en": "Generative 3D models face challenges in animation rigging, which this work addresses by introducing SkinTokensâ€”a learned discrete representation for skinning weightsâ€”and TokenRig, a unified autoregressive framework that models skeletons and skin deformations together, improving rigging accuracy through reinforcement learning.",
      "summary_zh": "ç”Ÿæˆå¼3Dæ¨¡å‹åœ¨åŠ¨ç”»ç»‘å®šä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œæœ¬å·¥ä½œé€šè¿‡å¼•å…¥SkinTokensï¼ˆä¸€ç§é’ˆå¯¹è’™çš®æƒé‡çš„ä¹ å¾—ç¦»æ•£è¡¨ç¤ºï¼‰å’ŒTokenRigï¼ˆä¸€ç§è”åˆå»ºæ¨¡éª¨éª¼ä¸çš®è‚¤å½¢å˜çš„ç»Ÿä¸€è‡ªå›å½’æ¡†æ¶ï¼‰æ¥è§£å†³è¯¥é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡ç»‘å®šç²¾åº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04805",
      "arxiv_url": "https://arxiv.org/abs/2602.04805",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04805",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:42:13.869434+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04486",
      "title": "Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition",
      "authors": [
        "Jinlong Ma",
        "Yu Zhang",
        "Xuefeng Bai",
        "Kehai Chen",
        "Yuwei Wang",
        "Zeming Liu",
        "Jun Yu",
        "Min Zhang"
      ],
      "abstract": "MLLMs suffer from modality bias in GMNER tasks, which is addressed through a proposed method that enforces cross-modal reasoning via multi-style reasoning schema injection and constraint-guided verifiable optimization. Grounded Multimodal Named Entity Recognition ( GMNER ) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit modality bias , including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.",
      "summary_en": "MLLMs suffer from modality bias in GMNER tasks, which is addressed through a proposed method that enforces cross-modal reasoning via multi-style reasoning schema injection and constraint-guided verifiable optimization.",
      "summary_zh": "MLLMsåœ¨GMNERä»»åŠ¡ä¸­å­˜åœ¨æ¨¡æ€åè§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¤šé£æ ¼æ¨ç†æ¨¡å¼æ³¨å…¥ä¸çº¦æŸå¼•å¯¼çš„å¯éªŒè¯ä¼˜åŒ–æ¥å¼ºåˆ¶è·¨æ¨¡æ€æ¨ç†çš„æ–¹æ³•æ¥è§£å†³è¯¥é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04486",
      "arxiv_url": "https://arxiv.org/abs/2602.04486",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04486",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:27:42.029664+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.01849",
      "title": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models",
      "authors": [
        "Ziwei Luo",
        "Ziqi Jin",
        "Lei Wang",
        "Lidong Bing",
        "Thomas B. SchÃ¶n"
      ],
      "abstract": "Self-rewarding sequential Monte Carlo enables effective sampling of masked diffusion language models by using parallel diffusion processes and trajectory-level confidence signals to improve generation quality. This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.",
      "summary_en": "Self-rewarding sequential Monte Carlo enables effective sampling of masked diffusion language models by using parallel diffusion processes and trajectory-level confidence signals to improve generation quality.",
      "summary_zh": "è‡ªå¥–åŠ±åºè´¯è’™ç‰¹å¡æ´›åˆ©ç”¨å¹¶è¡Œæ‰©æ•£è¿‡ç¨‹å’Œè½¨è¿¹çº§ç½®ä¿¡ä¿¡å·ï¼Œå®ç°å¯¹æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆé‡‡æ ·ï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01849",
      "arxiv_url": "https://arxiv.org/abs/2602.01849",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01849",
      "github_url": "https://github.com/Algolzw/self-rewarding-smc",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:27:18.680734+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02350",
      "title": "Context Learning for Multi-Agent Discussion",
      "authors": [
        "Xingyuan Hua",
        "Sheng Yue",
        "Xinyi Li",
        "Yizhe Zhao",
        "Jinrui Zhang",
        "Ju Ren"
      ],
      "abstract": "",
      "summary_en": "This is an automated message from the Librarian Bot . I found the following papers similar to this paper.",
      "summary_zh": "è¿™æ˜¯æ¥è‡ª Librarian Bot çš„è‡ªåŠ¨æ¶ˆæ¯ã€‚æˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¸æœ¬æ–‡ç›¸ä¼¼çš„è®ºæ–‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02350",
      "arxiv_url": "https://arxiv.org/abs/2602.02350",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02350",
      "github_url": "https://github.com/HansenHua/M2CL-ICLR26",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:27:22.396051+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04883",
      "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
      "authors": [
        "Yanru Qu",
        "Cheng-Yen Hsieh",
        "Zaixiang Zheng",
        "Ge Liu",
        "Quanquan Gu"
      ],
      "abstract": "PAR is a multi-scale autoregressive framework for protein backbone generation that uses hierarchical structure modeling, autoregressive transformers, and flow-based decoding to produce high-quality protein structures with improved generalization and reduced exposure bias. We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias , caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling , enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization , supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark , PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.",
      "summary_en": "PAR is a multi-scale autoregressive framework for protein backbone generation that uses hierarchical structure modeling, autoregressive transformers, and flow-based decoding to produce high-quality protein structures with improved generalization and reduced exposure bias.",
      "summary_zh": "PARæ˜¯ä¸€ç§ç”¨äºè›‹ç™½è´¨éª¨æ¶ç”Ÿæˆçš„å¤šå°ºåº¦è‡ªå›å½’æ¡†æ¶ï¼Œé€šè¿‡å±‚æ¬¡åŒ–ç»“æ„å»ºæ¨¡ã€è‡ªå›å½’Transformerå’ŒåŸºäºæµçš„è§£ç ç”Ÿæˆé«˜è´¨é‡è›‹ç™½è´¨ç»“æ„ï¼Œæå‡äº†æ³›åŒ–èƒ½åŠ›å¹¶é™ä½äº†æ›å…‰åå·®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04883",
      "arxiv_url": "https://arxiv.org/abs/2602.04883",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04883",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:42:18.141226+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04442",
      "title": "No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data",
      "authors": [
        "Dmitry Karpov"
      ],
      "abstract": "Machine translation experiments for Turkic languages using nllb-200, LoRA fine-tuning, and prompt-based approaches achieved varying chrF++ scores across language pairs. We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200 -distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.",
      "summary_en": "Machine translation experiments for Turkic languages using nllb-200, LoRA fine-tuning, and prompt-based approaches achieved varying chrF++ scores across language pairs.",
      "summary_zh": "ä½¿ç”¨ nllb-200ã€LoRA å¾®è°ƒå’ŒåŸºäºæç¤ºçš„æ–¹æ³•å¼€å±•çš„çªå¥è¯­æ—è¯­è¨€æœºå™¨ç¿»è¯‘å®éªŒåœ¨ä¸åŒè¯­è¨€å¯¹ä¸Šå–å¾—äº†ä¸åŒçš„ chrF++ åˆ†æ•°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04442",
      "arxiv_url": "https://arxiv.org/abs/2602.04442",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04442",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:27:41.313551+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04289",
      "title": "Proxy Compression for Language Modeling",
      "authors": [
        "Lin Zheng",
        "Xinyu Li",
        "Qian Liu",
        "Xiachong Feng",
        "Lingpeng Kong"
      ],
      "abstract": "Proxy compression trains language models on both raw byte sequences and compressed views, enabling efficient training with end-to-end raw-byte inference while maintaining model robustness. Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer , an external lossless compressor often over UTF-8 byte sequences , thereby coupling the model to that compressor. This work introduces proxy compression , an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time . During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors ; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling .",
      "summary_en": "Proxy compression trains language models on both raw byte sequences and compressed views, enabling efficient training with end-to-end raw-byte inference while maintaining model robustness.",
      "summary_zh": "ä»£ç†å‹ç¼©åœ¨åŸå§‹å­—èŠ‚åºåˆ—å’Œå‹ç¼©è§†å›¾ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒä¸ç«¯åˆ°ç«¯åŸå§‹å­—èŠ‚æ¨ç†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹é²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04289",
      "arxiv_url": "https://arxiv.org/abs/2602.04289",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04289",
      "github_url": "https://github.com/LZhengisme/proxy-compression",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:27:40.581880+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.01031",
      "title": "HalluHard: A Hard Multi-Turn Hallucination Benchmark",
      "authors": [
        "Dongyang Fan",
        "Sebastien Delsad",
        "Nicolas Flammarion",
        "Maksym Andriushchenko"
      ],
      "abstract": "Large language models continue to generate plausible but ungrounded factual claims in multi-turn dialogue, with hallucinations remaining significant even when utilizing web search for verification across high-stakes domains. Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce HalluHard, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions . To support reliable evaluation in open-ended settings , we propose a judging pipeline that iteratively retrieves evidence via web search . It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucination s remain substantial even with web search (approx 30% for the strongest configuration, Opus-4.5 with web search ), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity , turn position , effective reasoning , and the type of knowledge required.",
      "summary_en": "Large language models continue to generate plausible but ungrounded factual claims in multi-turn dialogue, with hallucinations remaining significant even when utilizing web search for verification across high-stakes domains.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­æŒç»­ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†ç¼ºä¹äº‹å®ä¾æ®çš„é™ˆè¿°ï¼Œå³ä½¿åœ¨é«˜é£é™©é¢†åŸŸåˆ©ç”¨ç½‘ç»œæœç´¢è¿›è¡ŒéªŒè¯ï¼Œå¹»è§‰ç°è±¡ä¾ç„¶æ˜¾è‘—ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01031",
      "arxiv_url": "https://arxiv.org/abs/2602.01031",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01031",
      "github_url": "https://github.com/epfml/halluhard",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:27:17.223875+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02495",
      "title": "Reward-free Alignment for Conflicting Objectives",
      "authors": [
        "Peter Chen",
        "Xiaopeng Li",
        "Xi Chen",
        "Tianyi Lin"
      ],
      "abstract": "A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures. Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent . We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
      "summary_en": "A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures.",
      "summary_zh": "ä¸€ç§æ— å¥–åŠ±å¯¹é½æ¡†æ¶é€šè¿‡å¸¦è£å‰ªçš„å†²çªè§„é¿æ¢¯åº¦ä¸‹é™è§£å†³è¯­è¨€æ¨¡å‹ä¸­çš„å¤šç›®æ ‡å†²çªï¼Œåœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸Šæ”¹è¿›äº†å¸•ç´¯æ‰˜æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02495",
      "arxiv_url": "https://arxiv.org/abs/2602.02495",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02495",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:27:24.006695+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.05000",
      "title": "EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models",
      "authors": [
        "Atula Tejaswi",
        "Litu Rout",
        "Constantine Caramanis",
        "Sanjay Shakkottai",
        "Sujay Sanghavi"
      ],
      "abstract": "Discrete diffusion language models use entropy-aware reward guidance to improve test-time adaptation by dynamically regulating gradient feedback from reward models.",
      "summary_en": "Discrete diffusion language models use entropy-aware reward guidance to improve test-time adaptation by dynamically regulating gradient feedback from reward models.",
      "summary_zh": "ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹åˆ©ç”¨ç†µæ„ŸçŸ¥å¥–åŠ±å¼•å¯¼ï¼Œé€šè¿‡åŠ¨æ€è°ƒèŠ‚æ¥è‡ªå¥–åŠ±æ¨¡å‹çš„æ¢¯åº¦åé¦ˆæ¥æ”¹è¿›æµ‹è¯•æ—¶è‡ªé€‚åº”ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.05000",
      "arxiv_url": "https://arxiv.org/abs/2602.05000",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05000",
      "github_url": "https://github.com/atutej/entrgi",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:42:19.395334+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04651",
      "title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF",
      "authors": [
        "Dipan Maity"
      ],
      "abstract": "A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management. Optimization ( PPO ) has been positioned by recent literature as the canonical method for the RL part of RLHF . PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM- RLHF in an ad-hoc manner and suffers form reward oscillations , entropy collapse , value function drift , and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM- RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation , and PID-controlled adaptive thresholds . Unlike standard PPO 's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE",
      "summary_en": "A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management.",
      "summary_zh": "ä¸€ç§ç”¨äºè¯­è¨€æ¨¡å‹å¯¹é½çš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å¢å¼ºçš„KLæ•£åº¦æ§åˆ¶å’Œè‡ªé€‚åº”å¥–åŠ±ç®¡ç†ï¼Œåœ¨ç¨³å®šæ€§å’Œæ€§èƒ½ä¸Šä¼˜äºPPOã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04651",
      "arxiv_url": "https://arxiv.org/abs/2602.04651",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04651",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:42:08.372643+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04605",
      "title": "RexBERT: Context Specialized Bidirectional Encoders for E-commerce",
      "authors": [
        "Rahul Bajaj",
        "Anuj Garg"
      ],
      "abstract": "RexBERT, a family of BERT-style encoders designed for e-commerce semantics, achieves superior performance on domain-specific tasks through specialized pretraining and high-quality in-domain data. Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics . We make three contributions. First, we release Ecom-niverse , a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT 's architectural advances. The recipe consists of three phases: general pre-training, context extension , and annealed domain specialization . Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification , semantic similarity , and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.",
      "summary_en": "RexBERT, a family of BERT-style encoders designed for e-commerce semantics, achieves superior performance on domain-specific tasks through specialized pretraining and high-quality in-domain data.",
      "summary_zh": "RexBERTæ˜¯ä¸€ç³»åˆ—ä¸“ä¸ºç”µå•†è¯­ä¹‰è®¾è®¡çš„BERTé£æ ¼ç¼–ç å™¨ï¼Œé€šè¿‡ä¸“é—¨é¢„è®­ç»ƒä¸é«˜è´¨é‡é¢†åŸŸæ•°æ®ï¼Œåœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04605",
      "arxiv_url": "https://arxiv.org/abs/2602.04605",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04605",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:45.769155+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04581",
      "title": "Trust The Typical",
      "authors": [
        "Debargha Ganguly",
        "Sreehari Sankar",
        "Biyao Zhang",
        "Vikash Singh",
        "Kanan Gupta",
        "Harshini Kavuru",
        "Alan Luo",
        "Weicong Chen",
        "Warren Morningstar",
        "Raghu Machiraju",
        "Vipin Chaudhary"
      ],
      "abstract": "A novel framework for LLM safety that treats safety as an out-of-distribution detection problem, achieving state-of-the-art performance without harmful example training through semantic space analysis and efficient GPU implementation. Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails . We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms , and over-refusal , reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM , enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.",
      "summary_en": "A novel framework for LLM safety that treats safety as an out-of-distribution detection problem, achieving state-of-the-art performance without harmful example training through semantic space analysis and efficient GPU implementation.",
      "summary_zh": "ä¸€ç§é’ˆå¯¹LLMå®‰å…¨çš„æ–°æ¡†æ¶ï¼Œå°†å®‰å…¨æ€§å»ºæ¨¡ä¸ºåˆ†å¸ƒå¤–æ£€æµ‹é—®é¢˜ï¼Œå€ŸåŠ©è¯­ä¹‰ç©ºé—´åˆ†æå’Œé«˜æ•ˆGPUå®ç°ï¼Œæ— éœ€æœ‰å®³æ ·æœ¬è®­ç»ƒå³å¯è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04581",
      "arxiv_url": "https://arxiv.org/abs/2602.04581",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04581",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:45.017131+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04547",
      "title": "OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis",
      "authors": [
        "Luca Zedda",
        "Andrea Loddo",
        "Cecilia Di Ruberto"
      ],
      "abstract": "OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability. Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability . We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualization s suggest improved feature clustering and modality-related separation.",
      "summary_en": "OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.",
      "summary_zh": "OmniRadæ˜¯ä¸€ç§åœ¨120ä¸‡å¼ åŒ»å­¦å½±åƒä¸Šé¢„è®­ç»ƒçš„è‡ªç›‘ç£æ”¾å°„å­¦åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡è¡¨å¾å¤ç”¨å’Œè·¨ä»»åŠ¡å¯è¿ç§»æ€§ï¼Œåœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04547",
      "arxiv_url": "https://arxiv.org/abs/2602.04547",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04547",
      "github_url": "https://github.com/unica-visual-intelligence-lab/OmniRad",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:43.461583+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.04271",
      "title": "SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization",
      "authors": [
        "Lifan Wu",
        "Ruijie Zhu",
        "Yubo Ai",
        "Tianzhu Zhang"
      ],
      "abstract": "SkeletonGaussian enables editable 4D generation by decomposing motion into rigid skeleton-driven and non-rigid fine-grained components using hexplane-based refinement. 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input . Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning , followed by a hexplane-based refinement for non-rigid deformations , enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing , establishing a new paradigm for editable 4D generation . Project page: https://wusar.github.io/projects/skeletongaussian/",
      "summary_en": "SkeletonGaussian enables editable 4D generation by decomposing motion into rigid skeleton-driven and non-rigid fine-grained components using hexplane-based refinement.",
      "summary_zh": "SkeletonGaussianåˆ©ç”¨åŸºäºå…­å¹³é¢çš„ç»†åŒ–å°†è¿åŠ¨åˆ†è§£ä¸ºåˆšæ€§éª¨éª¼é©±åŠ¨å’Œéåˆšæ€§ç»†ç²’åº¦ç»„ä»¶ï¼Œå®ç°äº†å¯ç¼–è¾‘çš„4Dç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04271",
      "arxiv_url": "https://arxiv.org/abs/2602.04271",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04271",
      "github_url": "https://github.com/wusar/SkeletonGaussian",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:39.125137+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02863",
      "title": "\"I May Not Have Articulated Myself Clearly\": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time",
      "authors": [
        "Jinkun Chen",
        "Fengxiang Cheng",
        "Sijia Han",
        "Vlado Keselj"
      ],
      "abstract": "Analysis of reasoning failures in large language models reveals that instability signals derived from token log probabilities and entropy can predict incorrect answers and distinguish between corrective and destructive instability based on timing of distribution shifts. Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model \"loses the thread\" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs ( token log probabilities ), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty ( entropy ), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA , instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer ( corrective instability ), whereas late instability is more often followed by failure ( destructive instability ), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.",
      "summary_en": "Analysis of reasoning failures in large language models reveals that instability signals derived from token log probabilities and entropy can predict incorrect answers and distinguish between corrective and destructive instability based on timing of distribution shifts.",
      "summary_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¤±è´¥çš„åˆ†æè¡¨æ˜ï¼Œç”± token å¯¹æ•°æ¦‚ç‡å’Œç†µæ¨å¯¼å‡ºçš„ä¸ç¨³å®šä¿¡å·å¯é¢„æµ‹é”™è¯¯ç­”æ¡ˆï¼Œå¹¶èƒ½æ ¹æ®åˆ†å¸ƒåç§»çš„æ—¶æœºåŒºåˆ†ä¿®æ­£æ€§ä¸ç¨³å®šä¸ç ´åæ€§ä¸ç¨³å®šã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02863",
      "arxiv_url": "https://arxiv.org/abs/2602.02863",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02863",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:25.585262+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2602.02341",
      "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
      "authors": [
        "Zhenpeng Huang",
        "Jiaqi Li",
        "Zihan Jia",
        "Xinhao Li",
        "Desen Meng",
        "Lingxue Song",
        "Xi Chen",
        "Liang Li",
        "Limin Wang"
      ],
      "abstract": "LongVPO is a two-stage Direct Preference Optimization framework that enables short-context vision-language models to understand ultra-long videos through synthetic preference triples and recursive captioning, achieving state-of-the-art performance with minimal human annotation. We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata , then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.",
      "summary_en": "LongVPO is a two-stage Direct Preference Optimization framework that enables short-context vision-language models to understand ultra-long videos through synthetic preference triples and recursive captioning, achieving state-of-the-art performance with minimal human annotation.",
      "summary_zh": "LongVPOæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µç›´æ¥åå¥½ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡åˆæˆåå¥½ä¸‰å…ƒç»„ä¸é€’å½’å­—å¹•ç”Ÿæˆï¼Œä½¿çŸ­ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç†è§£è¶…é•¿è§†é¢‘ï¼Œåœ¨æå°‘äººå·¥æ ‡æ³¨ä¸‹è¾¾åˆ°å½“å‰æœ€ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02341",
      "arxiv_url": "https://arxiv.org/abs/2602.02341",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02341",
      "github_url": "https://github.com/MCG-NJU/LongVPO",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:21.519265+00:00"
    },
    {
      "date": "2026-02-05",
      "paper_id": "2601.22596",
      "title": "FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data",
      "authors": [
        "Abdelrrahman Moubane"
      ],
      "abstract": "A large-scale building change detection dataset named FOTBCD is introduced, covering 28 French departments with high-resolution imagery and comprehensive annotations for both binary and instance-level change detection tasks. We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift , with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection .",
      "summary_en": "A large-scale building change detection dataset named FOTBCD is introduced, covering 28 French departments with high-resolution imagery and comprehensive annotations for both binary and instance-level change detection tasks.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºFOTBCDçš„å¤§è§„æ¨¡å»ºç­‘ç‰©å˜åŒ–æ£€æµ‹æ•°æ®é›†ï¼Œæ¶µç›–28ä¸ªæ³•å›½çœä»½çš„é«˜åˆ†è¾¨ç‡å½±åƒï¼Œå¹¶ä¸ºäºŒå€¼ä¸å®ä¾‹çº§å˜åŒ–æ£€æµ‹ä»»åŠ¡æä¾›äº†å…¨é¢çš„æ ‡æ³¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22596",
      "arxiv_url": "https://arxiv.org/abs/2601.22596",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22596",
      "github_url": "https://github.com/abdelpy/FOTBCD-datasets",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:27:15.161340+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01785",
      "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding",
      "authors": [
        "Yuling Shi",
        "Chaoxiang Xie",
        "Zhensu Sun",
        "Yeheng Chen",
        "Chenxu Zhang",
        "Longfei Yun",
        "Chengcheng Wan",
        "Hongyu Zhang",
        "David Lo",
        "Xiaodong Gu"
      ],
      "abstract": "Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.",
      "summary_en": "Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.",
      "summary_zh": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆç†è§£ä»¥å‹ç¼©å›¾åƒå½¢å¼è¡¨ç¤ºçš„æºä»£ç ï¼Œåœ¨å®ç°æ˜¾è‘— token ç¼©å‡çš„åŒæ—¶ï¼Œäºä»£ç ç†è§£ä»»åŠ¡ä¸Šä¿æŒæˆ–æå‡æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01785",
      "arxiv_url": "https://arxiv.org/abs/2602.01785",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01785",
      "github_url": "",
      "upvotes": 93,
      "fetched_at": "2026-02-19T05:38:57.629302+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03786",
      "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
      "authors": [
        "Jianhao Ruan",
        "Zhihao Xu",
        "Yiran Peng",
        "Fashen Ren",
        "Zhaoyang Yu",
        "Xinbing Liang",
        "Jinyu Xiang",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo",
        "Jiayi Zhang"
      ],
      "abstract": "AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management. Language agents have shown strong promise for task automation . Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving . However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation . Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient . Across three challenging benchmarks ( GAIA , SWE-Bench , Terminal-Bench ), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
      "summary_en": "AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.",
      "summary_zh": "AOrchestraæ˜¯ä¸€ä¸ªä¸æ¡†æ¶æ— å…³çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé‡‡ç”¨åŸºäºå…ƒç»„çš„æŠ½è±¡åŠ¨æ€åˆ›å»ºä¸“ç”¨ä»»åŠ¡æ‰§è¡Œå™¨ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“åˆ›å»ºå’Œèµ„æºç®¡ç†ï¼Œåœ¨å¤æ‚åŸºå‡†æµ‹è¯•ä¸Šå®ç°æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03786",
      "arxiv_url": "https://arxiv.org/abs/2602.03786",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03786",
      "github_url": "https://github.com/FoundationAgents/AOrchestra",
      "upvotes": 85,
      "fetched_at": "2026-02-19T05:40:49.023194+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02103",
      "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs",
      "authors": [
        "Liyan Xu",
        "Mo Yu",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition. This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning . To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens , applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics , and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/ tele-lens .",
      "summary_en": "Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.",
      "summary_zh": "ç ”ç©¶é€šè¿‡åä¸º Tele-Lens çš„æ¢æµ‹æ–¹æ³•æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåœ¨è§„åˆ’åŠ¨æ€ï¼Œæ­ç¤ºäº†æœ‰é™çš„å…¨å±€è§„åˆ’ç°è±¡ï¼Œå¹¶å®ç°äº†æ”¹è¿›çš„ä¸ç¡®å®šæ€§ä¼°è®¡å’Œ CoT ç»•è¿‡è¯†åˆ«ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02103",
      "arxiv_url": "https://arxiv.org/abs/2602.02103",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02103",
      "github_url": "https://github.com/lxucs/tele-lens",
      "upvotes": 70,
      "fetched_at": "2026-02-19T05:38:59.372171+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02660",
      "title": "MARS: Modular Agent with Reflective Search for Automated AI Research",
      "authors": [
        "Jiefeng Chen",
        "Bhavana Dalvi Mishra",
        "Jaehyun Nam",
        "Rui Meng",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "abstract": "MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research. Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search ( MCTS ) to explicitly balance performance with execution expense; (2) Modular Construction , employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory , which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer , demonstrating that the agent effectively generalizes insights across search paths.",
      "summary_en": "MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.",
      "summary_zh": "MARSæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„äººå·¥æ™ºèƒ½ç ”ç©¶è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œé‡‡ç”¨é¢„ç®—æ„ŸçŸ¥è§„åˆ’ã€æ¨¡å—åŒ–æ„å»ºå’Œåæ€æ€§è®°å¿†ï¼Œåœ¨è‡ªä¸»æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02660",
      "arxiv_url": "https://arxiv.org/abs/2602.02660",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02660",
      "github_url": "https://github.com/jfc43/MARS",
      "upvotes": 63,
      "fetched_at": "2026-02-19T05:39:19.105884+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03796",
      "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation",
      "authors": [
        "Zhixue Fang",
        "Xu He",
        "Songlin Tang",
        "Haoxian Zhang",
        "Qingfeng Li",
        "Xiaoqiang Liu",
        "Pengfei Wan",
        "Kun Gai"
      ],
      "abstract": "3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors. Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL ) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generator s. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens , injected semantically via cross-attention . To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control , significantly surpassing existing methods in both motion fidelity and visual quality .",
      "summary_en": "3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.",
      "summary_zh": "3DiMoé€šè¿‡è”åˆè®­ç»ƒè¿åŠ¨ç¼–ç å™¨ä¸é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆå™¨ï¼Œå°†é©±åŠ¨å¸§è’¸é¦ä¸ºä¸ç”Ÿæˆå™¨ç©ºé—´å…ˆéªŒå¯¹é½çš„ç´§å‡‘è¿åŠ¨ä»¤ç‰Œï¼Œå®ç°äº†è§†é¢‘ç”Ÿæˆä¸­è§†è§’æ— å…³çš„äººä½“è¿åŠ¨æ§åˆ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03796",
      "arxiv_url": "https://arxiv.org/abs/2602.03796",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03796",
      "github_url": "",
      "upvotes": 57,
      "fetched_at": "2026-02-19T05:27:07.523133+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02619",
      "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently",
      "authors": [
        "Mohan Jiang",
        "Dayuan Fu",
        "Junhao Shi",
        "Ji Zeng",
        "Weiye Si",
        "Keyu Li",
        "Xuefeng Li",
        "Yang Xiao",
        "Wenjie Li",
        "Dequan Wang",
        "Pengfei Liu"
      ],
      "abstract": "Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix histories. While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics --existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories . Building on this, we propose daVinci-Agency , which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits , (2) long-term consistency enforcement through unified functional objectives , and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency 's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon . Beyond benchmark performance, our analysis confirms...",
      "summary_en": "Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix histories.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹å› ç¼ºä¹çœŸå®é•¿ä¾èµ–è®­ç»ƒæ•°æ®è€Œåœ¨é•¿ç¨‹æ™ºèƒ½ä½“å·¥ä½œæµä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶é€šè¿‡åˆ©ç”¨ Pull Request åºåˆ—è¿›è¡Œç»“æ„åŒ–ç›‘ç£æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå…·ä½“æ–¹æ³•åŒ…æ‹¬æ¸è¿›å¼åˆ†è§£ã€ä¸€è‡´æ€§çº¦æŸä»¥åŠåŸºäºé”™è¯¯ä¿®å¤å†å²çš„ä¼˜åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02619",
      "arxiv_url": "https://arxiv.org/abs/2602.02619",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02619",
      "github_url": "https://github.com/GAIR-NLP/daVinci-Agency",
      "upvotes": 50,
      "fetched_at": "2026-02-19T05:39:15.443899+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01630",
      "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
      "authors": [
        "Bohan Zeng",
        "Kaixin Zhu",
        "Daili Hua",
        "Bozhou Li",
        "Chengzhuo Tong",
        "Yuran Wang",
        "Xinyi Huang",
        "Yifan Dai",
        "Zixiang Zhang",
        "Yifan Yang",
        "Zhou Liu",
        "Hao Liang",
        "Xiaochen Ma",
        "Ruichuan An",
        "Tianyi Bai",
        "Hongcheng Gao",
        "Junbo Niu",
        "Yang Shi",
        "Xinlong Chen",
        "Yue Ding",
        "Minglei Shi",
        "Kai Zeng"
      ],
      "abstract": "Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation. World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction , 3D estimation , or symbol grounding , rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models . We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation . This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.",
      "summary_en": "Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.",
      "summary_zh": "å½“å‰ä¸–ç•Œæ¨¡å‹è™½åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå–å¾—è¿›å±•ï¼Œä½†ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ•´åˆäº¤äº’ã€æ„ŸçŸ¥ã€ç¬¦å·æ¨ç†ä¸ç©ºé—´è¡¨å¾çš„ç»¼åˆæ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01630",
      "arxiv_url": "https://arxiv.org/abs/2602.01630",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01630",
      "github_url": "https://github.com/OpenDCAI/DataFlow",
      "upvotes": 46,
      "fetched_at": "2026-02-19T05:38:54.196429+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03139",
      "title": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis",
      "authors": [
        "Tianhe Wu",
        "Ruibin Li",
        "Lei Zhang",
        "Kede Ma"
      ],
      "abstract": "A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead. Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse , as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction ) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.",
      "summary_en": "A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.",
      "summary_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º DP-DMD çš„æ–°å‹è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡åˆ†ç¦»è’¸é¦æ­¥éª¤çš„è§’è‰²ï¼Œä½¿ç”¨ v-prediction ä¿æŒå¤šæ ·æ€§ï¼Œå¹¶åˆ©ç”¨æ ‡å‡† DMD loss è¿›è¡Œè´¨é‡ä¼˜åŒ–ï¼Œåœ¨ä¸å¢åŠ é¢å¤–è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå®ç°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æ ·æœ¬å¤šæ ·æ€§ä¿æŒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03139",
      "arxiv_url": "https://arxiv.org/abs/2602.03139",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03139",
      "github_url": "https://github.com/Multimedia-Analytics-Laboratory/dpdmd",
      "upvotes": 41,
      "fetched_at": "2026-02-19T05:26:55.664893+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03419",
      "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
      "authors": [
        "Shuang Sun",
        "Huatong Song",
        "Lisheng Huang",
        "Jinhao Jiang",
        "Ran Le",
        "Zhihao Lv",
        "Zongchao Chen",
        "Yiwen Hu",
        "Wenyang Luo",
        "Wayne Xin Zhao",
        "Yang Song",
        "Hongteng Xu",
        "Tao Zhang",
        "Ji-Rong Wen"
      ],
      "abstract": "A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup. Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents . SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
      "summary_en": "A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.",
      "summary_zh": "ä¸€ç§æ— éœ€Dockerçš„æ¡†æ¶ï¼Œç”¨å­¦ä¹ å¾—åˆ°çš„æ›¿ä»£æ¨¡å‹å–ä»£ç‰©ç†æ‰§è¡Œç¯å¢ƒä»¥è®­ç»ƒè½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒä¸æµ‹è¯•æ—¶æ‰©å±•ï¼Œä¸”æ— éœ€æ˜‚è´µçš„å®¹å™¨é…ç½®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03419",
      "arxiv_url": "https://arxiv.org/abs/2602.03419",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03419",
      "github_url": "",
      "upvotes": 39,
      "fetched_at": "2026-02-19T05:27:00.940258+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03411",
      "title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training",
      "authors": [
        "Huatong Song",
        "Lisheng Huang",
        "Shuang Sun",
        "Jinhao Jiang",
        "Ran Le",
        "Daixuan Cheng",
        "Guoxin Chen",
        "Yiwen Hu",
        "Zongchao Chen",
        "Wayne Xin Zhao",
        "Yang Song",
        "Tao Zhang",
        "Ji-Rong Wen"
      ],
      "abstract": "SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.",
      "summary_en": "SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.",
      "summary_zh": "SWE-Masteræå‡ºäº†ä¸€ç§å¯å¤ç°çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ™ºèƒ½ä½“å¼€å‘å¤šé˜¶æ®µè¿›è¡Œç³»ç»ŸåŒ–ä¼˜åŒ–æ¥å¼€å‘è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“ï¼Œåœ¨è½¯ä»¶ä»»åŠ¡è§£å†³åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03411",
      "arxiv_url": "https://arxiv.org/abs/2602.03411",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03411",
      "github_url": "",
      "upvotes": 37,
      "fetched_at": "2026-02-19T05:27:00.189793+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03048",
      "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs",
      "authors": [
        "Zhiyuan Yao",
        "Yi-Kai Zhang",
        "Yuxin Chen",
        "Yueqing Sun",
        "Zishan Xu",
        "Yu Yang",
        "Tianhao Hu",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai"
      ],
      "abstract": "CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.",
      "summary_en": "CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.",
      "summary_zh": "CoBA-RL é€šè¿‡è¯„ä¼°æ ·æœ¬è®­ç»ƒä»·å€¼ï¼Œå¹¶å€ŸåŠ©é¢å‘èƒ½åŠ›çš„ä»·å€¼å‡½æ•°ä¸è´ªå¿ƒç­–ç•¥ä¼˜åŒ–èµ„æºåˆ†é…ï¼Œè‡ªé€‚åº”è°ƒæ•´ LLM è®­ç»ƒçš„ rollout é¢„ç®—åˆ†é…ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03048",
      "arxiv_url": "https://arxiv.org/abs/2602.03048",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03048",
      "github_url": "https://github.com/Within-yao/CoBA-RL",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:26:53.986537+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03845",
      "title": "Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing",
      "authors": [
        "Tong Zheng",
        "Chengsong Huang",
        "Runpeng Dai",
        "Yun He",
        "Rui Liu",
        "Xin Ni",
        "Huiwen Bao",
        "Kaishen Wang",
        "Hongtu Zhu",
        "Jiaxin Huang",
        "Furong Huang",
        "Heng Huang"
      ],
      "abstract": "Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy. Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking . Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling . Compared to standard majority voting , it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.",
      "summary_en": "Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.",
      "summary_zh": "Parallel-Probeæ˜¯ä¸€ç§å…è®­ç»ƒçš„æ§åˆ¶å™¨ï¼Œé€šè¿‡åŸºäºå…±è¯†çš„æ—©æœŸåœæ­¢å’ŒåŸºäºåå·®çš„åˆ†æ”¯å‰ªææ¥ä¼˜åŒ–å¹¶è¡Œæ€è€ƒï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03845",
      "arxiv_url": "https://arxiv.org/abs/2602.03845",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03845",
      "github_url": "https://github.com/zhengkid/Parallel-Probe",
      "upvotes": 26,
      "fetched_at": "2026-02-19T05:27:11.131433+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03619",
      "title": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation",
      "authors": [
        "Changze Lv",
        "Jie Zhou",
        "Wentao Zhao",
        "Jingwen Xu",
        "Zisu Huang",
        "Muzhao Tian",
        "Shihan Dou",
        "Tao Gui",
        "Le Tian",
        "Xiao Zhou",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "",
      "summary_en": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation (2026)",
      "summary_zh": "RubricHubï¼šé€šè¿‡è‡ªåŠ¨åŒ–ç²—åˆ°ç²¾ç”Ÿæˆæ„å»ºçš„å…¨é¢ä¸”é«˜åŒºåˆ†åº¦Rubricæ•°æ®é›†ï¼ˆ2026ï¼‰",
      "hf_url": "https://huggingface.co/papers/2602.03619",
      "arxiv_url": "https://arxiv.org/abs/2602.03619",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03619",
      "github_url": "",
      "upvotes": 26,
      "fetched_at": "2026-02-19T05:40:39.288615+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02380",
      "title": "Unified Personalized Reward Model for Vision Generation",
      "authors": [
        "Yibin Wang",
        "Yuhang Zang",
        "Feng Han",
        "Jiazi Bu",
        "Yujie Zhou",
        "Cheng Jin",
        "Jiaqi Wang"
      ],
      "abstract": "UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence. Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation . Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning . However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning . Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence , then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis , and extensive results demonstrate its superiority.",
      "summary_en": "UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.",
      "summary_zh": "UnifiedReward-Flex ç»“åˆå¥–åŠ±å»ºæ¨¡ä¸çµæ´»ã€ä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„æ¨ç†ï¼ŒåŸºäºè¯­ä¹‰æ„å›¾å’Œè§†è§‰è¯æ®åŠ¨æ€æ„å»ºåˆ†å±‚è¯„ä¼°ä»¥æ”¹è¿›è§†è§‰ç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02380",
      "arxiv_url": "https://arxiv.org/abs/2602.02380",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02380",
      "github_url": "https://github.com/CodeGoat24/UnifiedReward",
      "upvotes": 20,
      "fetched_at": "2026-02-19T05:39:03.589619+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02444",
      "title": "RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval",
      "authors": [
        "Tyler Skow",
        "Alexander Martin",
        "Benjamin Van Durme",
        "Rama Chellappa",
        "Reno Kriz"
      ],
      "abstract": "RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach. Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking , reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise , pairwise , and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.",
      "summary_en": "RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach.",
      "summary_zh": "RANKVIDEOæ˜¯ä¸€ç§åŸºäºæ¨ç†çš„è§†é¢‘æ£€ç´¢ç³»ç»Ÿï¼Œé€šè¿‡æ˜¾å¼çš„æŸ¥è¯¢-è§†é¢‘å¯¹åˆ†æå’Œå¤šç›®æ ‡è®­ç»ƒæ–¹æ³•æ”¹è¿›äº†ä¼ ç»Ÿä¸¤é˜¶æ®µæ¡†æ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02444",
      "arxiv_url": "https://arxiv.org/abs/2602.02444",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02444",
      "github_url": "https://github.com/tskow99/RANKVIDEO-Reasoning-Reranker",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:39:09.242447+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03086",
      "title": "Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning",
      "authors": [
        "Jiayao Mai",
        "Bangyan Liao",
        "Zhenjun Zhao",
        "Yingping Zeng",
        "Haoang Li",
        "Javier Civera",
        "Tailin Wu",
        "Yi Zhou",
        "Peidong Liu"
      ],
      "abstract": "Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training. The Homotopy paradigm , a general principle for solving challenging problems, appears across diverse domains such as robust optimization , global optimization , polynomial root-finding, and sampling . Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver . Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.",
      "summary_en": "Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training.",
      "summary_zh": "ç¥ç»é¢„æµ‹-æ ¡æ­£æ¡†æ¶ç»Ÿä¸€äº†è·¨å¤šä¸ªé¢†åŸŸçš„åŒä¼¦æ–¹æ³•ï¼Œå¹¶é€šè¿‡å­¦ä¹ ç­–ç•¥ä¸æ‘Šé”€è®­ç»ƒä¼˜äºç»å…¸æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03086",
      "arxiv_url": "https://arxiv.org/abs/2602.03086",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03086",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:26:54.934293+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02636",
      "title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling",
      "authors": [
        "Ziyang Huang",
        "Haolin Ren",
        "Xiaowei Yuan",
        "Jiawei Wang",
        "Zhongtao Jiang",
        "Kun Xu",
        "Shizhu He",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints. Search intelligence is evolving from Deep Research to Wide Research , a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench , a General Broad Information Seeking ( GBIS ) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL . Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.",
      "summary_en": "Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.",
      "summary_zh": "Wide Research é€šè¿‡ä¸“ç”¨åŸºå‡†ä¸å¤šæ™ºèƒ½ä½“æ¶æ„æ¨è¿›æœç´¢æ™ºèƒ½ï¼Œå®ç°å¤æ‚çº¦æŸä¸‹çš„å¹¶è¡Œä¿¡æ¯æ£€ç´¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02636",
      "arxiv_url": "https://arxiv.org/abs/2602.02636",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02636",
      "github_url": "https://github.com/hzy312/WideSeek",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:39:17.520746+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01362",
      "title": "Balancing Understanding and Generation in Discrete Diffusion Models",
      "authors": [
        "Yue Liu",
        "Yuzhong Zhao",
        "Zheyong Xie",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Yao Hu",
        "Shaosheng Cao",
        "Yunfan Liu"
      ],
      "abstract": "XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality. In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel . XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities . Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation ( FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model , XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM",
      "summary_en": "XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.",
      "summary_zh": "XDLM é€šè¿‡å¹³ç¨³å™ªå£°æ ¸ç»Ÿä¸€äº†æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸å‡åŒ€å™ªå£°æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¯­ä¹‰ç†è§£å’Œç”Ÿæˆè´¨é‡ä¸Šå‡å–å¾—äº†æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01362",
      "arxiv_url": "https://arxiv.org/abs/2602.01362",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01362",
      "github_url": "https://github.com/MzeroMiko/XDLM",
      "upvotes": 14,
      "fetched_at": "2026-02-19T05:38:48.043281+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03747",
      "title": "LIVE: Long-horizon Interactive Video World Modeling",
      "authors": [
        "Junchao Huang",
        "Ziyang Ye",
        "Xinting Hu",
        "Tianyu He",
        "Guiyu Zhang",
        "Shaoshuai Shi",
        "Jiang Bian",
        "Li Jiang"
      ],
      "abstract": "LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation. Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective , thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.",
      "summary_en": "LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation.",
      "summary_zh": "LIVEæ˜¯ä¸€ç§é•¿ç¨‹è§†é¢‘ä¸–ç•Œæ¨¡å‹ï¼Œåˆ©ç”¨å¾ªç¯ä¸€è‡´æ€§å’Œæ‰©æ•£æŸå¤±æ§åˆ¶é•¿è§†é¢‘ç”Ÿæˆä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03747",
      "arxiv_url": "https://arxiv.org/abs/2602.03747",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03747",
      "github_url": "https://github.com/Junchao-cs/LIVE",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:40:46.725114+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03216",
      "title": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection",
      "authors": [
        "Dongwon Jo",
        "Beomseok Kang",
        "Jiwon Song",
        "Jae-Joon Kim"
      ],
      "abstract": "Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss. The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention , a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention . Our approach is fully compatible with dense attention implementations, including Flash Attention , and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference .",
      "summary_en": "Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.",
      "summary_zh": "Tokenç¨€ç–æ³¨æ„åŠ›é€šè¿‡åœ¨Tokençº§åˆ«åŠ¨æ€å‹ç¼©å’Œè§£å‹æ³¨æ„åŠ›å¼ é‡ï¼Œå®ç°é«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ï¼Œåœ¨ç²¾åº¦æŸå¤±æå°çš„æƒ…å†µä¸‹è·å¾—æ˜¾è‘—åŠ é€Ÿã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03216",
      "arxiv_url": "https://arxiv.org/abs/2602.03216",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03216",
      "github_url": "https://github.com/dongwonjo/Token-Sparse-Attention",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:26:57.281053+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2601.21244",
      "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
      "authors": [
        "Yiju Guo",
        "Tianyi Hu",
        "Zexu Sun",
        "Yankai Lin"
      ],
      "abstract": "LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability. Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budget s, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework ( LENS ), which first prompts by identifying and removing interference tokens . then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO , delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.",
      "summary_en": "LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.",
      "summary_zh": "LENSæ¡†æ¶é€šè¿‡è¯†åˆ«å¹¶ç§»é™¤å¹²æ‰°tokenæ¥æ”¹è¿›å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œå¢å¼ºæ¢ç´¢æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21244",
      "arxiv_url": "https://arxiv.org/abs/2601.21244",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21244",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:38:34.854530+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03183",
      "title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch",
      "authors": [
        "Hyunwoo Kim",
        "Niloofar Mireshghallah",
        "Michael Duan",
        "Rui Xin",
        "Shuyue Stella Li",
        "Jaehun Jung",
        "David Acuna",
        "Qi Pang",
        "Hanshen Xiao",
        "G. Edward Suh",
        "Sewoong Oh",
        "Yulia Tsvetkov",
        "Pang Wei Koh",
        "Yejin Choi"
      ],
      "abstract": "A large-scale synthetic dataset called Privasis is introduced to address privacy concerns in AI research, enabling more effective text sanitization with compact models that outperform existing large language models. Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models , such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.",
      "summary_en": "A large-scale synthetic dataset called Privasis is introduced to address privacy concerns in AI research, enabling more effective text sanitization with compact models that outperform existing large language models.",
      "summary_zh": "ä¸ºè§£å†³AIç ”ç©¶ä¸­çš„éšç§é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†åä¸ºPrivasisçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œä½¿ç´§å‡‘æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ–‡æœ¬è„±æ•ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰å¤§è¯­è¨€æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03183",
      "arxiv_url": "https://arxiv.org/abs/2602.03183",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03183",
      "github_url": "https://github.com/skywalker023/privasis",
      "upvotes": 11,
      "fetched_at": "2026-02-19T05:26:56.463611+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03798",
      "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation",
      "authors": [
        "Zimu Lu",
        "Houxing Ren",
        "Yunqiao Yang",
        "Ke Wang",
        "Zhuofan Zong",
        "Mingjie Zhan",
        "Hongsheng Li"
      ],
      "abstract": "A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities. Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing , codebase navigation , and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend , backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend , backend , and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.",
      "summary_en": "A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFullStack-Agentçš„ç»Ÿä¸€æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¢å¼ºè§„åˆ’ã€ä»£ç ç¼–è¾‘å’Œè‡ªæˆ‘æ”¹è¿›èƒ½åŠ›æ¥åº”å¯¹å…¨æ ˆå¼€å‘æŒ‘æˆ˜ï¼ŒååŠ©éä¸“ä¸šç”¨æˆ·å¼€å‘å¤æ‚çš„äº¤äº’å¼ç½‘ç«™ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03798",
      "arxiv_url": "https://arxiv.org/abs/2602.03798",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03798",
      "github_url": "https://github.com/mnluzimu/FullStack-Agent",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:27:08.146966+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02676",
      "title": "AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process",
      "authors": [
        "Xintong Zhang",
        "Xiaowen Zhang",
        "Jongrong Wu",
        "Zhi Gao",
        "Shilin Yan",
        "Zhenxin Diao",
        "Kunpeng Gao",
        "Xuanyan Chen",
        "Yuwei Wu",
        "Yunde Jia",
        "Qing Li"
      ],
      "abstract": "AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation. Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage , tool effectiveness , and computational efficiency . Our evaluation reveals that while adaptive mode selection scales with model capacity , it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.",
      "summary_en": "AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.",
      "summary_zh": "AdaptMMBench æå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­è‡ªé€‚åº”å¤šæ¨¡æ€æ¨ç†çš„ç»¼åˆåŸºå‡†ï¼Œé€šè¿‡åŠ¨æ€éš¾åº¦è¯„ä¼°å’Œå¤šç»´è¿‡ç¨‹è¯„ä¼°æ¥è¡¡é‡æ¨ç†æ¨¡å¼é€‰æ‹©çš„åˆç†æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02676",
      "arxiv_url": "https://arxiv.org/abs/2602.02676",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02676",
      "github_url": "https://github.com/xtong-zhang/AdaptMMBench",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:39:21.060216+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.00747",
      "title": "Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training",
      "authors": [
        "Shengrui Li",
        "Fei Zhao",
        "Kaiyan Zhao",
        "Jieying Ye",
        "Haifeng Liu",
        "Fangcheng Shi",
        "Zheyong Xie",
        "Yao Hu",
        "Shaosheng Cao"
      ],
      "abstract": "DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency. Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training , where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging . This paradigm decouples search from training costs , enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials . Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.",
      "summary_en": "DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.",
      "summary_zh": "DeMixæ˜¯ä¸€ç§åˆ©ç”¨æ¨¡å‹åˆå¹¶é¢„æµ‹LLMé¢„è®­ç»ƒæœ€ä¼˜æ•°æ®é…æ¯”çš„æ¡†æ¶ï¼Œé€šè¿‡å°†æœç´¢ä¸è®­ç»ƒæˆæœ¬è§£è€¦æ¥æå‡æ··åˆå‘ç°æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00747",
      "arxiv_url": "https://arxiv.org/abs/2602.00747",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00747",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:38:42.210389+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.04541",
      "title": "LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding",
      "authors": [
        "Gang Lin",
        "Dongfang Li",
        "Zhuoen Chen",
        "Yukun Shi",
        "Xuhui Chen",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "LycheeDecode improves long-context LLM decoding efficiency through a fine-grained hybrid-head attention mechanism that dynamically identifies crucial tokens while maintaining attention head diversity.",
      "summary_en": "LycheeDecode improves long-context LLM decoding efficiency through a fine-grained hybrid-head attention mechanism that dynamically identifies crucial tokens while maintaining attention head diversity.",
      "summary_zh": "LycheeDecodeé€šè¿‡ç»†ç²’åº¦æ··åˆå¤´æ³¨æ„åŠ›æœºåˆ¶æå‡é•¿ä¸Šä¸‹æ–‡LLMçš„è§£ç æ•ˆç‡ï¼ŒåŠ¨æ€è¯†åˆ«å…³é”®tokenå¹¶ä¿æŒæ³¨æ„åŠ›å¤´çš„å¤šæ ·æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04541",
      "arxiv_url": "https://arxiv.org/abs/2602.04541",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04541",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:27:11.938409+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03709",
      "title": "No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding",
      "authors": [
        "Vynska Amalia Permadi",
        "Xingwei Tan",
        "Nafise Sadat Moosavi",
        "Nikos Aletras"
      ],
      "abstract": "Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains. Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline , combining expert review and LLM-as-a-judge filtering , ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.",
      "summary_en": "Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.",
      "summary_zh": "å¤šè·³é—®ç­”æ•°æ®é›†ID-MoCQAé€šè¿‡åŒ…å«å¤šæ ·åŒ–æ¨ç†é“¾çš„å°å°¼ä¼ ç»Ÿè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–ç†è§£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03709",
      "arxiv_url": "https://arxiv.org/abs/2602.03709",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03709",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:40:44.724414+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01053",
      "title": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents",
      "authors": [
        "Hyesung Jeon",
        "Hyeongju Ha",
        "Jae-Joon Kim"
      ],
      "abstract": "LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.",
      "summary_en": "LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.",
      "summary_zh": "LRAgentæ˜¯ä¸€ç§é¢å‘å¤šLoRAæ™ºèƒ½ä½“çš„KVç¼“å­˜å…±äº«æ¡†æ¶ï¼Œé€šè¿‡å°†ç¼“å­˜åˆ†è§£ä¸ºå…±äº«ç»„ä»¶å’Œé€‚é…å™¨ç›¸å…³ç»„ä»¶ï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶é™ä½å†…å­˜ä¸è®¡ç®—å¼€é”€ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01053",
      "arxiv_url": "https://arxiv.org/abs/2602.01053",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01053",
      "github_url": "https://github.com/hjeon2k/LRAgent",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:38:44.193269+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03677",
      "title": "Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration",
      "authors": [
        "Yu Zhang",
        "Mufan Xu",
        "Xuefeng Bai",
        "Kehai chen",
        "Pengfei Zhang",
        "Yang Xiang",
        "Min Zhang"
      ],
      "abstract": "Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent. Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens . Our findings reveal that instruction tokens function as structural anchors for modality arbitration : Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia , acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere 5% of these critical heads can decrease the modality-following ratio by 60% through blocking, or increase it by 60% through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.",
      "summary_en": "Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.",
      "summary_zh": "ç ”ç©¶è¡¨æ˜ï¼ŒæŒ‡ä»¤tokenåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­å……å½“ç»“æ„é”šç‚¹ï¼Œå…¶ä¸­æµ…å±‚æ‰§è¡Œéé€‰æ‹©æ€§ä¿¡æ¯ä¼ é€’ï¼Œæ·±å±‚åˆ™åœ¨æŒ‡ä»¤æ„å›¾å¼•å¯¼ä¸‹è§£å†³æ¨¡æ€ç«äº‰ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03677",
      "arxiv_url": "https://arxiv.org/abs/2602.03677",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03677",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:40:42.771316+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03647",
      "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
      "authors": [
        "Bowei He",
        "Minda Hu",
        "Zenan Xu",
        "Hongru Wang",
        "Licheng Zong",
        "Yankai Chen",
        "Chen Ma",
        "Xue Liu",
        "Pluto Zhou",
        "Irwin King"
      ],
      "abstract": "Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning. Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy , proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.",
      "summary_en": "Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.",
      "summary_zh": "Search-R2æ¡†æ¶é€šè¿‡Actor-Refineråä½œã€é’ˆå¯¹æ€§å¹²é¢„å’Œç»†ç²’åº¦å¥–åŠ±ç›‘ç£ä¼˜åŒ–å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…ï¼Œä»è€Œæå‡è¯­è¨€æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03647",
      "arxiv_url": "https://arxiv.org/abs/2602.03647",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03647",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:40:41.182465+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02537",
      "title": "WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models",
      "authors": [
        "Runjie Zhou",
        "Youbo Shao",
        "Haoyu Lu",
        "Bowei Xing",
        "Tongtong Bai",
        "Yujie Chen",
        "Jie Zhao",
        "Lin Sui",
        "Haotian Yao",
        "Zijia Zhao",
        "Hao Yang",
        "Haoning Wu",
        "Zaida Zhou",
        "Jinguo Zhu",
        "Zhiqi Huang",
        "Yiping Bao",
        "Yangyang Liu",
        "Y. Charles",
        "Xinyu Zhou"
      ],
      "abstract": "WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts. We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning , WorldVQA decouples these capabilities to strictly measure \"what the model memorizes.\" The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality , thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.",
      "summary_en": "WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.",
      "summary_zh": "WorldVQAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†è§‰ä¸–ç•ŒçŸ¥è¯†çš„åŸºå‡†ï¼Œé€šè¿‡åˆ†ç¦»è§†è§‰çŸ¥è¯†æ£€ç´¢ä¸æ¨ç†æ¥è¡¡é‡è®°å¿†çš„äº‹å®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02537",
      "arxiv_url": "https://arxiv.org/abs/2602.02537",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02537",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:39:13.542051+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.00359",
      "title": "Position: Agentic Evolution is the Path to Evolving LLMs",
      "authors": [
        "Minhua Lin",
        "Hanqing Lu",
        "Zhan Shi",
        "Bing He",
        "Rui Mao",
        "Zhiwei Zhang",
        "Zongyu Wu",
        "Xianfeng Tang",
        "Hui Liu",
        "Zhenwei Dai",
        "Xiang Zhang",
        "Suhang Wang",
        "Benoit Dumoulin",
        "Jian Pei"
      ],
      "abstract": "Large language models face limitations in adapting to changing real-world environments, necessitating a new approach called agentic evolution that treats deployment-time improvement as a goal-directed optimization process. As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation , lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve , which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis : the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.",
      "summary_en": "Large language models face limitations in adapting to changing real-world environments, necessitating a new approach called agentic evolution that treats deployment-time improvement as a goal-directed optimization process.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨é€‚åº”ä¸æ–­å˜åŒ–çš„ç°å®ä¸–ç•Œç¯å¢ƒæ—¶é¢ä¸´å±€é™æ€§ï¼Œéœ€è¦ä¸€ç§ç§°ä¸ºæ™ºèƒ½ä½“æ¼”åŒ–çš„æ–°æ–¹æ³•ï¼Œå°†éƒ¨ç½²æ—¶çš„æ”¹è¿›è§†ä¸ºç›®æ ‡å¯¼å‘çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00359",
      "arxiv_url": "https://arxiv.org/abs/2602.00359",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00359",
      "github_url": "https://github.com/ventr1c/agentic-evoluiton",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:38:36.503606+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03837",
      "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
      "authors": [
        "David P. Woodruff",
        "Vincent Cohen-Addad",
        "Lalit Jain",
        "Jieming Mao",
        "Song Zuo",
        "MohammadHossein Bateni",
        "Simina Branzei",
        "Michael P. Brenner",
        "Lin Chen",
        "Ying Feng",
        "Lance Fortnow",
        "Gang Fu",
        "Ziyi Guan",
        "Zahra Hadizadeh",
        "Mohammad T. Hajiaghayi",
        "Mahdi JafariRaviz",
        "Adel Javanmard",
        "Karthik C. S.",
        "Ken-ichi Kawarabayashi",
        "Ravi Kumar",
        "Silvio Lattanzi",
        "Euiwoong Lee"
      ],
      "abstract": "Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution. Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science , as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement , problem decomposition , and cross-disciplinary knowledge transfer . While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.",
      "summary_en": "Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.",
      "summary_zh": "å…ˆè¿›AIæ¨¡å‹å±•ç°å‡ºé€šè¿‡æ¶‰åŠè¯æ˜éªŒè¯å’Œè‡ªåŠ¨ä»£ç æ‰§è¡Œçš„åä½œæ–¹æ³•æ”¯æŒä¸“å®¶çº§æ•°å­¦å‘ç°ä¸ç§‘å­¦ç ”ç©¶çš„èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03837",
      "arxiv_url": "https://arxiv.org/abs/2602.03837",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03837",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:27:10.400263+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03806",
      "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
      "authors": [
        "Ziru Chen",
        "Dongdong Chen",
        "Ruinan Jin",
        "Yingbin Liang",
        "Yujia Xie",
        "Huan Sun"
      ],
      "abstract": "Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.",
      "summary_en": "Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.",
      "summary_zh": "ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç»“åˆä¸Šä¸‹æ–‡èµŒåšæœºå­¦ä¹ ä¸éƒ¨åˆ†è½¨è¿¹ï¼Œæå‡å¤šè½®ä»£ç ç”Ÿæˆæ€§èƒ½å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03806",
      "arxiv_url": "https://arxiv.org/abs/2602.03806",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03806",
      "github_url": "https://github.com/OSU-NLP-Group/cobalt",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:27:08.954330+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02905",
      "title": "FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights",
      "authors": [
        "Zhen Wang",
        "Fan Bai",
        "Zhongyan Luo",
        "Jinyan Su",
        "Kaiser Sun",
        "Xinle Yu",
        "Jieyuan Liu",
        "Kun Zhou",
        "Claire Cardie",
        "Mark Dredze",
        "Eric P. Xing",
        "Zhiting Hu"
      ],
      "abstract": "Researchers developed FIRE-Bench, a comprehensive evaluation framework that challenges autonomous agents to rediscover established scientific findings through complete research cycles involving hypothesis generation, experimentation, coding, and evidence-based conclusion drawing.",
      "summary_en": "Researchers developed FIRE-Bench, a comprehensive evaluation framework that challenges autonomous agents to rediscover established scientific findings through complete research cycles involving hypothesis generation, experimentation, coding, and evidence-based conclusion drawing.",
      "summary_zh": "ç ”ç©¶äººå‘˜å¼€å‘äº†FIRE-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæŒ‘æˆ˜è‡ªä¸»æ™ºèƒ½ä½“é€šè¿‡åŒ…å«å‡è®¾ç”Ÿæˆã€å®éªŒã€ç¼–ç¨‹å’ŒåŸºäºè¯æ®å¾—å‡ºç»“è®ºçš„å®Œæ•´ç ”ç©¶å‘¨æœŸï¼Œé‡æ–°å‘ç°æ—¢å®šç§‘å­¦å‘ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02905",
      "arxiv_url": "https://arxiv.org/abs/2602.02905",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02905",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:26:52.581598+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02751",
      "title": "Scaling Small Agents Through Strategy Auctions",
      "authors": [
        "Lisa Alazraki",
        "William F. Shen",
        "Yoram Bachrach",
        "Akhil Mathur"
      ],
      "abstract": "Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms. Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI , with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity , when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents' performance fails to scale with task complexity on deep search and coding tasks , and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces . In SALE, agents bid with short strategic plans , which are scored by a systematic cost-value mechanism and refined via a shared auction memory , enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent's pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively \"scaled up\" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.",
      "summary_en": "Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.",
      "summary_zh": "å°å‹è¯­è¨€æ¨¡å‹éš¾ä»¥å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†å¯é€šè¿‡å—å¸‚åœºå¯å‘çš„æ¡†æ¶æœ‰æ•ˆåè°ƒï¼Œå€ŸåŠ©ç­–ç•¥æ€§ç«ä»·å’Œè‡ªæˆ‘æ”¹è¿›æœºåˆ¶é™ä½æˆæœ¬å¹¶æå‡æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02751",
      "arxiv_url": "https://arxiv.org/abs/2602.02751",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02751",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:26:51.862446+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01753",
      "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings",
      "authors": [
        "Shenghao Fu",
        "Yukun Su",
        "Fengyun Rao",
        "Jing Lyu",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
      ],
      "abstract": "ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.",
      "summary_en": "ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.",
      "summary_zh": "ObjEmbedæ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åµŒå…¥æ–¹æ³•ï¼Œé€šè¿‡å°†å›¾åƒåˆ†è§£ä¸ºåŒºåŸŸåµŒå…¥ï¼Œæå‡ç‰©ä½“çº§è§†è§‰ç†è§£ä¸æ£€ç´¢ä»»åŠ¡çš„æ•ˆæœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01753",
      "arxiv_url": "https://arxiv.org/abs/2602.01753",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01753",
      "github_url": "https://github.com/WeChatCV/ObjEmbed",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:38:55.894561+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03295",
      "title": "POP: Prefill-Only Pruning for Efficient Large Model Inference",
      "authors": [
        "Junhui He",
        "Zhihui Fu",
        "Jun Wang",
        "Qingan Li"
      ],
      "abstract": "Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy. Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stage s. By introducing a virtual gate mechanism , our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage . To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.",
      "summary_en": "Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.",
      "summary_zh": "é¢å‘å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰-è¯­è¨€æ¨¡å‹çš„é˜¶æ®µæ„ŸçŸ¥å‰ªææ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸åŒå¤„ç†é˜¶æ®µé€‰æ‹©æ€§ç§»é™¤å±‚ä»¥æå‡æ•ˆç‡å¹¶ä¿æŒå‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03295",
      "arxiv_url": "https://arxiv.org/abs/2602.03295",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03295",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:26:58.848956+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02419",
      "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
      "authors": [
        "Qingni Wang",
        "Yue Fan",
        "Xin Eric Wang"
      ],
      "abstract": "SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates. Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibration s before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38% percentage points over Gemini-only inference.",
      "summary_en": "SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.",
      "summary_zh": "SafeGroundæ˜¯ä¸€ç§é¢å‘GUIå®šä½æ¨¡å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¡†æ¶ï¼Œåˆ©ç”¨åˆ†å¸ƒæ„ŸçŸ¥çš„ä¸ç¡®å®šæ€§é‡åŒ–å’Œæ ¡å‡†ï¼Œå®ç°å…·æœ‰å—æ§é”™è¯¯å‘ç°ç‡çš„é£é™©æ„ŸçŸ¥é¢„æµ‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02419",
      "arxiv_url": "https://arxiv.org/abs/2602.02419",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02419",
      "github_url": "https://github.com/Cece1031/SAFEGROUND",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:39:07.476858+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.00398",
      "title": "MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers",
      "authors": [
        "Ajay Jaiswal",
        "Lauren Hannah",
        "Han-Byul Kim",
        "Duc Hoang",
        "Arnav Kundu",
        "Mehrdad Farajtabar",
        "Minsik Cho"
      ],
      "abstract": "MemoryLLM decouples feed-forward networks from self-attention in transformers, enabling context-free token-wise neural retrieval memory that improves inference efficiency through pre-computed lookups. Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM , which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory . In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings . This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM , positioning it between a conventional transformer design and MemoryLLM . This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.",
      "summary_en": "MemoryLLM decouples feed-forward networks from self-attention in transformers, enabling context-free token-wise neural retrieval memory that improves inference efficiency through pre-computed lookups.",
      "summary_zh": "MemoryLLMå°†Transformerä¸­çš„å‰é¦ˆç½‘ç»œä¸è‡ªæ³¨æ„åŠ›è§£è€¦ï¼Œå®ç°äº†æ— ä¸Šä¸‹æ–‡çš„é€tokenç¥ç»æ£€ç´¢è®°å¿†ï¼Œé€šè¿‡é¢„è®¡ç®—æŸ¥æ‰¾æå‡æ¨ç†æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00398",
      "arxiv_url": "https://arxiv.org/abs/2602.00398",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00398",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:38:38.134589+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2601.19103",
      "title": "Glance and Focus Reinforcement for Pan-cancer Screening",
      "authors": [
        "Linshan Wu",
        "Jiaxin Zhuang",
        "Hao Chen"
      ],
      "abstract": "A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning. Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives . Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening . GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model . To optimize the Glance model , we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives . In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening . Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).",
      "summary_en": "A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.",
      "summary_zh": "ä¸€ç§ç»“åˆæ‰«è§†ä¸èšç„¦æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡è§£å†³å‰æ™¯-èƒŒæ™¯ä¸å¹³è¡¡å¹¶é‡‡ç”¨ç»„ç›¸å¯¹å­¦ä¹ é™ä½å‡é˜³æ€§ï¼Œæ”¹è¿›äº†CTæ‰«æä¸­çš„æ³›ç™Œç­›æŸ¥ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.19103",
      "arxiv_url": "https://arxiv.org/abs/2601.19103",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.19103",
      "github_url": "https://github.com/Luffy03/GF-Screen",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:38:33.100382+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03454",
      "title": "Contextualized Visual Personalization in Vision-Language Models",
      "authors": [
        "Yeongtak Oh",
        "Sangwon Yu",
        "Junsung Park",
        "Han Cheol Moon",
        "Jisoo Mok",
        "Sungroh Yoon"
      ],
      "abstract": "CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation. Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization , which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation . We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context . Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization .",
      "summary_en": "CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.",
      "summary_zh": "CoViP é€šè¿‡å°†ä¸ªæ€§åŒ–å›¾åƒæè¿°ä½œä¸ºæ ¸å¿ƒä»»åŠ¡ï¼Œå¹¶å€ŸåŠ©åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒä¸æè¿°å¢å¼ºç”Ÿæˆæ¥æå‡èƒ½åŠ›ï¼Œä»è€Œåº”å¯¹æƒ…å¢ƒåŒ–è§†è§‰ä¸ªæ€§åŒ–é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03454",
      "arxiv_url": "https://arxiv.org/abs/2602.03454",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03454",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:40:37.310057+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01212",
      "title": "SimpleGPT: Improving GPT via A Simple Normalization Strategy",
      "authors": [
        "Marco Chen",
        "Xianbiao Qi",
        "Yelin He",
        "Jiaquan Ye",
        "Rong Xiao"
      ],
      "abstract": "SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance. In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale , the Hessian matrix , and the maximum tolerable learning rate . We introduce a simple normalization strategy, termed SimpleNorm , which stabilizes intermediate activation scale s by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rate s. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT , our SimpleNorm -based network, tolerates learning rate s 3times-10times larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm , reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/ SimpleGPT .",
      "summary_en": "SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.",
      "summary_zh": "SimpleNorm å½’ä¸€åŒ–ç­–ç•¥é€šè¿‡é™ä½ Hessian è°±èŒƒæ•°ç¨³å®šæ¿€æ´»å°ºåº¦ï¼Œä½¿ Transformer æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨æ›´å¤§ä¸”ç¨³å®šçš„å­¦ä¹ ç‡ï¼Œä»è€Œæå‡è®­ç»ƒæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01212",
      "arxiv_url": "https://arxiv.org/abs/2602.01212",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01212",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:38:45.997007+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03320",
      "title": "MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning",
      "authors": [
        "Shengyuan Liu",
        "Liuxin Bao",
        "Qi Yang",
        "Wanting Geng",
        "Boyun Zheng",
        "Chenxin Li",
        "Wenting Chen",
        "Houwen Peng",
        "Yixuan Yuan"
      ],
      "abstract": "MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.",
      "summary_en": "MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.",
      "summary_zh": "MedSAM-Agent åˆ©ç”¨æ··åˆæç¤ºå’Œå¼•å…¥è¿‡ç¨‹å¥–åŠ±çš„ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œå°†åŒ»å­¦å›¾åƒåˆ†å‰²é‡æ–°å®šä¹‰ä¸ºå¤šæ­¥å†³ç­–è¿‡ç¨‹ï¼Œä»¥æå‡è‡ªä¸»æ¨ç†ä¸ä¼˜åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03320",
      "arxiv_url": "https://arxiv.org/abs/2602.03320",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03320",
      "github_url": "https://github.com/CUHK-AIM-Group/MedSAM-Agent",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:26:59.545994+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02914",
      "title": "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction",
      "authors": [
        "Wenqi Guo",
        "Shan Du"
      ],
      "abstract": "FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection. Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction , measured by PSNR and SSIM . We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.",
      "summary_en": "FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.",
      "summary_zh": "FaceLinkGenæ”»å‡»è¡¨æ˜ï¼Œå°½ç®¡åƒç´ çº§å¤±çœŸæŒ‡æ ‡è¡¨æ˜å·²æä¾›å……åˆ†ä¿æŠ¤ï¼Œå½“å‰çš„éšç§ä¿æŠ¤äººè„¸è¯†åˆ«æ–¹æ³•ä»æœªèƒ½ä¿æŠ¤èº«ä»½ä¿¡æ¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02914",
      "arxiv_url": "https://arxiv.org/abs/2602.02914",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02914",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:26:53.283796+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02405",
      "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
      "authors": [
        "Ethan Mendes",
        "Jungsoo Park",
        "Alan Ritter"
      ],
      "abstract": "Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data. Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization .",
      "summary_en": "Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.",
      "summary_zh": "åˆ†å¸ƒå¯¹é½æ¨¡ä»¿å­¦ä¹ ï¼ˆDAILï¼‰é€šè¿‡å°†ä¸“å®¶è§£å†³æ–¹æ¡ˆè½¬æ¢ä¸ºåˆ†å¸ƒå†…è½¨è¿¹ï¼Œå¹¶ä½¿ç”¨å¯¹æ¯”å­¦ä¹ èšç„¦ä¸“å®¶æ–¹æ³•ï¼Œåœ¨ä»…éœ€æå°‘ä¸“å®¶æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ˜¾è‘—æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02405",
      "arxiv_url": "https://arxiv.org/abs/2602.02405",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02405",
      "github_url": "https://github.com/ethanm88/DAIL",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:39:05.756339+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03238",
      "title": "The Necessity of a Unified Framework for LLM-Based Agent Evaluation",
      "authors": [
        "Pengyu Zhu",
        "Li Sun",
        "Philip S. Yu",
        "Sen Su"
      ],
      "abstract": "Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment. With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts , toolset configurations , and environmental dynamics . Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.",
      "summary_en": "Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹æ¨åŠ¨äº†é€šç”¨æ™ºèƒ½ä½“çš„å‘å±•ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†å­˜åœ¨æ··æ‚å› ç´ ä¸”ç¼ºä¹æ ‡å‡†åŒ–ï¼ŒäºŸéœ€ç»Ÿä¸€æ¡†æ¶ä»¥å®ç°ä¸¥è°¨è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03238",
      "arxiv_url": "https://arxiv.org/abs/2602.03238",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03238",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:26:58.176754+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02494",
      "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
      "authors": [
        "Dulhan Jayalath",
        "Oiwi Parker Jones"
      ],
      "abstract": "MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data. Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context . Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models . We find that models pre-trained with longer contexts learn representations that transfer better to word decoding . Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .",
      "summary_en": "MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.",
      "summary_zh": "MEG-XLé€šè¿‡2.5åˆ†é’ŸMEGä¸Šä¸‹æ–‡çš„æ‰©å±•é¢„è®­ç»ƒæå‡äº†è„‘åˆ°æ–‡æœ¬è§£ç æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä½¿ç”¨æ›´å°‘ä¸Šä¸‹æ–‡æ•°æ®çš„å…ˆå‰æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02494",
      "arxiv_url": "https://arxiv.org/abs/2602.02494",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02494",
      "github_url": "https://github.com/neural-processing-lab/MEG-XL",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:39:11.346986+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.02220",
      "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
      "authors": [
        "Bo Miao",
        "Weijia Liu",
        "Jun Luo",
        "Lachlan Shinnick",
        "Jian Liu",
        "Thomas Hamilton-Smith",
        "Yuhe Yang",
        "Zijie Wu",
        "Vanja Videnovic",
        "Feras Dayoub",
        "Anton van den Hengel"
      ],
      "abstract": "HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments. The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity , open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels : scene , room , region , and instance . To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation . Project: https://bo-miao.github.io/LangMap",
      "summary_en": "HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.",
      "summary_zh": "HieraNavæå‡ºäº†ä¸€é¡¹å¤šç²’åº¦ã€å¼€æ”¾è¯æ±‡çš„å¯¼èˆªä»»åŠ¡ï¼Œå¹¶æ¨å‡ºäº†LangMapåŸºå‡†ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨3Dç¯å¢ƒä¸­éµå¾ªè·¨ä¸åŒè¯­ä¹‰å±‚çº§çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02220",
      "arxiv_url": "https://arxiv.org/abs/2602.02220",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02220",
      "github_url": "https://github.com/bo-miao/LangMap",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:39:01.391649+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01405",
      "title": "Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents",
      "authors": [
        "Nikhil Sharma",
        "Zheng Zhang",
        "Daniel Lee",
        "Namita Krishnan",
        "Guang-Jie Ren",
        "Ziang Xiao",
        "Yunyao Li"
      ],
      "abstract": "High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.",
      "summary_en": "High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality.",
      "summary_zh": "é«˜è´¨é‡åé¦ˆå¯¹äºæœ‰æ•ˆçš„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚å®ƒå¼¥åˆçŸ¥è¯†å·®è·ã€çº æ­£åå·®å¹¶å¡‘é€ ç³»ç»Ÿè¡Œä¸ºï¼›æ— è®ºæ˜¯åœ¨äº¤äº’è¿‡ç¨‹ä¸­è¿˜æ˜¯åœ¨æ•´ä¸ªæ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶ååˆ†é‡è¦ï¼Œäººç±»å¯¹AIçš„åé¦ˆå¾€å¾€ç¨€å°‘ä¸”è´¨é‡ä½ä¸‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01405",
      "arxiv_url": "https://arxiv.org/abs/2602.01405",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01405",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:38:50.434421+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.00682",
      "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
      "authors": [
        "Yuecheng Li",
        "Hengwei Ju",
        "Zeyu Song",
        "Wei Yang",
        "Chi Lu",
        "Peng Jiang",
        "Kun Gai"
      ],
      "abstract": "A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning. Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning . However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation , which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.",
      "summary_en": "A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.",
      "summary_zh": "ä¸€ç§é¢å‘LLMå¢å¼ºå¤šæ¨¡æ€æ¨èçš„æ–°å‹åŒé‡è¯­ä¹‰å¯¹é½æ¡†æ¶ï¼Œé€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œä¸è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ è§£å†³å¤§æ¨¡å‹ä¸æ¨èç³»ç»Ÿä¹‹é—´çš„è¡¨ç¤ºå·®å¼‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00682",
      "arxiv_url": "https://arxiv.org/abs/2602.00682",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00682",
      "github_url": "https://github.com/6lyc/RecGOAT-LLM4Rec",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:38:40.207167+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.03817",
      "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
      "authors": [
        "Oscar Ovanger",
        "Levi Harris",
        "Timothy H. Keitt"
      ],
      "abstract": "A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches. Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor . FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics . The resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback . Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md{anonymous-repository}}",
      "summary_en": "A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.",
      "summary_zh": "ä¸€ç§åä¸ºFINCHçš„èåˆæ¡†æ¶é€šè¿‡åŸºäºå¯é æ€§ä¼°è®¡çš„è‡ªé€‚åº”è¯æ®åŠ æƒï¼Œç»“åˆéŸ³é¢‘å’Œæ—¶ç©ºé¢„æµ‹å™¨è¿›è¡Œç”Ÿç‰©å£°å­¦åˆ†ç±»ï¼Œæ€§èƒ½ä¼˜äºå›ºå®šæƒé‡æ–¹æ³•å’Œä»…éŸ³é¢‘æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03817",
      "arxiv_url": "https://arxiv.org/abs/2602.03817",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03817",
      "github_url": "https://github.com/leharris3/birdnoise",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:27:09.671362+00:00"
    },
    {
      "date": "2026-02-04",
      "paper_id": "2602.01519",
      "title": "You Need an Encoder for Native Position-Independent Caching",
      "authors": [
        "Shiju Zhao",
        "Junhao Hu",
        "Jiaqi Zheng",
        "Guihai Chen"
      ],
      "abstract": "Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy. The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs . Our code is available at https://github.com/shijuzhao/Comb.",
      "summary_en": "Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.",
      "summary_zh": "åŸç”Ÿä½ç½®æ— å…³ç¼“å­˜é€šè¿‡é‡æ–°å¼•å…¥ç¼–ç å™¨å¹¶å¼€å‘ç¼“å­˜ç³»ç»Ÿï¼Œåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶é™ä½å»¶è¿Ÿï¼Œä»è€Œæå‡ LLM æ¨ç†æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01519",
      "arxiv_url": "https://arxiv.org/abs/2602.01519",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01519",
      "github_url": "https://github.com/shijuzhao/Comb",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:38:52.007805+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00919",
      "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
      "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin"
      ],
      "abstract": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning. We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding , (R0) multi-embodiment pretraining , (R1) embodiment-specific adaptation , and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction , out-of-distribution detection , and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "summary_en": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "summary_zh": "Green-VLAæ˜¯ä¸€ä¸ªç”¨äºçœŸå®ä¸–ç•Œæœºå™¨äººéƒ¨ç½²çš„äº”é˜¶æ®µè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œé€šè¿‡å¤šæ¨¡æ€è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å®ç°è·¨ä¸åŒæœºå™¨äººæœ¬ä½“çš„æ³›åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00919",
      "arxiv_url": "https://arxiv.org/abs/2602.00919",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00919",
      "github_url": "https://github.com/greenvla/GreenVLA",
      "upvotes": 280,
      "fetched_at": "2026-02-19T05:36:32.100057+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02276",
      "title": "Kimi K2.5: Visual Agentic Intelligence",
      "authors": [
        "Kimi Team",
        "Tongtong Bai",
        "Yifan Bai",
        "Yiping Bao",
        "S. H. Cai",
        "Yuan Cao",
        "Y. Charles",
        "H. S. Che",
        "Cheng Chen",
        "Guanduo Chen",
        "Huarong Chen",
        "Jia Chen",
        "Jiahao Chen",
        "Jianlong Chen",
        "Jun Chen",
        "Kefan Chen",
        "Liang Chen",
        "Ruijue Chen",
        "Xinhao Chen",
        "Yanru Chen",
        "Yanxu Chen",
        "Yicun Chen"
      ],
      "abstract": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution. We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training , zero-vision SFT , and joint text-vision reinforcement learning . Building on this multimodal foundation, K2.5 introduces Agent Swarm , a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "summary_en": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.",
      "summary_zh": "Kimi K2.5 æ˜¯ä¸€æ¬¾å¼€æºçš„å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¨¡å‹ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æŠ€æœ¯å¢å¼ºæ–‡æœ¬ä¸è§†è§‰å¤„ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥ Agent Swarm å®ç°å¹¶è¡Œä»»åŠ¡æ‰§è¡Œã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02276",
      "arxiv_url": "https://arxiv.org/abs/2602.02276",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02276",
      "github_url": "https://github.com/MoonshotAI/Kimi-K2.5",
      "upvotes": 233,
      "fetched_at": "2026-02-19T05:37:47.945680+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22060",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "authors": [
        "Wenxuan Huang",
        "Yu Zeng",
        "Qiuchen Wang",
        "Zhen Fang",
        "Shaosheng Cao",
        "Zheng Chu",
        "Qingyu Yin",
        "Shuang Chen",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Yao Hu",
        "Philip Torr",
        "Feng Zhao",
        "Wanli Ouyang"
      ],
      "abstract": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning. Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by `` reasoning-then-tool-call '' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "summary_en": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.",
      "summary_zh": "Vision-DeepResearchæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œé€šè¿‡å†·å¯åŠ¨ç›‘ç£ä¸å¼ºåŒ–å­¦ä¹ é›†æˆæ·±åº¦ç ”ç©¶èƒ½åŠ›ï¼Œå®ç°å¤šè½®ã€å¤šå®ä½“ã€å¤šå°ºåº¦çš„è§†è§‰ä¸æ–‡æœ¬æœç´¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22060",
      "arxiv_url": "https://arxiv.org/abs/2601.22060",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22060",
      "github_url": "https://github.com/Osilly/Vision-DeepResearch",
      "upvotes": 153,
      "fetched_at": "2026-02-19T05:36:06.346246+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02185",
      "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Zhen Fang",
        "Shuang Chen",
        "Yufan Shen",
        "Yishuo Cai",
        "Xiaoman Wang",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Shiting Huang",
        "Yiming Zhao",
        "Yao Hu",
        "Philip Torr",
        "Wanli Ouyang",
        "Shaosheng Cao"
      ],
      "abstract": "Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow. Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding . However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems . The code will be released in https://github.com/Osilly/ Vision-DeepResearch .",
      "summary_en": "Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.",
      "summary_zh": "Vision-DeepResearchåŸºå‡†é€šè¿‡å¼•å…¥çœŸå®è¯„ä¼°æ¡ä»¶å¹¶é‡‡ç”¨å¤šè½®è£å‰ªæœç´¢å·¥ä½œæµæ”¹è¿›è§†è§‰æ£€ç´¢ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹è§†è§‰-æ–‡æœ¬æœç´¢èƒ½åŠ›è¯„ä¼°çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02185",
      "arxiv_url": "https://arxiv.org/abs/2602.02185",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02185",
      "github_url": "",
      "upvotes": 125,
      "fetched_at": "2026-02-19T05:37:42.232250+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02084",
      "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
      "authors": [
        "Jane Luo",
        "Chengyu Yin",
        "Xin Zhang",
        "Qingtao Li",
        "Steven Liu",
        "Yiming Huang",
        "Jie Wu",
        "Hao Liu",
        "Yangyu Huang",
        "Yu Kang",
        "Fangkai Yang",
        "Ying Xin",
        "Scarlett Li"
      ],
      "abstract": "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy. Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder , a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies ; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation . In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.",
      "summary_en": "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.",
      "summary_zh": "RPG-Encoderæ¡†æ¶é€šè¿‡å°†ä»£ç ç¼–ç ä¸ºé«˜ä¿çœŸRepository Planning Graphè¡¨ç¤ºï¼Œå°†ä»£ç åº“ç†è§£ä¸ç”Ÿæˆè½¬åŒ–ä¸ºç»Ÿä¸€çš„å¾ªç¯ï¼Œä»è€Œæå‡ç†è§£ä¸é‡å»ºçš„å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02084",
      "arxiv_url": "https://arxiv.org/abs/2602.02084",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02084",
      "github_url": "https://github.com/microsoft/RPG-ZeroRepo",
      "upvotes": 82,
      "fetched_at": "2026-02-19T05:37:33.706738+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02437",
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Chaofan Ma",
        "Feng Han",
        "Size Wu",
        "Wei Song",
        "Yibin Wang",
        "Zhixiong Zhang",
        "Tianhang Wang",
        "Siyuan Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks. Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm . We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation , mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction . Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE , KrisBench and UniREditBench , while maintaining superior general synthesis capabilities.",
      "summary_en": "UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.",
      "summary_zh": "UniReasoné€šè¿‡åŒé‡æ¨ç†èŒƒå¼æ•´åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸å›¾åƒç¼–è¾‘ï¼Œåˆ©ç”¨ä¸–ç•ŒçŸ¥è¯†å¢å¼ºè§„åˆ’å¹¶ä½¿ç”¨ç¼–è¾‘è¿›è¡Œè§†è§‰ä¼˜åŒ–ï¼Œåœ¨æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02437",
      "arxiv_url": "https://arxiv.org/abs/2602.02437",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02437",
      "github_url": "",
      "upvotes": 76,
      "fetched_at": "2026-02-19T05:38:00.634902+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02361",
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "authors": [
        "Mouxiang Chen",
        "Lei Zhang",
        "Yunlong Feng",
        "Xuwu Wang",
        "Wenting Zhao",
        "Ruisheng Cao",
        "Jiaxi Yang",
        "Jiawei Chen",
        "Mingze Li",
        "Zeyao Ma",
        "Hao Ge",
        "Zongmeng Zhang",
        "Zeyu Cui",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Jianling Sun",
        "Junyang Lin",
        "Binyuan Hui"
      ],
      "abstract": "A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities. We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model . This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning . Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified . Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
      "summary_en": "A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.",
      "summary_zh": "ä¸€ç§å¯æ‰©å±•æ¡†æ¶ï¼Œåˆ©ç”¨å…·å¤‡è‡ªéªŒè¯å’Œé»‘å®¢æ”»å‡»æ£€æµ‹èƒ½åŠ›çš„é«˜æ•ˆæ„å»ºæ™ºèƒ½ä½“ï¼Œä» GitHub pull requests æ„å»ºçœŸå®è½¯ä»¶å·¥ç¨‹ç¯å¢ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02361",
      "arxiv_url": "https://arxiv.org/abs/2602.02361",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02361",
      "github_url": "",
      "upvotes": 60,
      "fetched_at": "2026-02-19T05:37:57.044088+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01566",
      "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
      "authors": [
        "Chiwei Zhu",
        "Benfeng Xu",
        "Mingxuan Du",
        "Shaohan Wang",
        "Xiaorui Wang",
        "Zhendong Mao",
        "Yongdong Zhang"
      ],
      "abstract": "A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.",
      "summary_en": "A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.",
      "summary_zh": "åŸºäºæ–‡ä»¶ç³»ç»Ÿçš„åŒæ™ºèƒ½ä½“æ¡†æ¶é€šè¿‡å°†æŒä¹…åŒ–å­˜å‚¨ä½œä¸ºå¤–éƒ¨è®°å¿†ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“èƒ½å¤Ÿçªç ´ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼Œæ‰§è¡Œæ‰©å±•ç ”ç©¶ä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01566",
      "arxiv_url": "https://arxiv.org/abs/2602.01566",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01566",
      "github_url": "",
      "upvotes": 48,
      "fetched_at": "2026-02-19T05:36:57.934456+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02472",
      "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
      "authors": [
        "Qifan Yu",
        "Xinyu Ma",
        "Zhijian Zhuo",
        "Minrui Wang",
        "Deyi Liu",
        "Shiyi Zhan",
        "Yiyuan Ma",
        "Liang Xiang",
        "Xingyan Bin",
        "Di He"
      ],
      "abstract": "SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs. Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings , yet it remains a formidable challenge due to severe training instabilities . Empirically, we show that naive initialization at this stage disrupts activation statistics , triggering loss spikes , while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion . Our method achieves signal preservation via RMS-scale consistency , stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup . Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion .",
      "summary_en": "SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.",
      "summary_zh": "SPARKLINGæ˜¯ä¸€ç§ç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æœŸå®½åº¦æ‰©å±•çš„æ¡†æ¶ï¼Œé€šè¿‡ä¿æŒä¿¡å·ä¿ç•™å¹¶æ‰“ç ´å¯¹ç§°æ€§æ¥ç¨³å®šè®­ç»ƒå¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02472",
      "arxiv_url": "https://arxiv.org/abs/2602.02472",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02472",
      "github_url": "",
      "upvotes": 44,
      "fetched_at": "2026-02-19T05:38:04.314322+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02493",
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "abstract": "PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations. Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAE s in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion model s. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision . Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision , PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to- image generation with a GenEval score of 0.79. PixelGen requires no VAE s, no latent representations , and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "summary_en": "PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.",
      "summary_zh": "PixelGen æ˜¯ä¸€ç§åƒç´ ç©ºé—´æ‰©æ•£æ¡†æ¶ï¼Œåˆ©ç”¨ LPIPS å’ŒåŸºäº DINO çš„æŸå¤±å‡½æ•°è¿›è¡Œæ„ŸçŸ¥ç›‘ç£ï¼Œæ— éœ€ VAE æˆ–æ½œåœ¨è¡¨ç¤ºå³å¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02493",
      "arxiv_url": "https://arxiv.org/abs/2602.02493",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02493",
      "github_url": "https://github.com/Zehong-Ma/PixelGen",
      "upvotes": 42,
      "fetched_at": "2026-02-19T05:38:11.506490+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01576",
      "title": "Generative Visual Code Mobile World Models",
      "authors": [
        "Woosung Koh",
        "Sungjun Han",
        "Segyu Lee",
        "Se-Young Yun",
        "Jamin Shin"
      ],
      "abstract": "Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches. Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity , while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation , where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework ( gWorld ) that automatically synthesizes code-based training data . In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.",
      "summary_en": "Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.",
      "summary_zh": "é€šè¿‡è§†è§‰-è¯­è¨€æ¨¡å‹ç”Ÿæˆå¯æ¸²æŸ“ä»£ç ï¼Œæ”¹è¿›äº†ç§»åŠ¨GUIæ™ºèƒ½ä½“çš„è§†è§‰ä¸–ç•Œæ¨¡å‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œä»¥æ›´å°çš„æ¨¡å‹å°ºå¯¸å®ç°äº†æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01576",
      "arxiv_url": "https://arxiv.org/abs/2602.01576",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01576",
      "github_url": "https://github.com/trillion-labs/gWorld",
      "upvotes": 41,
      "fetched_at": "2026-02-19T05:36:59.609126+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02338",
      "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
      "authors": [
        "Yu Liang",
        "Zhongjin Zhang",
        "Yuxuan Zhu",
        "Kerui Zhang",
        "Zhiluohan Guo",
        "Wenhang Zhou",
        "Zonqi Yang",
        "Kangle Wu",
        "Yabo Ni",
        "Anxiang Zeng",
        "Cong Fu",
        "Jianxin Wang",
        "Jiazhi Xia"
      ],
      "abstract": "ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability. Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems , but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction , and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling . To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability , without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.",
      "summary_en": "ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.",
      "summary_zh": "ReSID æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨èåŸç”Ÿæ¡†æ¶ï¼Œé€šè¿‡å­¦**ä¹ é¢„æµ‹æ€§ç‰©å“è¡¨ç¤ºå¹¶ä¼˜åŒ–é‡åŒ–ä»¥ä¿ç•™ä¿¡æ¯å¹¶ä¿æŒåºåˆ—å¯é¢„æµ‹æ€§ï¼Œä»è€Œæ”¹è¿›åºåˆ—æ¨èã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02338",
      "arxiv_url": "https://arxiv.org/abs/2602.02338",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02338",
      "github_url": "https://github.com/FuCongResearchSquad/ReSID",
      "upvotes": 40,
      "fetched_at": "2026-02-19T05:37:51.565193+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02053",
      "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
      "authors": [
        "Pengyu Wang",
        "Benfeng Xu",
        "Licheng Zhang",
        "Shaohan Wang",
        "Mingxuan Du",
        "Chiwei Zhu",
        "Zhendong Mao"
      ],
      "abstract": "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types. Graph-based Retrieval-Augmented Generation ( GraphRAG ) organizes external knowledge as a hierarchical graph , enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge , failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents . To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia 's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization . Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.",
      "summary_en": "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.",
      "summary_zh": "WildGraphBench ä½¿ç”¨ Wikipedia çš„ç»“æ„åŒ–å†…å®¹ï¼Œåœ¨çœŸå®åœºæ™¯ä¸­è¯„ä¼° GraphRAG è·¨å¤šç§æ–‡æ¡£ç±»å‹çš„å¤šäº‹å®èšåˆä¸æ€»ç»“èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02053",
      "arxiv_url": "https://arxiv.org/abs/2602.02053",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02053",
      "github_url": "https://github.com/BstWPY/WildGraphBench",
      "upvotes": 40,
      "fetched_at": "2026-02-19T05:37:32.161403+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01058",
      "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
      "authors": [
        "Dylan Zhang",
        "Yufeng Xu",
        "Haojin Wang",
        "Qingzhi Chen",
        "Hao Peng"
      ],
      "abstract": "Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss. Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone. We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts. We propose PEAR ( Policy Evaluation -inspired Algorithm for Offline Learning Loss Re-weighting ), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected. We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek -distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.",
      "summary_en": "Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.",
      "summary_zh": "æ¨ç†å¤§è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒå¯é€šè¿‡å¯¹SFTæŸå¤±è¿›è¡Œé‡è¦æ€§é‡‡æ ·é‡åŠ æƒï¼Œçº æ­£ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…ï¼Œä»è€Œå¾—åˆ°æ”¹è¿›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01058",
      "arxiv_url": "https://arxiv.org/abs/2602.01058",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01058",
      "github_url": "",
      "upvotes": 40,
      "fetched_at": "2026-02-19T05:36:35.802552+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02453",
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "authors": [
        "Andong Chen",
        "Wenxin Zhu",
        "Qiuyu Ding",
        "Yuchen Song",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance. Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure , while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure , embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning .",
      "summary_en": "Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.",
      "summary_zh": "Thinking with Comics æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§†è§‰æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨æ¼«ç”»ç»“æ„æ¡¥æ¥å›¾åƒä¸è§†é¢‘ï¼Œä»¥æå‡å¤šæ¨¡æ€æ¨ç†çš„æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02453",
      "arxiv_url": "https://arxiv.org/abs/2602.02453",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02453",
      "github_url": "https://github.com/andongBlue/Think-with-Comics",
      "upvotes": 35,
      "fetched_at": "2026-02-19T05:38:02.172058+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01590",
      "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
      "authors": [
        "Shaohan Wang",
        "Benfeng Xu",
        "Licheng Zhang",
        "Mingxuan Du",
        "Chiwei Zhu",
        "Xiaorui Wang",
        "Zhendong Mao",
        "Yongdong Zhang"
      ],
      "abstract": "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.",
      "summary_en": "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.",
      "summary_zh": "Deep Research Agentså±•ç°äº†è‡ªä¸»ä¿¡æ¯æ£€ç´¢çš„èƒ½åŠ›ï¼Œä½†åœ¨ä½¿ç”¨æ–°çš„å®æ—¶åŸºå‡†æµ‹è¯•å’Œç»¼åˆè¯„ä¼°æ¡†æ¶å¯¹ç…§ä¸“å®¶çº§Wikipediaæ–‡ç« è¿›è¡Œè¯„ä¼°æ—¶ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01590",
      "arxiv_url": "https://arxiv.org/abs/2602.01590",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01590",
      "github_url": "https://github.com/WangShao2000/Wiki_Live_Challenge",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:37:01.181665+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02488",
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "authors": [
        "Yinjie Wang",
        "Tianbao Xie",
        "Ke Shen",
        "Mengdi Wang",
        "Ling Yang"
      ],
      "abstract": "RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training. We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization , amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals , while the reward model is jointly optimized via consistency feedback , which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench , respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
      "summary_en": "RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.",
      "summary_zh": "RLAnythingé€šè¿‡åŠ¨æ€æ¨¡å‹ä¼˜åŒ–å’Œé—­ç¯åé¦ˆæœºåˆ¶ï¼Œå¢å¼ºé¢å‘LLMå’Œæ™ºèƒ½ä½“çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ”¹è¿›ç­–ç•¥ä¸å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02488",
      "arxiv_url": "https://arxiv.org/abs/2602.02488",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02488",
      "github_url": "https://github.com/Gen-Verse/Open-AgentRL",
      "upvotes": 32,
      "fetched_at": "2026-02-19T05:38:09.586429+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02383",
      "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
      "authors": [
        "Maksim Afanasyev",
        "Illarion Iov"
      ],
      "abstract": "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints. Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback ( RLHF ) for aligning Large Language Models ( LLMs ). Latest approaches have streamlined the alignment process by deriving implicit reward functions , yet they often suffer from a critical objective mismatch : optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood . This can lead to `` unlearning '', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and `` formatting collapse '' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability .",
      "summary_en": "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.",
      "summary_zh": "SLIMEæ˜¯ä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹æ— å‚è€ƒå¯¹é½ç›®æ ‡ï¼Œé€šè¿‡ç»“åˆä¼¼ç„¶æœ€å¤§åŒ–ã€æ¦‚ç‡ç¨³å®šåŒ–å’ŒåŒè¾¹ç•Œçº¦æŸçš„ä¸‰æ–¹é¢æ–¹æ³•ï¼Œå°†åå¥½å­¦ä¹ ä¸ç”Ÿæˆè´¨é‡è§£è€¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02383",
      "arxiv_url": "https://arxiv.org/abs/2602.02383",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02383",
      "github_url": "https://github.com/fpsigma/trl-slime",
      "upvotes": 29,
      "fetched_at": "2026-02-19T05:37:58.687613+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01801",
      "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
      "authors": [
        "Dvir Samuel",
        "Issar Tzachor",
        "Matan Levy",
        "Micahel Green",
        "Gal Chechik",
        "Rami Ben-Ari"
      ],
      "abstract": "Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance. Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; Ann CA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor ( ANN ) matching; and Ann SA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN . Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.",
      "summary_en": "Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.",
      "summary_zh": "è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹å› KVç¼“å­˜å¢é•¿å’Œæ³¨æ„åŠ›è®¡ç®—å†—ä½™é¢ä¸´æ•ˆç‡æŒ‘æˆ˜ï¼ŒTempCacheã€AnnCAå’ŒAnnSAæŠ€æœ¯é€šè¿‡é™ä½è®¡ç®—éœ€æ±‚å¹¶ä¿æŒè§†è§‰è´¨é‡ä¸ç¨³å®šæ€§èƒ½è§£å†³è¿™äº›é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01801",
      "arxiv_url": "https://arxiv.org/abs/2602.01801",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01801",
      "github_url": "",
      "upvotes": 28,
      "fetched_at": "2026-02-19T05:37:12.726742+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02214",
      "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
      "authors": [
        "Hongzhou Zhu",
        "Min Zhao",
        "Guande He",
        "Hang Su",
        "Chongxuan Li",
        "Jun Zhu"
      ],
      "abstract": "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance. To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention . However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation , which requires frame-level injectivity , where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution , which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}",
      "summary_en": "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.",
      "summary_zh": "ä¸€ç§æ–°é¢–çš„å› æœå¼ºåˆ¶æ–¹æ³•é€šè¿‡ä½¿ç”¨ARæ•™å¸ˆæ¨¡å‹è¿›è¡ŒODEåˆå§‹åŒ–ï¼Œè§£å†³äº†å°†åŒå‘è§†é¢‘æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºè‡ªå›å½’æ¨¡å‹æ—¶å­˜åœ¨çš„æ¶æ„å·®è·é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02214",
      "arxiv_url": "https://arxiv.org/abs/2602.02214",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02214",
      "github_url": "https://github.com/thu-ml/Causal-Forcing",
      "upvotes": 24,
      "fetched_at": "2026-02-19T05:37:44.312354+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01624",
      "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
      "authors": [
        "Minh-Quan Le",
        "Gaurav Mittal",
        "Cheng Zhao",
        "David Gu",
        "Dimitris Samaras",
        "Mei Chen"
      ],
      "abstract": "PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations. Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning .",
      "summary_en": "PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.",
      "summary_zh": "PISCESæ˜¯ä¸€ç§æ— éœ€æ ‡æ³¨çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡åŒé‡æœ€ä¼˜ä¼ è¾“å¯¹é½å¥–åŠ±æå‡è§†è§‰è´¨é‡ä¸è¯­ä¹‰å¯¹é½ï¼Œæ— éœ€äººç±»åå¥½æ ‡æ³¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01624",
      "arxiv_url": "https://arxiv.org/abs/2602.01624",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01624",
      "github_url": "",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:37:04.889547+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01395",
      "title": "Rethinking Selective Knowledge Distillation",
      "authors": [
        "Almog Tavor",
        "Itay Ebenspanger",
        "Neil Cnaan",
        "Mor Geva"
      ],
      "abstract": "Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements. Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation , which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation . Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.",
      "summary_en": "Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.",
      "summary_zh": "åŸºäºå­¦ç”Ÿç†µå¼•å¯¼ä½ç½®é€‰æ‹©çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦å¯æå‡å‡†ç¡®ç‡ä¸æ•ˆç‡ï¼ŒåŒæ—¶é™ä½å†…å­˜å’Œå­˜å‚¨éœ€æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01395",
      "arxiv_url": "https://arxiv.org/abs/2602.01395",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01395",
      "github_url": "https://github.com/almogtavor/SE-KD3x",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:36:46.441147+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01756",
      "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
      "authors": [
        "Jun He",
        "Junyan Ye",
        "Zilong Huang",
        "Dongzhi Jiang",
        "Chenjue Zhang",
        "Leqi Zhu",
        "Renrui Zhang",
        "Xiang Zhang",
        "Weijia Li"
      ],
      "abstract": "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.",
      "summary_en": "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.",
      "summary_zh": "Mind-Brush æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„ agentic æ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œè¯¥æ¡†æ¶åŠ¨æ€æ£€ç´¢å¤šæ¨¡æ€è¯æ®å¹¶é‡‡ç”¨æ¨ç†å·¥å…·ï¼Œä»¥æå‡å¯¹éšå¼ç”¨æˆ·æ„å›¾å’Œå¤æ‚çŸ¥è¯†æ¨ç†çš„ç†è§£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01756",
      "arxiv_url": "https://arxiv.org/abs/2602.01756",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01756",
      "github_url": "https://github.com/PicoTrex/Mind-Brush",
      "upvotes": 22,
      "fetched_at": "2026-02-19T05:37:10.900845+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02486",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches. LLM-based deep research agents are largely built on the ReAct framework . This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning , reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning , achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "summary_en": "Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.",
      "summary_zh": "Re-TRACæ˜¯ä¸€ç§æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–çŠ¶æ€è¡¨ç¤ºå®ç°è·¨è½¨è¿¹æ¢ç´¢ä¸è¿­ä»£åæ€ï¼Œä»è€Œå¢å¼ºåŸºäºLLMçš„ç ”ç©¶æ™ºèƒ½ä½“ï¼Œç›¸è¾ƒäºä¼ ç»ŸReActæ–¹æ³•èƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆæœ‰æ•ˆçš„é—®é¢˜æ±‚è§£ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02486",
      "arxiv_url": "https://arxiv.org/abs/2602.02486",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02486",
      "github_url": "https://github.com/microsoft/InfoAgent",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:38:07.551172+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02092",
      "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
      "authors": [
        "FSVideo Team",
        "Qingyu Chen",
        "Zhiyuan Fang",
        "Haibin Huang",
        "Xinwei Huang",
        "Tong Jin",
        "Minxuan Lin",
        "Bo Liu",
        "Celong Liu",
        "Chongyang Ma",
        "Xing Mei",
        "Xiaohui Shen",
        "Yaojie Shen",
        "Fuwen Tan",
        "Angtian Wang",
        "Xiao Yang",
        "Yiding Yang",
        "Jiamin Yuan",
        "Lingxi Zhang",
        "Yuxin Zhang"
      ],
      "abstract": "FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.",
      "summary_en": "FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.",
      "summary_zh": "FSVideoæ˜¯ä¸€ç§åŸºäºtransformerçš„å¿«é€Ÿå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œé‡‡ç”¨å‹ç¼©è§†é¢‘è‡ªç¼–ç å™¨ã€å…·å¤‡å¢å¼ºå±‚è®°å¿†çš„æ‰©æ•£transformeræ¶æ„å’Œå¤šåˆ†è¾¨ç‡ç”Ÿæˆç­–ç•¥ï¼Œåœ¨æ˜¾è‘—å‡å°‘è®¡ç®—æ—¶é—´çš„åŒæ—¶å®ç°é«˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02092",
      "arxiv_url": "https://arxiv.org/abs/2602.02092",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02092",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:37:35.937436+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01479",
      "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
      "authors": [
        "Xueqing Peng",
        "Ruoyu Xiang",
        "Fan Zhang",
        "Mingzi Song",
        "Mingyang Jiang",
        "Yan Wang",
        "Lingfei Qian",
        "Taiki Hara",
        "Yuqing Guo",
        "Jimin Huang",
        "Junichi Tsujii",
        "Sophia Ananiadou"
      ],
      "abstract": "A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities. Japanese finance combines agglutinative, head-final linguistic structure , mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment , posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A , and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures . We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.",
      "summary_en": "A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities.",
      "summary_zh": "ä»‹ç»äº†ä¸€ä¸ªåä¸ºEbisuçš„æ—¥æœ¬é‡‘èè¯­è¨€ç†è§£åŸºå‡†ï¼ŒåŒ…å«ä¸¤ä¸ªä¸“å®¶æ ‡æ³¨ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°éšæ€§æ‰¿è¯ºè¯†åˆ«å’Œå±‚æ¬¡åŒ–é‡‘èæœ¯è¯­æŠ½å–ï¼Œæ­ç¤ºäº†å½“å‰è¯­è¨€æ¨¡å‹å°½ç®¡å…·å¤‡å…ˆè¿›èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´æŒç»­æ€§æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01479",
      "arxiv_url": "https://arxiv.org/abs/2602.01479",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01479",
      "github_url": "",
      "upvotes": 17,
      "fetched_at": "2026-02-19T05:36:50.350950+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01851",
      "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
      "authors": [
        "Huanyu Zhang",
        "Xuehai Bai",
        "Chengzu Li",
        "Chen Liang",
        "Haochen Tian",
        "Haodong Li",
        "Ruichuan An",
        "Yifan Zhang",
        "Anna Korhonen",
        "Zhang Zhang",
        "Liang Wang",
        "Tieniu Tan"
      ],
      "abstract": "Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models. Recent generative models have achieved remarkable progress in image editing . However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding , morphological manipulation , and causal reasoning . Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following . We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.",
      "summary_en": "Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.",
      "summary_zh": "é¢å‘å›¾åƒç¼–è¾‘çš„è§†è§‰æŒ‡ä»¤åŸºå‡†æå‡ºäº†ä¸€ä¸ªä¸‰çº§äº¤äº’å±‚æ¬¡ç»“æ„ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„è§†è§‰æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01851",
      "arxiv_url": "https://arxiv.org/abs/2602.01851",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01851",
      "github_url": "https://github.com/hwanyu112/VIBE-Benchmark",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:37:17.917457+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01541",
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "authors": [
        "Boyi Li",
        "Yifan Shen",
        "Yuanzhe Liu",
        "Yifan Xu",
        "Jiateng Liu",
        "Xinzhuo Li",
        "Zhengyuan Li",
        "Jingyuan Zhu",
        "Yunhan Zhong",
        "Fangzhou Lan",
        "Jianguo Cao",
        "James M. Rehg",
        "Heng Ji",
        "Ismini Lourentzou",
        "Xu Cao"
      ],
      "abstract": "MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.",
      "summary_en": "MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.",
      "summary_zh": "é…å¤‡è®¤çŸ¥è¶…æ„ŸçŸ¥ä¸æ½œåœ¨è§†è§‰æ„è±¡é¢„æµ‹çš„MLLMsé€šè¿‡é›†æˆçš„è§†è§‰å’Œæ–‡æœ¬æ¨ç†è·¯å¾„å±•ç°å‡ºå¢å¼ºçš„è®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01541",
      "arxiv_url": "https://arxiv.org/abs/2602.01541",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01541",
      "github_url": "https://github.com/PediaMedAI/Cognition-MLLM",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:36:55.996785+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01335",
      "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
      "authors": [
        "Yu Xu",
        "Yuxin Zhang",
        "Juan Cao",
        "Lin Gao",
        "Chunyu Wang",
        "Oliver Deussen",
        "Tong-Yee Lee",
        "Fan Tang"
      ],
      "abstract": "Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory. A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \" creative essence \" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic , component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.",
      "summary_en": "Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.",
      "summary_zh": "è§†è§‰éšå–»è¿ç§»ä½¿åˆ›é€ æ€§AIç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡åŸºäºè®¤çŸ¥ç†è®ºçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œä»å‚è€ƒå›¾åƒä¸­åˆ†è§£æŠ½è±¡æ¦‚å¿µå…³ç³»ï¼Œå¹¶å°†å…¶é‡æ–°åº”ç”¨äºæ–°ä¸»ä½“ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01335",
      "arxiv_url": "https://arxiv.org/abs/2602.01335",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01335",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:36:42.893765+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01538",
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "authors": [
        "Youliang Zhang",
        "Zhengguang Zhou",
        "Zhentao Yu",
        "Ziyao Huang",
        "Teng Hu",
        "Sen Liang",
        "Guozhen Zhang",
        "Ziqiao Peng",
        "Shunkai Li",
        "Yi Chen",
        "Zixiang Zhou",
        "Yuan Zhou",
        "Qinglin Lu",
        "Xiu Li"
      ],
      "abstract": "A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.",
      "summary_en": "A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åä¸º InteractAvatar çš„åŒæµæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆèƒ½å¤Ÿä¸ç¯å¢ƒç‰©ä½“äº¤äº’çš„è¯´è¯å¤´åƒï¼Œé€šè¿‡è§£è€¦çš„æ„ŸçŸ¥ä¸è§„åˆ’æ¨¡å—ï¼Œè§£å†³äº†çœŸå®åœºæ™¯äººç‰©äº¤äº’ä¸­çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01538",
      "arxiv_url": "https://arxiv.org/abs/2602.01538",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01538",
      "github_url": "https://github.com/angzong/InteractAvatar",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:36:54.161500+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01511",
      "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
      "authors": [
        "Ran Xu",
        "Tianci Liu",
        "Zihan Dong",
        "Tony You",
        "Ilgee Hong",
        "Carl Yang",
        "Linjun Zhang",
        "Tao Zhao",
        "Haoyu Wang"
      ],
      "abstract": "Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks. Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback . Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.",
      "summary_en": "Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.",
      "summary_zh": "Rubric-ARMæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆä¼˜åŒ–è¯„åˆ†æ ‡å‡†ç”Ÿæˆä¸è¯„åˆ¤ï¼Œä»¥æå‡åˆ›æ„æ€§å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸­çš„å›å¤è´¨é‡è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01511",
      "arxiv_url": "https://arxiv.org/abs/2602.01511",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01511",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-19T05:36:52.236345+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02343",
      "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach. Methods for controlling large language models (LLMs), including local weight fine-tuning , LoRA-based adaptation , and activation-based interventions , are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal , placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples . Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold . Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/ SPLIT .md.",
      "summary_en": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.",
      "summary_zh": "å¤§è¯­è¨€æ¨¡å‹æ§åˆ¶æ–¹æ³•ç»Ÿä¸€äºåŠ¨æ€æƒé‡æ›´æ–°æ¡†æ¶ï¼Œæ­ç¤ºåå¥½-æ•ˆç”¨æƒè¡¡ï¼Œå¹¶é€šè¿‡SPLITæ–¹æ³•å®ç°æ”¹è¿›çš„å¼•å¯¼ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02343",
      "arxiv_url": "https://arxiv.org/abs/2602.02343",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02343",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:37:53.607267+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00986",
      "title": "Sparse Reward Subsystem in Large Language Models",
      "authors": [
        "Guowei Xu",
        "Mert Yuksekgonul",
        "James Zou"
      ],
      "abstract": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments , we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.",
      "summary_en": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments , we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.",
      "summary_zh": "æœ¬æ–‡è¯†åˆ«å‡ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éšè—çŠ¶æ€ä¸­å­˜åœ¨ä¸€ä¸ªç¨€ç–å¥–åŠ±å­ç³»ç»Ÿï¼Œå¹¶å°†å…¶ç±»æ¯”äºäººè„‘ä¸­çš„ç”Ÿç‰©å¥–åŠ±å­ç³»ç»Ÿã€‚æˆ‘ä»¬è¯æ˜è¯¥å­ç³»ç»ŸåŒ…å«ä»·å€¼ç¥ç»å…ƒï¼Œè¿™äº›ç¥ç»å…ƒè¡¨å¾æ¨¡å‹å¯¹çŠ¶æ€ä»·å€¼çš„å†…éƒ¨é¢„æœŸï¼Œå¹¶é€šè¿‡å¹²é¢„å®éªŒç¡®ç«‹äº†è¿™äº›ç¥ç»å…ƒå¯¹æ¨ç†çš„é‡è¦æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›ä»·å€¼ç¥ç»å…ƒåœ¨å¤šæ ·åŒ–æ•°æ®é›†ã€ä¸åŒæ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸Šå‡å…·æœ‰é²æ£’æ€§ï¼›æ­¤å¤–ï¼Œå®ƒä»¬åœ¨æ¥è‡ªåŒä¸€åŸºç¡€æ¨¡å‹çš„ä¸åŒæ•°æ®é›†å’Œå¾®è°ƒæ¨¡å‹ä¹‹é—´è¡¨ç°å‡ºæ˜¾è‘—çš„å¯è¿ç§»æ€§ã€‚é€šè¿‡æ£€éªŒä»·å€¼é¢„æµ‹ä¸å®é™…å¥–åŠ±å‡ºç°åˆ†æ­§çš„æ¡ˆä¾‹ï¼Œæˆ‘ä»¬åœ¨å¥–åŠ±å­ç³»ç»Ÿä¸­è¯†åˆ«å‡ºç¼–ç å¥–åŠ±é¢„æµ‹è¯¯å·®ï¼ˆRPEï¼‰çš„å¤šå·´èƒºç¥ç»å…ƒã€‚å½“å¥–åŠ±é«˜äºé¢„æœŸæ—¶ï¼Œè¿™äº›ç¥ç»å…ƒè¡¨ç°å‡ºé«˜æ¿€æ´»ï¼›å½“å¥–åŠ±ä½äºé¢„æœŸæ—¶ï¼Œåˆ™è¡¨ç°å‡ºä½æ¿€æ´»ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00986",
      "arxiv_url": "https://arxiv.org/abs/2602.00986",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00986",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:36:34.126929+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.21123",
      "title": "CUA-Skill: Develop Skills for Computer Using Agent",
      "authors": [
        "Tianyi Chen",
        "Yinheng Li",
        "Michael Solodko",
        "Sen Wang",
        "Nan Jiang",
        "Tingyuan Cui",
        "Junheng Hao",
        "Jongwoo Ko",
        "Sara Abdali",
        "Suzhen Zheng",
        "Leon Xu",
        "Hao Fan",
        "Pashmina Cameron",
        "Justin Wagle",
        "Kazuhito Koishida"
      ],
      "abstract": "CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks. Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs . CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base , we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval , argument instantiation , and memory-aware failure recovery . Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena , CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.",
      "summary_en": "CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.",
      "summary_zh": "CUA-Skill å¼•å…¥äº†ä¸€ä¸ªå¤§è§„æ¨¡å·¥ç¨‹åŒ–è®¡ç®—æœºä½¿ç”¨æŠ€èƒ½åº“ï¼Œå¯æå‡æ™ºèƒ½ä½“åœ¨åŸºäºWindowsçš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21123",
      "arxiv_url": "https://arxiv.org/abs/2601.21123",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21123",
      "github_url": "https://github.com/microsoft/cua_skill",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:36:00.778763+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02156",
      "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
      "authors": [
        "Wen-Jie Shu",
        "Xuerui Qiu",
        "Rui-Jie Zhu",
        "Harold Haodong Chen",
        "Yexin Liu",
        "Harry Yang"
      ],
      "abstract": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters. Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark . However, we argue that the feed-forward architecture , where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence . Loop-ViT iterates a weight-tied Hybrid Block , combining local convolutions and global attention , to form a latent chain of thought . Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy : the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
      "summary_en": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.",
      "summary_zh": "Loop-ViTå¼•å…¥äº†ä¸€ç§é€’å½’è§†è§‰Transformeræ¶æ„ï¼Œé€šè¿‡æƒé‡å…±äº«çš„é€’å½’å’ŒåŠ¨æ€é€€å‡ºæœºåˆ¶å°†æ¨ç†æ·±åº¦ä¸æ¨¡å‹å®¹é‡è§£è€¦ï¼Œä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†æ›´ä¼˜çš„è§†è§‰æ¨ç†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02156",
      "arxiv_url": "https://arxiv.org/abs/2602.02156",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02156",
      "github_url": "https://github.com/WenjieShu/LoopViT",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:37:39.963904+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02477",
      "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
      "authors": [
        "Xiao Liang",
        "Zhong-Zhi Li",
        "Zhenghao Lin",
        "Eric Hancheng Jiang",
        "Hengyuan Zhang",
        "Yelong Shen",
        "Kai-Wei Chang",
        "Ying Nian Wu",
        "Yeyun Gong",
        "Weizhu Chen"
      ],
      "abstract": "An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks. Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability . A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability , surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.",
      "summary_en": "An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.",
      "summary_zh": "ä¸€ç§ç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡åˆ†æ²»ç­–ç•¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥ç­–ç•¥åœ¨æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02477",
      "arxiv_url": "https://arxiv.org/abs/2602.02477",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02477",
      "github_url": "https://github.com/MasterVito/DAC-RL",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:38:05.830960+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02227",
      "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
      "authors": [
        "Harold Haodong Chen",
        "Xinxiang Yin",
        "Wen-Jie Shu",
        "Hongfei Zhang",
        "Zixin Zhang",
        "Chenfei Liao",
        "Litao Guo",
        "Qifeng Chen",
        "Ying-Cong Chen"
      ],
      "abstract": "LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.",
      "summary_en": "LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.",
      "summary_zh": "LatentMorphé€šè¿‡å››ä¸ªè½»é‡çº§ç»„ä»¶å°†éšå¼æ½œåœ¨æ¨ç†æ•´åˆè‡³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œå®ç°è‡ªé€‚åº”è‡ªæˆ‘ä¼˜åŒ–ï¼Œå¹¶æå‡æ•ˆç‡ä¸è®¤çŸ¥å¯¹é½åº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02227",
      "arxiv_url": "https://arxiv.org/abs/2602.02227",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02227",
      "github_url": "https://github.com/EnVision-Research/LatentMorph",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:37:45.878233+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.20613",
      "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
      "authors": [
        "Kaiyuan Chen",
        "Qimin Wu",
        "Taiyu Hou",
        "Tianhao Tang",
        "Xueyu Hu",
        "Yuchen Hou",
        "Bikun Li",
        "Chengming Qian",
        "Guoyin Wang",
        "Haolin Chen",
        "Haotong Tian",
        "Haoye Zhang",
        "Haoyu Bian",
        "Hongbing Pan",
        "Hongkang Zhang",
        "Hongyi Zhou",
        "Jiaqi Cai",
        "Jiewu Rao",
        "Jiyuan Ren",
        "Keduan Huang",
        "Lucia Zhu Huang",
        "Mingyu Yuan"
      ],
      "abstract": "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories. The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution , which assesses adherence to explicit and complex workflows; Latent Instruction , which requires agents to infer implicit instructions from attachments; and Iterative Refinement , which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities , enabling AI application teams to develop cutting-edge Agent products.",
      "summary_en": "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.",
      "summary_zh": "AgentIF-OneDayè¯„ä¼°AIæ™ºèƒ½ä½“é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¤„ç†å¤šæ ·åŒ–æ—¥å¸¸ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¦æ±‚å…·å¤‡é—®é¢˜è§£å†³ã€é™„ä»¶ç†è§£å’ŒåŸºäºæ–‡ä»¶çš„è¾“å‡ºèƒ½åŠ›ï¼Œæ¶µç›–ä¸‰ä¸ªä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„ç±»åˆ«ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.20613",
      "arxiv_url": "https://arxiv.org/abs/2601.20613",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.20613",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:35:59.058238+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01675",
      "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
      "authors": [
        "Yuanzhe Shen",
        "Zisu Huang",
        "Zhengyuan Wang",
        "Muzhao Tian",
        "Zhengkang Guo",
        "Chenyang Zhang",
        "Shuaiyu Zhou",
        "Zengjie Hu",
        "Dailin Li",
        "Jingwen Xu",
        "Kaimin Wang",
        "Wenhao Liu",
        "Tianlong Li",
        "Fengpeng Yue",
        "Feng Hong",
        "Cao Liu",
        "Ke Zeng"
      ],
      "abstract": "TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues. As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions . To bridge this gap, we introduce TRIP-Bench, a long-horizon benchmark grounded in realistic travel-planning scenarios . TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation . It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose GTPO, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing . Applied to Qwen2.5-32B-Instruct , GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.",
      "summary_en": "TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.",
      "summary_zh": "TRIP-Benchæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ—…è¡Œè§„åˆ’é•¿æœŸåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMæ™ºèƒ½ä½“åœ¨å¤æ‚å¤šè½®äº¤äº’ä¸­çš„è¡¨ç°ï¼›GTPOåˆ™æä¾›äº†ä¸€ç§åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¢å¼ºæ‰©å±•å¯¹è¯ä¸­çš„çº¦æŸæ»¡è¶³å’Œé²æ£’æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01675",
      "arxiv_url": "https://arxiv.org/abs/2602.01675",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01675",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:37:08.962987+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01382",
      "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation",
      "authors": [
        "Fu-Yun Wang",
        "Han Zhang",
        "Michael Gharbi",
        "Hongsheng Li",
        "Taesung Park"
      ],
      "abstract": "Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements. Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting , where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval , 0.98 on OCR accuracy , and 24.05 on PickScore . Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2times fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/ UniRL .",
      "summary_en": "Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.",
      "summary_zh": "é¢å‘æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æµåŒ¹é…æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¾—åˆ°å¢å¼ºï¼Œè¯¥æ¡†æ¶å¼•å…¥è¯­è¨€æ¨¡å‹è¿›è¡Œæç¤ºè¯ä¼˜åŒ–ï¼Œè§£å†³äº†æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œæç¤ºè¯è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œåœ¨é™ä½è®¡ç®—éœ€æ±‚çš„åŒæ—¶å®ç°äº†æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01382",
      "arxiv_url": "https://arxiv.org/abs/2602.01382",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01382",
      "github_url": "https://github.com/G-U-N/UniRL",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:36:44.632399+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01322",
      "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
      "authors": [
        "Panagiotis Koromilas",
        "Andreas D. Demou",
        "James Oldfield",
        "Yannis Panagakis",
        "Mihalis Nicolaou"
      ],
      "abstract": "PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability. Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms . However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10times larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. r = 0.82 for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition , largely independent of surface statistics.",
      "summary_en": "PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.",
      "summary_zh": "PolySAE é€šè¿‡å¤šé¡¹å¼è§£ç æ‰©å±•ç¨€ç–è‡ªç¼–ç å™¨ï¼Œåœ¨ä¿æŒçº¿æ€§ç¼–ç å™¨ä»¥ç¡®ä¿å¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæ•æ‰ç‰¹å¾äº¤äº’ä¸ç»„åˆç»“æ„ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01322",
      "arxiv_url": "https://arxiv.org/abs/2602.01322",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01322",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:36:41.245119+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01660",
      "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
      "authors": [
        "Zhongyuan Peng",
        "Caijun Xu",
        "Changyi Xiao",
        "Shibo Hong",
        "Eli Zhang",
        "Stephen Huang",
        "Yixin Cao"
      ],
      "abstract": "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance. Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions . However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation ), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation , making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance , verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus , CoDiQ-Generator , and implementations to support related research.",
      "summary_en": "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.",
      "summary_zh": "ä¸€ç§åä¸ºCoDiQçš„æ–°é¢–æ¡†æ¶é€šè¿‡æµ‹è¯•æ—¶ç¼©æ”¾å®ç°ç«èµ›çº§é—®é¢˜çš„å¯æ§éš¾åº¦ç”Ÿæˆï¼Œç”±æ­¤äº§ç”Ÿçš„è¯­æ–™åº“æ˜¾è‘—æå‡äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01660",
      "arxiv_url": "https://arxiv.org/abs/2602.01660",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01660",
      "github_url": "https://github.com/ALEX-nlp/CoDiQ",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:37:07.065405+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00269",
      "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models",
      "authors": [
        "Keisuke Kamahori",
        "Wei-Tzu Lee",
        "Atindra Jha",
        "Rohan Kadekodi",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Baris Kasikci"
      ],
      "abstract": "VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines. Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency , high throughput , and strong guarantees of streamability . Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.",
      "summary_en": "VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.",
      "summary_zh": "VoxServeæ˜¯ä¸€ä¸ªé¢å‘è¯­éŸ³è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æœåŠ¡ç³»ç»Ÿï¼Œé€šè¿‡æ¨¡å‹æ‰§è¡ŒæŠ½è±¡ã€æµå¼æ„ŸçŸ¥è°ƒåº¦å’Œå¼‚æ­¥æ¨ç†æµæ°´çº¿æå‡æµå¼æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00269",
      "arxiv_url": "https://arxiv.org/abs/2602.00269",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00269",
      "github_url": "https://github.com/vox-serve/vox-serve",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:36:26.308271+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02110",
      "title": "An Empirical Study of World Model Quantization",
      "authors": [
        "Zhongqian Fu",
        "Tianyi Zhao",
        "Kai Han",
        "Hang Zhou",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "abstract": "Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability. World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizon s up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts , activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor module s. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.",
      "summary_en": "Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.",
      "summary_zh": "ä¸–ç•Œæ¨¡å‹ä¸­çš„è®­ç»ƒåé‡åŒ–æ•ˆåº”å‘ˆç°å‡ºç‹¬ç‰¹çš„å¤±æ•ˆæ¨¡å¼ï¼Œä»¥åŠç²¾åº¦ã€ä½å®½ä¸è§„åˆ’æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼–ç å™¨-é¢„æµ‹å™¨æ¨¡å—ä¸å¯¹ç§°æ€§å’Œä½æ¯”ç‰¹ rollout ç¨³å®šæ€§æ–¹é¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02110",
      "arxiv_url": "https://arxiv.org/abs/2602.02110",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02110",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:37:38.177263+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02039",
      "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
      "authors": [
        "Wei Liu",
        "Peijie Yu",
        "Michele Orini",
        "Yali Du",
        "Yulan He"
      ],
      "abstract": "Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries. The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence , distinguishing it from executional intelligence , which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.",
      "summary_en": "Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.",
      "summary_zh": "æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹éœ€è¦æ¢ç©¶æ™ºèƒ½æ¥å®ç°è‡ªä¸»æ•°æ®åˆ†æï¼ŒDeep Data Research åŸºå‡†é€šè¿‡è¯„ä¼°å…¶åœ¨æ²¡æœ‰æ˜¾å¼æŸ¥è¯¢çš„æƒ…å†µä¸‹ä»æ•°æ®åº“ä¸­æå–æ´å¯Ÿçš„èƒ½åŠ›éªŒè¯äº†è¿™ä¸€ç‚¹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02039",
      "arxiv_url": "https://arxiv.org/abs/2602.02039",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02039",
      "github_url": "https://github.com/thinkwee/DDR_Bench",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:37:30.291928+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01984",
      "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling",
      "authors": [
        "Minyoung Lee",
        "Yeji Park",
        "Dongjun Hwang",
        "Yejin Kim",
        "Seong Joon Oh",
        "Junsuk Choe"
      ],
      "abstract": "Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance. Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage , where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage . To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens . This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions . Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis , MuirBench , MIRB , and QBench2 . We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench , MultiNews , and WCEP-10 . Notably, our method requires no additional training or inference cost.",
      "summary_en": "Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.",
      "summary_zh": "ç¼©æ”¾è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­åˆ†éš”ç¬¦tokençš„éšè—çŠ¶æ€ï¼Œå¯å‡å°‘è·¨å›¾åƒä¿¡æ¯æ³„æ¼å¹¶æå‡å¤šå›¾åƒæ¨ç†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01984",
      "arxiv_url": "https://arxiv.org/abs/2602.01984",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01984",
      "github_url": "https://github.com/MYMY-young/DelimScaling",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:37:25.927516+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00759",
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "authors": [
        "Zhipeng Chen",
        "Xiaobo Qin",
        "Wayne Xin Zhao",
        "Youbin Wu",
        "Ji-Rong Wen"
      ],
      "abstract": "Adaptive Ability Decomposing (AÂ²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model. Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration , which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions . Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer , revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner 's exploration and exploitation abilities.",
      "summary_en": "Adaptive Ability Decomposing (AÂ²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.",
      "summary_zh": "AÂ²Dï¼ˆè‡ªé€‚åº”èƒ½åŠ›åˆ†è§£ï¼‰é€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜æ¥å¢å¼ºå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¼•å¯¼å¼æ¢ç´¢æå‡LLMæ¨ç†èƒ½åŠ›ï¼Œæ— éœ€æ•™å¸ˆæ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00759",
      "arxiv_url": "https://arxiv.org/abs/2602.00759",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00759",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:30.170477+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22674",
      "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
      "authors": [
        "Hanxun Yu",
        "Wentong Li",
        "Xuan Qu",
        "Song Wang",
        "Junbo Chen",
        "Jianke Zhu"
      ],
      "abstract": "VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss. Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens , particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
      "summary_en": "VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.",
      "summary_zh": "VisionTrimæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©ä¸»å¯¼è§†è§‰tokenå¹¶å°†å…¶ä¸æ–‡æœ¬å¼•å¯¼çš„è¡¥å……ä¿¡æ¯ç›¸èåˆï¼ŒåŠ é€Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸æŸå¤±æ€§èƒ½çš„æƒ…å†µä¸‹æå‡æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22674",
      "arxiv_url": "https://arxiv.org/abs/2601.22674",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22674",
      "github_url": "https://github.com/hanxunyu/VisionTrim",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:14.905394+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22599",
      "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
      "authors": [
        "Kai Li",
        "Jintao Cheng",
        "Chang Zeng",
        "Zijun Yan",
        "Helin Wang",
        "Zixiong Su",
        "Bo Zheng",
        "Xiaolin Hu"
      ],
      "abstract": "Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements. Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events . These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol . Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency , offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.",
      "summary_en": "Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.",
      "summary_zh": "åˆ©ç”¨æ¥è‡ªçœŸå®åœºæ™¯æ•°æ®é›†çš„é«˜çº¯åº¦å•äº‹ä»¶ç‰‡æ®µè¿›è¡Œå£°éŸ³åˆ†ç¦»çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼Œä»¥æ˜¾è‘—é™ä½çš„æ•°æ®éœ€æ±‚å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22599",
      "arxiv_url": "https://arxiv.org/abs/2601.22599",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22599",
      "github_url": "https://github.com/ShandaAI/Hive",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:12.691886+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22588",
      "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
      "authors": [
        "Zhuochun Li",
        "Yong Zhang",
        "Ming Li",
        "Yuelyu Ji",
        "Yiming Zeng",
        "Ning Cheng",
        "Yun Zhu",
        "Yanmeng Wang",
        "Shaojun Wang",
        "Jing Xiao",
        "Daqing He"
      ],
      "abstract": "Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework. Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \" LLM-as-a-Judge \" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states . This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge , a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.",
      "summary_en": "Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.",
      "summary_zh": "å°å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å†…éƒ¨è¡¨å¾è€Œéç”Ÿæˆå›å¤æ¥æœ‰æ•ˆè¯„ä¼°è¾“å‡ºï¼Œé€šè¿‡åŸºäºæ¢é’ˆçš„æ¡†æ¶å®ç°æ›´é«˜æ•ˆä¸”å¯è§£é‡Šçš„è¯„ä¼°æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22588",
      "arxiv_url": "https://arxiv.org/abs/2601.22588",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22588",
      "github_url": "https://github.com/zhuochunli/Representation-as-a-judge",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:10.479757+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01997",
      "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
      "authors": [
        "Safal Shrestha",
        "Anubhav Shrestha",
        "Aadim Nepal",
        "Minwu Kim",
        "Keith Ross"
      ],
      "abstract": "Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses. Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks . Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses . This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.",
      "summary_en": "Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.",
      "summary_zh": "å±‚å‰ªæèƒ½å¤Ÿå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹å¹¶ä¿æŒåˆ†ç±»æ€§èƒ½ï¼Œä½†ä¼šå¯¼è‡´ç”Ÿæˆå¼æ¨ç†ä»»åŠ¡çš„æ˜¾è‘—é€€åŒ–ï¼Œä¸”é€šè¿‡è‡ªç”Ÿæˆå›å¤çš„ç›‘ç£å¾®è°ƒä»…èƒ½æœ‰é™æ¢å¤ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01997",
      "arxiv_url": "https://arxiv.org/abs/2602.01997",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01997",
      "github_url": "https://github.com/safal312/on-the-limits-of-layer-pruning",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:37:28.103392+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01296",
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "authors": [
        "Zeran Ke",
        "Bin Tan",
        "Gui-Song Xia",
        "Yujun Shen",
        "Nan Xue"
      ],
      "abstract": "LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments. 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch . We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping , not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization , establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.",
      "summary_en": "LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.",
      "summary_zh": "LiP-Mapæå‡ºäº†ä¸€ç§çº¿-é¢è”åˆä¼˜åŒ–æ¡†æ¶ï¼Œæ˜¾å¼å»ºæ¨¡å¯å­¦ä¹ çš„çº¿åŸºå…ƒå’Œå¹³é¢åŸºå…ƒï¼Œä»¥åœ¨äººé€ ç¯å¢ƒä¸­å®ç°ç²¾ç¡®çš„3Dçº¿åœ°å›¾æ„å»ºã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01296",
      "arxiv_url": "https://arxiv.org/abs/2602.01296",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01296",
      "github_url": "https://github.com/calmke/LiPMAP",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:36:39.324302+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01842",
      "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
      "authors": [
        "Jinbin Bai",
        "Yixuan Li",
        "Yuchen Zhu",
        "Yi Xin",
        "Qingyu Shi",
        "Aosong Feng",
        "Xiaohong Liu",
        "Molei Tao",
        "Jianru Xue",
        "Xiangtai Li",
        "Ming-Hsuan Yang"
      ],
      "abstract": "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms. Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding , which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window , (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
      "summary_en": "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.",
      "summary_zh": "é’ˆå¯¹ç¦»æ•£æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPrismçš„æ–°å‹æµ‹è¯•æ—¶æ‰©å±•æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚è½¨è¿¹æœç´¢ã€å±€éƒ¨åˆ†æ”¯ä¸éƒ¨åˆ†é‡æ©ç åŠè‡ªéªŒè¯åé¦ˆæœºåˆ¶æå‡æ¨ç†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01842",
      "arxiv_url": "https://arxiv.org/abs/2602.01842",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01842",
      "github_url": "https://github.com/viiika/Prism",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:37:16.135176+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01077",
      "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
      "authors": [
        "Haopeng Li",
        "Shitong Shao",
        "Wenliang Zhong",
        "Zikai Zhou",
        "Lichen Bai",
        "Hui Xiong",
        "Zeke Xie"
      ],
      "abstract": "PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality. Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention . While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub- quadratic complexity . Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion . This design allows PISA to serve as a faithful proxy to full attention , effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality . Code is available at: https://github.com/xie-lab-ml/piecewise-sparse- attention .",
      "summary_en": "PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.",
      "summary_zh": "PISAæ˜¯ä¸€ç§æ–°é¢–çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œé€šè¿‡è¿‘ä¼¼éå…³é”®æ³¨æ„åŠ›å—è€Œéç›´æ¥ä¸¢å¼ƒï¼Œæå‡æ‰©æ•£Transformerçš„æ•ˆç‡ï¼Œåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶å®ç°æ›´å¿«å¤„ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01077",
      "arxiv_url": "https://arxiv.org/abs/2602.01077",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01077",
      "github_url": "https://github.com/xie-lab-ml/piecewise-sparse-attention",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:36:37.264517+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00130",
      "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
      "authors": [
        "Sumit Yadav"
      ],
      "abstract": "Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy. We investigate the relationship between representation geometry and neural network performance . Analyzing 52 pretrained ImageNet models across 13 architecture families , we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance , computed entirely without labels.",
      "summary_en": "Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.",
      "summary_zh": "æœ‰æ•ˆç»´åº¦æ˜¯ä¸€ç§æ— ç›‘ç£å‡ ä½•åº¦é‡ï¼Œå¯å¼ºé¢„æµ‹ä¸åŒæ¶æ„ä¸é¢†åŸŸä¸­çš„ç¥ç»ç½‘ç»œæ€§èƒ½ï¼Œå¹¶æ­ç¤ºè¡¨ç¤ºå‡ ä½•ä¸å‡†ç¡®ç‡ä¹‹é—´çš„åŒå‘å› æœå…³ç³»ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00130",
      "arxiv_url": "https://arxiv.org/abs/2602.00130",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00130",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:36:20.243558+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01983",
      "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
      "authors": [
        "Xintian Shen",
        "Jiawei Chen",
        "Lihao Zheng",
        "Hao Ma",
        "Tao Wei",
        "Kun Zhan"
      ],
      "abstract": "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation. Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
      "summary_en": "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.",
      "summary_zh": "ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡†æ¶ä½¿è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªåŠ¨åˆ›å»ºå’Œä¼˜åŒ–å·¥å…·ï¼Œé€šè¿‡è‡ªæˆ‘è¿›åŒ–å’Œè®°å¿†å·©å›ºæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01983",
      "arxiv_url": "https://arxiv.org/abs/2602.01983",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01983",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:37:24.084259+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01970",
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "authors": [
        "Yun Qu",
        "Qi Wang",
        "Yixiu Mao",
        "Heming Zou",
        "Yuhang Jiang",
        "Weijie Liu",
        "Clive Bai",
        "Kai Yang",
        "Yangkun Chen",
        "Saiyong Yang",
        "Xiangyang Ji"
      ],
      "abstract": "Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance. Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization . Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency . However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency , final performance, and test-time efficiency over superior baseline methods.",
      "summary_en": "Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.",
      "summary_zh": "å¯æ³›åŒ–é¢„æµ‹æ€§æç¤ºé€‰æ‹©ï¼ˆGPSï¼‰åˆ©ç”¨è´å¶æ–¯æ¨æ–­å’Œè½»é‡çº§ç”Ÿæˆæ¨¡å‹ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è¯­è¨€æ¨¡å‹é«˜æ•ˆé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„æç¤ºï¼Œä»è€Œæå‡è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01970",
      "arxiv_url": "https://arxiv.org/abs/2602.01970",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01970",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:37:21.870713+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01618",
      "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
      "authors": [
        "Panuthep Tasawong",
        "Jian Gang Ngui",
        "Alham Fikri Aji",
        "Trevor Cohn",
        "Peerat Limkonchotiwat"
      ],
      "abstract": "Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance. Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.",
      "summary_en": "Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.",
      "summary_zh": "ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§æ–°å‹æ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºä¸ºä¸œå—äºšåˆ›å»ºå…·æœ‰æ–‡åŒ–æ ¹åŸºçš„å®‰å…¨æ•°æ®é›†ï¼Œç”±æ­¤æ„å»ºçš„å¤šè¯­è¨€å®‰å…¨é˜²æŠ¤æ¨¡å‹åœ¨æ£€æµ‹åŒºåŸŸæ€§æ•æ„Ÿå†…å®¹æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†é€šç”¨å®‰å…¨æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01618",
      "arxiv_url": "https://arxiv.org/abs/2602.01618",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01618",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:37:02.879281+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.23000",
      "title": "Mano: Restriking Manifold Optimization for LLM Training",
      "authors": [
        "Yufei Gu",
        "Zeke Xie"
      ],
      "abstract": "A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements. While large language models ( LLMs ) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizer s, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs , which may address both optimizer s' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold , we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizer s. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity , respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.",
      "summary_en": "A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åä¸ºManoçš„æ–°å‹ä¼˜åŒ–å™¨ï¼Œå®ƒç»“åˆäº†æµå½¢ä¼˜åŒ–ä¸åˆ‡ç©ºé—´ä¸Šçš„åŠ¨é‡æŠ•å½±ï¼Œåœ¨æ€§èƒ½ä¸Šä¼˜äºAdamWå’ŒMuonï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23000",
      "arxiv_url": "https://arxiv.org/abs/2601.23000",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23000",
      "github_url": "https://github.com/xie-lab-ml/Mano-Restriking-Manifold-Optimization-for-LLM-Training",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:36:18.427272+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22801",
      "title": "Clipping-Free Policy Optimization for Large Language Models",
      "authors": [
        "Ã–mer Veysel Ã‡aÄŸatan",
        "BarÄ±ÅŸ AkgÃ¼n",
        "GÃ¶zde GÃ¼l Åahin",
        "Xuandong Zhao"
      ],
      "abstract": "Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss. Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking , and training instability . We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.",
      "summary_en": "Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.",
      "summary_zh": "æ— è£å‰ªç­–ç•¥ä¼˜åŒ–ä½¿ç”¨å‡¸äºŒæ¬¡æƒ©ç½šæ›¿ä»£å¯å‘å¼è£å‰ªï¼Œåœ¨ç¨³å®šå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„åŒæ—¶é¿å…æ€§èƒ½æŸå¤±ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22801",
      "arxiv_url": "https://arxiv.org/abs/2601.22801",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22801",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:36:16.755891+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.21968",
      "title": "OVD: On-policy Verbal Distillation",
      "authors": [
        "Jing Xiong",
        "Hui Shen",
        "Shansan Gong",
        "Yuxin Cheng",
        "Jianghan Shen",
        "Chaofan Tao",
        "Haochen Tan",
        "Haoli Bai",
        "Lifeng Shang",
        "Ngai Wong"
      ],
      "abstract": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models ; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models , which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning . We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models . OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment , allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io",
      "summary_en": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints.",
      "summary_zh": "ç­–ç•¥å†…è¯­è¨€è’¸é¦ï¼ˆOVDï¼‰é€šè¿‡ä»¥ç¦»æ•£è¯­è¨€è¯„åˆ†è¿›è¡Œè½¨è¿¹åŒ¹é…æ¥æ›¿ä»£tokençº§æ¦‚ç‡åŒ¹é…ï¼Œå®ç°ä»æ•™å¸ˆæ¨¡å‹åˆ°å­¦ç”Ÿæ¨¡å‹çš„é«˜æ•ˆçŸ¥è¯†è¿ç§»ï¼Œé™ä½å†…å­˜æ¶ˆè€—ï¼Œå¹¶æ”¯æŒæ— tokenå¯¹é½çº¦æŸçš„è‡ªç”±æ¢ç´¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21968",
      "arxiv_url": "https://arxiv.org/abs/2601.21968",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21968",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:36:04.617166+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02354",
      "title": "Implicit neural representation of textures",
      "authors": [
        "Albert Kwok",
        "Zheyuan Hu",
        "Dounia Hammou"
      ],
      "abstract": "Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks. Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space . Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality , with considerable memory usage and rendering inference time . We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation .",
      "summary_en": "Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.",
      "summary_zh": "éšå¼ç¥ç»è¡¨ç¤ºåœ¨UVåæ ‡ç©ºé—´ä¸Šè¿ç»­è¿ç®—ï¼Œåœ¨å¹³è¡¡å†…å­˜ä½¿ç”¨ä¸æ¸²æŸ“æ—¶é—´çš„åŒæ—¶å±•ç°å‡ºè‰¯å¥½çš„å›¾åƒè´¨é‡ï¼Œå¯åº”ç”¨äºå®æ—¶æ¸²æŸ“å’Œä¸‹æ¸¸ä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02354",
      "arxiv_url": "https://arxiv.org/abs/2602.02354",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02354",
      "github_url": "https://github.com/PeterHUistyping/INR-Tex",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:37:55.211040+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02287",
      "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages",
      "authors": [
        "Isaac Chung",
        "Linda Freienthal"
      ],
      "abstract": "Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks. Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences. This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages , motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data , and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.",
      "summary_en": "Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.",
      "summary_zh": "å—æ§è·¨è¯­è¨€è¯„ä¼°æ˜¾ç¤ºï¼Œé’ˆå¯¹å½¢æ€ä¸°å¯Œè¯­è¨€æ—¶ï¼ŒLLMè¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸ç¨³å®šæ€§ï¼Œè¡¨æ˜é›¶æ ·æœ¬è¯„åˆ¤è¿ç§»åœ¨è¯è¯­çº§ä»»åŠ¡ä¸­å¹¶ä¸å¯é ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.02287",
      "arxiv_url": "https://arxiv.org/abs/2602.02287",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02287",
      "github_url": "https://github.com/isaac-chung/cross-lingual-stability-judges",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:37:49.562313+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01815",
      "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
      "authors": [
        "Yunhui Jang",
        "Seonghyun Park",
        "Jaehyung Kim",
        "Sungsoo Ahn"
      ],
      "abstract": "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches. Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal , critique , and voting phases . Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
      "summary_en": "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.",
      "summary_zh": "ä½¿ç”¨åŸºäºå‘è¡¨è®ºæ–‡å’Œåˆ†å­å†å²çš„ä¸ªæ€§åŒ–ç§‘å­¦å®¶æ¡£æ¡ˆçš„åˆ†å­å‘ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¼˜äºä¼ ç»Ÿçš„åŸºäºè§’è‰²çš„æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01815",
      "arxiv_url": "https://arxiv.org/abs/2602.01815",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01815",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:37:14.394286+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00192",
      "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
      "authors": [
        "Elif Nebioglu",
        "Emirhan BilgiÃ§",
        "Adrian Popescu"
      ],
      "abstract": "VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance. Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks . Our findings highlight the need for content-aware detection . Indeed, training on our dataset yields better generalization and localization than standard inpainting . Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.",
      "summary_en": "VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.",
      "summary_zh": "åŸºäºVAEçš„ä¿®å¤ä¼šäº§ç”Ÿé¢‘è°±åç§»ï¼Œä»è€Œæ¬ºéª—æ£€æµ‹ç³»ç»Ÿï¼›é€šè¿‡Inpainting Exchangeå¯ç¼“è§£è¯¥ç°è±¡ï¼Œè¿›è€Œæå‡å†…å®¹æ„ŸçŸ¥æ£€æµ‹æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00192",
      "arxiv_url": "https://arxiv.org/abs/2602.00192",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00192",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:24.323074+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00168",
      "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation",
      "authors": [
        "Ranjan Sapkota",
        "Manoj Karkee"
      ],
      "abstract": "YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities. This paper presents YOLOE -26, a unified framework that integrates the deployment-optimized YOLO26 (or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free , end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE -26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation , followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head , which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE -26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.",
      "summary_en": "YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.",
      "summary_zh": "YOLOE-26é›†æˆYOLO26æ¶æ„ä¸å¼€æ”¾è¯æ±‡å­¦ä¹ ä»¥å®ç°å®æ—¶å®ä¾‹åˆ†å‰²ï¼Œé‡‡ç”¨å·ç§¯éª¨å¹²ç½‘ç»œã€ç«¯åˆ°ç«¯å›å½’åŠå…·å¤‡æ–‡æœ¬ä¸è§†è§‰æç¤ºèƒ½åŠ›çš„ç›®æ ‡åµŒå…¥å¤´ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00168",
      "arxiv_url": "https://arxiv.org/abs/2602.00168",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00168",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:22.323200+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22296",
      "title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
      "authors": [
        "Matteo Pinna",
        "Giacomo Lagomarsini",
        "Andrea Ceni",
        "Claudio Gallicchio"
      ],
      "abstract": "Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains. Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing . However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space , enabling parallel processing of temporal data . We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.",
      "summary_en": "Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.",
      "summary_zh": "å¹¶è¡Œå›å£°çŠ¶æ€ç½‘ç»œï¼ˆParalESNï¼‰é€šè¿‡ä½¿ç”¨å¯¹è§’çº¿æ€§é€’å½’å®ç°å¹¶è¡Œæ—¶åºå¤„ç†ï¼Œè§£å†³äº†å‚¨å¤‡æ± è®¡ç®—çš„å±€é™æ€§ï¼Œåœ¨ä¿æŒç†è®ºä¿è¯çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„è®¡ç®—æ•ˆç‡æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22296",
      "arxiv_url": "https://arxiv.org/abs/2601.22296",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22296",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:08.327948+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.21759",
      "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
      "authors": [
        "Meet Doshi",
        "Vishwajeet Kumar",
        "Yulong Li",
        "Jaydeep Sen"
      ],
      "abstract": "An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs. General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models . We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2 , even when starting from expert-assigned weights on a large pool of training datasets.",
      "summary_en": "An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.",
      "summary_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„é‡‡æ ·æ¡†æ¶é€šè¿‡è‡ªé€‚åº”é‡åŠ æƒè®­ç»ƒæ•°æ®é›†ï¼Œåœ¨æå‡åµŒå…¥æ¨¡å‹æ€§èƒ½çš„åŒæ—¶é™ä½GPUæˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21759",
      "arxiv_url": "https://arxiv.org/abs/2601.21759",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21759",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:02.489694+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.16513",
      "title": "Competing Visions of Ethical AI: A Case Study of OpenAI",
      "authors": [
        "Melissa Wilfley",
        "Mengting Ai",
        "Madelyn Rose Sanfilippo"
      ],
      "abstract": "",
      "summary_en": "",
      "summary_zh": "",
      "hf_url": "https://huggingface.co/papers/2601.16513",
      "arxiv_url": "https://arxiv.org/abs/2601.16513",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.16513",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:23:42.091308+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01897",
      "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
      "authors": [
        "Sungheon Jeong",
        "Sanggeon Yun",
        "Ryozo Masukawa",
        "Wenjun Haung",
        "Hanning Chen",
        "Mohsen Imani"
      ],
      "abstract": "Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model. Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce internal flow signatures that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring , then summarizes trajectories in compact moving readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport , yielding depth-comparable transported step lengths , turning angles , and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement : the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. Code is available at github.com/EavnJeong/Internal-Flow-Signatures-for- Self-Checking -and-Refinement-in-LLMs.",
      "summary_en": "Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.",
      "summary_zh": "å†…éƒ¨æµç‰¹å¾åˆ†æå¤§è¯­è¨€æ¨¡å‹çš„æ·±åº¦åŠ¨æ€ï¼Œä»¥åœ¨ä¸ä¿®æ”¹åŸºç¡€æ¨¡å‹çš„æƒ…å†µä¸‹å®ç°è‡ªæˆ‘æ£€æŸ¥ä¸é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01897",
      "arxiv_url": "https://arxiv.org/abs/2602.01897",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01897",
      "github_url": "https://github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:37:20.012786+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01418",
      "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas",
      "authors": [
        "Christoffer Koo Ã˜hrstrÃ¸m",
        "Rafael I. Cabral Muchacho",
        "Yifei Dong",
        "Filippos Moumtzidellis",
        "Ronja GÃ¼ldenring",
        "Florian T. Pokorny",
        "Lazaros Nalpantidis"
      ],
      "abstract": "Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles. We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures . Given a set of vision tokens -such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities . Prior works have largely extended position encoding s from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance , rotation invariance (PaPE-RI), distance decay , directionality , and context awareness . We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding . Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.",
      "summary_en": "Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.",
      "summary_zh": "æŠ›ç‰©çº¿ä½ç½®ç¼–ç ï¼ˆPaPEï¼‰æ˜¯ä¸€ç§é¢å‘è§†è§‰æ¨¡æ€çš„æ–°å‹ä½ç½®ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡èåˆå¹³ç§»ä¸å˜æ€§ã€æ—‹è½¬ä¸å˜æ€§ã€è·ç¦»è¡°å‡ã€æ–¹å‘æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥åŸåˆ™ï¼Œæ”¹è¿›äº†ç°æœ‰æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.01418",
      "arxiv_url": "https://arxiv.org/abs/2602.01418",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01418",
      "github_url": "https://github.com/DTU-PAS/parabolic-position-encoding",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:36:48.384472+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00521",
      "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
      "authors": [
        "Junhyuk Choi",
        "Sohhyung Park",
        "Chanhee Cho",
        "Hyeonchu Park",
        "Bugeun Kim"
      ],
      "abstract": "A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment. While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge , grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency , defined as the stability of measurement behavior under prompt variations, and (2) human alignment , capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of un reliability .",
      "summary_en": "A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åŸºäºé¡¹ç›®ååº”ç†è®ºå’Œç­‰çº§ååº”æ¨¡å‹çš„ä¸¤é˜¶æ®µè¯Šæ–­æ¡†æ¶ï¼Œé€šè¿‡æ£€éªŒå†…åœ¨ä¸€è‡´æ€§å’Œäººç±»å¯¹é½æ¥è¯„ä¼° LLM-as-a-Judge çš„å¯é æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00521",
      "arxiv_url": "https://arxiv.org/abs/2602.00521",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00521",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:36:28.470769+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.14691",
      "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
      "authors": [
        "Muhammad Khalifa",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Sungryull Sohn",
        "Yunxiang Zhang",
        "Moontae Lee",
        "Hao Peng",
        "Lu Wang",
        "Honglak Lee"
      ],
      "abstract": "Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations. Large language models (LLMs) are increasingly used as judges to evaluate agent performance , particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute , which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.",
      "summary_en": "Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.",
      "summary_zh": "ç”¨ä½œæ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°è¯„åˆ¤å™¨çš„å¤§è¯­è¨€æ¨¡å‹æ˜“å—æ¨ç†è½¨è¿¹æ“çºµï¼Œä¸”åŸºäºå†…å®¹çš„ä¼ªé€ æ¯”åŸºäºé£æ ¼çš„ä¿®æ”¹æ›´æœ‰æ•ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2601.14691",
      "arxiv_url": "https://arxiv.org/abs/2601.14691",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.14691",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:35:57.482881+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23265",
      "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
      "authors": [
        "Dawei Zhu",
        "Rui Meng",
        "Yale Song",
        "Xiyu Wei",
        "Sujian Li",
        "Tomas Pfister",
        "Jinsung Yoon"
      ],
      "abstract": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques. Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models , PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique . To rigorously evaluate our framework, we introduce PaperBananaBench , comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots . Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations .",
      "summary_en": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.",
      "summary_zh": "_paperbanana æ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œè‡ªåŠ¨åŒ–åˆ›å»ºå‡ºç‰ˆçº§å­¦æœ¯æ’å›¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23265",
      "arxiv_url": "https://arxiv.org/abs/2601.23265",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23265",
      "github_url": "https://github.com/dwzhu-pku/PaperBanana",
      "upvotes": 188,
      "fetched_at": "2026-02-19T05:35:33.531327+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22975",
      "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text",
      "authors": [
        "Ximing Lu",
        "David Acuna",
        "Jaehun Jung",
        "Jian Hu",
        "Di Zhang",
        "Shizhe Diao",
        "Yunheng Zou",
        "Shaokun Zhang",
        "Brandon Cui",
        "Mingjie Liu",
        "Hyunwoo Kim",
        "Prithviraj Ammanabrolu",
        "Jan Kautz",
        "Yi Dong",
        "Yejin Choi"
      ],
      "abstract": "Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains. Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task . Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M , a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.",
      "summary_en": "Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.",
      "summary_zh": "Golden Gooseé€šè¿‡å°†fill-in-the-middleä»»åŠ¡è½¬æ¢ä¸ºå¤šé¡¹é€‰æ‹©é—®ç­”å½¢å¼ï¼ŒåŸºäºä¸å¯éªŒè¯çš„äº’è”ç½‘æ–‡æœ¬åˆæˆæ— é™çš„RLVRä»»åŠ¡ï¼Œå®ç°å¤§è§„æ¨¡è®­ç»ƒï¼Œå¹¶åœ¨ç½‘ç»œå®‰å…¨åŠå…¶ä»–é¢†åŸŸè¾¾åˆ°SOTAæ°´å¹³ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22975",
      "arxiv_url": "https://arxiv.org/abs/2601.22975",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22975",
      "github_url": "",
      "upvotes": 100,
      "fetched_at": "2026-02-19T05:35:19.227572+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21558",
      "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
      "authors": [
        "Xiaoyu Tian",
        "Haotian Wang",
        "Shuaiting Chen",
        "Hao Zhou",
        "Kaichi Yu",
        "Yudian Zhang",
        "Jade Ouyang",
        "Junxi Yin",
        "Jiong Chen",
        "Baoyan Guo",
        "Lei Zhang",
        "Junjie Tao",
        "Yuansheng Song",
        "Ming Cui",
        "Chengwei Liu"
      ],
      "abstract": "ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities. Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making , yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning . ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule- verifiable environments , enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.",
      "summary_en": "ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.",
      "summary_zh": "ASTRAæ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œä½¿ç”¨åˆæˆæ•°æ®å’Œå¯éªŒè¯çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå·¥å…·å¢å¼ºçš„è¯­è¨€æ¨¡å‹ï¼Œä»¥æå‡å¤šæ­¥å†³ç­–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21558",
      "arxiv_url": "https://arxiv.org/abs/2601.21558",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21558",
      "github_url": "https://github.com/LianjiaTech/astra",
      "upvotes": 58,
      "fetched_at": "2026-02-19T05:34:40.873243+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22813",
      "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation",
      "authors": [
        "Andrei Panferov",
        "Erik Schultheis",
        "Soroush Tabesh",
        "Dan Alistarh"
      ],
      "abstract": "Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution. The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs , promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats , called MS-EDEN , that has more than 2x lower quantization error than SR. We integrate it into a novel fully- NVFP4 quantization scheme for linear layers , called Quartet II . We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications , both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4 . We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .",
      "summary_en": "Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.",
      "summary_zh": "é‡åŒ–è®­ç»ƒæ–¹æ³• Quartet II é€šè¿‡å¢å¼ºæ¢¯åº¦ä¼°è®¡å’ŒåŠ é€Ÿ GPU æ‰§è¡Œï¼Œæå‡äº† NVFP4 æ ¼å¼åœ¨å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒä¸­çš„åˆ©ç”¨ç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22813",
      "arxiv_url": "https://arxiv.org/abs/2601.22813",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22813",
      "github_url": "https://github.com/IST-DASLab/Quartet-II",
      "upvotes": 56,
      "fetched_at": "2026-02-19T05:35:13.218441+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23143",
      "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models",
      "authors": [
        "Seanie Lee",
        "Sangwoo Park",
        "Yumin Choi",
        "Gyeongman Kim",
        "Minki Kang",
        "Jihun Yun",
        "Dongmin Park",
        "Jongho Park",
        "Sung Ju Hwang"
      ],
      "abstract": "ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs. Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation , yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering , guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency . Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost . Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.",
      "summary_en": "ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.",
      "summary_zh": "ThinkSafeæ˜¯ä¸€ç§è‡ªå¯¹é½æ¡†æ¶ï¼Œé€šè¿‡è½»é‡çº§æ‹’ç»å¼•å¯¼å’Œå¯¹è‡ªç”Ÿæˆå›å¤çš„å¾®è°ƒæ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œåœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23143",
      "arxiv_url": "https://arxiv.org/abs/2601.23143",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23143",
      "github_url": "https://github.com/seanie12/ThinkSafe",
      "upvotes": 38,
      "fetched_at": "2026-02-19T05:35:22.984332+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23184",
      "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought",
      "authors": [
        "Fanmeng Wang",
        "Haotian Liu",
        "Guojiang Zhao",
        "Hongteng Xu",
        "Zhifeng Gao"
      ],
      "abstract": "ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance. While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution , thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning , providing a new and insightful solution to latent reasoning . Code: https://github.com/FanmengWang/ReGuLaR.",
      "summary_en": "ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.",
      "summary_zh": "ReGuLaRæå‡ºäº†ä¸€ç§å˜åˆ†è‡ªç¼–ç æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹å‹ç¼©è‡³éšç©ºé—´ï¼ŒåŒæ—¶é€šè¿‡å›¾åƒæ¸²æŸ“çš„æ˜¾å¼æ¨ç†é“¾è¿›è¡Œå¼•å¯¼ä»¥ç»´æŒæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23184",
      "arxiv_url": "https://arxiv.org/abs/2601.23184",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23184",
      "github_url": "https://github.com/FanmengWang/ReGuLaR",
      "upvotes": 36,
      "fetched_at": "2026-02-19T05:35:28.464803+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22628",
      "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving",
      "authors": [
        "Chengyi Yang",
        "Zhishang Xiang",
        "Yunbo Tang",
        "Zongpei Teng",
        "Chengsong Huang",
        "Fei Long",
        "Yuhan Liu",
        "Jinsong Su"
      ],
      "abstract": "TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards. Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels , and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver . These policies evolve through iterative optimization : the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training . Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.",
      "summary_en": "TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.",
      "summary_zh": "TTCSæ˜¯ä¸€ç§ååŒæ¼”åŒ–çš„æµ‹è¯•æ—¶è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜å˜ä½“ï¼Œå¹¶åˆ©ç”¨è‡ªä¸€è‡´æ€§å¥–åŠ±æ›´æ–°æ¨ç†æ±‚è§£å™¨ï¼Œä»è€Œå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22628",
      "arxiv_url": "https://arxiv.org/abs/2601.22628",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22628",
      "github_url": "https://github.com/XMUDeepLIT/TTCS",
      "upvotes": 35,
      "fetched_at": "2026-02-19T05:35:01.502553+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21998",
      "title": "Causal World Modeling for Robot Control",
      "authors": [
        "Lin Li",
        "Qihang Zhang",
        "Yiming Luo",
        "Shuai Yang",
        "Ruilin Wang",
        "Fei Han",
        "Mingrui Yu",
        "Zelin Gao",
        "Nan Xue",
        "Xing Zhu",
        "Yujun Shen",
        "Yinghao Xu"
      ],
      "abstract": "Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms. This work highlights that video world modeling , alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space , integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism , allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline , parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation , data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.",
      "summary_en": "Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.",
      "summary_zh": "è§†é¢‘ä¸–ç•Œå»ºæ¨¡é€šè¿‡ç»Ÿä¸€æ¡†æ¶å®ç°æœºå™¨äººå­¦ä¹ ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å…±äº«æ½œç©ºé—´å’Œé—­ç¯åé¦ˆæœºåˆ¶åŒæ—¶é¢„æµ‹å¸§å¹¶æ‰§è¡Œç­–ç•¥ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21998",
      "arxiv_url": "https://arxiv.org/abs/2601.21998",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21998",
      "github_url": "",
      "upvotes": 30,
      "fetched_at": "2026-02-19T05:34:51.318209+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21192",
      "title": "Do Reasoning Models Enhance Embedding Models?",
      "authors": [
        "Wun Yu Chan",
        "Shaojin Chen",
        "Huihao Jing",
        "Kwun Hang Lau",
        "Elton Chun-Chai Li",
        "Zihao Wang",
        "Haoran Li",
        "Yangqiu Song"
      ],
      "abstract": "Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization. State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning . Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold 's local geometry reorganization and reversible coordinate basis drift , it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term ** Manifold Realignment **. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.",
      "summary_en": "Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.",
      "summary_zh": "ä»¥RLVRè°ƒä¼˜æ¨ç†æ¨¡å‹åˆå§‹åŒ–çš„åµŒå…¥æ¨¡å‹ç›¸è¾ƒåŸºç¡€æ¨¡å‹æœªå±•ç°æ€§èƒ½ä¼˜åŠ¿ï¼ŒHRSAæ­ç¤ºå°½ç®¡å±€éƒ¨å‡ ä½•é‡ç»„ï¼Œå…¨å±€å‡ ä½•ä¸çº¿æ€§è¯»å‡ºä»ä¿æŒã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21192",
      "arxiv_url": "https://arxiv.org/abs/2601.21192",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21192",
      "github_url": "https://github.com/HKUST-KnowComp/Reasoning-Embedding",
      "upvotes": 25,
      "fetched_at": "2026-02-19T05:34:27.984586+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21468",
      "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning",
      "authors": [
        "Yaorui Shi",
        "Shugui Liu",
        "Yu Yang",
        "Wenyu Mao",
        "Yuxin Chen",
        "Qi GU",
        "Hui Su",
        "Xunliang Cai",
        "Xiang Wang",
        "An Zhang"
      ],
      "abstract": "MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints. Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window . Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout . Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.",
      "summary_en": "MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.",
      "summary_zh": "MemOCRæ˜¯ä¸€ç§å¤šæ¨¡æ€è®°å¿†æ™ºèƒ½ä½“ï¼Œé€šè¿‡å°†äº¤äº’å†å²è‡ªé€‚åº”å‹ç¼©ä¸ºè§†è§‰å¸ƒå±€æ¥å¢å¼ºé•¿ç¨‹æ¨ç†ï¼Œä»è€Œåœ¨ä¸¥æ ¼çš„é¢„ç®—çº¦æŸä¸‹å®ç°é«˜æ•ˆçš„ä¸Šä¸‹æ–‡åˆ©ç”¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21468",
      "arxiv_url": "https://arxiv.org/abs/2601.21468",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21468",
      "github_url": "https://github.com/syr-cn/MemOCR",
      "upvotes": 22,
      "fetched_at": "2026-02-19T05:34:34.109410+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22636",
      "title": "Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling",
      "authors": [
        "Mingqian Feng",
        "Xiaodong Liu",
        "Weiwei Yang",
        "Chenliang Xu",
        "Christopher White",
        "Jianfeng Gao"
      ],
      "abstract": "A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs. Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting , which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling . We model sample-level success probabilities using a Beta distribution , the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rate s from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.",
      "summary_en": "A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åä¸ºSABERçš„è§„æ¨¡æ„ŸçŸ¥é£é™©ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡Best-of-Né‡‡æ ·é¢„æµ‹è¯­è¨€æ¨¡å‹çš„å¤§è§„æ¨¡å¯¹æŠ—æ€§è„†å¼±æ€§ï¼Œä»è€Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°å‡†ç¡®è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22636",
      "arxiv_url": "https://arxiv.org/abs/2601.22636",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22636",
      "github_url": "",
      "upvotes": 21,
      "fetched_at": "2026-02-19T05:35:03.173489+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23182",
      "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation",
      "authors": [
        "Siyang He",
        "Qiqi Wang",
        "Xiaoran Liu",
        "Hongnan Ma",
        "Yiwei Shi",
        "Yuerong Song",
        "Ying Zhu",
        "Tianyi Liang",
        "Zengfeng Huang",
        "Ziwei He",
        "Xipeng Qiu"
      ],
      "abstract": "Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.",
      "summary_en": "Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.",
      "summary_zh": "æ‰©æ•£è¯­è¨€æ¨¡å‹çš„é¢‘åŸŸåˆ†æè¡¨æ˜ï¼Œä½é¢‘æˆåˆ†ç¼–ç å…¨å±€ç»“æ„ï¼Œé«˜é¢‘æˆåˆ†æ•æ‰å±€éƒ¨ç»†èŠ‚ï¼Œé€šè¿‡FourierSamplerçš„åŠ¨æ€é¢‘åŸŸæ»‘åŠ¨çª—å£æœºåˆ¶å¯æå‡ç”Ÿæˆè´¨é‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23182",
      "arxiv_url": "https://arxiv.org/abs/2601.23182",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23182",
      "github_url": "https://github.com/ShirleYoung/FourierSampler",
      "upvotes": 20,
      "fetched_at": "2026-02-19T05:35:26.676966+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21957",
      "title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
      "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
      ],
      "abstract": "A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality. We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR",
      "summary_en": "A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.",
      "summary_zh": "ä¸€ç§ç´§å‡‘å‹è§†è§‰è¯­è¨€æ¨¡å‹é€šè¿‡ä¸“é—¨çš„åŸºå‡†æµ‹è¯•å’Œæ‰©å±•åŠŸèƒ½ï¼Œåœ¨æ–‡æ¡£ç†è§£ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒæ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21957",
      "arxiv_url": "https://arxiv.org/abs/2601.21957",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21957",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T05:34:48.943153+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22904",
      "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
      "authors": [
        "Hun Chang",
        "Byunghee Cha",
        "Jong Chul Ye"
      ],
      "abstract": "A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency. Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders , showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder ( DINO -SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors , while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere , we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold . Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality , reaching 0.37 rFID and 26.2 dB PSNR , while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching -based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.",
      "summary_en": "A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.",
      "summary_zh": "ä¸€ç§æ–°é¢–çš„è§†è§‰è‡ªç¼–ç å™¨æ¡†æ¶åˆ©ç”¨çƒé¢æ½œåœ¨ç©ºé—´å’Œé»æ›¼æµåŒ¹é…ï¼Œå°†è¯­ä¹‰è¡¨ç¤ºä¸åƒç´ çº§é‡å»ºç›¸ç»“åˆï¼Œä»è€Œæå‡ä¿çœŸåº¦å’Œæ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22904",
      "arxiv_url": "https://arxiv.org/abs/2601.22904",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22904",
      "github_url": "https://github.com/wkdgnsgo/dino-sae",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:35:17.577359+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.20218",
      "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
      "authors": [
        "Haoyou Deng",
        "Keyu Yan",
        "Chaojie Mao",
        "Xiang Wang",
        "Yu Liu",
        "Changxin Gao",
        "Nong Sang"
      ],
      "abstract": "DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration. Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem : the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards , which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach . This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards , a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space . Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler , ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.",
      "summary_en": "DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.",
      "summary_zh": "DenseGRPOé€šè¿‡å¼•å…¥ä¸­é—´å»å™ªæ­¥éª¤çš„å¯†é›†å¥–åŠ±å’Œè‡ªé€‚åº”æ¢ç´¢æ ¡å‡†ï¼Œè§£å†³äº†flow matchingæ¨¡å‹ä¸­çš„ç¨€ç–å¥–åŠ±é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.20218",
      "arxiv_url": "https://arxiv.org/abs/2601.20218",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.20218",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:34:24.165409+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22664",
      "title": "Real-Time Aligned Reward Model beyond Semantics",
      "authors": [
        "Zixuan Huang",
        "Xin Xia",
        "Yuxi Ren",
        "Jianbin Zheng",
        "Xuefeng Xiao",
        "Hongyan Xie",
        "Li Huaqiu",
        "Songshi Liang",
        "Zhongxiang Dai",
        "Fuzhen Zhuang",
        "Jianxin Li",
        "Yikun Ban",
        "Deqing Wang"
      ],
      "abstract": "RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training. Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization , in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts . This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization . To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback ) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models .",
      "summary_en": "RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.",
      "summary_zh": "RLHFå› å¥–åŠ±æ¨¡å‹ä¸ç­–ç•¥æ¨¡å‹æœªå¯¹é½è€Œå­˜åœ¨å¥–åŠ±è¿‡åº¦ä¼˜åŒ–é—®é¢˜ï¼ŒR2Mé€šè¿‡å¼•å…¥å®æ—¶ç­–ç•¥åé¦ˆï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å¥–åŠ±å»ºæ¨¡ä»¥è§£å†³è¯¥é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22664",
      "arxiv_url": "https://arxiv.org/abs/2601.22664",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22664",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:35:07.592237+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21716",
      "title": "DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning",
      "authors": [
        "Mingshuang Luo",
        "Shuang Liang",
        "Zhengkun Rong",
        "Yuxuan Luo",
        "Tianshu Hu",
        "Ruibing Hou",
        "Hong Chang",
        "Yong Li",
        "Yuan Zhang",
        "Mingyuan Gao"
      ],
      "abstract": "DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters. Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a \"see-saw\", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space , enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs , facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation . This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization . Project Page: https://grisoon.github.io/DreamActor-M2/",
      "summary_en": "DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.",
      "summary_zh": "DreamActor-M2 æå‡ºäº†ä¸€ç§é€šç”¨è§’è‰²åŠ¨ç”»æ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè‡ªä¸¾æ•°æ®åˆæˆè§£å†³è¿åŠ¨æ³¨å…¥çš„æƒè¡¡é—®é¢˜ä¸å§¿æ€å…ˆéªŒçš„å±€é™æ€§ï¼Œä»è€Œæå‡å¯¹å¤šæ ·åŒ–è§’è‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21716",
      "arxiv_url": "https://arxiv.org/abs/2601.21716",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21716",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:34:47.043028+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22491",
      "title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
      "authors": [
        "Jinyang Wu",
        "Changpeng Yang",
        "Yuhao Shen",
        "Fangzhi Xu",
        "Bolin Ni",
        "Chonghua Liao",
        "Yuchen Liu",
        "Hongzhen Wang",
        "Shuai Nie",
        "Shuai Zhang",
        "Haoran Luo",
        "Jiaming Xu"
      ],
      "abstract": "Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability. Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio , thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability . Our work establishes SSL as a general principle for training capable and robust agents.",
      "summary_en": "Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.",
      "summary_zh": "Sweet Spot Learningï¼ˆSSLï¼‰æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åˆ†å±‚å¥–åŠ±å¼•å¯¼æ™ºèƒ½ä½“ä¼˜åŒ–è‡³è§£ç©ºé—´çš„æœ€ä¼˜åŒºåŸŸï¼Œä»è€Œæé«˜æ ·æœ¬æ•ˆç‡ä¸è·¨ä»»åŠ¡å¯è¿ç§»æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22491",
      "arxiv_url": "https://arxiv.org/abs/2601.22491",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22491",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:34:59.689452+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23161",
      "title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
      "authors": [
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Shiwan Zhao",
        "Yuhang Jia",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
      ],
      "abstract": "DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training. Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder , employs dual semantic and acoustic adapters , and is trained with a four-stage curriculum that combines semantic and acoustic alignment , large-scale supervised fine-tuning , and variance-reduced preference optimization , using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.",
      "summary_en": "DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.",
      "summary_zh": "DIFFA-2æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¢å¼ºç¼–ç ã€åŒé€‚é…å™¨å’Œåˆ†é˜¶æ®µè®­ç»ƒï¼Œåœ¨éŸ³é¢‘ç†è§£æ€§èƒ½ä¸Šå…·å¤‡ç«äº‰åŠ›ï¼Œä¸”æ•ˆç‡ä¼˜äºè‡ªå›å½’æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23161",
      "arxiv_url": "https://arxiv.org/abs/2601.23161",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23161",
      "github_url": "https://github.com/NKU-HLT/DIFFA",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:35:24.778323+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22837",
      "title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
      "authors": [
        "Bin Wu",
        "Mengqi Huang",
        "Weinan Jia",
        "Zhendong Mao"
      ],
      "abstract": "NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation. VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization , which enforces causal dependencies during tokenization . Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling , and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.",
      "summary_en": "NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.",
      "summary_zh": "NativeTokæå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰åˆ†è¯æ–¹æ³•ï¼Œåœ¨å›¾åƒç¼–ç æ—¶å¼ºåˆ¶å»ºç«‹å› æœä¾èµ–å…³ç³»ï¼Œåˆ©ç”¨Meta Image Transformerå’ŒMixture of Causal Expert Transformerå®ç°é«˜æ•ˆä¸”è¿è´¯çš„å›¾åƒç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22837",
      "arxiv_url": "https://arxiv.org/abs/2601.22837",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22837",
      "github_url": "https://github.com/wangbei1/Nativetok",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:35:15.258481+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22642",
      "title": "Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification",
      "authors": [
        "Chuxue Cao",
        "Jinluan Yang",
        "Haoran Li",
        "Kunhao Pan",
        "Zijian Zhao",
        "Zhengyu Chen",
        "Yuchen Tian",
        "Lijun Wu",
        "Conghui He",
        "Sirui Han",
        "Yike Guo"
      ],
      "abstract": "A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models. Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification -guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain . We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification -guided supervised fine-tuning and policy optimization . Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.",
      "summary_en": "A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.",
      "summary_zh": "ä¸€ç§å½¢å¼é€»è¾‘éªŒè¯å¼•å¯¼çš„æ¡†æ¶åŠ¨æ€äº¤é”™ç¬¦å·éªŒè¯ä¸è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å‡†ç¡®æ€§å¹¶å‡å°‘é”™è¯¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22642",
      "arxiv_url": "https://arxiv.org/abs/2601.22642",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22642",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:35:05.516758+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.18241",
      "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance",
      "authors": [
        "Elena Bruches",
        "Vadim Alperovich",
        "Dari Baturova",
        "Roman Derunets",
        "Daniil Grebenkin",
        "Georgy Mkrtchyan",
        "Oleg Sedukhin",
        "Mikhail Klementev",
        "Ivan Bondarenko",
        "Nikolay Bushkov",
        "Stanislav Moiseev"
      ],
      "abstract": "TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages. While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction , neglecting the broader challenge of test suite maintenance . We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows , using a reference-free protocol based on test suite pass rate , code coverage , and mutation testing . Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.",
      "summary_en": "TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.",
      "summary_zh": "TAM-Evalæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨è·¨å¤šç§ç¼–ç¨‹è¯­è¨€çš„å…¨é¢æµ‹è¯•å¥—ä»¶ç»´æŠ¤ä»»åŠ¡ï¼ˆåŒ…æ‹¬åˆ›å»ºã€ä¿®å¤å’Œæ›´æ–°ï¼‰ä¸Šçš„æ¡†æ¶å’ŒåŸºå‡†ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.18241",
      "arxiv_url": "https://arxiv.org/abs/2601.18241",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.18241",
      "github_url": "https://github.com/trndcenter/TAM-Eval",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:34:22.510354+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.15625",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "authors": [
        "Zhiwei Zhang",
        "Fei Zhao",
        "Rui Wang",
        "Zezhong Wang",
        "Bin Liang",
        "Jiakang Wang",
        "Yao Hu",
        "Shaosheng Cao",
        "Kam-Fai Wong"
      ],
      "abstract": "A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training. Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution : following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO , a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator , then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "summary_en": "A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.",
      "summary_zh": "æå‡ºäº†ä¸€ç§åä¸º Fission-GRPO çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å°†æ‰§è¡Œé”™è¯¯è½¬åŒ–ä¸ºçº æ­£æ€§ç›‘ç£ï¼Œä»¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„å¤šè½®å·¥å…·æ‰§è¡Œèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.15625",
      "arxiv_url": "https://arxiv.org/abs/2601.15625",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.15625",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:34:20.795603+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.13097",
      "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
      "authors": [
        "Elena Bruches",
        "Daniil Grebenkin",
        "Mikhail Klementev",
        "Vadim Alperovich",
        "Roman Derunets",
        "Dari Baturova",
        "Georgy Mkrtchyan",
        "Oleg Sedukhin",
        "Ivan Bondarenko",
        "Nikolay Bushkov",
        "Stanislav Moiseev"
      ],
      "abstract": "RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods. We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests . Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals : (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage , and (3) whether the generated test cases improve the mutation kill rate . To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files , test files , and candidate test additions labeled by an execution-based pipeline , and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes ( zero-shot , full fine-tuning , and PEFT via LoRA ), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.",
      "summary_en": "RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.",
      "summary_zh": "RM-RFæ˜¯ä¸€ç§è½»é‡çº§å¥–åŠ±æ¨¡å‹ï¼Œä»…å‡­æºä»£ç å³å¯é¢„æµ‹æ‰§è¡Œç»“æœï¼Œç›¸æ¯”ä¼ ç»Ÿç¼–è¯‘è¿è¡Œæ–¹æ³•è¯„ä¼°æ›´å¿«ä¸”æˆæœ¬æ›´ä½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.13097",
      "arxiv_url": "https://arxiv.org/abs/2601.13097",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.13097",
      "github_url": "https://github.com/trndcenter/RM-RF-unit-tests",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:34:16.382009+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2602.00158",
      "title": "RAPTOR: Ridge-Adaptive Logistic Probes",
      "authors": [
        "Ziqi Gao",
        "Yaotian Zhu",
        "Qingcheng Zeng",
        "Xu Zhao",
        "Ziqing Wang",
        "Feng Ruan",
        "Kaize Ding"
      ],
      "abstract": "RACTOR, a ridge-adaptive logistic probe, achieves accurate and stable concept vector estimation for activation steering in frozen LLMs with reduced training costs, supported by theoretical analysis of ridge logistic regression in high-dimensional settings. Probing studies what information is encoded in a frozen LLM 's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLM s and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem ( CGMT ), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.",
      "summary_en": "RACTOR, a ridge-adaptive logistic probe, achieves accurate and stable concept vector estimation for activation steering in frozen LLMs with reduced training costs, supported by theoretical analysis of ridge logistic regression in high-dimensional settings.",
      "summary_zh": "RACTORæ˜¯ä¸€ç§å²­è‡ªé€‚åº”é€»è¾‘æ¢é’ˆï¼Œå¯åœ¨å†»ç»“LLMä¸­ä»¥è¾ƒä½è®­ç»ƒæˆæœ¬å®ç°å‡†ç¡®ç¨³å®šçš„æ¦‚å¿µå‘é‡ä¼°è®¡ç”¨äºæ¿€æ´»å¼•å¯¼ï¼Œå¹¶æœ‰é«˜ç»´åœºæ™¯ä¸‹å²­é€»è¾‘å›å½’çš„ç†è®ºåˆ†æä½œä¸ºæ”¯æ’‘ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.00158",
      "arxiv_url": "https://arxiv.org/abs/2602.00158",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00158",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:35:35.835950+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23228",
      "title": "Scaling Multiagent Systems with Process Rewards",
      "authors": [
        "Ed Li",
        "Junyu Ren",
        "Cat Yan"
      ],
      "abstract": "Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks. While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.",
      "summary_en": "Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.",
      "summary_zh": "é€šè¿‡AIåé¦ˆçš„é€åŠ¨ä½œè¿‡ç¨‹å¥–åŠ±(MAPPA)æ”¹è¿›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¢å¼ºå¤æ‚ä»»åŠ¡çš„ä¿¡ç”¨åˆ†é…ä¸æ ·æœ¬æ•ˆç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23228",
      "arxiv_url": "https://arxiv.org/abs/2601.23228",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23228",
      "github_url": "https://github.com/ltjed/multiagent-coaching",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:35:31.807777+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23188",
      "title": "Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience",
      "authors": [
        "Zhongxiang Sun",
        "Qipeng Wang",
        "Weijie Yu",
        "Jingxuan Yang",
        "Haolang Lu",
        "Jun Xu"
      ],
      "abstract": "Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions. Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval , reasoning , and long-horizon task execution . However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection . In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor , which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor , which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories . By embedding monitoring directly into the reasoning -retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.",
      "summary_en": "Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.",
      "summary_zh": "å…·å¤‡å±‚çº§å…ƒè®¤çŸ¥ç›‘æ§çš„æ·±åº¦æœç´¢æ™ºèƒ½ä½“é€šè¿‡å¿«é€Ÿä¸€è‡´æ€§æ£€æŸ¥ä¸ç»éªŒé©±åŠ¨çš„çº æ­£å¹²é¢„ï¼Œå¢å¼ºæ¨ç†ä¸æ£€ç´¢æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23188",
      "arxiv_url": "https://arxiv.org/abs/2601.23188",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23188",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:35:30.259784+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21358",
      "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
      "authors": [
        "Jiecong Wang",
        "Hao Peng",
        "Chunyang Liu"
      ],
      "abstract": "PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches. Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces . Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states . However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states , while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search .",
      "summary_en": "PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.",
      "summary_zh": "PLaT æå‡ºäº†ä¸€ç§æ½œåœ¨æ¨ç†æ¡†æ¶ï¼Œå°†æ¨ç†ä¸è¨€è¯­åŒ–è§£è€¦ï¼Œæ”¯æŒåŠ¨æ€ç»ˆæ­¢å¹¶æå‡äº†ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21358",
      "arxiv_url": "https://arxiv.org/abs/2601.21358",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21358",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:34:30.074646+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21525",
      "title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
      "authors": [
        "Meet Doshi",
        "Aashka Trivedi",
        "Vishwajeet Kumar",
        "Parul Awasthy",
        "Yulong Li",
        "Jaydeep Sen",
        "Radu Florian",
        "Sachindra Joshi"
      ],
      "abstract": "Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods. Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator , most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance . To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.",
      "summary_en": "Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.",
      "summary_zh": "Landmarkæ± åŒ–é€šè¿‡å°†åºåˆ—åˆ†å—å¹¶ä½¿ç”¨landmarkæ ‡è®°æ¥ä¿ç•™å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ï¼Œæ¯”ä¼ ç»Ÿæ± åŒ–æ–¹æ³•æ›´æœ‰æ•ˆåœ°æ”¹è¿›äº†é•¿ä¸Šä¸‹æ–‡è¡¨ç¤ºå­¦ä¹ ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21525",
      "arxiv_url": "https://arxiv.org/abs/2601.21525",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21525",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:34:36.410844+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21419",
      "title": "Revisiting Diffusion Model Predictions Through Dimensionality",
      "authors": [
        "Qing Jin",
        "Chaoyang Wang"
      ],
      "abstract": "Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data. Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension . Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff , a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance .",
      "summary_en": "Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.",
      "summary_zh": "åœ¨é«˜ç»´è®¾ç½®ä¸‹ï¼Œä½¿ç”¨ç›´æ¥æ•°æ®é¢„æµ‹çš„æ‰©æ•£æ¨¡å‹ä¼˜äºä¼ ç»Ÿçš„å™ªå£°æˆ–é€Ÿåº¦é¢„æµ‹ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å¯è‡ªåŠ¨ä»æ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜é¢„æµ‹å‚æ•°ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21419",
      "arxiv_url": "https://arxiv.org/abs/2601.21419",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21419",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:34:32.043908+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.20732",
      "title": "Continual GUI Agents",
      "authors": [
        "Ziwei Liu",
        "Borui Kang",
        "Hangjie Yuan",
        "Zixiang Zhao",
        "Wei Li",
        "Yifan Zhu",
        "Tao Feng"
      ],
      "abstract": "Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions. As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents , a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux ( APR-iF ) and Anchoring Region Reward in Flux ( ARR-iF ). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents , revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents .",
      "summary_en": "Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.",
      "summary_zh": "Continual GUI Agents æ¡†æ¶é€šè¿‡å¼ºåŒ–å¾®è°ƒåŠæ–°é¢–çš„é”šå®šå¥–åŠ±è§£å†³åŠ¨æ€æ•°å­—ç¯å¢ƒä¸­çš„æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œç¨³å®šè·¨å˜åŒ– UI åŸŸå’Œåˆ†è¾¨ç‡çš„å­¦ä¹ ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.20732",
      "arxiv_url": "https://arxiv.org/abs/2601.20732",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.20732",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:34:26.286498+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22666",
      "title": "ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding",
      "authors": [
        "Junyi Hu",
        "Tian Bai",
        "Fengyi Wu",
        "Wenyan Li",
        "Zhenming Peng",
        "Yi Zhang"
      ],
      "abstract": "ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations. Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities , enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization . Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation , particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split , outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.",
      "summary_en": "ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.",
      "summary_zh": "ExpAlign æå‡ºäº†ä¸€ç§è§†è§‰-è¯­è¨€å¯¹é½æ¡†æ¶ï¼Œåˆ©ç”¨å¤šç¤ºä¾‹å­¦ä¹ ä¸æ³¨æ„åŠ›æ± åŒ–ï¼Œåœ¨æ— éœ€é¢å¤–æ ‡æ³¨çš„æƒ…å†µä¸‹æå‡å¼€æ”¾è¯æ±‡æ£€æµ‹ä¸é›¶æ ·æœ¬å®ä¾‹åˆ†å‰²æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22666",
      "arxiv_url": "https://arxiv.org/abs/2601.22666",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22666",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:35:09.468707+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22032",
      "title": "Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving",
      "authors": [
        "Linhan Wang",
        "Zichong Yang",
        "Chen Bai",
        "Guoxiang Zhang",
        "Xiaotong Liu",
        "Xiaoyin Zheng",
        "Xiao-Xiao Long",
        "Chang-Tien Lu",
        "Cheng Lu"
      ],
      "abstract": "Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving. End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors . In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture ( V-JEPA ) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM , the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.",
      "summary_en": "Drive-JEPA combines V-JEPA video pretraining with multimodal trajectory distillation to achieve state-of-the-art performance in end-to-end autonomous driving.",
      "summary_zh": "Drive-JEPA ç»“åˆ V-JEPA è§†é¢‘é¢„è®­ç»ƒä¸å¤šæ¨¡æ€è½¨è¿¹è’¸é¦ï¼Œåœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22032",
      "arxiv_url": "https://arxiv.org/abs/2601.22032",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22032",
      "github_url": "https://github.com/linhanwang/Drive-JEPA",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:34:53.395277+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.15394",
      "title": "Memorization Dynamics in Knowledge Distillation for Language Models",
      "authors": [
        "Jaydeep Borkar",
        "Karan Chadha",
        "Niloofar Mireshghallah",
        "Yuchen Zhang",
        "Irina-Elena Veliche",
        "Archi Mitra",
        "David A. Smith",
        "Zheng Xu",
        "Diego Garcia-Olano"
      ],
      "abstract": "Knowledge distillation reduces training data memorization compared to standard fine-tuning while maintaining performance, with distinct memorization patterns and predictability based on input characteristics. Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy , KL divergence , and perplexity ; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits 2.7times more teacher-specific examples than soft distillation . Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.",
      "summary_en": "Knowledge distillation reduces training data memorization compared to standard fine-tuning while maintaining performance, with distinct memorization patterns and predictability based on input characteristics.",
      "summary_zh": "ä¸æ ‡å‡†å¾®è°ƒç›¸æ¯”ï¼ŒçŸ¥è¯†è’¸é¦åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘äº†è®­ç»ƒæ•°æ®è®°å¿†ï¼Œå¹¶å…·æœ‰ç‹¬ç‰¹çš„è®°å¿†æ¨¡å¼ä¸åŸºäºè¾“å…¥ç‰¹å¾çš„å¯é¢„æµ‹æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.15394",
      "arxiv_url": "https://arxiv.org/abs/2601.15394",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.15394",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:34:18.626843+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22680",
      "title": "Visual Personalization Turing Test",
      "authors": [
        "Rameen Abdal",
        "James Burgess",
        "Sergey Tulyakov",
        "Kuan-Chieh Jackson Wang"
      ],
      "abstract": "A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.",
      "summary_en": "A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.",
      "summary_zh": "ä¸€ä¸ªåä¸ºVPTTçš„æ–°å‹è¯„ä¼°æ¡†æ¶åˆ©ç”¨åŸºå‡†æµ‹è¯•ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå™¨å’Œç»è¿‡æ ¡å‡†çš„åŸºäºæ–‡æœ¬çš„æŒ‡æ ‡ï¼Œé€šè¿‡ä¸äººç±»åˆ›ä½œå†…å®¹çš„æ„ŸçŸ¥ä¸å¯åŒºåˆ†æ€§æ¥è¯„ä¼°ä¸Šä¸‹æ–‡è§†è§‰ä¸ªæ€§åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22680",
      "arxiv_url": "https://arxiv.org/abs/2601.22680",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22680",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:35:11.621863+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22141",
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "authors": [
        "Grzegorz Stefanski",
        "Alberto Presta",
        "Michal Byra"
      ],
      "abstract": "Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues. In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets , that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets , each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse , a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.",
      "summary_en": "Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.",
      "summary_zh": "Routing the Lottery æ¡†æ¶å‘ç°å¤šä¸ªé’ˆå¯¹ä¸åŒæ•°æ®æ¡ä»¶å®šåˆ¶çš„ä¸“ç”¨å­ç½‘ç»œï¼Œåœ¨ä½¿ç”¨æ›´å°‘å‚æ•°çš„åŒæ—¶æ€§èƒ½ä¼˜äºä¼ ç»Ÿå‰ªææ–¹æ³•ï¼Œå¹¶è¯†åˆ«å‡ºå­ç½‘ç»œå´©æºƒé—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22141",
      "arxiv_url": "https://arxiv.org/abs/2601.22141",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22141",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:34:57.546616+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21709",
      "title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis",
      "authors": [
        "Qingyue Yang",
        "Jie Wang",
        "Xing Li",
        "Yinqi Bai",
        "Xialiang Tong",
        "Huiling Zhen",
        "Jianye Hao",
        "Mingxuan Yuan",
        "Bin Li"
      ],
      "abstract": "Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity. Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis ( TAPPA ), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings ( RoPE ). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM- TAPPA .",
      "summary_en": "Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity.",
      "summary_zh": "æ—¶åºæ³¨æ„åŠ›æ¨¡å¼å¯é¢„æµ‹æ€§åˆ†æï¼ˆTAPPAï¼‰é€šè¿‡ä»æ—¶é—´è§†è§’åˆ†æå¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦å…¬å¼ï¼ŒåŸºäºæŸ¥è¯¢è‡ªç›¸ä¼¼æ€§åŒºåˆ†å¯é¢„æµ‹ä¸ä¸å¯é¢„æµ‹çš„æ¨¡å¼ï¼Œä¸ºç†è§£æ³¨æ„åŠ›æ¨¡å¼æä¾›äº†ç»Ÿä¸€æ¡†æ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21709",
      "arxiv_url": "https://arxiv.org/abs/2601.21709",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21709",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:34:45.050898+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21666",
      "title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding",
      "authors": [
        "Ahmed Y. Radwan",
        "Christos Emmanouilidis",
        "Hina Tabassum",
        "Deval Pandya",
        "Shaina Raza"
      ],
      "abstract": "A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata. Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding . We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard",
      "summary_en": "A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.",
      "summary_zh": "ä¸€é¡¹ç»¼åˆåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå¯¹è¯é¢†åŸŸä¸­å¯¹æ—¶åºéŸ³è§†é¢‘æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼ŒåŒ…å«äººå·¥éªŒè¯çš„æ ‡æ³¨ä¸äººå£ç»Ÿè®¡å­¦å…ƒæ•°æ®ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21666",
      "arxiv_url": "https://arxiv.org/abs/2601.21666",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21666",
      "github_url": "https://github.com/VectorInstitute/sonic-o1",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:34:42.747217+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.21526",
      "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
      "authors": [
        "Alireza Nadaf",
        "Alireza Mohammadshahi",
        "Majid Yazdani"
      ],
      "abstract": "KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.",
      "summary_en": "KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.",
      "summary_zh": "KAPSO æ˜¯ä¸€ä¸ªç”¨äºè‡ªä¸»ç¨‹åºåˆæˆçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé‡‡ç”¨åŒ…å«å®éªŒè¿½è¸ªã€çŸ¥è¯†æ•´åˆå’Œè®¤çŸ¥è®°å¿†çš„è¿­ä»£ä¼˜åŒ–å¾ªç¯ï¼Œä»¥æ”¹è¿›æ‰©å±•ä»»åŠ¡ä¸­çš„ä»£ç ç”Ÿæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2601.21526",
      "arxiv_url": "https://arxiv.org/abs/2601.21526",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21526",
      "github_url": "https://github.com/Leeroo-AI/kapso",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:34:38.640660+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.23134",
      "title": "Machine Learning for Energy-Performance-aware Scheduling",
      "authors": [
        "Zheyuan Hu",
        "Yifei Shi"
      ],
      "abstract": "A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs. In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis ( fANOVA ) and comparing different covariance kernels (e.g., MatÃ©rn vs. RBF ), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.",
      "summary_en": "A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.",
      "summary_zh": "åˆ©ç”¨é«˜æ–¯è¿‡ç¨‹çš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•å¯åœ¨å¼‚æ„å¤šæ ¸ç³»ç»Ÿä¸Šè‡ªåŠ¨ä¼˜åŒ–è°ƒåº¦é…ç½®ï¼Œå¹¶é€¼è¿‘èƒ½è€—-æ—¶é—´æƒè¡¡çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.23134",
      "arxiv_url": "https://arxiv.org/abs/2601.23134",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23134",
      "github_url": "https://github.com/PeterHUistyping/ml-cpu-sched",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:35:21.098560+00:00"
    },
    {
      "date": "2026-02-02",
      "paper_id": "2601.22108",
      "title": "Value-Based Pre-Training with Downstream Feedback",
      "authors": [
        "Shuqi Ke",
        "Giulia Fanti"
      ],
      "abstract": "V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs. Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction ), which can misallocate compute away from downstream capabilities of interest. We introduce V- Pretraining : a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation . The V- Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V- Pretraining of 0.5B--7B language models improves reasoning ( GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining .",
      "summary_en": "V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.",
      "summary_zh": "V-Pretrainingåˆ©ç”¨ä¸‹æ¸¸ä»»åŠ¡æ¢¯åº¦é‡å¡‘é¢„è®­ç»ƒç›®æ ‡ï¼Œä»…éœ€å°‘é‡æ ‡æ³¨æ•°æ®å¹¶é™ä½è®¡ç®—æˆæœ¬å³å¯æå‡æ¨¡å‹èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2601.22108",
      "arxiv_url": "https://arxiv.org/abs/2601.22108",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22108",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:34:55.441961+00:00"
    }
  ]
}