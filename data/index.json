{
  "generated_at": "2026-02-19T02:00:33.889843+00:00",
  "count": 106,
  "dates": [
    "2026-02-19",
    "2026-02-18",
    "2026-02-17",
    "2026-02-16"
  ],
  "daily_summary": {
    "date": "2026-02-19",
    "content": "Overview\n- Date: 2026-02-19\n- Total Papers: 25\n- Total Upvotes: 234\n- Papers with GitHub: 13\n- Papers with AI Summary: 25\n\nKey Takeaways\n1. Sparse Autoencoders (SAEs) çš„å¯è§£é‡Šæ€§æœ‰æ•ˆæ€§é­å—ç³»ç»Ÿæ€§è´¨ç–‘ï¼Œç ”ç©¶è¡¨æ˜é«˜é‡å»ºæ€§èƒ½å¹¶ä¸ç­‰åŒäºå¯é çš„ç¥ç»ç½‘ç»œå†…éƒ¨ç‰¹å¾åˆ†è§£èƒ½åŠ›ã€‚\n2. Agentèƒ½åŠ›è¯„ä¼°å‘å¤šç»´åº¦æ‰©å±•ï¼Œä»æŠ€èƒ½è·¨ä»»åŠ¡æ³›åŒ–åˆ°çœŸå®ç ”ç©¶ç¯å¢ƒå‡æ­ç¤ºå½“å‰æ¨¡å‹æ˜¾è‘—çš„èƒ½åŠ›-å¯é æ€§å·®è·ã€‚\n3. å¤šæ¨¡æ€ç»Ÿä¸€æ¶æ„ä¸Test-time ScalingæŠ€æœ¯æ·±åº¦èåˆï¼Œé€šè¿‡Chain-of-Thoughtæœºåˆ¶å®ç°è¿­ä»£æ¨ç†ä¸å¤æ‚åœºæ™¯ä¸‹çš„æ€§èƒ½ä¼˜åŒ–ã€‚\n4. åŸºç¡€æ¨¡å‹å·¥ç¨‹åŒ–è¿›å±•æ˜¾è‘—ï¼ŒGLM-5é€šè¿‡DSAæ¶æ„ä¸å¼‚æ­¥RLHFå®ç°æˆæœ¬é™ä½å’Œå¯¹é½æ€§èƒ½æå‡ï¼Œæ¨åŠ¨Agentå·¥ç¨‹å®è·µèƒ½åŠ›å‘å±•ã€‚\n\nNotable Papers\n- [2602.14111] Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines? (ğŸ‘51): å°½ç®¡é‡å»ºæ€§èƒ½å¼ºåŠ²ï¼ŒSparse Autoencodersåœ¨åˆæˆä¸çœŸå®åœºæ™¯ä¸­å‡æ— æ³•å¯é åœ°åˆ†è§£ç¥ç»ç½‘ç»œå†…éƒ¨æœºåˆ¶ã€‚\n- [2602.12670] SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks (ğŸ‘38): è·¨86é¡¹ä»»åŠ¡è¯„ä¼°è¡¨æ˜ç²¾é€‰AgentæŠ€èƒ½è™½æ˜¾è‘—æå‡æ€§èƒ½ä½†ä¸€è‡´æ€§ä¸è¶³ï¼Œè‡ªç”ŸæˆæŠ€èƒ½æ•ˆæœæœ‰é™ã€‚\n- [2602.15763] GLM-5: from Vibe Coding to Agentic Engineering (ğŸ‘31): é‡‡ç”¨DSAæ¶æ„é™ä½æˆæœ¬å¹¶ç»“åˆå¼‚æ­¥RLHFå¢å¼ºå¯¹é½ï¼Œæå‡ç¼–ç ä¸Agentå·¥ç¨‹èƒ½åŠ›ã€‚\n- [2602.14299] Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook (ğŸ‘21): ç½‘ç»œç¯å¢ƒä¸­çš„LLM Agentç»´æŒä¸ªä½“å¤šæ ·æ€§å¹¶å±•ç°åŠ¨æ€ç¨³å®šæ€§ï¼Œä½†æœªå½¢æˆçœŸæ­£çš„ç¤¾ä¼šè¶‹åŒã€‚\n- [2602.12279] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling (ğŸ‘16): ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡Chain-of-Thought Test-time Scalingå®ç°è¿­ä»£æ¨ç†ä¸ä¼˜åŒ–ï¼Œæå‡ç”Ÿæˆä¸ç†è§£èƒ½åŠ›ã€‚",
    "source": "openrouter",
    "model": "moonshotai/kimi-k2.5",
    "generated_at": "2026-02-19T01:58:17.300320+00:00"
  },
  "daily_summaries": {
    "2026-02-19": {
      "date": "2026-02-19",
      "content": "Overview\n- Date: 2026-02-19\n- Total Papers: 25\n- Total Upvotes: 234\n- Papers with GitHub: 13\n- Papers with AI Summary: 25\n\nKey Takeaways\n1. Sparse Autoencoders (SAEs) çš„å¯è§£é‡Šæ€§æœ‰æ•ˆæ€§é­å—ç³»ç»Ÿæ€§è´¨ç–‘ï¼Œç ”ç©¶è¡¨æ˜é«˜é‡å»ºæ€§èƒ½å¹¶ä¸ç­‰åŒäºå¯é çš„ç¥ç»ç½‘ç»œå†…éƒ¨ç‰¹å¾åˆ†è§£èƒ½åŠ›ã€‚\n2. Agentèƒ½åŠ›è¯„ä¼°å‘å¤šç»´åº¦æ‰©å±•ï¼Œä»æŠ€èƒ½è·¨ä»»åŠ¡æ³›åŒ–åˆ°çœŸå®ç ”ç©¶ç¯å¢ƒå‡æ­ç¤ºå½“å‰æ¨¡å‹æ˜¾è‘—çš„èƒ½åŠ›-å¯é æ€§å·®è·ã€‚\n3. å¤šæ¨¡æ€ç»Ÿä¸€æ¶æ„ä¸Test-time ScalingæŠ€æœ¯æ·±åº¦èåˆï¼Œé€šè¿‡Chain-of-Thoughtæœºåˆ¶å®ç°è¿­ä»£æ¨ç†ä¸å¤æ‚åœºæ™¯ä¸‹çš„æ€§èƒ½ä¼˜åŒ–ã€‚\n4. åŸºç¡€æ¨¡å‹å·¥ç¨‹åŒ–è¿›å±•æ˜¾è‘—ï¼ŒGLM-5é€šè¿‡DSAæ¶æ„ä¸å¼‚æ­¥RLHFå®ç°æˆæœ¬é™ä½å’Œå¯¹é½æ€§èƒ½æå‡ï¼Œæ¨åŠ¨Agentå·¥ç¨‹å®è·µèƒ½åŠ›å‘å±•ã€‚\n\nNotable Papers\n- [2602.14111] Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines? (ğŸ‘51): å°½ç®¡é‡å»ºæ€§èƒ½å¼ºåŠ²ï¼ŒSparse Autoencodersåœ¨åˆæˆä¸çœŸå®åœºæ™¯ä¸­å‡æ— æ³•å¯é åœ°åˆ†è§£ç¥ç»ç½‘ç»œå†…éƒ¨æœºåˆ¶ã€‚\n- [2602.12670] SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks (ğŸ‘38): è·¨86é¡¹ä»»åŠ¡è¯„ä¼°è¡¨æ˜ç²¾é€‰AgentæŠ€èƒ½è™½æ˜¾è‘—æå‡æ€§èƒ½ä½†ä¸€è‡´æ€§ä¸è¶³ï¼Œè‡ªç”ŸæˆæŠ€èƒ½æ•ˆæœæœ‰é™ã€‚\n- [2602.15763] GLM-5: from Vibe Coding to Agentic Engineering (ğŸ‘31): é‡‡ç”¨DSAæ¶æ„é™ä½æˆæœ¬å¹¶ç»“åˆå¼‚æ­¥RLHFå¢å¼ºå¯¹é½ï¼Œæå‡ç¼–ç ä¸Agentå·¥ç¨‹èƒ½åŠ›ã€‚\n- [2602.14299] Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook (ğŸ‘21): ç½‘ç»œç¯å¢ƒä¸­çš„LLM Agentç»´æŒä¸ªä½“å¤šæ ·æ€§å¹¶å±•ç°åŠ¨æ€ç¨³å®šæ€§ï¼Œä½†æœªå½¢æˆçœŸæ­£çš„ç¤¾ä¼šè¶‹åŒã€‚\n- [2602.12279] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling (ğŸ‘16): ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡Chain-of-Thought Test-time Scalingå®ç°è¿­ä»£æ¨ç†ä¸ä¼˜åŒ–ï¼Œæå‡ç”Ÿæˆä¸ç†è§£èƒ½åŠ›ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-19T01:58:17.300320+00:00"
    },
    "2026-02-18": {
      "date": "2026-02-18",
      "content": "Overview\n- Date: 2026-02-18\n- Total Papers: 19\n- Total Upvotes: 140\n- Papers with GitHub: 9\n- Papers with AI Summary: 19\n\nKey Takeaways\n1. Agentèƒ½åŠ›è¯„ä¼°æˆä¸ºç„¦ç‚¹ï¼Œå¤šé¡¹ç ”ç©¶æ­ç¤ºå½“å‰AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸å¯é æ€§å·®è·ã€‚\n2. åŸºç¡€æ¨¡å‹è®­ç»ƒæ–¹æ³•æŒç»­æ¼”è¿›ï¼ŒGLM-5é€šè¿‡å¼‚æ­¥å¼ºåŒ–å­¦ä¹ ä¸DSAæ¶æ„ä¼˜åŒ–å®ç°å·¥ç¨‹åŒ–çªç ´ã€‚\n3. ç¥ç»ç½‘ç»œå¯è§£é‡Šæ€§é¢ä¸´æŒ‘æˆ˜ï¼ŒSparse Autoencodersåœ¨åˆ†è§£ç½‘ç»œå†…éƒ¨è¡¨ç¤ºæ–¹é¢æœªæ˜¾è‘—ä¼˜äºéšæœºåŸºçº¿ã€‚\n4. å¤šæ¨¡æ€ä¸è¡¨ç¤ºå­¦ä¹ ç ”ç©¶æ·±å…¥ï¼Œæ¶µç›–Test-time Scalingã€åµŒå…¥è’¸é¦åŠPlatonic Representation Hypothesisçš„é‡æ–°å®¡è§†ã€‚\n\nNotable Papers\n- [2602.12670] SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks (ğŸ‘26): é’ˆå¯¹86é¡¹ä»»åŠ¡è¯„ä¼°AgentæŠ€èƒ½ï¼Œå‘ç°äººå·¥ curated skills è™½æ˜¾è‘—æå‡æ€§èƒ½ä½†è¡¨ç°ä¸ä¸€è‡´ï¼Œè€Œè‡ªåŠ¨ç”ŸæˆæŠ€èƒ½å¯é æ€§ä¸è¶³ã€‚\n- [2602.15763] GLM-5: from Vibe Coding to Agentic Engineering (ğŸ‘25): é€šè¿‡DSAæ¶æ„é™ä½æ¨ç†æˆæœ¬ï¼Œç»“åˆå¼‚æ­¥å¼ºåŒ–å­¦ä¹ æå‡å¯¹é½æ•ˆæœï¼Œå¹¶å¢å¼ºä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œæ¨åŠ¨åŸºç¡€æ¨¡å‹å‘å·¥ç¨‹åŒ–æ™ºèƒ½ä½“æ¼”è¿›ã€‚\n- [2602.14111] Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines? (ğŸ‘22): ç³»ç»Ÿæ€§éªŒè¯è¡¨æ˜Sparse Autoencoderså°½ç®¡é‡å»ºæ€§èƒ½ä¼˜å¼‚ï¼Œä½†åœ¨åˆ†è§£ç¥ç»ç½‘ç»œå†…éƒ¨ç»“æ„æ–¹é¢æœªèƒ½ç¨³å®šè¶…è¶ŠéšæœºåŸºçº¿ã€‚\n- [2602.14299] Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook (ğŸ‘17): ç½‘ç»œç¯å¢ƒä¸­çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å‘ˆç°åŠ¨æ€ç¨³å®šæ€§è€ŒéçœŸæ­£çš„ç¤¾ä¼šæ”¶æ•›ï¼Œä¿æŒä¸ªä½“å¤šæ ·æ€§ä½†ç¼ºä¹é›†ä½“è§„èŒƒå½¢æˆã€‚\n- [2602.15112] ResearchGym: Evaluating Language Model Agents on Real-World AI Research (ğŸ‘11): æ„å»ºç«¯åˆ°ç«¯AIç ”ç©¶ä»»åŠ¡åŸºå‡†ç¯å¢ƒï¼Œæ­ç¤ºå½“å‰è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“åœ¨å¤æ‚ç§‘ç ”æµç¨‹ä¸­çš„èƒ½åŠ›-å¯é æ€§é¸¿æ²Ÿã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-18T14:13:05.287620+00:00"
    },
    "2026-02-17": {
      "date": "2026-02-17",
      "content": "Overview\n- Date: 2026-02-17\n- Total Papers: 28\n- Total Upvotes: 156\n- Papers with GitHub: 17\n- Papers with AI Summary: 28\n\nKey Takeaways\n1. å¤šæ¨¡æ€Agentä¸è§†è§‰æ£€ç´¢ç³»ç»Ÿæˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚\n2. å¼ºåŒ–å­¦ä¹ èŒƒå¼æŒç»­æ¼”è¿›ï¼Œä»ç»éªŒåæ€åˆ°æ¨ç†é©±åŠ¨çš„è¡¨å¾å­¦ä¹ ï¼Œæå‡ç¨€ç–å¥–åŠ±åœºæ™¯ä¸‹çš„å­¦ä¹ æ•ˆç‡ã€‚\n3. æ¨¡å‹æ•ˆç‡ä¸æ¶æ„åˆ›æ–°å¹¶é‡ï¼ŒäºŒè¿›åˆ¶Tokenè¡¨ç¤ºï¼ˆBinary Tokensï¼‰å’Œå°è§„æ¨¡é«˜æ€§èƒ½æ¨¡å‹ï¼ˆ3Bå‚æ•°ï¼‰å—åˆ°å…³æ³¨ã€‚\n4. è·¨é¢†åŸŸåº”ç”¨æ‹“å±•æ˜¾è‘—ï¼Œæ¶µç›–é‡å­æ•°æ®åº“ï¼ˆQuantum-Native Databaseï¼‰ã€ç§‘ç ”æ•°æ®è¯„ä¼°ä¸è‡ªåŠ¨åŒ–æœç´¢æ¡†æ¶ã€‚\n\nNotable Papers\n- [2602.10809] DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories (ğŸ‘24): æå‡ºæ¨¡å—åŒ–Agentæ¡†æ¶ï¼Œé€šè¿‡å¤šæ­¥æ¨ç†è§£å†³ä¼ ç»Ÿè¯­ä¹‰åŒ¹é…åœ¨è§†è§‰å†å²æ£€ç´¢ä¸­çš„å±€é™æ€§ã€‚\n- [2602.14492] Query as Anchor: Scenario-Adaptive User Representation via Large Language Model (ğŸ‘14): å°†ç”¨æˆ·å»ºæ¨¡ä»é™æ€ç¼–ç è½¬å˜ä¸ºåŸºäºLLMçš„åŠ¨æ€æŸ¥è¯¢æ„ŸçŸ¥åˆæˆï¼Œå®ç°åœºæ™¯è‡ªé€‚åº”è¡¨ç¤ºã€‚\n- [2602.13949] Experiential Reinforcement Learning (ğŸ‘14): å¼•å…¥æ˜¾å¼ç»éªŒ-åæ€-å·©å›ºå¾ªç¯æœºåˆ¶ï¼Œæ˜¾è‘—æå‡ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸‹çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚\n- [2602.14699] Qute: Towards Quantum-Native Database (ğŸ‘13): é¦–ä¸ªå°†é‡å­è®¡ç®—ä½œä¸ºä¸€ç­‰æ‰§è¡Œé€‰é¡¹çš„æ•°æ®åº“ç³»ç»Ÿï¼Œæ”¯æŒæ‰©å±•SQLåˆ°é—¨çº§é‡å­ç”µè·¯çš„é«˜æ•ˆç¼–è¯‘ä¸æ··åˆä¼˜åŒ–ã€‚\n- [2602.14234] REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents (ğŸ‘13): é€šè¿‡ä»»åŠ¡åˆæˆã€å·¥å…·å¢å¼ºæŸ¥è¯¢å’Œæ¨¡æ‹Ÿç¯å¢ƒä¼˜åŒ–ï¼Œæ„å»ºå¯æ‰©å±•çš„é•¿ç¨‹æœç´¢Agentæ¡†æ¶ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-17T16:24:46.833682+00:00"
    },
    "2026-02-16": {
      "date": "2026-02-16",
      "content": "Overview\n- Date: 2026-02-16\n- Total Papers: 32\n- Total Upvotes: 639\n- Papers with GitHub: 20\n- Papers with AI Summary: 32\n\nKey Takeaways\n1. å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰æŒç»­å‘å‚ç›´é¢†åŸŸæ·±åŒ–ï¼ŒåŒ»ç–—è¯Šæ–­ä¸ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥æˆä¸ºé‡ç‚¹çªç ´æ–¹å‘ã€‚\n2. å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ç»“åˆç´§å¯†ï¼Œåœ¨æœºå™¨äººæ“æ§å’Œè§†è§‰æ¨ç†ä¸­æ¢ç´¢èƒ½åŠ›è¾¹ç•Œã€‚\n3. æ•°æ®æ•ˆç‡ç ”ç©¶å—å…³æ³¨ï¼Œç‰¹å¾ç©ºé—´æ•°æ®åˆæˆä¸ç¼–è§£ç å™¨å¯¹é½çš„ç¨€ç–æ€§æ–¹æ³•æå‡è®­ç»ƒä¸æ¨ç†æ•ˆç‡ã€‚\n4. é²æ£’æ€§è¯„ä¼°åŸºå‡†å»ºè®¾åŠ é€Ÿï¼Œé’ˆå¯¹è¯­éŸ³æ£€ç´¢å’Œåœ°ç†å®šä½ç­‰åœºæ™¯çš„ç³»ç»ŸåŒ–æµ‹è¯•æ¡†æ¶ç›¸ç»§æå‡ºã€‚\n\nNotable Papers\n- [2602.10388] Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs (ğŸ‘203): æå‡ºFeature Activation CoverageæŒ‡æ ‡ï¼Œåœ¨LLMå¯è§£é‡Šç‰¹å¾ç©ºé—´åˆæˆå¤šæ ·åŒ–æ•°æ®ï¼Œæ— éœ€å¢åŠ æ•°æ®é‡å³å¯æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚\n- [2602.12783] SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise (ğŸ‘134): æ„å»ºåŒ…å«37,317ä¸ªç‹¬ç‰¹æŸ¥è¯¢çš„è·¨è¯­è¨€é²æ£’æ€§åŸºå‡†ï¼Œç³»ç»Ÿè¯„ä¼°è¯­éŸ³æŸ¥è¯¢æ£€ç´¢åœ¨å£°å­¦å™ªå£°ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚\n- [2602.12705] MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs (ğŸ‘56): é€šè¿‡å®ä½“æ„ŸçŸ¥æŒç»­é¢„è®­ç»ƒã€RLHFå’Œå·¥å…·å¢å¼ºæ™ºèƒ½ä½“è®­ç»ƒï¼Œæ„å»ºé¢å‘å¯é åŒ»ç–—è¯Šæ–­çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚\n- [2602.11858] Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception (ğŸ‘52): æå‡ºRegion-to-Image Distillationæ–¹æ³•ï¼Œä½¿MLLMåœ¨æ¨ç†æ—¶å†…éƒ¨æ‰§è¡Œè¿­ä»£ç¼©æ”¾ï¼Œæ— éœ€é‡å¤å·¥å…·è°ƒç”¨å³å¯å®ç°ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥ã€‚\n- [2602.08683] OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence (ğŸ‘40): åŸºäºè§†é¢‘å‹ç¼©çš„ä¿¡æ¯è®ºåŸç†ï¼Œåˆ©ç”¨Codecå¯¹é½çš„ç¨€ç–æ€§é©±åŠ¨ç¼–ç ï¼Œåœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§ä¸Šè¶…è¶Šä¼ ç»Ÿè§†è§‰ç¼–ç æ–¹æ³•ã€‚",
      "source": "openrouter",
      "model": "moonshotai/kimi-k2.5",
      "generated_at": "2026-02-18T11:40:58.973200+00:00"
    }
  },
  "papers": [
    {
      "date": "2026-02-19",
      "paper_id": "2602.14111",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "authors": [
        "Anton Korznikov",
        "Andrey Galichin",
        "Alexey Dontsov",
        "Oleg Rogov",
        "Ivan Oseledets",
        "Elena Tutubalina"
      ],
      "abstract": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations. Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance , showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations , we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.",
      "summary_en": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.",
      "summary_zh": "ç¨€ç–è‡ªç¼–ç å™¨å°½ç®¡é‡æ„æ€§èƒ½å¼ºåŠ²ï¼Œå´æ— æ³•å¯é åˆ†è§£ç¥ç»ç½‘ç»œå†…éƒ¨æœºåˆ¶ï¼Œè¿™ä¸€ç‚¹é€šè¿‡åˆæˆä¸çœŸå®æ¿€æ´»è¯„ä¼°å¾—åˆ°äº†è¯å®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14111",
      "arxiv_url": "https://arxiv.org/abs/2602.14111",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14111",
      "github_url": "",
      "upvotes": 51,
      "fetched_at": "2026-02-19T01:55:38.241233+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.12670",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "authors": [
        "Xiangyi Li",
        "Wenbo Chen",
        "Yimin Liu",
        "Shenghan Zheng",
        "Xiaokun Chen",
        "Yifeng He",
        "Yubo Li",
        "Bingran You",
        "Haotian Shen",
        "Jiankai Sun",
        "Shuyi Wang",
        "Qunhong Zeng",
        "Di Wang",
        "Xuandong Zhao",
        "Yuanli Wang",
        "Roey Ben Chaim",
        "Zonglin Di",
        "Yipeng Gao",
        "Junwei He",
        "Yizhuo He",
        "Liqiang Jing",
        "Luyang Kong"
      ],
      "abstract": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions. Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench , a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills , and self-generated Skills . We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "summary_en": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.",
      "summary_zh": "SkillsBenchåœ¨86é¡¹ä»»åŠ¡ä¸Šè¯„ä¼°æ™ºèƒ½ä½“æŠ€èƒ½ï¼Œå‘ç°ç²¾é€‰æŠ€èƒ½è™½èƒ½æ˜¾è‘—æå‡æ€§èƒ½ä½†æ•ˆæœä¸ä¸€ï¼Œè€Œè‡ªç”ŸæˆæŠ€èƒ½åˆ™æ¯«æ— åŠ©ç›Šï¼Œè¿™è¡¨æ˜æ¨¡å‹è™½èƒ½ä»ç²¾é€‰æŠ€èƒ½ä¸­è·ç›Šï¼Œå´éš¾ä»¥è‡ªè¡Œåˆ›å»ºæœ‰ç”¨çš„ç¨‹åºæ€§çŸ¥è¯†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12670",
      "arxiv_url": "https://arxiv.org/abs/2602.12670",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12670",
      "github_url": "https://github.com/benchflow-ai/skillsbench",
      "upvotes": 38,
      "fetched_at": "2026-02-19T01:55:35.285668+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15763",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "authors": [
        "GLM-5 Team",
        "Aohan Zeng",
        "Xin Lv",
        "Zhenyu Hou",
        "Zhengxiao Du",
        "Qinkai Zheng",
        "Bin Chen",
        "Da Yin",
        "Chendi Ge",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Gengzheng Pan",
        "Hao Zeng",
        "Haoke Zhang",
        "Haoran Wang",
        "Huilong Chen",
        "Jiajie Zhang",
        "Jian Jiao",
        "Jiaqi Guo",
        "Jingsen Wang",
        "Jingzhao Du",
        "Jinzhu Wu"
      ],
      "abstract": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering. We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering . Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks . Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "summary_en": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.",
      "summary_zh": "GLM-5 é€šè¿‡ DSA é™ä½æˆæœ¬ã€å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æå‡å¯¹é½ä»¥åŠå¢å¼ºé¢å‘çœŸå®è½¯ä»¶å·¥ç¨‹çš„ç¼–ç¨‹èƒ½åŠ›ï¼Œæ¨è¿›åŸºç¡€æ¨¡å‹å‘å±•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15763",
      "arxiv_url": "https://arxiv.org/abs/2602.15763",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15763",
      "github_url": "https://github.com/zai-org/GLM-5",
      "upvotes": 31,
      "fetched_at": "2026-02-19T01:55:54.127746+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14299",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "authors": [
        "Ming Li",
        "Xirui Li",
        "Tianyi Zhou"
      ],
      "abstract": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures. As large language model agents increasingly populate networked environments , a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies , measuring semantic stabilization , lexical turnover , individual inertia , influence persistence , and collective consensus . Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover , defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies .",
      "summary_en": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.",
      "summary_zh": "ç½‘ç»œç¯å¢ƒä¸­çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å‘ˆç°åŠ¨æ€ç¨³å®šæ€§ï¼Œä½†æœªå®ç°çœŸæ­£çš„ç¤¾ä¼šæ”¶æ•›ï¼Œåœ¨ä¿æŒä¸ªä½“å¤šæ ·æ€§çš„åŒæ—¶ç¼ºä¹é›†ä½“å½±å“ç»“æ„ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14299",
      "arxiv_url": "https://arxiv.org/abs/2602.14299",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14299",
      "github_url": "https://github.com/MingLiiii/Moltbook_Socialization",
      "upvotes": 21,
      "fetched_at": "2026-02-19T01:55:39.715212+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.12279",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha",
        "Xiaoliang Dai",
        "Jialiang Wang",
        "Zecheng He",
        "Jianwei Yang",
        "Chunyuan Li",
        "Junzhe Sun",
        "Chu Wang",
        "Serena Yeung-Levy",
        "Felix Juefei-Xu"
      ],
      "abstract": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities. Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis , unified model training , and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning . These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models .",
      "summary_en": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.",
      "summary_zh": "UniTæ¡†æ¶ä½¿ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ€ç»´é“¾æµ‹è¯•æ—¶ç¼©æ”¾è¿›è¡Œè¿­ä»£æ¨ç†ä¸ä¼˜åŒ–ï¼Œä»è€Œæå‡ç”Ÿæˆä¸ç†è§£èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12279",
      "arxiv_url": "https://arxiv.org/abs/2602.12279",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12279",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T01:55:33.664750+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15112",
      "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi Patwardhan",
        "Arman Cohan"
      ],
      "abstract": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance. We introduce ResearchGym , a benchmark and execution environment for evaluating AI agents on end-to-end research . To instantiate this, we repurpose five oral and spotlight papers from ICML , ICLR , and ACL . From each paper's repository, we preserve the datasets , evaluation harness , and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5 , we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length . Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex ( GPT-5 .2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
      "summary_en": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.",
      "summary_zh": "ResearchGymæå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“ç«¯åˆ°ç«¯ç ”ç©¶ä»»åŠ¡çš„åŸºå‡†ç¯å¢ƒï¼Œæ­ç¤ºäº†å½“å‰è‡ªä¸»æ™ºèƒ½ä½“å°½ç®¡å¶å°”èƒ½å–å¾—æœ€ä¼˜æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—çš„èƒ½åŠ›-å¯é æ€§å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15112",
      "arxiv_url": "https://arxiv.org/abs/2602.15112",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15112",
      "github_url": "https://github.com/Anikethh/ResearchGym",
      "upvotes": 14,
      "fetched_at": "2026-02-19T01:55:43.087568+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15547",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
      "authors": [
        "Mohammad Kalim Akram",
        "Saba Sturua",
        "Nastia Havriushenko",
        "Quentin Herreros",
        "Michael GÃ¼nther",
        "Maximilian Werk",
        "Han Xiao"
      ],
      "abstract": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization. Text embedding models are widely used for semantic similarity tasks, including information retrieval , clustering , and classification . General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models . Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization . Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "summary_en": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization.",
      "summary_zh": "ç´§å‡‘å‹æ–‡æœ¬åµŒå…¥æ¨¡å‹é€šè¿‡è’¸é¦ä¸å¯¹æ¯”æŸå¤±ç›¸ç»“åˆçš„è®­ç»ƒæ–¹æ³•å¼€å‘ï¼Œåœ¨å®ç°æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶æ”¯æŒé•¿ä¸Šä¸‹æ–‡åºåˆ—å’Œé«˜æ•ˆé‡åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15547",
      "arxiv_url": "https://arxiv.org/abs/2602.15547",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15547",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T01:55:51.191144+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14486",
      "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
      "authors": [
        "Fabian GrÃ¶ger",
        "Shuo Wen",
        "Maria BrbiÄ‡"
      ],
      "abstract": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics. The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity , but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis : representations in neural networks are converging to shared local neighborhood relationships.",
      "summary_en": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.",
      "summary_zh": "ç»æ ¡å‡†çš„ç›¸ä¼¼æ€§åº¦é‡æ˜¾ç¤ºï¼Œç½‘ç»œè¡¨å¾æ”¶æ•›äºå…±äº«çš„å±€éƒ¨é‚»åŸŸå…³ç³»ï¼Œè€Œéå…¨å±€ç»Ÿè®¡æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14486",
      "arxiv_url": "https://arxiv.org/abs/2602.14486",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14486",
      "github_url": "https://github.com/mlbio-epfl/aristotelian",
      "upvotes": 7,
      "fetched_at": "2026-02-19T01:55:42.035681+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15772",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
      "authors": [
        "Sen Ye",
        "Mengde Xu",
        "Shuyang Gu",
        "Di He",
        "Liwei Wang",
        "Han Hu"
      ],
      "abstract": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation. Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma , achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models . Code is available at https://github.com/sen-ye/R3.",
      "summary_en": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.",
      "summary_zh": "æ¨ç†-åæ€-ç²¾ç‚¼æ¡†æ¶é€šè¿‡å°†å•æ­¥ç”Ÿæˆé‡æ–°è¡¨è¿°ä¸ºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¾å¼èå…¥ç†è§£çš„å¤šæ­¥è¿‡ç¨‹ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15772",
      "arxiv_url": "https://arxiv.org/abs/2602.15772",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15772",
      "github_url": "https://github.com/sen-ye/R3",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:55.413014+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15322",
      "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
      "authors": [
        "Taejong Joo",
        "Wenhan Xia",
        "Cheolmin Kim",
        "Ming Zhang",
        "Eugene Ie"
      ],
      "abstract": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers. Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners . We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment . Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
      "summary_en": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.",
      "summary_zh": "éšæœºå‚æ•°æ›´æ–°æ©ç é€šè¿‡è¯±å¯¼æ›²ç‡ç›¸å…³çš„æ­£åˆ™åŒ–ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹å®ç°æ›´ä¼˜ä¼˜åŒ–ï¼Œå…¶åŠ¨é‡å¯¹é½å˜ä½“ç›¸è¾ƒæœ€å…ˆè¿›çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15322",
      "arxiv_url": "https://arxiv.org/abs/2602.15322",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15322",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:46.990692+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15200",
      "title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
      "authors": [
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Ammar Ali",
        "Baher Mohammad",
        "Stamatios Lefkimmiatis"
      ],
      "abstract": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods. Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization . COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.",
      "summary_en": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.",
      "summary_zh": "COMPOT æ˜¯ä¸€ç§é¢å‘ Transformer æ¨¡å‹çš„å…è®­ç»ƒå‹ç¼©æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ­£äº¤å­—å…¸ä¸é—­å¼æ›´æ–°çš„ç¨€ç–å­—å…¸å­¦ä¹ ï¼Œå®ç°äº†ä¼˜äºä¼ ç»Ÿä½ç§©æ–¹æ³•çš„è´¨é‡-å‹ç¼©æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15200",
      "arxiv_url": "https://arxiv.org/abs/2602.15200",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15200",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:44.392487+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15156",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "abstract": "Panini enables efficient and accurate language model reasoning through a non-parametric continual learning framework that uses generative semantic workspaces to store and retrieve knowledge, achieving superior performance with reduced computational overhead.",
      "summary_en": "Panini enables efficient and accurate language model reasoning through a non-parametric continual learning framework that uses generative semantic workspaces to store and retrieve knowledge, achieving superior performance with reduced computational overhead.",
      "summary_zh": "Paninié€šè¿‡éå‚æ•°æŒç»­å­¦ä¹ æ¡†æ¶å®ç°é«˜æ•ˆå‡†ç¡®çš„è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆå¼è¯­ä¹‰å·¥ä½œç©ºé—´å­˜å‚¨å’Œæ£€ç´¢çŸ¥è¯†ï¼Œåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶å–å¾—æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15156",
      "arxiv_url": "https://arxiv.org/abs/2602.15156",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15156",
      "github_url": "https://github.com/roychowdhuryresearch/gsw-memory",
      "upvotes": 5,
      "fetched_at": "2026-02-19T01:55:43.772365+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15449",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
      ],
      "abstract": "Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure. Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "summary_en": "Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure.",
      "summary_zh": "é¢å‘ä»£ç ç”Ÿæˆçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å››å±‚æµ‹è¯•å¥—ä»¶ç»“æ„åŸºäºæ¨¡å‹èƒ½åŠ›è¿›è¡Œè‡ªé€‚åº”è¯¾ç¨‹è®¾è®¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15449",
      "arxiv_url": "https://arxiv.org/abs/2602.15449",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15449",
      "github_url": "https://github.com/deep-diver/TAROT",
      "upvotes": 4,
      "fetched_at": "2026-02-19T01:55:50.538737+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15620",
      "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
      "authors": [
        "Shiqi Liu",
        "Zeyu He",
        "Guojian Zhan",
        "Letian Tao",
        "Zhilong Zheng",
        "Jiang Wu",
        "Yinuo Wang",
        "Yang Guan",
        "Kehua Sheng",
        "Bo Zhang",
        "Keqiang Li",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "abstract": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance. Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy . Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens . When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates . Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO , 20- Entropy and JustRL .",
      "summary_en": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.",
      "summary_zh": "ç ”ç©¶å‘ç°ï¼Œå¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„è®­ç»ƒä¸ç¨³å®šæ€§æºäºè™šå‡ tokenï¼Œå¹¶æå‡ºé€šè¿‡é€‰æ‹©æ€§æ©ç é—®é¢˜æ¢¯åº¦æ›´æ–°ä»¥æå‡æ¨ç†æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15620",
      "arxiv_url": "https://arxiv.org/abs/2602.15620",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15620",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T01:55:52.727674+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15278",
      "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
      "authors": [
        "Manuel Cherep",
        "Pranav M R",
        "Pattie Maes",
        "Nikhil Singh"
      ],
      "abstract": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns. The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference : choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization , adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities , safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
      "summary_en": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.",
      "summary_zh": "é€šè¿‡å—æ§å›¾åƒé€‰æ‹©ä»»åŠ¡ä¸ç³»ç»Ÿæ€§è¾“å…¥æ‰°åŠ¨ç ”ç©¶è§†è§‰è¯­è¨€æ¨¡å‹çš„å†³ç­–åå¥½ï¼Œæ­ç¤ºå…¶è§†è§‰æ¼æ´ä¸å®‰å…¨é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15278",
      "arxiv_url": "https://arxiv.org/abs/2602.15278",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15278",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T01:55:45.581697+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15382",
      "title": "The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems",
      "authors": [
        "Xiaoze Liu",
        "Ruowang Zhang",
        "Weichen Yu",
        "Siheng Xiong",
        "Liu He",
        "Feijie Wu",
        "Hoin Jung",
        "Matt Fredrikson",
        "Xiaoqian Wang",
        "Jing Gao"
      ],
      "abstract": "A Vision Wormhole framework enables efficient, model-agnostic communication in multi-agent systems by using visual-language models to transfer reasoning states through a shared latent space, reducing computational overhead while maintaining reasoning accuracy. Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec , we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway , effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas",
      "summary_en": "A Vision Wormhole framework enables efficient, model-agnostic communication in multi-agent systems by using visual-language models to transfer reasoning states through a shared latent space, reducing computational overhead while maintaining reasoning accuracy.",
      "summary_zh": "Vision Wormholeæ¡†æ¶é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å…±äº«éšç©ºé—´ä¸­ä¼ é€’æ¨ç†çŠ¶æ€ï¼Œå®ç°äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­é«˜æ•ˆä¸”ä¸æ¨¡å‹æ— å…³çš„é€šä¿¡ï¼Œåœ¨é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15382",
      "arxiv_url": "https://arxiv.org/abs/2602.15382",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15382",
      "github_url": "https://github.com/xz-liu/heterogeneous-latent-mas",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:49.347712+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.15327",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "authors": [
        "Hanlin Zhang",
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade"
      ],
      "abstract": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks. For deploying foundation models, practitioners increasingly need prescriptive scaling laws : given a pre training compute budget , what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance , we estimate capability boundaries , high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization . We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budget s into reliable performance expectations and for monitoring when capability boundaries shift across time.",
      "summary_en": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.",
      "summary_zh": "å¤§è§„æ¨¡è§‚å¯Ÿæ€§åˆ†æä½¿ç”¨åˆ†ä½æ•°å›å½’ä¼°è®¡åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œä¸æ€§èƒ½é¢„æµ‹ï¼Œå¹¶è¯„ä¼°è·¨ä»»åŠ¡çš„æ—¶é—´ç¨³å®šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15327",
      "arxiv_url": "https://arxiv.org/abs/2602.15327",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15327",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:47.959989+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.13964",
      "title": "HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam",
      "authors": [
        "Weiqi Zhai",
        "Zhihai Wang",
        "Jinghang Wang",
        "Boyu Yang",
        "Xiaogang Li",
        "Xiang Xu",
        "Bohan Wang",
        "Peng Wang",
        "Xingzhe Wu",
        "Anfeng Li",
        "Qiyuan Feng",
        "Yuhao Zhou",
        "Shoulin Han",
        "Wenjie Luo",
        "Yiyuan Li",
        "Yaxuan Wang",
        "Ruixian Luo",
        "Guojie Lin",
        "Peiyao Xiao",
        "Chengliang Xu",
        "Ben Wang",
        "Zeyu Wang"
      ],
      "abstract": "HLE-Verified presents a validated and revised version of the HLE benchmark with improved reliability through expert review and model-based checks, demonstrating significant accuracy improvements in language model evaluations. Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy . Our construction follows a two-stage validation -and-repair workflow resulting in a certified benchmark . In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks , yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs , model-assisted auditing , and final adjudication , resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified",
      "summary_en": "HLE-Verified presents a validated and revised version of the HLE benchmark with improved reliability through expert review and model-based checks, demonstrating significant accuracy improvements in language model evaluations.",
      "summary_zh": "HLE-Verifiedæ˜¯HLEåŸºå‡†ç»è¿‡éªŒè¯ä¸ä¿®è®¢çš„ç‰ˆæœ¬ï¼Œé€šè¿‡ä¸“å®¶å®¡æ ¸å’ŒåŸºäºæ¨¡å‹çš„æ£€æŸ¥æå‡äº†å¯é æ€§ï¼Œåœ¨è¯­è¨€æ¨¡å‹è¯„ä¼°ä¸­å±•ç°å‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13964",
      "arxiv_url": "https://arxiv.org/abs/2602.13964",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13964",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:37.496105+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.12978",
      "title": "Learning Native Continuation for Action Chunking Flow Policies",
      "authors": [
        "Yufeng Liu",
        "Hang Yu",
        "Juntu Zhao",
        "Bocheng Li",
        "Di Zhang",
        "Mingzhu Li",
        "Wenxuan Wu",
        "Yingdong Hu",
        "Junyuan Xie",
        "Junliang Guo",
        "Dequan Wang",
        "Yang Gao"
      ],
      "abstract": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution. Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time . Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time .",
      "summary_en": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.",
      "summary_zh": "Legato åˆ©ç”¨è®­ç»ƒæ—¶å»¶ç»­æ–¹æ³•æ”¹è¿›åŠ¨ä½œåˆ†å—çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œç¡®ä¿è½¨è¿¹å¹³æ»‘å¹¶å‡å°‘å®æ—¶æ‰§è¡Œä¸­çš„å¤šæ¨¡æ€åˆ‡æ¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12978",
      "arxiv_url": "https://arxiv.org/abs/2602.12978",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12978",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:36.092392+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "authors": [
        "Heejeong Nam",
        "Quentin Le Lidec",
        "Lucas Maes",
        "Yann LeCun",
        "Randall Balestriero"
      ],
      "abstract": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.",
      "summary_en": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.",
      "summary_zh": "C-JEPAå°†æ©ç è”åˆåµŒå…¥é¢„æµ‹æ‰©å±•è‡³ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨å¾ï¼Œé€šè¿‡å¼•å…¥å› æœå½’çº³åç½®çš„å¯¹è±¡çº§æ©ç å®ç°ç¨³å¥çš„å…³ç³»ç†è§£ï¼Œå¹¶æå‡æ¨ç†ä¸æ§åˆ¶ä»»åŠ¡çš„è¡¨ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11389",
      "arxiv_url": "https://arxiv.org/abs/2602.11389",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11389",
      "github_url": "https://github.com/galilai-group/cjepa",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:32.256322+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.09653",
      "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
      "authors": [
        "Shiwei Lyu",
        "Xidong Wang",
        "Lei Liu",
        "Hao Zhu",
        "Chaohe Zhang",
        "Jian Wang",
        "Jinjie Gu",
        "Benyou Wang",
        "Yue Shen"
      ],
      "abstract": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning. Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics , a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples : 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision . A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard , outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
      "summary_en": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.",
      "summary_zh": "ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶é€šè¿‡åŒ»ç”ŸéªŒè¯çš„ç¤ºä¾‹å’Œè’¸é¦å¾—åˆ°çš„ä¸´åºŠåŸåˆ™ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹ä¸ä¸´åºŠåŒ»ç”Ÿåå¥½çš„å¯¹é½ï¼Œä»¥æ”¹è¿›åŒ»å­¦æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09653",
      "arxiv_url": "https://arxiv.org/abs/2602.09653",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09653",
      "github_url": "https://github.com/AQ-MedAI/ClinAlign",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:30.816675+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.07854",
      "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model",
      "authors": [
        "Chendong Xiang",
        "Jiajun Liu",
        "Jintao Zhang",
        "Xiao Yang",
        "Zhengwei Fang",
        "Shizun Wang",
        "Zijun Wang",
        "Yingtian Zou",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization. Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings , which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers . By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention , which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift . Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.",
      "summary_en": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.",
      "summary_zh": "ViewRopeæ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡å°†ç›¸æœºå°„çº¿æ–¹å‘æ³¨å…¥è§†é¢‘Transformerçš„æ³¨æ„åŠ›å±‚æ¥å¢å¼ºé¢„æµ‹æ€§ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨ç›¸å¯¹å°„çº¿å‡ ä½•å‚æ•°åŒ–è§£å†³ç©ºé—´æŒä¹…æ€§é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07854",
      "arxiv_url": "https://arxiv.org/abs/2602.07854",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07854",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T01:55:29.881857+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.14364",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "authors": [
        "Tianyu Chen",
        "Dongrui Liu",
        "Xia Hu",
        "Jingyi Yu",
        "Wenjie Wang"
      ],
      "abstract": "Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks. Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench ) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge ( AgentDoG-Qwen3-4B ) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "summary_en": "Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks.",
      "summary_zh": "Clawdbotæ˜¯ä¸€æ¬¾å…·å¤‡å¤šæ ·åŒ–å·¥å…·èƒ½åŠ›çš„è‡ªæ‰˜ç®¡AIæ™ºèƒ½ä½“ï¼Œå…¶åœ¨ä¸åŒé£é™©ç»´åº¦ä¸Šçš„å®‰å…¨æ€§èƒ½è¡¨ç°å„å¼‚ï¼Œå°¤å…¶åœ¨å¤„ç†æ¨¡ç³Šæˆ–å¯¹æŠ—æ€§è¾“å…¥æ—¶å­˜åœ¨å›°éš¾ï¼Œå°½ç®¡åœ¨æŒ‡å®šä»»åŠ¡ä¸­ä¿æŒä¸€è‡´çš„å¯é æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14364",
      "arxiv_url": "https://arxiv.org/abs/2602.14364",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14364",
      "github_url": "https://github.com/tychenn/clawdbot_report",
      "upvotes": 1,
      "fetched_at": "2026-02-19T01:55:40.666586+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.12235",
      "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
      "authors": [
        "Julia Belikova",
        "Danila Rozhevskii",
        "Dennis Svirin",
        "Konstantin Polev",
        "Alexander Panchenko"
      ],
      "abstract": "Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors. Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens . Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA , SQuADv2 , and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
      "summary_en": "Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors.",
      "summary_zh": "é¢å‘é•¿ä¸Šä¸‹æ–‡LLMçš„è½¯å‹ç¼©æ¶æ„ä½¿ç”¨æŸ¥è¯¢æ„ŸçŸ¥æ¢æµ‹åˆ†ç±»å™¨æ£€æµ‹tokenæº¢å‡ºå¹¶ç¼“è§£å‹ç¼©å¼•èµ·çš„è¯¯å·®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12235",
      "arxiv_url": "https://arxiv.org/abs/2602.12235",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12235",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T01:55:32.934470+00:00"
    },
    {
      "date": "2026-02-19",
      "paper_id": "2602.10210",
      "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge",
      "authors": [
        "Junhong Lin",
        "Bing Zhang",
        "Song Wang",
        "Ziyan Liu",
        "Dan Gutfreund",
        "Julian Shun",
        "Yada Zhu"
      ],
      "abstract": "HybridRAG-Bench evaluates retrieval-intensive multi-hop reasoning in large language models by combining unstructured text and structured knowledge graphs from recent scientific literature, providing a contamination-aware benchmark that distinguishes genuine retrieval and reasoning from parametric recall. Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning . Augmenting LLMs with hybrid external knowledge , such as unstructured text and structured knowledge graphs , offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall . We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall , offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.",
      "summary_en": "HybridRAG-Bench evaluates retrieval-intensive multi-hop reasoning in large language models by combining unstructured text and structured knowledge graphs from recent scientific literature, providing a contamination-aware benchmark that distinguishes genuine retrieval and reasoning from parametric recall.",
      "summary_zh": "HybridRAG-Benchç»“åˆæœ€æ–°ç§‘å­¦æ–‡çŒ®ä¸­çš„éç»“æ„åŒ–æ–‡æœ¬ä¸ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼Œè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¯†é›†å‹å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œæä¾›äº†ä¸€ä¸ªé˜²æ±¡æŸ“åŸºå‡†ï¼Œç”¨äºåŒºåˆ†çœŸå®çš„æ£€ç´¢æ¨ç†ä¸å‚æ•°åŒ–è®°å¿†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10210",
      "arxiv_url": "https://arxiv.org/abs/2602.10210",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10210",
      "github_url": "https://github.com/junhongmit/HybridRAG-Bench",
      "upvotes": 1,
      "fetched_at": "2026-02-19T01:55:31.510579+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12670",
      "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
      "authors": [
        "Xiangyi Li",
        "Wenbo Chen",
        "Yimin Liu",
        "Shenghan Zheng",
        "Xiaokun Chen",
        "Yifeng He",
        "Yubo Li",
        "Bingran You",
        "Haotian Shen",
        "Jiankai Sun",
        "Shuyi Wang",
        "Qunhong Zeng",
        "Di Wang",
        "Xuandong Zhao",
        "Yuanli Wang",
        "Roey Ben Chaim",
        "Zonglin Di",
        "Yipeng Gao",
        "Junwei He",
        "Yizhuo He",
        "Liqiang Jing",
        "Luyang Kong"
      ],
      "abstract": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions. Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench , a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills , and self-generated Skills . We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
      "summary_en": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.",
      "summary_zh": "SkillsBenchåœ¨86é¡¹ä»»åŠ¡ä¸­è¯„ä¼°äº†æ™ºèƒ½ä½“æŠ€èƒ½ï¼Œå‘ç°ç²¾é€‰æŠ€èƒ½è™½èƒ½æ˜¾è‘—æå‡æ€§èƒ½ä½†æ•ˆæœä¸ä¸€ï¼Œè€Œè‡ªç”ŸæˆæŠ€èƒ½åˆ™æ¯«æ— åŠ©ç›Šï¼Œè¿™è¡¨æ˜å°½ç®¡æ¨¡å‹èƒ½ä»ç²¾é€‰ç‰ˆæœ¬ä¸­è·ç›Šï¼Œå´éš¾ä»¥åˆ›å»ºæœ‰ç”¨çš„ç¨‹åºæ€§çŸ¥è¯†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12670",
      "arxiv_url": "https://arxiv.org/abs/2602.12670",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12670",
      "github_url": "https://github.com/benchflow-ai/skillsbench",
      "upvotes": 26,
      "fetched_at": "2026-02-18T14:10:21.303859+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15763",
      "title": "GLM-5: from Vibe Coding to Agentic Engineering",
      "authors": [
        "GLM-5 Team",
        "Aohan Zeng",
        "Xin Lv",
        "Zhenyu Hou",
        "Zhengxiao Du",
        "Qinkai Zheng",
        "Bin Chen",
        "Da Yin",
        "Chendi Ge",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Gengzheng Pan",
        "Hao Zeng",
        "Haoke Zhang",
        "Haoran Wang",
        "Huilong Chen",
        "Jiajie Zhang",
        "Jian Jiao",
        "Jiaqi Guo",
        "Jingsen Wang",
        "Jingzhao Du",
        "Jinzhu Wu"
      ],
      "abstract": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering. We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering . Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks . Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.",
      "summary_en": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.",
      "summary_zh": "GLM-5é€šè¿‡DSAé™ä½æˆæœ¬ï¼Œåˆ©ç”¨å¼‚æ­¥å¼ºåŒ–å­¦ä¹ æ”¹è¿›å¯¹é½ï¼Œå¹¶å¢å¼ºç¼–ç¨‹èƒ½åŠ›ä»¥æ”¯æŒçœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ï¼Œä»è€Œæ¨è¿›åŸºç¡€æ¨¡å‹å‘å±•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15763",
      "arxiv_url": "https://arxiv.org/abs/2602.15763",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15763",
      "github_url": "https://github.com/zai-org/GLM-5",
      "upvotes": 25,
      "fetched_at": "2026-02-18T14:10:37.169842+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14111",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "authors": [
        "Anton Korznikov",
        "Andrey Galichin",
        "Alexey Dontsov",
        "Oleg Rogov",
        "Ivan Oseledets",
        "Elena Tutubalina"
      ],
      "abstract": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations. Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance , showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations , we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.",
      "summary_en": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.",
      "summary_zh": "ç¨€ç–è‡ªç¼–ç å™¨å°½ç®¡å…·æœ‰è¾ƒå¼ºçš„é‡å»ºæ€§èƒ½ï¼Œå´æ— æ³•å¯é åœ°åˆ†è§£ç¥ç»ç½‘ç»œå†…éƒ¨ç»“æ„ï¼Œè¿™ä¸€ç‚¹åœ¨åˆæˆä¸çœŸå®æ¿€æ´»è¯„ä¼°ä¸­å¾—åˆ°äº†è¯å®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14111",
      "arxiv_url": "https://arxiv.org/abs/2602.14111",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14111",
      "github_url": "",
      "upvotes": 22,
      "fetched_at": "2026-02-18T14:10:23.623049+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14299",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "authors": [
        "Ming Li",
        "Xirui Li",
        "Tianyi Zhou"
      ],
      "abstract": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures. As large language model agents increasingly populate networked environments , a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies , measuring semantic stabilization , lexical turnover , individual inertia , influence persistence , and collective consensus . Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover , defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies .",
      "summary_en": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.",
      "summary_zh": "ç½‘ç»œç¯å¢ƒä¸­çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“è¡¨ç°å‡ºåŠ¨æ€ç¨³å®šæ€§ï¼Œä½†æœªå®ç°çœŸæ­£çš„ç¤¾ä¼šæ”¶æ•›ï¼Œåœ¨ä¿æŒä¸ªä½“å¤šæ ·æ€§çš„åŒæ—¶ç¼ºä¹é›†ä½“å½±å“ç»“æ„ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14299",
      "arxiv_url": "https://arxiv.org/abs/2602.14299",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14299",
      "github_url": "https://github.com/MingLiiii/Moltbook_Socialization",
      "upvotes": 17,
      "fetched_at": "2026-02-18T14:10:24.916655+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15112",
      "title": "ResearchGym: Evaluating Language Model Agents on Real-World AI Research",
      "authors": [
        "Aniketh Garikaparthi",
        "Manasi Patwardhan",
        "Arman Cohan"
      ],
      "abstract": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance. We introduce ResearchGym , a benchmark and execution environment for evaluating AI agents on end-to-end research . To instantiate this, we repurpose five oral and spotlight papers from ICML , ICLR , and ACL . From each paper's repository, we preserve the datasets , evaluation harness , and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5 , we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length . Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex ( GPT-5 .2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.",
      "summary_en": "ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.",
      "summary_zh": "ResearchGymæå‡ºäº†ä¸€ä¸ªç”¨äºè¯„ä¼°AIæ™ºèƒ½ä½“ç«¯åˆ°ç«¯ç ”ç©¶ä»»åŠ¡çš„åŸºå‡†ç¯å¢ƒï¼Œæ­ç¤ºäº†å½“å‰è‡ªä¸»æ™ºèƒ½ä½“å­˜åœ¨æ˜¾è‘—çš„èƒ½åŠ›-å¯é æ€§å·®è·ï¼Œå°½ç®¡å…¶å¶å°”èƒ½è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15112",
      "arxiv_url": "https://arxiv.org/abs/2602.15112",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15112",
      "github_url": "https://github.com/Anikethh/ResearchGym",
      "upvotes": 11,
      "fetched_at": "2026-02-18T14:10:27.726332+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12279",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha",
        "Xiaoliang Dai",
        "Jialiang Wang",
        "Zecheng He",
        "Jianwei Yang",
        "Chunyuan Li",
        "Junzhe Sun",
        "Chu Wang",
        "Serena Yeung-Levy",
        "Felix Juefei-Xu"
      ],
      "abstract": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities. Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis , unified model training , and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning . These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models .",
      "summary_en": "UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.",
      "summary_zh": "UniTæ¡†æ¶ä½¿ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ€ç»´é“¾æµ‹è¯•æ—¶ç¼©æ”¾è¿›è¡Œè¿­ä»£æ¨ç†ä¸ä¼˜åŒ–ï¼Œæå‡ç”Ÿæˆä¸ç†è§£èƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12279",
      "arxiv_url": "https://arxiv.org/abs/2602.12279",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12279",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-18T14:10:19.898742+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15200",
      "title": "COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression",
      "authors": [
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Ammar Ali",
        "Baher Mohammad",
        "Stamatios Lefkimmiatis"
      ],
      "abstract": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods. Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization . COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.",
      "summary_en": "COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.",
      "summary_zh": "COMPOTæ˜¯ä¸€ç§é¢å‘Transformeræ¨¡å‹çš„å…è®­ç»ƒå‹ç¼©æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ­£äº¤å­—å…¸ä¸é—­å¼æ›´æ–°çš„ç¨€ç–å­—å…¸å­¦ä¹ ï¼Œå®ç°äº†ä¼˜äºä¼ ç»Ÿä½ç§©æ–¹æ³•çš„è´¨é‡-å‹ç¼©æƒè¡¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15200",
      "arxiv_url": "https://arxiv.org/abs/2602.15200",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15200",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-18T14:10:28.955015+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14486",
      "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
      "authors": [
        "Fabian GrÃ¶ger",
        "Shuo Wen",
        "Maria BrbiÄ‡"
      ],
      "abstract": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics. The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity , but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis : representations in neural networks are converging to shared local neighborhood relationships.",
      "summary_en": "Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.",
      "summary_zh": "ç»æ ¡å‡†çš„ç›¸ä¼¼æ€§åº¦é‡æ­ç¤ºï¼Œç½‘ç»œè¡¨å¾æ”¶æ•›äºå…±äº«çš„å±€éƒ¨é‚»åŸŸå…³ç³»ï¼Œè€Œéå…¨å±€ç»Ÿè®¡æ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14486",
      "arxiv_url": "https://arxiv.org/abs/2602.14486",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14486",
      "github_url": "https://github.com/mlbio-epfl/aristotelian",
      "upvotes": 5,
      "fetched_at": "2026-02-18T14:10:26.042097+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15547",
      "title": "jina-embeddings-v5-text: Task-Targeted Embedding Distillation",
      "authors": [
        "Mohammad Kalim Akram",
        "Saba Sturua",
        "Nastia Havriushenko",
        "Quentin Herreros",
        "Michael GÃ¼nther",
        "Maximilian Werk",
        "Han Xiao"
      ],
      "abstract": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization. Text embedding models are widely used for semantic similarity tasks, including information retrieval , clustering , and classification . General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models . Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization . Model weights are publicly available, hopefully inspiring further advances in embedding model development.",
      "summary_en": "Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization.",
      "summary_zh": "ç´§å‡‘å‹æ–‡æœ¬åµŒå…¥æ¨¡å‹é€šè¿‡è’¸é¦ä¸å¯¹æ¯”æŸå¤±ç›¸ç»“åˆçš„è”åˆè®­ç»ƒæ–¹æ³•å¼€å‘ï¼Œåœ¨å®ç°æœ€å…ˆè¿›æ€§èƒ½çš„åŒæ—¶æ”¯æŒé•¿ä¸Šä¸‹æ–‡åºåˆ—å’Œé«˜æ•ˆé‡åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15547",
      "arxiv_url": "https://arxiv.org/abs/2602.15547",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15547",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-18T14:10:35.239756+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15772",
      "title": "Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models",
      "authors": [
        "Sen Ye",
        "Mengde Xu",
        "Shuyang Gu",
        "Di He",
        "Liwei Wang",
        "Han Hu"
      ],
      "abstract": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation. Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma , achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models . Code is available at https://github.com/sen-ye/R3.",
      "summary_en": "The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.",
      "summary_zh": "æ¨ç†-åæ€-ç²¾ç‚¼æ¡†æ¶é€šè¿‡å°†å•æ­¥ç”Ÿæˆé‡æ„ä¸ºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¾å¼èå…¥ç†è§£çš„å¤šæ­¥è¿‡ç¨‹ï¼Œè§£å†³äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­ç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15772",
      "arxiv_url": "https://arxiv.org/abs/2602.15772",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15772",
      "github_url": "https://github.com/sen-ye/R3",
      "upvotes": 3,
      "fetched_at": "2026-02-18T14:10:37.829876+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15322",
      "title": "On Surprising Effectiveness of Masking Updates in Adaptive Optimizers",
      "authors": [
        "Taejong Joo",
        "Wenhan Xia",
        "Cheolmin Kim",
        "Ming Zhang",
        "Eugene Ie"
      ],
      "abstract": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers. Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners . We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment . Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.",
      "summary_en": "Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.",
      "summary_zh": "éšæœºå‚æ•°æ›´æ–°æ©ç é€šè¿‡è¯±å¯¼æ›²ç‡ç›¸å…³æ­£åˆ™åŒ–ï¼Œåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†æ›´ä¼˜çš„ä¼˜åŒ–ï¼Œå…¶åŠ¨é‡å¯¹é½å˜ä½“ç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„è‡ªé€‚åº”ä¼˜åŒ–å™¨å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15322",
      "arxiv_url": "https://arxiv.org/abs/2602.15322",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15322",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-18T14:10:31.823721+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15620",
      "title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
      "authors": [
        "Shiqi Liu",
        "Zeyu He",
        "Guojian Zhan",
        "Letian Tao",
        "Zhilong Zheng",
        "Jiang Wu",
        "Yinuo Wang",
        "Yang Guan",
        "Kehua Sheng",
        "Bo Zhang",
        "Keqiang Li",
        "Jingliang Duan",
        "Shengbo Eben Li"
      ],
      "abstract": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance. Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy . Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens . When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates . Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO , 20- Entropy and JustRL .",
      "summary_en": "Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.",
      "summary_zh": "ç ”ç©¶å‘ç°è™šå‡tokenæ˜¯å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­è®­ç»ƒä¸ç¨³å®šçš„åŸå› ï¼Œå¹¶æå‡ºé€‰æ‹©æ€§æ©ç é—®é¢˜æ¢¯åº¦æ›´æ–°çš„è§£å†³æ–¹æ¡ˆä»¥æå‡æ¨ç†æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15620",
      "arxiv_url": "https://arxiv.org/abs/2602.15620",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15620",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-18T14:10:36.166252+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15278",
      "title": "Visual Persuasion: What Influences Decisions of Vision-Language Models?",
      "authors": [
        "Manuel Cherep",
        "Pranav M R",
        "Pattie Maes",
        "Nikhil Singh"
      ],
      "abstract": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns. The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference : choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization , adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities , safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",
      "summary_en": "Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.",
      "summary_zh": "é€šè¿‡å—æ§çš„å›¾åƒé€‰æ‹©ä»»åŠ¡ä¸ç³»ç»Ÿæ€§è¾“å…¥æ‰°åŠ¨ï¼Œç ”ç©¶è§†è§‰-è¯­è¨€æ¨¡å‹çš„å†³ç­–åå¥½ï¼Œæ­ç¤ºå…¶è§†è§‰æ¼æ´ä¸å®‰å…¨é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15278",
      "arxiv_url": "https://arxiv.org/abs/2602.15278",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15278",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-18T14:10:30.443374+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12978",
      "title": "Learning Native Continuation for Action Chunking Flow Policies",
      "authors": [
        "Yufeng Liu",
        "Hang Yu",
        "Juntu Zhao",
        "Bocheng Li",
        "Di Zhang",
        "Mingzhu Li",
        "Wenxuan Wu",
        "Yingdong Hu",
        "Junyuan Xie",
        "Junliang Guo",
        "Dequan Wang",
        "Yang Gao"
      ],
      "abstract": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution. Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time . Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time .",
      "summary_en": "Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.",
      "summary_zh": "Legato é€šè¿‡è®­ç»ƒæ—¶å»¶ç»­æ–¹æ³•æ”¹è¿›åŠ¨ä½œåˆ†å—çš„è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼Œç¡®ä¿è½¨è¿¹å¹³æ»‘å¹¶å‡å°‘å®æ—¶æ‰§è¡Œä¸­çš„å¤šæ¨¡æ€åˆ‡æ¢ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12978",
      "arxiv_url": "https://arxiv.org/abs/2602.12978",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12978",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-18T14:10:22.813129+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.09653",
      "title": "ClinAlign: Scaling Healthcare Alignment from Clinician Preference",
      "authors": [
        "Shiwei Lyu",
        "Xidong Wang",
        "Lei Liu",
        "Hao Zhu",
        "Chaohe Zhang",
        "Jian Wang",
        "Jinjie Gu",
        "Benyou Wang",
        "Yue Shen"
      ],
      "abstract": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning. Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics , a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples : 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision . A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard , outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.",
      "summary_en": "A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.",
      "summary_zh": "ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶é€šè¿‡åŒ»ç”ŸéªŒè¯çš„ç¤ºä¾‹å’Œè’¸é¦çš„ä¸´åºŠåŸåˆ™ï¼Œå®ç°å¤§è¯­è¨€æ¨¡å‹ä¸ä¸´åºŠåŒ»ç”Ÿåå¥½çš„å¯¹é½ï¼Œä»¥æ”¹è¿›åŒ»å­¦æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09653",
      "arxiv_url": "https://arxiv.org/abs/2602.09653",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09653",
      "github_url": "https://github.com/AQ-MedAI/ClinAlign",
      "upvotes": 2,
      "fetched_at": "2026-02-18T14:10:17.323608+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.07854",
      "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model",
      "authors": [
        "Chendong Xiang",
        "Jiajun Liu",
        "Jintao Zhang",
        "Xiao Yang",
        "Zhengwei Fang",
        "Shizun Wang",
        "Zijun Wang",
        "Yingtian Zou",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization. Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings , which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers . By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention , which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift . Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.",
      "summary_en": "ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.",
      "summary_zh": "ViewRopeæ˜¯ä¸€ç§å‡ ä½•æ„ŸçŸ¥ç¼–ç æ–¹æ³•ï¼Œé€šè¿‡å‘è§†é¢‘Transformeræ³¨æ„åŠ›å±‚æ³¨å…¥ç›¸æœºå°„çº¿æ–¹å‘æ¥å¢å¼ºé¢„æµ‹æ€§ä¸–ç•Œæ¨¡å‹çš„é•¿æœŸä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨ç›¸å¯¹å°„çº¿å‡ ä½•å‚æ•°åŒ–è§£å†³ç©ºé—´æŒä¹…æ€§é—®é¢˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07854",
      "arxiv_url": "https://arxiv.org/abs/2602.07854",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07854",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-18T14:10:16.127210+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15449",
      "title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
      "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
      ],
      "abstract": "Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure. Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",
      "summary_en": "Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure.",
      "summary_zh": "é¢å‘ä»£ç ç”Ÿæˆçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å››å±‚æµ‹è¯•å¥—ä»¶ç»“æ„åŸºäºæ¨¡å‹èƒ½åŠ›è‡ªé€‚åº”è°ƒæ•´è¯¾ç¨‹è®¾è®¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15449",
      "arxiv_url": "https://arxiv.org/abs/2602.15449",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15449",
      "github_url": "https://github.com/deep-diver/TAROT",
      "upvotes": 1,
      "fetched_at": "2026-02-18T14:10:34.370167+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.15327",
      "title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
      "authors": [
        "Hanlin Zhang",
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade"
      ],
      "abstract": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks. For deploying foundation models, practitioners increasingly need prescriptive scaling laws : given a pre training compute budget , what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance , we estimate capability boundaries , high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization . We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budget s into reliable performance expectations and for monitoring when capability boundaries shift across time.",
      "summary_en": "Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.",
      "summary_zh": "å¤§è§„æ¨¡è§‚å¯Ÿæ€§åˆ†æä½¿ç”¨åˆ†ä½æ•°å›å½’ä¼°è®¡åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œå’Œæ€§èƒ½é¢„æµ‹ï¼Œå¹¶è¯„ä¼°è·¨ä»»åŠ¡çš„æ—¶é—´ç¨³å®šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.15327",
      "arxiv_url": "https://arxiv.org/abs/2602.15327",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.15327",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-18T14:10:32.947339+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.14364",
      "title": "A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)",
      "authors": [
        "Tianyu Chen",
        "Dongrui Liu",
        "Xia Hu",
        "Jingyi Yu",
        "Wenjie Wang"
      ],
      "abstract": "Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks. Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench ) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge ( AgentDoG-Qwen3-4B ) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.",
      "summary_en": "Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks.",
      "summary_zh": "Clawdbotæ˜¯ä¸€ä¸ªå…·å¤‡å¤šæ ·åŒ–å·¥å…·èƒ½åŠ›çš„è‡ªæ‰˜ç®¡AIæ™ºèƒ½ä½“ï¼Œåœ¨ä¸åŒé£é™©ç»´åº¦ä¸Šå‘ˆç°å‡ºå·®å¼‚åŒ–çš„å®‰å…¨æ€§èƒ½ï¼Œå°½ç®¡åœ¨ç‰¹å®šä»»åŠ¡ä¸­ä¿æŒç¨³å®šå¯é ï¼Œä½†åœ¨å¤„ç†æ¨¡ç³Šæˆ–å¯¹æŠ—æ€§è¾“å…¥æ—¶è¡¨ç°ä¸ä½³ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14364",
      "arxiv_url": "https://arxiv.org/abs/2602.14364",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14364",
      "github_url": "https://github.com/tychenn/clawdbot_report",
      "upvotes": 1,
      "fetched_at": "2026-02-18T15:48:04.320976+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.12235",
      "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
      "authors": [
        "Julia Belikova",
        "Danila Rozhevskii",
        "Dennis Svirin",
        "Konstantin Polev",
        "Alexander Panchenko"
      ],
      "abstract": "Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors. Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens . Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA , SQuADv2 , and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.",
      "summary_en": "Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors.",
      "summary_zh": "é¢å‘é•¿ä¸Šä¸‹æ–‡LLMçš„è½¯å‹ç¼©æ¶æ„ä½¿ç”¨æŸ¥è¯¢æ„ŸçŸ¥æ¢æµ‹åˆ†ç±»å™¨æ£€æµ‹tokenæº¢å‡ºå¹¶ç¼“è§£å‹ç¼©å¼•èµ·çš„é”™è¯¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12235",
      "arxiv_url": "https://arxiv.org/abs/2602.12235",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12235",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-18T15:19:42.403592+00:00"
    },
    {
      "date": "2026-02-18",
      "paper_id": "2602.11389",
      "title": "Causal-JEPA: Learning World Models through Object-Level Latent Interventions",
      "authors": [
        "Heejeong Nam",
        "Quentin Le Lidec",
        "Lucas Maes",
        "Yann LeCun",
        "Randall Balestriero"
      ],
      "abstract": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.",
      "summary_en": "C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.",
      "summary_zh": "C-JEPAå°†æ©ç è”åˆåµŒå…¥é¢„æµ‹æ‰©å±•è‡³ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨å¾ï¼Œé€šè¿‡å¯¹è±¡çº§æ©ç å¼•å…¥å› æœå½’çº³åç½®ï¼Œå®ç°ç¨³å¥çš„å…³ç³»ç†è§£ï¼Œå¹¶æ”¹è¿›æ¨ç†å’Œæ§åˆ¶ä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11389",
      "arxiv_url": "https://arxiv.org/abs/2602.11389",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11389",
      "github_url": "https://github.com/galilai-group/cjepa",
      "upvotes": 0,
      "fetched_at": "2026-02-18T14:10:18.399338+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.10809",
      "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories",
      "authors": [
        "Chenlong Deng",
        "Mengjie Deng",
        "Junjie Wu",
        "Dun Zeng",
        "Teng Wang",
        "Qingsong Xie",
        "Jiadeng Huang",
        "Shengjie Ma",
        "Changwang Zhang",
        "Zhaoxiang Wang",
        "Jun Wang",
        "Yutao Zhu",
        "Zhicheng Dou"
      ],
      "abstract": "DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system. Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams , where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues . We construct DISBench , a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations , effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation . Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.",
      "summary_en": "DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system.",
      "summary_zh": "DeepImageSearch æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒæ£€ç´¢çš„æ™ºèƒ½ä½“æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é…å¤‡åŒè®°å¿†ç³»ç»Ÿçš„æ¨¡å—åŒ–æ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ç°å¯¹è§†è§‰å†å²çš„å¤šæ­¥æ¨ç†ï¼Œä»è€Œè§£å†³ä¼ ç»Ÿè¯­ä¹‰åŒ¹é…çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10809",
      "arxiv_url": "https://arxiv.org/abs/2602.10809",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10809",
      "github_url": "https://github.com/RUC-NLPIR/DeepImageSearch",
      "upvotes": 24,
      "fetched_at": "2026-02-17T15:57:40.909289+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14492",
      "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
      "authors": [
        "Jiahao Yuan",
        "Yike Xu",
        "Jinyong Wen",
        "Baokun Wang",
        "Ziyi Gao",
        "Xiaotong Lin",
        "Yun Liu",
        "Xing Fu",
        "Yu Cheng",
        "Yongchao Liu",
        "Weiqiang Wang",
        "Zhongle Xie"
      ],
      "abstract": "A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods. Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU , an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
      "summary_en": "A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods.",
      "summary_zh": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Query-as-Anchor çš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å…·æœ‰ä¸“é—¨æ¶æ„å’Œè®­ç»ƒæ–¹æ³•çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå°†ç”¨æˆ·å»ºæ¨¡ä»é™æ€ç¼–ç è½¬å˜ä¸ºåŠ¨æ€ã€æŸ¥è¯¢æ„ŸçŸ¥çš„åˆæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14492",
      "arxiv_url": "https://arxiv.org/abs/2602.14492",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14492",
      "github_url": "https://github.com/JhCircle/Q-Anchor",
      "upvotes": 14,
      "fetched_at": "2026-02-17T15:57:58.685056+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13949",
      "title": "Experiential Reinforcement Learning",
      "authors": [
        "Taiwei Shi",
        "Sihao Chen",
        "Bowen Jiang",
        "Linxin Song",
        "Longqi Yang",
        "Jieyu Zhao"
      ],
      "abstract": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs. Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback , and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision , improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.",
      "summary_en": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.",
      "summary_zh": "Experiential Reinforcement Learning å¼•å…¥äº†ä¸€ç§æ˜¾å¼çš„ experience-reflection-consolidation å¾ªç¯ï¼Œé€šè¿‡åœ¨ä¸å¢åŠ é¢å¤–æ¨ç†æˆæœ¬çš„æƒ…å†µä¸‹å®ç°ç»“æ„åŒ–è¡Œä¸ºä¿®æ­£ï¼Œæå‡äº†ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡ä¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13949",
      "arxiv_url": "https://arxiv.org/abs/2602.13949",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13949",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-17T15:57:50.538181+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14699",
      "title": "Qute: Towards Quantum-Native Database",
      "authors": [
        "Muzhi Chen",
        "Xuanhe Zhou",
        "Wei Zhou",
        "Bangrui Xu",
        "Surui Tang",
        "Guoliang Li",
        "Bingsheng He",
        "Yeye He",
        "Yitong Song",
        "Fan Wu"
      ],
      "abstract": "This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.",
      "summary_en": "This paper presents Qute, a quantum database that treats quantum computation as a first-class execution option. Qute compiles extended SQL into gate-efficient quantum circuits and employs a hybrid optimizer, selective quantum indexing, and fidelity-preserving storage to mitigate qubit constraints. Deployed on the origin_wukong quantum processor, Qute outperforms classical baselines at scale, and the authors release an open-source prototype.",
      "summary_zh": "æœ¬æ–‡æå‡ºQuteï¼Œä¸€ç§å°†é‡å­è®¡ç®—è§†ä¸ºä¸€çº§æ‰§è¡Œé€‰é¡¹çš„é‡å­æ•°æ®åº“ã€‚Quteå°†æ‰©å±•SQLç¼–è¯‘ä¸ºé—¨é«˜æ•ˆçš„é‡å­çº¿è·¯ï¼Œå¹¶é‡‡ç”¨æ··åˆä¼˜åŒ–å™¨ã€é€‰æ‹©æ€§é‡å­ç´¢å¼•å’Œä¿çœŸåº¦ä¿æŒå­˜å‚¨æ¥ç¼“è§£é‡å­æ¯”ç‰¹çº¦æŸã€‚éƒ¨ç½²äºorigin_wukongé‡å­å¤„ç†å™¨åï¼ŒQuteåœ¨å¤§è§„æ¨¡ä¸‹æ€§èƒ½ä¼˜äºç»å…¸åŸºçº¿ï¼Œä¸”ä½œè€…å‘å¸ƒäº†å¼€æºåŸå‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14699",
      "arxiv_url": "https://arxiv.org/abs/2602.14699",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14699",
      "github_url": "https://github.com/weAIDB/Qute",
      "upvotes": 13,
      "fetched_at": "2026-02-17T15:58:02.921371+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14234",
      "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
      "authors": [
        "Zheng Chu",
        "Xiao Wang",
        "Jack Hong",
        "Huiming Fan",
        "Yuqi Huang",
        "Yue Yang",
        "Guohai Xu",
        "Chenxiao Zhao",
        "Cheng Xiang",
        "Shengchao Hu",
        "Dongdong Kuang",
        "Ming Liu",
        "Bing Qin",
        "Xing Yu"
      ],
      "abstract": "REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks. Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis , midtraining , and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion , allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining , we strengthen core atomic capabilities knowledge , planning , and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents , we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.",
      "summary_en": "REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.",
      "summary_zh": "REDSearcher æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡æ”¹è¿›çš„ä»»åŠ¡åˆæˆã€å·¥å…·å¢å¼ºæŸ¥è¯¢ã€ä¸­æœŸè®­ç»ƒèƒ½åŠ›å¢å¼ºå’Œæ¨¡æ‹Ÿç¯å¢ƒæ¥ä¼˜åŒ–æœç´¢æ™ºèƒ½ä½“ï¼Œä»¥åº”å¯¹é•¿ç¨‹æœç´¢ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14234",
      "arxiv_url": "https://arxiv.org/abs/2602.14234",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14234",
      "github_url": "https://github.com/RedSearchAgent/REDSearcher",
      "upvotes": 13,
      "fetched_at": "2026-02-17T15:57:56.424041+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14367",
      "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem",
      "authors": [
        "Shuofei Qiao",
        "Yunxiang Wei",
        "Xuehai Wang",
        "Bin Wu",
        "Boyang Xue",
        "Ningyu Zhang",
        "Hossein A. Rahmani",
        "Yanshan Wang",
        "Qiang Zhang",
        "Keyan Ding",
        "Jeff Z. Pan",
        "Huajun Chen",
        "Emine Yilmaz"
      ],
      "abstract": "InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation. The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation . The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.",
      "summary_en": "InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.",
      "summary_zh": "InnoEval æ˜¯ä¸€ç§æ·±åº¦åˆ›æ–°è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åŸºäºçŸ¥è¯†çš„å¤šè§†è§’æ¨ç†ã€å¼‚æ„æ·±åº¦çŸ¥è¯†æ£€ç´¢ä¸å¤šç»´è§£è€¦è¯„ä¼°ï¼Œæ¨¡æ‹Ÿäººç±»æ°´å¹³çš„åˆ›æ„è¯„ä¼°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14367",
      "arxiv_url": "https://arxiv.org/abs/2602.14367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14367",
      "github_url": "https://github.com/zjunlp/InnoEval",
      "upvotes": 12,
      "fetched_at": "2026-02-17T15:57:57.913951+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14041",
      "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens",
      "authors": [
        "Yuang Ai",
        "Jiaming Han",
        "Shaobin Zhuang",
        "Weijia Mao",
        "Xuefeng Hu",
        "Ziyan Yang",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen"
      ],
      "abstract": "BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance. We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents , BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head : instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion , a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion , BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation , BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.",
      "summary_en": "BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance.",
      "summary_zh": "BitDanceæ˜¯ä¸€ç§å¯æ‰©å±•çš„è‡ªå›å½’å›¾åƒç”Ÿæˆå™¨ï¼Œåˆ©ç”¨äºŒè¿›åˆ¶è§†è§‰ä»¤ç‰Œå’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆï¼Œå¹¶åœ¨é€Ÿåº¦å’Œæ€§èƒ½æ–¹é¢å‡æœ‰æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14041",
      "arxiv_url": "https://arxiv.org/abs/2602.14041",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14041",
      "github_url": "https://github.com/shallowdream204/BitDance",
      "upvotes": 10,
      "fetched_at": "2026-02-17T15:57:52.051908+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.07824",
      "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training",
      "authors": [
        "Yiwei Qin",
        "Zhen Huang",
        "Tiantian Mi",
        "Weiye Si",
        "Chenyang Zhou",
        "Qipeng Guo",
        "Siyuan Feng",
        "Pengfei Liu"
      ],
      "abstract": "Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text. Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution : advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 ( Generative Refinement ) and L5 ( Cognitive Completion ) using frontier LLMs to explicate reasoning and terminology. To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training , Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks . Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.",
      "summary_en": "Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.",
      "summary_zh": "Data Darwinism æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„æ•°æ®-æ¨¡å‹ååŒè¿›åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åå±‚åˆ†ç±»ä½“ç³»ï¼Œè¯æ˜å…ˆè¿›å¤„ç†æŠ€æœ¯èƒ½å¤Ÿæ˜¾è‘—æå‡åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦æ–‡æœ¬ä¸Šçš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07824",
      "arxiv_url": "https://arxiv.org/abs/2602.07824",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07824",
      "github_url": "https://github.com/GAIR-NLP/Data-Darwinism",
      "upvotes": 10,
      "fetched_at": "2026-02-17T15:57:36.747333+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14178",
      "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model",
      "authors": [
        "Shaobin Zhuang",
        "Yuang Ai",
        "Jiaming Han",
        "Weijia Mao",
        "Xiaohui Li",
        "Fangyikang Wang",
        "Xiao Wang",
        "Yan Li",
        "Shanchuan Lin",
        "Kun Xu",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen",
        "Yali Wang"
      ],
      "abstract": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements. Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function . SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss . We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding , image generation ( DPG Score : UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing ( GEdit Overall Score : UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
      "summary_en": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.",
      "summary_zh": "UniWeTokå¼•å…¥äº†ä¸€ç§ç»Ÿä¸€ç¦»æ•£tokenizerï¼Œå…¶é‡‡ç”¨å¤§è§„æ¨¡äºŒè¿›åˆ¶ç æœ¬å’Œæ–°é¢–çš„è®­ç»ƒæŠ€æœ¯ï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14178",
      "arxiv_url": "https://arxiv.org/abs/2602.14178",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14178",
      "github_url": "https://github.com/shallowdream204/BitDance",
      "upvotes": 6,
      "fetched_at": "2026-02-17T15:57:55.097541+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13367",
      "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
      "authors": [
        "Chen Yang",
        "Guangyue Peng",
        "Jiaying Zhu",
        "Ran Le",
        "Ruixiang Feng",
        "Tao Zhang",
        "Xiyun Xu",
        "Yang Song",
        "Yiming Jia",
        "Yuntao Wen",
        "Yunzhi Xu",
        "Zekai Wang",
        "Zhenwei An",
        "Zhicong Sun",
        "Zongchao Chen"
      ],
      "abstract": "Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques. We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior , code generation , and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling , ensuring high-quality, human-aligned responses . For code generation , we design complexity-aware rewards in Reinforcement Learning , optimizing both correctness and efficiency. In deep search , we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.",
      "summary_en": "Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques.",
      "summary_zh": "Nanbeige4.1-3Bæ˜¯ä¸€ä¸ª3Bå‚æ•°çš„ç»Ÿä¸€è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å…ˆè¿›çš„å¥–åŠ±å»ºæ¨¡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œåœ¨agenticè¡Œä¸ºã€ä»£ç ç”Ÿæˆå’Œæ¨ç†æ–¹é¢å±•ç°å‡ºç›¸è¾ƒäºæ›´å¤§æ¨¡å‹æ›´ä¼˜çš„æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13367",
      "arxiv_url": "https://arxiv.org/abs/2602.13367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13367",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-17T15:57:47.965838+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13823",
      "title": "Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings",
      "authors": [
        "Haonan Jiang",
        "Yuji Wang",
        "Yongjie Zhu",
        "Xin Lu",
        "Wenyu Qin",
        "Meng Wang",
        "Pengfei Wan",
        "Yansong Tang"
      ],
      "abstract": "A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance. Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks . Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner , ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment , effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.",
      "summary_en": "A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.",
      "summary_zh": "ä¸€ç§æ¨ç†é©±åŠ¨çš„é€šç”¨å¤šæ¨¡æ€ embedding æ¡†æ¶æ•´åˆäº† embedder-guided å¼ºåŒ–å­¦ä¹ ä¸å¯è¿½æº¯ chain-of-thoughtï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§å’Œæ£€ç´¢æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13823",
      "arxiv_url": "https://arxiv.org/abs/2602.13823",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13823",
      "github_url": "https://github.com/ZoengHN/Embed-RL",
      "upvotes": 5,
      "fetched_at": "2026-02-17T15:57:49.818770+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.12876",
      "title": "BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
      "authors": [
        "Huanyao Zhang",
        "Jiepeng Zhou",
        "Bo Li",
        "Bowen Zhou",
        "Yanzhe Dan",
        "Haishan Lu",
        "Zhiyong Cao",
        "Jiaoyang Chen",
        "Yuqian Han",
        "Zinan Sheng",
        "Zhengwei Tao",
        "Hao Liang",
        "Jialong Wu",
        "Yang Shi",
        "Yuanpeng He",
        "Jiaye Lin",
        "Qintong Zhang",
        "Guochen Yan",
        "Runhao Zhao",
        "Zhengpin Li",
        "Xiaohan Yu",
        "Lang Mei"
      ],
      "abstract": "A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities. Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception . Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.",
      "summary_en": "A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.",
      "summary_zh": "ä¸€é¡¹åä¸º BrowseComp-V3 çš„æ–°åŸºå‡†é€šè¿‡éœ€è¦åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€é—´è¿›è¡Œæ·±åº¦æœç´¢çš„å¤æ‚å¤šè·³æ¨ç†ä»»åŠ¡æŒ‘æˆ˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰èƒ½åŠ›çš„æ˜¾è‘—å·®è·ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12876",
      "arxiv_url": "https://arxiv.org/abs/2602.12876",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12876",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-17T15:57:44.766475+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13195",
      "title": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision",
      "authors": [
        "Aadarsh Sahoo",
        "Georgia Gkioxari"
      ],
      "abstract": "Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding. Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding , and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/",
      "summary_en": "Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding.",
      "summary_zh": "å¯¹è¯å¼å›¾åƒåˆ†å‰²é€šè¿‡å¼•å…¥ç»“åˆåˆ†å‰²å…ˆéªŒä¸è¯­è¨€ç†è§£çš„æ–°åŸºå‡†å’Œæ¨¡å‹ï¼Œè§£å†³åŠŸèƒ½æ€§å’Œç‰©ç†æ€§æ¨ç†ä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13195",
      "arxiv_url": "https://arxiv.org/abs/2602.13195",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13195",
      "github_url": "https://github.com/AadSah/ConverSeg",
      "upvotes": 4,
      "fetched_at": "2026-02-17T15:57:46.229181+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14147",
      "title": "LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models",
      "authors": [
        "Shufan Li",
        "Yuchen Zhu",
        "Jiuxiang Gu",
        "Kangning Liu",
        "Zhe Lin",
        "Yongxin Chen",
        "Molei Tao",
        "Aditya Grover",
        "Jason Kuen"
      ],
      "abstract": "LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks. Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing , tree search , and complementary likelihood estimation , to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning , reason-intensive grounding , and image editing .",
      "summary_en": "LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.",
      "summary_zh": "LaViDa-R1 æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡æ–°é¢–çš„è®­ç»ƒæŠ€æœ¯ç»Ÿä¸€äº†ç›‘ç£å¾®è°ƒä¸å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œåœ¨è§†è§‰æ¨ç†å’Œç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æ€§èƒ½æå‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14147",
      "arxiv_url": "https://arxiv.org/abs/2602.14147",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14147",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T15:57:53.734573+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13344",
      "title": "FireRed-Image-Edit-1.0 Techinical Report",
      "authors": [
        "Super Intelligence Team",
        "Changhao Qiao",
        "Chao Hui",
        "Chen Li",
        "Cunzheng Wang",
        "Dejia Song",
        "Jiale Zhang",
        "Jing Li",
        "Qiang Xiang",
        "Runqi Wang",
        "Shuang Sun",
        "Wei Zhu",
        "Xu Tang",
        "Yao Hu",
        "Yibo Chen",
        "Yuhao Huang",
        "Yuxuan Duan",
        "Zhiyi Chen",
        "Ziyuan Guo"
      ],
      "abstract": "FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability. We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation , training methodology , and evaluation design . We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO , DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench , a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks ( ImgEdit and GEdit ) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.",
      "summary_en": "FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.",
      "summary_zh": "FireRed-Image-Edit é‡‡ç”¨ diffusion transformerï¼Œç»“åˆä¼˜åŒ–çš„æ•°æ®æ•´ç†ä¸è®­ç»ƒæ–¹æ³•ï¼Œåœ¨ instruction-based image editing ä¸­å®ç°äº† SOTA æ€§èƒ½ï¼Œå¹¶è¾…ä»¥ comprehensive benchmark ä»¥åŠæå‡æ•°æ®æ•ˆç‡å’Œä¼˜åŒ–ç¨³å®šæ€§çš„æ–°æŠ€æœ¯ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13344",
      "arxiv_url": "https://arxiv.org/abs/2602.13344",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13344",
      "github_url": "https://github.com/FireRedTeam/FireRed-Image-Edit",
      "upvotes": 3,
      "fetched_at": "2026-02-17T15:57:47.256611+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14721",
      "title": "WebWorld: A Large-Scale World Model for Web Agent Training",
      "authors": [
        "Zikai Xiao",
        "Jianhong Tu",
        "Chuhang Zou",
        "Yuxin Zuo",
        "Zhi Li",
        "Peng Wang",
        "Bowen Yu",
        "Fei Huang",
        "Junyang Lin",
        "Zuozhu Liu"
      ],
      "abstract": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open- web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation , we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation , Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search , outperforming GPT-5 as a world model . Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
      "summary_en": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open- web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation , we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation , Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search , outperforming GPT-5 as a world model . Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
      "summary_zh": "ç½‘é¡µæ™ºèƒ½ä½“éœ€è¦å¤§é‡è½¨è¿¹æ¥å®ç°æ³›åŒ–ï¼Œä½†çœŸå®ä¸–ç•Œè®­ç»ƒå—é™äºç½‘ç»œå»¶è¿Ÿã€é€Ÿç‡é™åˆ¶å’Œå®‰å…¨é£é™©ã€‚æˆ‘ä»¬æ¨å‡º WebWorld ç³»åˆ—ï¼Œé¦–ä¸ªå¤§è§„æ¨¡è®­ç»ƒçš„å¼€æ”¾ç½‘é¡µæ¨¡æ‹Ÿå™¨ã€‚ç°æœ‰æ¨¡æ‹Ÿå™¨å±€é™äºåŒ…å«æ•°åƒæ¡è½¨è¿¹çš„å°é—­ç¯å¢ƒï¼Œè€Œ WebWorld åˆ©ç”¨å¯æ‰©å±•çš„æ•°æ®æµæ°´çº¿ï¼ŒåŸºäº 100 ä¸‡+ å¼€æ”¾ç½‘é¡µäº¤äº’è¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒæ¨ç†ã€å¤šæ ¼å¼æ•°æ®ä»¥åŠ 30+ æ­¥çš„é•¿ç¨‹æ¨¡æ‹Ÿã€‚å¯¹äºå†…åœ¨è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥ WebWorld-Benchï¼Œå…¶åŒé‡æŒ‡æ ‡æ¶µç›–ä¹ä¸ªç»´åº¦ï¼ŒWebWorld åœ¨è¯¥åŸºå‡†ä¸Šè¾¾åˆ°ä¸ Gemini-3-Pro å¯æ¯”çš„æ¨¡æ‹Ÿæ€§èƒ½ã€‚å¯¹äºå¤–éƒ¨è¯„ä¼°ï¼Œåœ¨ WebWorld åˆæˆè½¨è¿¹ä¸Šè®­ç»ƒçš„ Qwen3-14B åœ¨ WebArena ä¸Šæå‡ +9.2%ï¼Œæ€§èƒ½è¾¾åˆ°ä¸ GPT-4o ç›¸å½“çš„æ°´å¹³ã€‚WebWorld å®ç°æœ‰æ•ˆçš„æ¨ç†æ—¶æœç´¢ï¼Œä½œä¸ºä¸–ç•Œæ¨¡å‹å…¶æ€§èƒ½ä¼˜äº GPT-5ã€‚é™¤ç½‘é¡µæ¨¡æ‹Ÿå¤–ï¼ŒWebWorld è¿˜å±•ç°å‡ºå¯¹ä»£ç ã€GUI å’Œæ¸¸æˆç¯å¢ƒçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä¸–ç•Œæ¨¡å‹æ„å»ºæä¾›å¯å¤ç°çš„æ–¹æ¡ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14721",
      "arxiv_url": "https://arxiv.org/abs/2602.14721",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14721",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:58:04.330698+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14534",
      "title": "MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation",
      "authors": [
        "Hongpeng Wang",
        "Zeyu Zhang",
        "Wenhao Li",
        "Hao Tang"
      ],
      "abstract": "A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility. Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards . Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets , MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.",
      "summary_en": "A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility.",
      "summary_zh": "é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç»Ÿä¸€å¤šæ¨¡æ€è¿åŠ¨æ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰å¯¹é½ã€æ¨ç†è¿è´¯æ€§å’Œç‰©ç†åˆç†æ€§ï¼Œæå‡äº†äººä½“è¿åŠ¨ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14534",
      "arxiv_url": "https://arxiv.org/abs/2602.14534",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14534",
      "github_url": "https://github.com/AIGeeksGroup/MoRL",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:57:59.624552+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14060",
      "title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
      "authors": [
        "Yang Liu",
        "Jiaye Yang",
        "Weikang Li",
        "Jiahui Liang",
        "Yang Li",
        "Lingyong Yan"
      ],
      "abstract": "LM-Lexicon improves definition modeling through data clustering, semantic expert learning, and sparse mixture-of-experts architecture, achieving higher BLEU scores and better expert specialization. We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering , semantic expert learning , and model merging using a sparse mixture-of-experts architecture . By decomposing the definition modeling task into specialized semantic domains , where small language models are trained as domain experts , LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.",
      "summary_en": "LM-Lexicon improves definition modeling through data clustering, semantic expert learning, and sparse mixture-of-experts architecture, achieving higher BLEU scores and better expert specialization.",
      "summary_zh": "LM-Lexiconé€šè¿‡æ•°æ®èšç±»ã€è¯­ä¹‰ä¸“å®¶å­¦ä¹ å’Œç¨€ç–æ··åˆä¸“å®¶æ¶æ„æ”¹è¿›å®šä¹‰å»ºæ¨¡ï¼Œå®ç°äº†æ›´é«˜çš„BLEUåˆ†æ•°å’Œæ›´å¥½çš„ä¸“å®¶ç‰¹åŒ–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14060",
      "arxiv_url": "https://arxiv.org/abs/2602.14060",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14060",
      "github_url": "https://github.com/jacklanda/LMLexicon",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:57:52.665140+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.09185",
      "title": "AIDev: Studying AI Coding Agents on GitHub",
      "authors": [
        "Hao Li",
        "Haoxiang Zhang",
        "Ahmed E. Hassan"
      ],
      "abstract": "AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios. AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories . AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity , and human-AI collaboration in the new era of software engineering. > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering , Agentic Engineering",
      "summary_en": "AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.",
      "summary_zh": "AIDevæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ”¶å½•äº†çœŸå®GitHubä»“åº“ä¸­ç”±agentç¼–å†™çš„pull requestï¼Œæ•æ‰äº†AI coding agentåœ¨å®é™…è½¯ä»¶å¼€å‘åœºæ™¯ä¸­çš„ä½¿ç”¨æƒ…å†µã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09185",
      "arxiv_url": "https://arxiv.org/abs/2602.09185",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09185",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T15:57:38.181506+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14689",
      "title": "Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks",
      "authors": [
        "Lukas Struppek",
        "Adam Gleave",
        "Kellin Pelrine"
      ],
      "abstract": "Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models. As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses , open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling , which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models . Our results show that prefill attacks are consistently effective against all major contemporary open-weight models , revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling , they remain vulnerable to tailored, model-specific strategies . Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.",
      "summary_en": "Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models.",
      "summary_zh": "Prefill attacks ä»£è¡¨äº†å¼€æ”¾æƒé‡è¯­è¨€æ¨¡å‹ä¸­ä¸€ç§é‡å¤§ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„æ¼æ´ï¼Œå½±å“ç€ä¸»æµå½“ä»£æ¨¡å‹ï¼Œå°½ç®¡å¤§å‹æ¨ç†æ¨¡å‹å¯¹æ­¤è¡¨ç°å‡ºä¸€å®šçš„æŠµæŠ—æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14689",
      "arxiv_url": "https://arxiv.org/abs/2602.14689",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14689",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:58:01.257492+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.12586",
      "title": "Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models",
      "authors": [
        "Joshua Ong Jun Leang",
        "Yu Zhao",
        "Mihaela CÄƒtÄƒlina Stoian",
        "Wenda Li",
        "Shay B. Cohen",
        "Eleonora Giunchiglia"
      ],
      "abstract": "McDiffuSE enhances Masked Diffusion Models by optimizing slot infilling order through Monte Carlo Tree Search, improving reasoning task performance through strategic exploration of generation sequences. While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders . Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.",
      "summary_en": "McDiffuSE enhances Masked Diffusion Models by optimizing slot infilling order through Monte Carlo Tree Search, improving reasoning task performance through strategic exploration of generation sequences.",
      "summary_zh": "McDiffuSEé€šè¿‡Monte Carlo Tree Searchä¼˜åŒ–slot infilling orderä»¥å¢å¼ºMasked Diffusion Modelsï¼Œå¹¶é€šè¿‡strategic exploration of generation sequencesæå‡reasoning task performanceã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12586",
      "arxiv_url": "https://arxiv.org/abs/2602.12586",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12586",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:43.762632+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.12299",
      "title": "Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization",
      "authors": [
        "Mandip Goswami"
      ],
      "abstract": "Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorous signal processing with intuitive visualization remain scarce. This paper presents AcoustiVision Pro, an open-source web-based platform for comprehensive room impulse response (RIR) analysis. The system computes twelve distinct acoustic parameters from uploaded or dataset-sourced RIRs, provides interactive 3D visualizations of early reflections, generates frequency-dependent decay characteristics through waterfall plots, and checks compliance against international standards including ANSI S12.60 and ISO 3382. We introduce the accompanying RIRMega and RIRMega Speech datasets hosted on Hugging Face, containing thousands of simulated room impulse responses with full metadata. The platform supports real-time auralization through FFT-based convolution, exports detailed PDF reports suitable for engineering documentation, and provides CSV data export for further analysis. We describe the mathematical foundations underlying each acoustic metric, detail the system architecture, and present preliminary case studies demonstrating the platform's utility across diverse application domains including classroom acoustics, healthcare facility design, and recording studio evaluation.",
      "summary_en": "This paper presents AcoustiVision Pro, an open-source web-based platform for room impulse response (RIR) analysis that computes twelve acoustic parameters, provides interactive 3D visualizations of early reflections, generates frequency-dependent waterfall plots, and checks compliance against ANSI S12.60 and ISO 3382 standards. The system supports real-time auralization through FFT-based convolution, exports PDF reports and CSV data, and is accompanied by the RIRMega and RIRMega Speech datasets hosted on Hugging Face containing thousands of simulated RIRs with full metadata. We describe the mathematical foundations underlying each metric, detail the system architecture, and present case studies demonstrating the platform's utility in classroom acoustics, healthcare facility design, and recording studio evaluation.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº† AcoustiVision Proï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„åŸºäº Web çš„æˆ¿é—´è„‰å†²å“åº” (RIR) åˆ†æå¹³å°ï¼Œå¯è®¡ç®—åäºŒé¡¹å£°å­¦å‚æ•°ï¼Œæä¾›æ—©æœŸåå°„çš„äº¤äº’å¼ 3D å¯è§†åŒ–ï¼Œç”Ÿæˆé¢‘åŸŸ waterfall plotsï¼Œå¹¶æ£€æŸ¥æ˜¯å¦ç¬¦åˆ ANSI S12.60 å’Œ ISO 3382 æ ‡å‡†ã€‚è¯¥ç³»ç»Ÿæ”¯æŒé€šè¿‡åŸºäº FFT çš„å·ç§¯å®ç°å®æ—¶ auralizationï¼Œå¯å¯¼å‡º PDF æŠ¥å‘Šå’Œ CSV æ•°æ®ï¼Œå¹¶é™„å¸¦æ‰˜ç®¡äº Hugging Face çš„ RIRMega å’Œ RIRMega Speech æ•°æ®é›†ï¼ŒåŒ…å«æ•°åƒæ¡å¸¦æœ‰å®Œæ•´å…ƒæ•°æ®çš„æ¨¡æ‹Ÿ RIRã€‚æˆ‘ä»¬æè¿°äº†å„é¡¹æŒ‡æ ‡èƒŒåçš„æ•°å­¦åŸºç¡€ï¼Œè¯¦ç»†ä»‹ç»äº†ç³»ç»Ÿæ¶æ„ï¼Œå¹¶å±•ç¤ºäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥å¹³å°åœ¨æ•™å®¤å£°å­¦ã€åŒ»ç–—è®¾æ–½è®¾è®¡å’Œå½•éŸ³æ£šè¯„ä¼°ä¸­çš„å®ç”¨æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12299",
      "arxiv_url": "https://arxiv.org/abs/2602.12299",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12299",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:42.450227+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.11968",
      "title": "DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling",
      "authors": [
        "Mariia Fedorova",
        "Andrey Kutuzov",
        "Khonzoda Umarova"
      ],
      "abstract": "In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.",
      "summary_en": "DHPLT is an open collection of diachronic corpora covering 41 diverse languages across three time periods (2011-2015, 2020-2021, and 2024-present), derived from HPLT web-crawled data using timestamps as temporal signals with one million documents per period per language. The resource provides pre-computed word type and token embeddings and lexical substitutions for target words while allowing researchers to define custom targets, aiming to fill the gap in multilingual diachronic corpora for semantic change modelling beyond high-resource languages. All datasets are available at https://data.hplt-project.org/three/diachronic/.",
      "summary_zh": "DHPLTæ˜¯ä¸€ä¸ªæ¶µç›–41ç§ä¸åŒè¯­è¨€ã€è·¨è¶Šä¸‰ä¸ªæ—¶æœŸï¼ˆ2011-2015ã€2020-2021å’Œ2024è‡³ä»Šï¼‰çš„å¼€æ”¾å†æ—¶è¯­æ–™åº“é›†åˆï¼ŒåŸºäºæ—¶é—´æˆ³ä½œä¸ºæ—¶é—´ä¿¡å·ä»HPLTç½‘ç»œçˆ¬å–æ•°æ®ä¸­æå–ï¼Œæ¯ç§è¯­è¨€æ¯ä¸ªæ—¶æœŸåŒ…å«ä¸€ç™¾ä¸‡ç¯‡æ–‡æ¡£ã€‚è¯¥èµ„æºæä¾›é¢„è®¡ç®—çš„word typeå’Œtoken embeddingsä»¥åŠç›®æ ‡è¯çš„è¯æ±‡æ›¿æ¢ï¼ŒåŒæ—¶å…è®¸ç ”ç©¶è€…å®šä¹‰è‡ªå®šä¹‰ç›®æ ‡ï¼Œæ—¨åœ¨å¡«è¡¥é’ˆå¯¹é«˜èµ„æºè¯­è¨€ä¹‹å¤–çš„è¯­è¨€ã€ç”¨äºè¯­ä¹‰å˜åŒ–å»ºæ¨¡çš„å¤šè¯­è¨€å†æ—¶è¯­æ–™åº“çš„ç©ºç™½ã€‚æ‰€æœ‰æ•°æ®é›†å‡å¯åœ¨ https://data.hplt-project.org/three/diachronic/ è·å–ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11968",
      "arxiv_url": "https://arxiv.org/abs/2602.11968",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11968",
      "github_url": "https://github.com/ltgoslo/scdisc_hplt",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:41.607451+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.09319",
      "title": "Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation",
      "authors": [
        "Zhisheng Qi",
        "Utkarsh Sahu",
        "Li Ma",
        "Haoyu Han",
        "Ryan Rossi",
        "Franck Dernoncourt",
        "Mahantesh Halappanavar",
        "Nesreen Ahmed",
        "Yushun Dong",
        "Yue Zhao",
        "Yu Zhang",
        "Yu Wang"
      ],
      "abstract": "A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols. Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models , and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies , representative retrieval embedding models , and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.",
      "summary_en": "A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.",
      "summary_zh": "ä»‹ç»äº†ä¸€ç§ç”¨äºè¯„ä¼°é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„çŸ¥è¯†æå–æ”»å‡»ï¼ˆknowledge-extraction attacksï¼‰çš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šç§æ”»å‡»ä¸é˜²å¾¡ç­–ç•¥ï¼Œè·¨è¶Šå¤šç§æ£€ç´¢ä¸ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨æ ‡å‡†åŒ–è¯„ä¼°åè®®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09319",
      "arxiv_url": "https://arxiv.org/abs/2602.09319",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09319",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:39.635562+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.07673",
      "title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation",
      "authors": [
        "Jiangnan Fang",
        "Cheng-Tse Liu",
        "Hanieh Deilamsalehy",
        "Nesreen K. Ahmed",
        "Puneet Mathur",
        "Nedim Lipka",
        "Franck Dernoncourt",
        "Ryan A. Rossi"
      ],
      "abstract": "LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.",
      "summary_en": "LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.",
      "summary_zh": "LLMè¯„åˆ¤å™¨åå‘ä¸å…¶è‡ªèº«ç”Ÿæˆç›¸ä¼¼çš„æ‘˜è¦ï¼Œä¸”éšç€æ‘˜è¦ä¸äººç±»å‚è€ƒçš„é‡å åº¦é™ä½ï¼Œå…¶æ€§èƒ½ä¼šä¸‹é™ï¼Œè¿™ä¸€ç°è±¡åœ¨å¤šç§æ¨¡å‹è§„æ¨¡å’Œæ¶æ„ä¸­å‡å­˜åœ¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07673",
      "arxiv_url": "https://arxiv.org/abs/2602.07673",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07673",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T15:57:35.290034+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14696",
      "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
      "authors": [
        "Nihal V. Nayak",
        "Paula Rodriguez-Diaz",
        "Neha Hulkund",
        "Sara Beery",
        "David Alvarez-Melis"
      ],
      "abstract": "Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets. Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms . Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representation s choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds . More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
      "summary_en": "Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.",
      "summary_zh": "é’ˆå¯¹LLMå¾®è°ƒçš„æŒ‡ä»¤é€‰æ‹©å¯é€šè¿‡ç³»ç»Ÿåˆ†ææ•°æ®è¡¨ç¤ºä¸é€‰æ‹©ç®—æ³•åŠ ä»¥æ”¹è¿›ï¼Œå…¶ä¸­åŸºäºæ¢¯åº¦çš„è¡¨ç¤ºä¸è´ªå¿ƒè½®è¯¢é€‰æ‹©åœ¨ä½é¢„ç®—ä¸‹è¡¨ç°æœ€ä½³ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14696",
      "arxiv_url": "https://arxiv.org/abs/2602.14696",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14696",
      "github_url": "https://github.com/dcml-lab/targeted-instruction-selection",
      "upvotes": 0,
      "fetched_at": "2026-02-17T15:58:01.965017+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.14560",
      "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
      "authors": [
        "Sandy H. S. Herho",
        "Rusmawan Suwarman",
        "Nurjanna J. Trilaksono",
        "Iwan P. Anwar",
        "Faiz R. Fajary"
      ],
      "abstract": "Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales. Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems . We treat El NiÃ±o-Southern Oscillation ( ENSO ), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the NiÃ±o 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems ( pelog and slendro ) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space . Recurrence-based diagnostics , convex hull geometry , and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO 's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro . Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
      "summary_en": "Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales.",
      "summary_zh": "ENSOæ•°æ®çš„å‚æ•°æ˜ å°„å£°åŒ–é€šè¿‡å£°å­¦ç›¸ç©ºé—´åˆ†æä¿ç•™äº†åŠ¨åŠ›å­¦ç‰¹å¾ï¼Œåœ¨ä¼ ç»ŸéŸ³é˜¶ä¸­æ­ç¤ºäº†ä¸åŒçš„è€¦åˆæœºåˆ¶ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.14560",
      "arxiv_url": "https://arxiv.org/abs/2602.14560",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14560",
      "github_url": "https://github.com/sandyherho/suppl-enso-javanese-sonification",
      "upvotes": 0,
      "fetched_at": "2026-02-17T15:58:00.286351+00:00"
    },
    {
      "date": "2026-02-17",
      "paper_id": "2602.13516",
      "title": "SPILLage: Agentic Oversharing on the Web",
      "authors": [
        "Jaechul Roh",
        "Eugene Bagdasarian",
        "Hamed Haddadi",
        "Ali Shahin Shamsabadi"
      ],
      "abstract": "Web agents inadvertently disclose user information through both content and behavioral traces, with behavioral oversharing being more prevalent than content oversharing, and this issue persists despite mitigation efforts. LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage , a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation . However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success . Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/ SPILLage .",
      "summary_en": "Web agents inadvertently disclose user information through both content and behavioral traces, with behavioral oversharing being more prevalent than content oversharing, and this issue persists despite mitigation efforts.",
      "summary_zh": "ç½‘é¡µæ™ºèƒ½ä½“é€šè¿‡å†…å®¹ç—•è¿¹å’Œè¡Œä¸ºç—•è¿¹æ— æ„ä¸­æ³„éœ²ç”¨æˆ·ä¿¡æ¯ï¼Œå…¶ä¸­è¡Œä¸ºè¿‡åº¦åˆ†äº«æ¯”å†…å®¹è¿‡åº¦åˆ†äº«æ›´ä¸ºæ™®éï¼Œä¸”å°½ç®¡é‡‡å–äº†ç¼“è§£æªæ–½ï¼Œè¯¥é—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13516",
      "arxiv_url": "https://arxiv.org/abs/2602.13516",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13516",
      "github_url": "https://github.com/jrohsc/SPILLage",
      "upvotes": 0,
      "fetched_at": "2026-02-17T15:57:48.870947+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.10388",
      "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
      "authors": [
        "Zhongzhi Li",
        "Xuansheng Wu",
        "Yijiang Li",
        "Lijie Hu",
        "Ninghao Liu"
      ],
      "abstract": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures. The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance . In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following , toxicity detection , reward modeling , and behavior steering . Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer . Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
      "summary_en": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.",
      "summary_zh": "Feature Activation Coverage åœ¨å¯è§£é‡Šçš„ç‰¹å¾ç©ºé—´ä¸­è¡¡é‡æ•°æ®å¤šæ ·æ€§ï¼Œå¹¶å®ç°å¤šæ ·æ€§é©±åŠ¨çš„æ•°æ®åˆæˆï¼Œä»è€Œæå‡è·¨å¤šç§è¯­è¨€æ¨¡å‹æ¶æ„çš„ä¸‹æ¸¸æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.10388",
      "arxiv_url": "https://arxiv.org/abs/2602.10388",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10388",
      "github_url": "https://github.com/Zhongzhi660/FAC-Synthesis",
      "upvotes": 203,
      "fetched_at": "2026-02-17T09:52:03.950719+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12783",
      "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise",
      "authors": [
        "Yuejie Li",
        "Ke Yang",
        "Yueying Hua",
        "Berlin Chen",
        "Jianhao Nie",
        "Yueping He",
        "Caixin Kang"
      ],
      "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.",
      "summary_en": "SQuTR is a robustness benchmark for spoken query retrieval that addresses limitations of existing datasets by aggregating 37,317 unique queries from six English and Chinese text retrieval datasets, synthesized using voice profiles from 200 real speakers and mixed with 17 categories of real-world environmental noise under controlled SNR levels. The benchmark provides a unified evaluation protocol for assessing both cascaded and end-to-end retrieval systems across conditions ranging from quiet to highly noisy environments. Experimental results demonstrate that retrieval performance degrades substantially as noise increases, with significant variation across different systems, and even large-scale models struggle under extreme noise, indicating that robustness remains a critical bottleneck in spoken query to text retrieval.",
      "summary_zh": "SQuTR æ˜¯ä¸€ä¸ªé¢å‘å£è¯­æŸ¥è¯¢æ£€ç´¢çš„é²æ£’æ€§åŸºå‡†æµ‹è¯•ï¼Œå®ƒé€šè¿‡ä»å…­ä¸ªè‹±æ–‡å’Œä¸­æ–‡æ–‡æœ¬æ£€ç´¢æ•°æ®é›†ä¸­èšåˆ 37,317 æ¡ç‹¬ç‰¹æŸ¥è¯¢ï¼Œä½¿ç”¨ 200 ä½çœŸå®è¯´è¯äººçš„è¯­éŸ³ç‰¹å¾åˆæˆï¼Œå¹¶åœ¨å—æ§ SNR æ°´å¹³ä¸‹æ··å…¥ 17 ç±»çœŸå®ç¯å¢ƒå™ªå£°ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†æä¾›äº†ç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼Œç”¨äºåœ¨ä»å®‰é™åˆ°é«˜å™ªå£°ç¯å¢ƒçš„å¤šç§æ¡ä»¶ä¸‹è¯„ä¼°çº§è”ï¼ˆcascadedï¼‰å’Œç«¯åˆ°ç«¯ï¼ˆend-to-endï¼‰æ£€ç´¢ç³»ç»Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€å™ªå£°å¢åŠ ï¼Œæ£€ç´¢æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä¸åŒç³»ç»Ÿé—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”å³ä½¿å¤§è§„æ¨¡æ¨¡å‹åœ¨æç«¯å™ªå£°ä¸‹ä¹Ÿè¡¨ç°ä¸ä½³ï¼Œè¿™è¡¨æ˜é²æ£’æ€§ä»ç„¶æ˜¯å£è¯­æŸ¥è¯¢åˆ°æ–‡æœ¬æ£€ç´¢ä¸­çš„å…³é”®ç“¶é¢ˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12783",
      "arxiv_url": "https://arxiv.org/abs/2602.12783",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12783",
      "github_url": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval",
      "upvotes": 134,
      "fetched_at": "2026-02-17T09:52:23.869416+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12705",
      "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
      "authors": [
        "Baorong Shi",
        "Bo Cui",
        "Boyuan Jiang",
        "Deli Yu",
        "Fang Qian",
        "Haihua Yang",
        "Huichao Wang",
        "Jiale Chen",
        "Jianfei Pan",
        "Jieqiong Cao",
        "Jinghao Lin",
        "Kai Wu",
        "Lin Yang",
        "Shengsheng Yao",
        "Tao Chen",
        "Xiaojun Xiao",
        "Xiaozhong Ji",
        "Xu Wang",
        "Yijun He",
        "Zhixiong Yang"
      ],
      "abstract": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "summary_en": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "summary_zh": "MedXIAOHE æ˜¯ä¸€ç§åŒ»ç–—è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ entity-aware continual pretrainingã€reinforcement learning å’Œ tool-augmented agentic training æ¥å¢å¼ºä¸´åºŠç†è§£ï¼Œå®ç°å¯é çš„è¯Šæ–­æ¨ç†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12705",
      "arxiv_url": "https://arxiv.org/abs/2602.12705",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12705",
      "github_url": "",
      "upvotes": 56,
      "fetched_at": "2026-02-17T09:52:22.809484+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11858",
      "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
      "authors": [
        "Lai Wei",
        "Liangbo He",
        "Jun Lan",
        "Lingzhong Dong",
        "Yutong Cai",
        "Siyuan Li",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Linghe Kong",
        "Yue Wang",
        "Zhuosheng Zhang",
        "Weiran Huang"
      ],
      "abstract": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks. Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception , where decisive evidence is small and easily overwhelmed by global context. Recent \" Thinking-with-Images \" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation , which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench , a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents . We further discuss when \" Thinking-with-Images \" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
      "summary_en": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.",
      "summary_zh": "Region-to-Image Distillation é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å†…éƒ¨æ‰§è¡Œè¿­ä»£ç¼©æ”¾ï¼Œä½¿ MLLMs èƒ½å¤Ÿå®ç°ç»†ç²’åº¦è§†è§‰æ„ŸçŸ¥ï¼Œæ¶ˆé™¤äº†é‡å¤å·¥å…·è°ƒç”¨å’Œè§†è§‰é‡ç¼–ç çš„éœ€æ±‚ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11858",
      "arxiv_url": "https://arxiv.org/abs/2602.11858",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11858",
      "github_url": "https://github.com/inclusionAI/Zooming-without-Zooming",
      "upvotes": 52,
      "fetched_at": "2026-02-17T09:52:10.555002+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.08683",
      "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
      "authors": [
        "Feilong Tang",
        "Xiang An",
        "Yunyao Yan",
        "Yin Xie",
        "Bin Qin",
        "Kaicheng Yang",
        "Yifei Shen",
        "Yuanhan Zhang",
        "Chunyuan Li",
        "Shikun Feng",
        "Changrui Chen",
        "Huajie Tan",
        "Ming Hu",
        "Manyuan Zhang",
        "Bo Li",
        "Ziyong Feng",
        "Ziwei Liu",
        "Zongyuan Ge",
        "Jiankang Deng"
      ],
      "abstract": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy. Hypothesis. Artificial general intelligence is, at its core, a compression problem . Effective compression demands resonance : deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information , the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification , OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts , jointly capturing object permanence and motion dynamics . Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM , it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data . Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
      "summary_en": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy.",
      "summary_zh": "è§†è§‰ç†è§£å¯é€šè¿‡å°†æ¶æ„ä¸è§†é¢‘å‹ç¼©çš„ä¿¡æ¯è®ºåŸç†å¯¹é½æ¥æ”¹è¿›ï¼Œåˆ©ç”¨åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸Šå‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç¨€ç–é©±åŠ¨ç¼–ç ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.08683",
      "arxiv_url": "https://arxiv.org/abs/2602.08683",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08683",
      "github_url": "https://github.com/EvolvingLMMs-Lab/OneVision-Encoder",
      "upvotes": 40,
      "fetched_at": "2026-02-17T09:51:59.876130+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13191",
      "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
      "authors": [
        "Sayan Deb Sarkar",
        "RÃ©mi Pautrat",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Iro Armeni",
        "Mahdi Rad",
        "Mihai Dusmanu"
      ],
      "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
      "summary_en": "Current Video Language Models (VideoLMs) rely on keyframe sampling that misses temporal details and incurs high computational costs from full-image processing. We propose leveraging video codec primitives (motion vectors and residuals) to encode sparsity without full-image encoding, using lightweight transformer encoders that aggregate these primitives and align them with image embeddings through pre-training. This approach reduces time-to-first-token by up to 86% and token usage by up to 93% while maintaining or exceeding performance on 14 video understanding benchmarks spanning question answering, temporal reasoning, and spatial understanding.",
      "summary_zh": "ç°æœ‰çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVideoLMsï¼‰ä¾èµ–å…³é”®å¸§é‡‡æ ·ï¼Œè¿™ä¼šä¸¢å¤±æ—¶åºç»†èŠ‚ï¼Œå¹¶å› å…¨å›¾å¤„ç†è€Œäº§ç”Ÿé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨è§†é¢‘ç¼–è§£ç åŸè¯­ï¼ˆè¿åŠ¨å‘é‡å’Œæ®‹å·®ï¼‰æ¥ç¼–ç ç¨€ç–æ€§ï¼Œæ— éœ€å®Œæ•´å›¾åƒç¼–ç ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§ transformer ç¼–ç å™¨èšåˆè¿™äº›åŸè¯­ï¼Œé€šè¿‡é¢„è®­ç»ƒå°†å…¶ä¸å›¾åƒåµŒå…¥å¯¹é½ã€‚è¯¥æ–¹æ³•å°† time-to-first-token é™ä½å¤šè¾¾ 86%ï¼Œtoken ä½¿ç”¨é‡å‡å°‘å¤šè¾¾ 93%ï¼ŒåŒæ—¶åœ¨æ¶µç›–é—®ç­”ã€æ—¶åºæ¨ç†å’Œç©ºé—´ç†è§£çš„ 14 ä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šä¿æŒæˆ–è¶…è¶Šæ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13191",
      "arxiv_url": "https://arxiv.org/abs/2602.13191",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13191",
      "github_url": "",
      "upvotes": 25,
      "fetched_at": "2026-02-17T09:52:30.980694+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12617",
      "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
      "authors": [
        "Modi Jin",
        "Yiming Zhang",
        "Boyuan Sun",
        "Dingwen Zhang",
        "MingMing Cheng",
        "Qibin Hou"
      ],
      "abstract": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency. This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics . To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process . Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
      "summary_en": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency.",
      "summary_zh": "GeoAgent é€šè¿‡ä¸“é—¨çš„æ•°æ®é›†å’Œå¥–åŠ±æœºåˆ¶å®ç°å“è¶Šçš„åœ°ç†å®šä½æ¨ç†æ€§èƒ½ï¼Œç¡®ä¿åœ°ç†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12617",
      "arxiv_url": "https://arxiv.org/abs/2602.12617",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12617",
      "github_url": "https://github.com/HVision-NKU/GeoAgent",
      "upvotes": 19,
      "fetched_at": "2026-02-17T09:52:19.372799+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.09146",
      "title": "SemanticMoments: Training-Free Motion Similarity via Third Moment Features",
      "authors": [
        "Saar Huberman",
        "Kfir Goldberg",
        "Or Patashnik",
        "Sagie Benaim",
        "Ron Mokady"
      ],
      "abstract": "Temporal statistics in semantic feature space provide a scalable approach for motion-centric video understanding, outperforming existing RGB, flow, and text-supervised methods.",
      "summary_en": "Temporal statistics in semantic feature space provide a scalable approach for motion-centric video understanding, outperforming existing RGB, flow, and text-supervised methods.",
      "summary_zh": "è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­çš„æ—¶åºç»Ÿè®¡ä¸ºä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„RGBã€flowå’Œæ–‡æœ¬ç›‘ç£æ–¹æ³•ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09146",
      "arxiv_url": "https://arxiv.org/abs/2602.09146",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09146",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-17T09:52:01.205751+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12395",
      "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
      "authors": [
        "Xirui Li",
        "Ming Li",
        "Tianyi Zhou"
      ],
      "abstract": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
      "summary_en": "While reinforcement learning (RL) with verifiable rewards is widely used to enhance visual reasoning in vision-language models, end-to-end benchmarks obscure what specific capabilities improve beyond supervised fine-tuning initialization. To disentangle these effects, the authors propose a Frankenstein-style framework combining causal probing, parameter comparison, and model merging to analyze functional localization, update characterization, and transferability. Their findings demonstrate that RL induces consistent inference-time shifts primarily in mid-to-late transformer layers, with these refinements proving both transferable via merging and necessary via freezing for RL gains. These results suggest RL systematically refines mid-to-late computation to improve vision-to-reasoning alignment rather than uniformly enhancing visual perception, highlighting limitations of benchmark-only evaluation for assessing multimodal reasoning.",
      "summary_zh": "è™½ç„¶åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«å¹¿æ³›ç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•æ©ç›–äº†ç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆå§‹åŒ–å…·ä½“å“ªäº›èƒ½åŠ›å¾—åˆ°äº†æå‡ã€‚ä¸ºäº†è§£è€¦è¿™äº›æ•ˆåº”ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ Frankenstein-style æ¡†æ¶ï¼Œç»“åˆå› æœæ¢æµ‹ï¼ˆcausal probingï¼‰ã€å‚æ•°æ¯”è¾ƒå’Œæ¨¡å‹åˆå¹¶ï¼ˆmodel mergingï¼‰ï¼Œä»¥åˆ†æåŠŸèƒ½å®šä½ï¼ˆfunctional localizationï¼‰ã€æ›´æ–°ç‰¹å¾åˆ»ç”»ï¼ˆupdate characterizationï¼‰å’Œå¯è¿ç§»æ€§ï¼ˆtransferabilityï¼‰ã€‚ä»–ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒRL ä¸»è¦åœ¨ä¸­åå±‚ Transformer å±‚å¼•å‘ä¸€è‡´çš„æ¨ç†æ—¶åç§»ï¼ˆinference-time shiftsï¼‰ï¼Œè¿™äº›ä¼˜åŒ–æ—¢å¯é€šè¿‡ merging å®ç°è¿ç§»ï¼Œä¹Ÿå¯é€šè¿‡ freezing éªŒè¯å…¶å¯¹ RL æ”¶ç›Šçš„å¿…è¦æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒRL ç³»ç»Ÿåœ°ä¼˜åŒ–ä¸­åå±‚è®¡ç®—ä»¥æ”¹è¿› vision-to-reasoning alignmentï¼Œè€Œéå‡åŒ€åœ°å¢å¼ºè§†è§‰æ„ŸçŸ¥ï¼Œå‡¸æ˜¾äº†ä»…ä¾èµ–åŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€æ¨ç†çš„å±€é™æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12395",
      "arxiv_url": "https://arxiv.org/abs/2602.12395",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12395",
      "github_url": "https://github.com/tianyi-lab/Frankenstein",
      "upvotes": 13,
      "fetched_at": "2026-02-17T09:52:15.581155+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11865",
      "title": "Intelligent AI Delegation",
      "authors": [
        "Nenad TomaÅ¡ev",
        "Matija Franklin",
        "Simon Osindero"
      ],
      "abstract": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms. AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web .",
      "summary_en": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms.",
      "summary_zh": "AIæ™ºèƒ½ä½“éœ€è¦è‡ªé€‚åº”æ¡†æ¶æ¥å®ç°ä»»åŠ¡åˆ†è§£ä¸å§”æ‰˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŠ¨æ€å“åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–æƒé™è½¬ç§»ä¸ä¿¡ä»»æœºåˆ¶å¤„ç†æ„å¤–æ•…éšœã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11865",
      "arxiv_url": "https://arxiv.org/abs/2602.11865",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11865",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-17T09:52:11.573500+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11236",
      "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
      "authors": [
        "Yandan Yang",
        "Shuang Zeng",
        "Tong Lin",
        "Xinyuan Chang",
        "Dekang Qi",
        "Junjin Xiao",
        "Haoyun Liu",
        "Ronghan Chen",
        "Yuzhi Chen",
        "Dongjie Huo",
        "Feng Xiong",
        "Xing Wei",
        "Zhiheng Ma",
        "Mu Xu"
      ],
      "abstract": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability. Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies , enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis : effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit , enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
      "summary_en": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability.",
      "summary_zh": "ABot-M0 æå‡ºäº†ä¸€ä¸ªç”¨äºå…·èº«æ™ºèƒ½ä½“å¼€å‘çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ‡å‡†åŒ–äº†å¤šæ ·åŒ–çš„æœºå™¨äººæ•°æ®ï¼Œå¹¶é‡‡ç”¨åŠ¨ä½œæµå½¢å­¦ä¹ ï¼ˆaction manifold learningï¼‰æ¥æå‡é¢„æµ‹æ•ˆç‡å’Œç¨³å®šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11236",
      "arxiv_url": "https://arxiv.org/abs/2602.11236",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11236",
      "github_url": "https://github.com/amap-cvlab/ABot-Manipulation",
      "upvotes": 10,
      "fetched_at": "2026-02-17T09:52:05.064119+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12628",
      "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
      "authors": [
        "Liangzhi Shi",
        "Shuaihang Chen",
        "Feng Gao",
        "Yinuo Chen",
        "Kang Chen",
        "Tonghe Zhang",
        "Hongzhi Zhang",
        "Weinan Zhang",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring. Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\textit{RL}-based sim-real \\textit{Co}-training (RL-Co) framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting . We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and Ï€_{0.5}, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on Ï€_{0.5}. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency , providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
      "summary_en": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring.",
      "summary_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„sim-realååŒè®­ç»ƒæ¡†æ¶é€šè¿‡äº¤äº’å¼ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œæ•°æ®é”šå®šæå‡vision-language-actionç­–ç•¥æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12628",
      "arxiv_url": "https://arxiv.org/abs/2602.12628",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12628",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-17T09:52:20.604775+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13013",
      "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "authors": [
        "Yunheng Li",
        "Hengrui Zhang",
        "Meng-Hao Guo",
        "Wenzhao Gao",
        "Shaoyong Jia",
        "Shaohui Jiao",
        "Qibin Hou",
        "Ming-Ming Cheng"
      ],
      "abstract": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning. Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning , attribute-wise captioning , caption-based QA , and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.",
      "summary_en": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning.",
      "summary_zh": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé¢å‘ç»†ç²’åº¦è§†å¬ç†è§£çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸æ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–æ ‡æ³¨å’Œç›‘ç£å¾®è°ƒï¼Œåœ¨æå‡æè¿°è´¨é‡çš„åŒæ—¶å‡å°‘äº†å¹»è§‰ç°è±¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13013",
      "arxiv_url": "https://arxiv.org/abs/2602.13013",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13013",
      "github_url": "https://github.com/ASID-Caption/ASID-Caption",
      "upvotes": 7,
      "fetched_at": "2026-02-17T09:52:27.207390+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.04163",
      "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
      "authors": [
        "Junyu Chen",
        "Jungang Li",
        "Jing Xiong",
        "Wenjie Wang",
        "Qingyao Yang",
        "He Xiao",
        "Zhen Li",
        "Taiqiang Wu",
        "Mengzhao Chen",
        "Zhen Peng",
        "Chaofan Tao",
        "Long Shi",
        "Hongxia Yang",
        "Ngai Wong"
      ],
      "abstract": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference. Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients , and iteratively refines them using approximate second-order information while progressively compensating quantization error s to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry . Code: github.com/KingdalfGoodman/BPDQ.",
      "summary_en": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference.",
      "summary_zh": "Bit-Plane Decomposition Quantization (BPDQ) åˆ©ç”¨æºè‡ªä½å¹³é¢å’Œæ ‡é‡ç³»æ•°çš„å¯å˜é‡åŒ–ç½‘æ ¼æ”¹è¿›ä½æ¯”ç‰¹é‡åŒ–ï¼Œåœ¨èµ„æºå—é™çš„ LLM æ¨ç†ä¸­å–å¾—äº†ä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„ç²¾åº¦ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04163",
      "arxiv_url": "https://arxiv.org/abs/2602.04163",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04163",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-17T09:51:56.455376+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11715",
      "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
      "authors": [
        "Haolei Bai",
        "Lingcheng Kong",
        "Xueyi Chen",
        "Jianmian Wang",
        "Zhiqiang Tao",
        "Huan Wang"
      ],
      "abstract": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework. Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation . This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation , spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation .",
      "summary_en": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework.",
      "summary_zh": "ç”¨äº CUDA å†…æ ¸ç”Ÿæˆçš„æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ (dLLMs) é€šè¿‡ä¸“é—¨çš„æ•°æ®é›†å’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶å®ç°äº†æ›´ä¼˜æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11715",
      "arxiv_url": "https://arxiv.org/abs/2602.11715",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11715",
      "github_url": "https://github.com/deadlykitten4/DICE",
      "upvotes": 5,
      "fetched_at": "2026-02-17T09:52:07.720244+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12984",
      "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
      "authors": [
        "Yujiong Shen",
        "Yajie Yang",
        "Zhiheng Xi",
        "Binze Hu",
        "Huayu Sha",
        "Jiazheng Zhang",
        "Qiyuan Peng",
        "Junlin Shang",
        "Jixuan Huang",
        "Yutao Fan",
        "Jingqi Tong",
        "Shihan Dou",
        "Ming Zhang",
        "Lei Bai",
        "Zhenfei Yin",
        "Tao Gui",
        "Xingjun Ma",
        "Qi Zhang",
        "Xuanjing Huang",
        "Yu-Gang Jiang"
      ],
      "abstract": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "summary_en": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "summary_zh": "SciAgentGym å’Œ SciAgentBench æ”¯æŒç§‘å­¦å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è¯„ä¼°ï¼Œè€Œ SciForge åˆ™é€šè¿‡ä¾èµ–å›¾å»ºæ¨¡å·¥å…·äº¤äº’æ¥æå‡æ™ºèƒ½ä½“æ€§èƒ½ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12984",
      "arxiv_url": "https://arxiv.org/abs/2602.12984",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12984",
      "github_url": "https://github.com/CMarsRover/SciAgentGYM",
      "upvotes": 4,
      "fetched_at": "2026-02-17T09:52:25.767440+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12829",
      "title": "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching",
      "authors": [
        "Lei Lv",
        "Yunfei Li",
        "Yu Luo",
        "Fuchun Sun",
        "Xiao Ma"
      ],
      "abstract": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized SchrÃ¶dinger bridge formulation. Iterative generative policies, such as diffusion models and flow matching , offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field . Our key insight is to formulate policy optimization as a Generalized SchrÃ¶dinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism . Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.",
      "summary_en": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized SchrÃ¶dinger bridge formulation.",
      "summary_zh": "Field Least-Energy Actor-Critic (FLAC) é€šè¿‡å¹¿ä¹‰è–›å®šè°”æ¡¥å½¢å¼åŒ–ï¼Œå°†åŠ¨èƒ½ä½œä¸ºç­–ç•¥éšæœºæ€§è°ƒèŠ‚çš„ä»£ç†ï¼Œè§£å†³äº†æœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ä¸­è¿­ä»£ç”Ÿæˆç­–ç•¥æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12829",
      "arxiv_url": "https://arxiv.org/abs/2602.12829",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12829",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:24.838753+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12684",
      "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
      "authors": [
        "Rui Cai",
        "Jun Guo",
        "Xinze He",
        "Piaopiao Jin",
        "Jie Li",
        "Bingxuan Lin",
        "Futeng Liu",
        "Wei Liu",
        "Fei Ma",
        "Kun Ma",
        "Feng Qiu",
        "Heng Qu",
        "Yifei Su",
        "Qiao Sun",
        "Dong Wang",
        "Donghao Wang",
        "Yunhong Wang",
        "Rujie Wu",
        "Diyun Xiang",
        "Yu Yang",
        "Hangjun Ye",
        "Yuan Zhang"
      ],
      "abstract": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "summary_en": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "summary_zh": "é¢å‘æœºå™¨äººå­¦çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒä¸ä¸“é—¨è®­ç»ƒæŠ€æœ¯ï¼Œå®ç°å®æ—¶æ‰§è¡Œå’Œé«˜æ€§èƒ½æ“ä½œä»»åŠ¡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12684",
      "arxiv_url": "https://arxiv.org/abs/2602.12684",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12684",
      "github_url": "https://github.com/XiaomiRobotics/Xiaomi-Robotics-0",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:21.667396+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12506",
      "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
      "authors": [
        "Rosie Zhao",
        "Anshul Shah",
        "Xiaoyu Zhu",
        "Xinke Deng",
        "Zhongyu Jiang",
        "Yang Yang",
        "Joerg Liebelt",
        "Arnab Mondal"
      ],
      "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
      "summary_en": "RL fine-tuning improves VLMs on visual reasoning benchmarks but leaves them vulnerable to weak visual grounding and hallucinations, where simple textual perturbations cause robustness drops that entropy-based metrics trace to reshaped uncertainty and probability mass, particularly when CoT consistency is evaluated. Analysis of RL dynamics reveals an accuracy-faithfulness trade-off: fine-tuning boosts accuracy while eroding CoT reliability and robustness to contextual shifts, and although adversarial augmentation improves robustness, it does not prevent faithfulness drift. Faithfulness-aware rewards restore alignment between answers and reasoning but risk training collapse when paired with augmentation, leaving robustness elusive. These findings highlight the limitations of accuracy-only evaluations and motivate protocols that jointly emphasize correctness, robustness, and faithfulness in visually grounded reasoning.",
      "summary_zh": "RLå¾®è°ƒæå‡äº†VLMåœ¨è§†è§‰æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ï¼Œä½†ä½¿å…¶æ˜“å—è§†è§‰æ¥åœ°è–„å¼±å’Œå¹»è§‰çš„å½±å“ï¼Œå…¶ä¸­ç®€å•çš„æ–‡æœ¬æ‰°åŠ¨ä¼šå¯¼è‡´é²æ£’æ€§ä¸‹é™ï¼Œè€ŒåŸºäºç†µçš„æŒ‡æ ‡å°†è¿™ç§ä¸‹é™è¿½æº¯è‡³ä¸ç¡®å®šæ€§å’Œæ¦‚ç‡è´¨é‡çš„é‡å¡‘ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°CoTä¸€è‡´æ€§æ—¶ã€‚å¯¹RLåŠ¨æ€çš„åˆ†ææ­ç¤ºäº†å‡†ç¡®æ€§ä¸å¿ å®åº¦ä¹‹é—´çš„æƒè¡¡ï¼šå¾®è°ƒæå‡äº†å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¾µèš€äº†CoTçš„å¯é æ€§ä»¥åŠå¯¹ä¸Šä¸‹æ–‡åç§»çš„é²æ£’æ€§ï¼›å°½ç®¡å¯¹æŠ—å¢å¼ºæå‡äº†é²æ£’æ€§ï¼Œä½†æ— æ³•é˜»æ­¢å¿ å®åº¦æ¼‚ç§»ã€‚å¿ å®åº¦æ„ŸçŸ¥å¥–åŠ±æ¢å¤äº†ç­”æ¡ˆä¸æ¨ç†ä¹‹é—´çš„å¯¹é½ï¼Œä½†åœ¨ä¸å¢å¼ºæ–¹æ³•ç»“åˆæ—¶å­˜åœ¨è®­ç»ƒå´©æºƒçš„é£é™©ï¼Œä½¿å¾—é²æ£’æ€§éš¾ä»¥å®ç°ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä»…å…³æ³¨å‡†ç¡®æ€§è¯„ä¼°çš„å±€é™æ€§ï¼Œå¹¶æ¨åŠ¨å»ºç«‹èƒ½å¤Ÿåœ¨è§†è§‰æ¥åœ°æ¨ç†ä¸­è”åˆå¼ºè°ƒæ­£ç¡®æ€§ã€é²æ£’æ€§å’Œå¿ å®åº¦çš„è¯„ä¼°åè®®ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12506",
      "arxiv_url": "https://arxiv.org/abs/2602.12506",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12506",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:17.485935+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11757",
      "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
      "authors": [
        "Yi Zhang",
        "Yunshuang Wang",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement. Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation . First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration . Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code . Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.",
      "summary_en": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement.",
      "summary_zh": "Code2Worldsé€šè¿‡å°†4DåŠ¨æ€åœºæ™¯ç”Ÿæˆå½¢å¼åŒ–ä¸ºè¯­è¨€åˆ°æ¨¡æ‹Ÿä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨åŒæµæ¶æ„ä¸ç‰©ç†æ„ŸçŸ¥é—­ç¯ä¼˜åŒ–æ¥å®ç°ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11757",
      "arxiv_url": "https://arxiv.org/abs/2602.11757",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11757",
      "github_url": "https://github.com/AIGeeksGroup/Code2Worlds",
      "upvotes": 3,
      "fetched_at": "2026-02-17T09:52:08.380082+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12612",
      "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
      "authors": [
        "Sein Kim",
        "Sangwu Park",
        "Hongseok Kang",
        "Wonjoong Kim",
        "Jimin Seo",
        "Yeonjun In",
        "Kanghoon Yoon",
        "Chanyoung Park"
      ],
      "abstract": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
      "summary_en": "Traditional Neural Architecture Search (NAS) methods are constrained by fixed search spaces defined by human priors, while recent LLM-driven code evolution frameworks rely on scalar metrics such as NDCG and Hit Ratio that lack qualitative insights for directional improvement. We propose Self-EvolveRec, which integrates a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative verification to establish directional feedback, and introduces a Diagnosis Tool - Model Co-Evolution strategy to dynamically adapt evaluation criteria as recommendation architectures evolve. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in recommendation performance and user satisfaction.",
      "summary_zh": "ä¼ ç»Ÿçš„ Neural Architecture Search (NAS) æ–¹æ³•å—é™äºç”±äººå·¥å…ˆéªŒå®šä¹‰çš„å›ºå®šæœç´¢ç©ºé—´ï¼Œè€Œè¿‘æœŸçš„ LLM-driven code evolution æ¡†æ¶ä¾èµ–äº NDCG å’Œ Hit Ratio ç­‰æ ‡é‡æŒ‡æ ‡ï¼Œç¼ºä¹ç”¨äºå®šå‘æ”¹è¿›çš„å®šæ€§æ´å¯Ÿã€‚æˆ‘ä»¬æå‡ºäº† Self-EvolveRecï¼Œå…¶æ•´åˆ User Simulator ä»¥æä¾›å®šæ€§æ‰¹è¯„ï¼Œå¹¶é›†æˆ Model Diagnosis Tool è¿›è¡Œå®šé‡éªŒè¯ï¼Œä»è€Œå»ºç«‹å®šå‘åé¦ˆï¼ŒåŒæ—¶å¼•å…¥ Diagnosis Tool - Model Co-Evolution ç­–ç•¥ï¼Œä»¥åœ¨æ¨èæ¶æ„æ¼”è¿›è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´è¯„ä¼°æ ‡å‡†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSelf-EvolveRec åœ¨æ¨èæ€§èƒ½å’Œç”¨æˆ·æ»¡æ„åº¦æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ NAS å’Œ LLM-driven code evolution åŸºçº¿ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12612",
      "arxiv_url": "https://arxiv.org/abs/2602.12612",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12612",
      "github_url": "https://github.com/Sein-Kim/self_evolverec",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:18.321404+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12221",
      "title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
      "authors": [
        "Onkar Susladkar",
        "Tushar Prakash",
        "Gayatri Deshmukh",
        "Kiet A. Nguyen",
        "Jiaxun Zhang",
        "Adheesh Juvekar",
        "Tianshu Bao",
        "Lin Chai",
        "Sparsh Mittal",
        "Inderjit S Dhillon",
        "Ismini Lourentzou"
      ],
      "abstract": "UniDFlow is a unified discrete flow-matching framework that decouples understanding and generation through low-rank adapters and uses reference-based alignment to improve multimodal tasks without retraining. We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding , generation , and editing . It decouples understanding and generation via task-specific low-rank adapters , avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting , in-context image generation , reference-based editing , and compositional generation , despite no explicit task-specific training.",
      "summary_en": "UniDFlow is a unified discrete flow-matching framework that decouples understanding and generation through low-rank adapters and uses reference-based alignment to improve multimodal tasks without retraining.",
      "summary_zh": "UniDFlow æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡ low-rank adapters è§£è€¦ç†è§£ä¸ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨ reference-based alignment æ”¹è¿›å¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12221",
      "arxiv_url": "https://arxiv.org/abs/2602.12221",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12221",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:14.257513+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Åukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "abstract": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering. Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts , such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "summary_en": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering.",
      "summary_zh": "ç ”ç©¶è¡¨æ˜ï¼ŒéŸ³é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å±‚æ§åˆ¶ç€ä¸åŒçš„éŸ³ä¹æ¦‚å¿µï¼Œä»è€Œèƒ½å¤Ÿé€šè¿‡æ¿€æ´»å¼•å¯¼ç²¾ç¡®æ“æ§éŸ³é¢‘ç‰¹å¾ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11910",
      "arxiv_url": "https://arxiv.org/abs/2602.11910",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11910",
      "github_url": "https://github.com/luk-st/steer-audio",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:13.145126+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11769",
      "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
      "authors": [
        "Zhenghuang Wu",
        "Kang Chen",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms. Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance , a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity . Second, to reinforce temporal consistency , we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.",
      "summary_en": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms.",
      "summary_zh": "Light4Dé€šè¿‡è§£è€¦å…‰æµå¼•å¯¼å’Œæ—¶é—´ä¸€è‡´æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†åœ¨ç›®æ ‡å…‰ç…§ä¸‹çš„ä¸€è‡´æ€§4Dè§†é¢‘åˆæˆã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11769",
      "arxiv_url": "https://arxiv.org/abs/2602.11769",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11769",
      "github_url": "https://github.com/AIGeeksGroup/Light4D",
      "upvotes": 2,
      "fetched_at": "2026-02-17T09:52:09.718197+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13022",
      "title": "Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels",
      "authors": [
        "Julius Pesonen",
        "Stefan Rua",
        "Josef Taher",
        "Niko KoivumÃ¤ki",
        "Xiaowei Yu",
        "Eija Honkavaara"
      ],
      "abstract": "Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.",
      "summary_en": "Individual tree crown mapping supports urban inventories and forest health monitoring but is challenging to automate from aerial imagery due to texture variations and crown overlaps. This study presents a method to train deep learning segmentation models on RGB and multispectral images using pseudo-labels derived from aerial laser scanning (ALS) data, which are enhanced by the zero-shot Segment Anything Model 2 (SAM 2). The approach eliminates manual annotation costs while producing domain-specific models that outperform general-domain alternatives on this task.",
      "summary_zh": "å•æœ¨æ ‘å† åˆ¶å›¾æ”¯æŒåŸå¸‚æ¸…æŸ¥å’Œæ£®æ—å¥åº·ç›‘æµ‹ï¼Œä½†ç”±äºçº¹ç†å˜åŒ–å’Œæ ‘å† é‡å ï¼Œéš¾ä»¥ä»èˆªç©ºå½±åƒä¸­å®ç°è‡ªåŠ¨åŒ–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æºè‡ªæœºè½½æ¿€å…‰é›·è¾¾ï¼ˆALSï¼‰æ•°æ®çš„ä¼ªæ ‡ç­¾åœ¨RGBå’Œå¤šå…‰è°±å½±åƒä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ åˆ†å‰²æ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™äº›ä¼ªæ ‡ç­¾ç”±é›¶æ ·æœ¬Segment Anything Model 2ï¼ˆSAM 2ï¼‰å¢å¼ºã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†äººå·¥æ ‡æ³¨æˆæœ¬ï¼ŒåŒæ—¶ç”Ÿæˆäº†åœ¨è¯¥ä»»åŠ¡ä¸Šä¼˜äºé€šç”¨é¢†åŸŸæ›¿ä»£æ–¹æ¡ˆçš„é¢†åŸŸç‰¹å®šæ¨¡å‹ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13022",
      "arxiv_url": "https://arxiv.org/abs/2602.13022",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13022",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:28.645454+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12500",
      "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
      "authors": [
        "AndrÃ© Storhaug",
        "Jiamou Sun",
        "Jingyue Li"
      ],
      "abstract": "Favia is a forensic, agent-based framework that combines scalable candidate ranking with deep semantic reasoning to accurately identify vulnerability-fixing commits by leveraging LLM agents with specialized tools and environmental context. Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs . Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning . Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent . By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.",
      "summary_en": "Favia is a forensic, agent-based framework that combines scalable candidate ranking with deep semantic reasoning to accurately identify vulnerability-fixing commits by leveraging LLM agents with specialized tools and environmental context.",
      "summary_zh": "Favia æ˜¯ä¸€ä¸ªå–è¯å‹ã€åŸºäº agent çš„æ¡†æ¶ï¼Œç»“åˆå¯æ‰©å±•çš„å€™é€‰æ’åºä¸æ·±åº¦è¯­ä¹‰æ¨ç†ï¼Œåˆ©ç”¨é…å¤‡ä¸“ç”¨å·¥å…·å’Œç¯å¢ƒä¸Šä¸‹æ–‡çš„ LLM agent ä»¥å‡†ç¡®è¯†åˆ«æ¼æ´ä¿®å¤æäº¤ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.12500",
      "arxiv_url": "https://arxiv.org/abs/2602.12500",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12500",
      "github_url": "https://github.com/andstor/agentic-security-patch-classification-replication-package",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:16.385253+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11609",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "abstract": "scPilot presents a framework for omics-native reasoning where large language models directly analyze single-cell RNA-seq data through step-by-step reasoning processes, improving accuracy and interpretability in cell-type annotation and developmental trajectory reconstruction. We present scPilot, the first systematic framework to practice omics-native reasoning : a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation , developmental-trajectory reconstruction , and transcription-factor targeting , into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence. To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic . By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses. Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "summary_en": "scPilot presents a framework for omics-native reasoning where large language models directly analyze single-cell RNA-seq data through step-by-step reasoning processes, improving accuracy and interpretability in cell-type annotation and developmental trajectory reconstruction.",
      "summary_zh": "scPilot æ„å»ºäº†ä¸€ä¸ªç”¨äº omics-native reasoning çš„æ¡†æ¶ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é€æ­¥æ¨ç†è¿‡ç¨‹ç›´æ¥åˆ†æ single-cell RNA-seq æ•°æ®ï¼Œä»è€Œåœ¨ç»†èƒç±»å‹æ³¨é‡Šå’Œå‘è‚²è½¨è¿¹é‡å»ºæ–¹é¢æå‡å‡†ç¡®æ€§ä¸å¯è§£é‡Šæ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.11609",
      "arxiv_url": "https://arxiv.org/abs/2602.11609",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11609",
      "github_url": "https://github.com/maitrix-org/scPilot",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:06.359879+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.09870",
      "title": "Steer2Edit: From Activation Steering to Component-Level Editing",
      "authors": [
        "Chung-En Sun",
        "Ge Yan",
        "Zimo Wang",
        "Tsui-Wei Weng"
      ],
      "abstract": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.",
      "summary_en": "Current steering methods apply fixed global modifications during inference, which induces unfavorable attribute-utility trade-offs by ignoring that behaviors are governed by small, heterogeneous subsets of components. Steer2Edit transforms steering vectors into diagnostic signals for component-level rank-1 weight editing, selectively redistributing behavioral influence across individual attention heads and MLP neurons while preserving the standard forward pass. This training-free framework achieves more favorable trade-offs than standard steering, improving safety by up to 17.2%, truthfulness by 9.8%, and reducing reasoning length by 12.2% on average across safety alignment, hallucination mitigation, and reasoning efficiency tasks.",
      "summary_zh": "ç°æœ‰çš„ steering æ–¹æ³•åœ¨ inference æœŸé—´æ–½åŠ å›ºå®šçš„å…¨å±€ä¿®æ”¹ï¼Œå› å…¶å¿½ç•¥äº†è¡Œä¸ºå—å°å‹ã€å¼‚è´¨çš„ç»„ä»¶å­é›†è°ƒæ§çš„äº‹å®ï¼Œä»è€Œå¯¼è‡´ä¸åˆ©çš„ attribute-utility trade-offsã€‚Steer2Edit å°† steering vectors è½¬åŒ–ä¸ºç”¨äºç»„ä»¶çº§ rank-1 weight editing çš„è¯Šæ–­ä¿¡å·ï¼Œåœ¨ä¿æŒæ ‡å‡† forward pass çš„åŒæ—¶ï¼Œé€‰æ‹©æ€§åœ°å°†è¡Œä¸ºå½±å“é‡æ–°åˆ†é…è‡³å„ä¸ª attention heads å’Œ MLP neuronsã€‚è¯¥ training-free æ¡†æ¶ç›¸æ¯”æ ‡å‡† steering å®ç°äº†æ›´ä¼˜çš„ trade-offsï¼Œåœ¨ safety alignmentã€hallucination mitigation å’Œ reasoning efficiency ä»»åŠ¡ä¸­ï¼Œå¹³å‡å°† safety æå‡æœ€å¤šè¾¾ 17.2%ï¼Œtruthfulness æå‡ 9.8%ï¼Œå¹¶å°† reasoning length å‡å°‘ 12.2%ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.09870",
      "arxiv_url": "https://arxiv.org/abs/2602.09870",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09870",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:52:02.393593+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.07298",
      "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
      "authors": [
        "Benyu Zhang",
        "Qiang Zhang",
        "Jianpeng Cheng",
        "Hong-You Chen",
        "Qifei Wang",
        "Wei Sun",
        "Shen Li",
        "Jia Li",
        "Jiahao Wu",
        "Xiangjun Fan",
        "Hong Yan"
      ],
      "abstract": "A novel layered framework generates high-quality synthetic data for large language models in recommender systems, demonstrating superior performance and predictable scaling laws compared to traditional methods. Large Language Models (LLMs) represent a promising frontier for recommender systems , yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform (+130% on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.",
      "summary_en": "A novel layered framework generates high-quality synthetic data for large language models in recommender systems, demonstrating superior performance and predictable scaling laws compared to traditional methods.",
      "summary_zh": "ä¸€ç§æ–°é¢–çš„åˆ†å±‚æ¡†æ¶å¯ä¸ºæ¨èç³»ç»Ÿä¸­çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å±•ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½å’Œå¯é¢„æµ‹çš„ scaling lawsã€‚",
      "hf_url": "https://huggingface.co/papers/2602.07298",
      "arxiv_url": "https://arxiv.org/abs/2602.07298",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07298",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:51:59.101035+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.04315",
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "authors": [
        "Guoqing Ma",
        "Siheng Wang",
        "Zeyu Zhang",
        "Shan Yu",
        "Hao Tang"
      ],
      "abstract": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection. Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability , which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning ), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM ( Affordance Segmentation Module ) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
      "summary_en": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection.",
      "summary_zh": "GeneralVLAæ˜¯ä¸€ç§åˆ†å±‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†å¼•å¯¼çš„è½¨è¿¹è§„åˆ’å®ç°é›¶æ ·æœ¬æœºå™¨äººæ“ä½œï¼Œæ— éœ€çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.04315",
      "arxiv_url": "https://arxiv.org/abs/2602.04315",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04315",
      "github_url": "https://github.com/AIGeeksGroup/GeneralVLA",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:51:57.328841+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.03120",
      "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
      "authors": [
        "Yinggan Xu",
        "Risto Miikkulainen",
        "Xin Qiu"
      ],
      "abstract": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .",
      "summary_en": "Post-Training Quantization (PTQ) enables deploying Large Language Models (LLMs) on memory-constrained devices but renders models static and incompatible with standard fine-tuning methods that require backpropagation through discrete, non-differentiable parameter spaces. This paper introduces Quantized Evolution Strategies (QES), which performs full-parameter fine-tuning directly in quantized space using accumulated error feedback to preserve high-precision gradient signals and stateless seed replay to reduce memory usage. QES significantly outperforms state-of-the-art zeroth-order methods on arithmetic reasoning tasks, enabling direct fine-tuning of quantized models and opening possibilities for scaling LLMs entirely in quantized space.",
      "summary_zh": "åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿéƒ¨ç½²äºå†…å­˜å—é™è®¾å¤‡ï¼Œä½†ä¼šä½¿æ¨¡å‹é™æ€åŒ–ï¼Œå¹¶ä¸éœ€è¦é€šè¿‡ç¦»æ•£ã€ä¸å¯å¾®å‚æ•°ç©ºé—´è¿›è¡Œåå‘ä¼ æ’­çš„æ ‡å‡†å¾®è°ƒæ–¹æ³•ä¸å…¼å®¹ã€‚æœ¬æ–‡æå‡ºé‡åŒ–è¿›åŒ–ç­–ç•¥ï¼ˆQESï¼‰ï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨é‡åŒ–ç©ºé—´ä¸­è¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œåˆ©ç”¨ç´¯ç§¯è¯¯å·®åé¦ˆä¿ç•™é«˜ç²¾åº¦æ¢¯åº¦ä¿¡å·ï¼Œå¹¶ä½¿ç”¨æ— çŠ¶æ€ç§å­é‡æ”¾å‡å°‘å†…å­˜ä½¿ç”¨ã€‚QESåœ¨ç®—æœ¯æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„zeroth-orderæ–¹æ³•ï¼Œå®ç°äº†å¯¹é‡åŒ–æ¨¡å‹çš„ç›´æ¥å¾®è°ƒï¼Œå¹¶ä¸ºå®Œå…¨åœ¨é‡åŒ–ç©ºé—´ä¸­æ‰©å±•LLMså¼€è¾Ÿäº†å¯èƒ½æ€§ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.03120",
      "arxiv_url": "https://arxiv.org/abs/2602.03120",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03120",
      "github_url": "https://github.com/dibbla/Quantized-Evolution-Strategies",
      "upvotes": 1,
      "fetched_at": "2026-02-17T09:51:55.095923+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13139",
      "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report",
      "authors": [
        "Mariia Fedorova",
        "Nikolay Arefyev",
        "Maja Buljan",
        "JindÅ™ich Helcl",
        "Stephan Oepen",
        "Egil RÃ¸nningstad",
        "Yves Scherrer"
      ],
      "abstract": "OpenLID-v3 improves language identification accuracy for closely related languages and low-resource variants through enhanced training data, cluster merging, and noise detection mechanisms. Language identification (LID) is an essential step in building high-quality multilingual datasets from web data . Existing LID tools (such as OpenLID or GlotLID ) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages . In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters , and introducing a special label for marking noise. We call this extended system OpenLID -v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages . OpenLID -v3 is available on https://huggingface.co/HPLT/ OpenLID -v3.",
      "summary_en": "OpenLID-v3 improves language identification accuracy for closely related languages and low-resource variants through enhanced training data, cluster merging, and noise detection mechanisms.",
      "summary_zh": "OpenLID-v3 é€šè¿‡å¢å¼ºçš„è®­ç»ƒæ•°æ®ã€èšç±»åˆå¹¶å’Œå™ªå£°æ£€æµ‹æœºåˆ¶ï¼Œæå‡äº†å¯¹è¿‘ç¼˜è¯­è¨€åŠä½èµ„æºå˜ä½“çš„è¯­è¨€è¯†åˆ«å‡†ç¡®ç‡ã€‚",
      "hf_url": "https://huggingface.co/papers/2602.13139",
      "arxiv_url": "https://arxiv.org/abs/2602.13139",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13139",
      "github_url": "https://github.com/hplt-project/openlid",
      "upvotes": 0,
      "fetched_at": "2026-02-17T09:52:30.260440+00:00"
    }
  ]
}