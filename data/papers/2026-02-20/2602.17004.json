{
  "date": "2026-02-20",
  "paper_id": "2602.17004",
  "title": "Arcee Trinity Large Technical Report",
  "authors": [
    "Varun Singh",
    "Lucas Krauss",
    "Sami Jaghouar",
    "Matej Sirovatka",
    "Charles Goddard",
    "Fares Obied",
    "Jack Min Ong",
    "Jannik Straube",
    "Fern",
    "Aria Harley",
    "Conner Stewart",
    "Colin Kealty",
    "Maziyar Panahi",
    "Simon Kirsten",
    "Anushka Deshpande",
    "Anneketh Vij",
    "Arthur Bresnu",
    "Pranav Veldurthi",
    "Raghav Ravishankar",
    "Hardik Bishnoi",
    "DatologyAI Team",
    "Arcee AI Team"
  ],
  "abstract": "Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.",
  "summary_en": "Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.",
  "summary_zh": "Arcee Trinity 模型为稀疏混合专家架构，参数量与激活模式各异，采用先进的注意力机制与训练优化技术。",
  "hf_url": "https://huggingface.co/papers/2602.17004",
  "arxiv_url": "https://arxiv.org/abs/2602.17004",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17004",
  "github_url": "",
  "upvotes": 10,
  "fetched_at": "2026-02-21T01:53:01.183988+00:00"
}