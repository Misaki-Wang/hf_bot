{
  "date": "2026-02-20",
  "paper_id": "2602.13515",
  "title": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning",
  "authors": [
    "Jintao Zhang",
    "Kai Jiang",
    "Chendong Xiang",
    "Weiqi Feng",
    "Yuezhou Hu",
    "Haocheng Xi",
    "Jianfei Chen",
    "Jun Zhu"
  ],
  "abstract": "A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning. Many training-free sparse attention methods are effective for accelerating diffusion models . Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p , fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention . Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.",
  "summary_en": "A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.",
  "summary_zh": "提出了一种名为SpargeAttention2的可训练稀疏注意力方法，该方法通过混合掩码规则和蒸馏启发的微调，在扩散模型中实现高稀疏性的同时保持生成质量。",
  "hf_url": "https://huggingface.co/papers/2602.13515",
  "arxiv_url": "https://arxiv.org/abs/2602.13515",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13515",
  "github_url": "https://github.com/thu-ml/SpargeAttn",
  "upvotes": 24,
  "fetched_at": "2026-02-21T01:52:46.205050+00:00"
}