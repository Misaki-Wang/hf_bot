{
  "date": "2026-02-20",
  "paper_id": "2602.16802",
  "title": "References Improve LLM Alignment in Non-Verifiable Domains",
  "authors": [
    "Kejian Shi",
    "Yixin Liu",
    "Peifeng Wang",
    "Alexander R. Fabbri",
    "Shafiq Joty",
    "Arman Cohan"
  ],
  "abstract": "Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques. While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment . In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM , a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard . These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
  "summary_en": "Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.",
  "summary_zh": "参考引导的LLM-evaluator作为软验证器增强LLM对齐，提升评判准确性，并通过自改进技术在不可验证领域实现有效的后训练。",
  "hf_url": "https://huggingface.co/papers/2602.16802",
  "arxiv_url": "https://arxiv.org/abs/2602.16802",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16802",
  "github_url": "https://github.com/yale-nlp/RLRR",
  "upvotes": 0,
  "fetched_at": "2026-02-21T01:52:53.385009+00:00"
}