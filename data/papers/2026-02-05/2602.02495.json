{
  "date": "2026-02-05",
  "paper_id": "2602.02495",
  "title": "Reward-free Alignment for Conflicting Objectives",
  "authors": [
    "Peter Chen",
    "Xiaopeng Li",
    "Xi Chen",
    "Tianyi Lin"
  ],
  "abstract": "A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures. Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent . We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.",
  "summary_en": "A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nA reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures.",
  "hf_url": "https://huggingface.co/papers/2602.02495",
  "arxiv_url": "https://arxiv.org/abs/2602.02495",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02495",
  "github_url": "",
  "upvotes": 2,
  "fetched_at": "2026-02-19T05:27:24.006695+00:00"
}