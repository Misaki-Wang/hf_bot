{
  "date": "2026-02-05",
  "paper_id": "2602.03143",
  "title": "Self-Hinting Language Models Enhance Reinforcement Learning",
  "authors": [
    "Baohao Liao",
    "Hanze Dong",
    "Xinxing Xu",
    "Christof Monz",
    "Jiang Bian"
  ],
  "abstract": "SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models. Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards , GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt x, the model samples a compact hint h (e.g., a plan or decomposition) and then generates a solution τ conditioned on (x,h). Crucially, the task reward R(x,τ) is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards . At test time, we set h=varnothing and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hint s serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.",
  "summary_en": "SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nSAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.",
  "hf_url": "https://huggingface.co/papers/2602.03143",
  "arxiv_url": "https://arxiv.org/abs/2602.03143",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03143",
  "github_url": "https://github.com/BaohaoLiao/SAGE",
  "upvotes": 29,
  "fetched_at": "2026-02-19T05:27:27.988537+00:00"
}