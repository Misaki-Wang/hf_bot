{
  "date": "2026-02-05",
  "paper_id": "2602.04145",
  "title": "Training Data Efficiency in Multimodal Process Reward Models",
  "authors": [
    "Jinyuan Li",
    "Chengsong Huang",
    "Langlin Huang",
    "Shaoyang Xu",
    "Haolin Liu",
    "Wenxuan Zhang",
    "Jiaxin Huang"
  ],
  "abstract": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data. Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench , BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.",
  "summary_en": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.",
  "summary_zh": "通过平衡信息评分高效训练多模态过程奖励模型，优先关注标签混合与可靠性，仅用10%训练数据即可实现全数据性能。",
  "hf_url": "https://huggingface.co/papers/2602.04145",
  "arxiv_url": "https://arxiv.org/abs/2602.04145",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04145",
  "github_url": "",
  "upvotes": 76,
  "fetched_at": "2026-02-19T05:27:38.392337+00:00"
}