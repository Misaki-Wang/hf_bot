{
  "date": "2026-02-05",
  "paper_id": "2602.04651",
  "title": "SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF",
  "authors": [
    "Dipan Maity"
  ],
  "abstract": "A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management. Optimization ( PPO ) has been positioned by recent literature as the canonical method for the RL part of RLHF . PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM- RLHF in an ad-hoc manner and suffers form reward oscillations , entropy collapse , value function drift , and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM- RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation , and PID-controlled adaptive thresholds . Unlike standard PPO 's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE",
  "summary_en": "A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nA new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management.",
  "hf_url": "https://huggingface.co/papers/2602.04651",
  "arxiv_url": "https://arxiv.org/abs/2602.04651",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04651",
  "github_url": "",
  "upvotes": 1,
  "fetched_at": "2026-02-19T05:42:08.372643+00:00"
}