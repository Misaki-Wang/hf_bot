{
  "date": "2026-02-13",
  "paper_id": "2602.12116",
  "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
  "authors": [
    "Pinyi Zhang",
    "Ting-En Lin",
    "Yuchuan Wu",
    "Jingyang Chen",
    "Zongqi Wang",
    "Hua Yang",
    "Ze Xu",
    "Fei Huang",
    "Kai Zhang",
    "Yongbin Li"
  ],
  "abstract": "Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy. Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning . A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling . P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user's scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset . Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.",
  "summary_en": "Personalized generative reward models address challenges in adapting language model responses to individual user preferences by using structured evaluation chains and dual-granularity scaling mechanisms for improved generalization and accuracy.",
  "summary_zh": "个性化生成式奖励模型通过结构化评估链与双粒度缩放机制，解决语言模型响应适配个体用户偏好的挑战，以提升泛化能力与准确性。",
  "hf_url": "https://huggingface.co/papers/2602.12116",
  "arxiv_url": "https://arxiv.org/abs/2602.12116",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12116",
  "github_url": "https://github.com/Tongyi-ConvAI/Qwen-Character",
  "upvotes": 4,
  "fetched_at": "2026-02-19T06:38:52.804208+00:00"
}