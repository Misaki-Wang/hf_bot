{
  "date": "2026-02-13",
  "paper_id": "2602.11964",
  "title": "Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments",
  "authors": [
    "Romain Froger",
    "Pierre Andrews",
    "Matteo Bettini",
    "Amar Budhiraja",
    "Ricardo Silveira Cabral",
    "Virginie Do",
    "Emilien Garreau",
    "Jean-Baptiste Gaya",
    "Hugo Laurençon",
    "Maxime Lecanu",
    "Kunal Malkan",
    "Dheeraj Mekala",
    "Pierre Ménard",
    "Gerard Moreno-Torres Bertran",
    "Ulyana Piterbarg",
    "Mikhail Plekhanov",
    "Mathieu Rita",
    "Andrey Rusakov",
    "Vladislav Vorotilov",
    "Mengjue Wang",
    "Ian Yu",
    "Amine Benhalloum"
  ],
  "abstract": "Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness. We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments . Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints , adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier , enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards . Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1 . These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the \"sim2real\" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.",
  "summary_en": "Gaia2 presents a benchmark for evaluating large language model agents in asynchronous, dynamic environments with temporal constraints and multi-agent collaboration, featuring a write-action verifier for reinforcement learning and revealing trade-offs between reasoning, efficiency, and robustness.",
  "summary_zh": "Gaia2提出了一个基准测试，用于评估大语言模型智能体在具有时间约束的异步动态多智能体协作环境中的表现，配备了用于强化学习的写动作验证器，并揭示了推理、效率与鲁棒性之间的权衡。",
  "hf_url": "https://huggingface.co/papers/2602.11964",
  "arxiv_url": "https://arxiv.org/abs/2602.11964",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11964",
  "github_url": "",
  "upvotes": 12,
  "fetched_at": "2026-02-19T06:38:42.089265+00:00"
}