{
  "date": "2026-02-13",
  "paper_id": "2602.10106",
  "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
  "authors": [
    "Modi Shi",
    "Shijia Peng",
    "Jin Chen",
    "Haoran Jiang",
    "Yinghui Li",
    "Di Huang",
    "Ping Luo",
    "Hongyang Li",
    "Li Chen"
  ],
  "abstract": "EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques. Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
  "summary_en": "EgoHumanoid enables humanoid loco-manipulation through co-training vision-language-action policies using egocentric human demonstrations and limited robot data, addressing embodiment gaps via view and action alignment techniques.",
  "summary_zh": "EgoHumanoid 利用第一人称视角人类演示和有限的机器人数据联合训练视觉-语言-动作策略，实现人形机器人移动操作，并通过视角与动作对齐技术解决具身差距。",
  "hf_url": "https://huggingface.co/papers/2602.10106",
  "arxiv_url": "https://arxiv.org/abs/2602.10106",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10106",
  "github_url": "",
  "upvotes": 20,
  "fetched_at": "2026-02-19T06:37:59.483327+00:00"
}