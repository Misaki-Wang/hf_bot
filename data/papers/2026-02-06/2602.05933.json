{
  "date": "2026-02-06",
  "paper_id": "2602.05933",
  "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
  "authors": [
    "Zhenghao Xu",
    "Qin Lu",
    "Changlong Yu",
    "Tuo Zhao"
  ],
  "abstract": "Policy mirror descent with mean approximation addresses challenges in training large language models by using adaptive regularization for more stable and efficient reinforcement learning. Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space . Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--χ^2 regularizer. This additional χ^2 regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors . Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.",
  "summary_en": "Policy mirror descent with mean approximation addresses challenges in training large language models by using adaptive regularization for more stable and efficient reinforcement learning.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nPolicy mirror descent with mean approximation addresses challenges in training large language models by using adaptive regularization for more stable and efficient reinforcement learning.",
  "hf_url": "https://huggingface.co/papers/2602.05933",
  "arxiv_url": "https://arxiv.org/abs/2602.05933",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05933",
  "github_url": "https://github.com/horizon-rl/OpenKimi",
  "upvotes": 5,
  "fetched_at": "2026-02-19T05:43:21.201875+00:00"
}