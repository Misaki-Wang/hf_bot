{
  "date": "2026-02-06",
  "paper_id": "2602.05073",
  "title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents",
  "authors": [
    "Changdae Oh",
    "Seongheon Park",
    "To Eun Kim",
    "Jiatong Li",
    "Wendi Li",
    "Samuel Yeh",
    "Xuefeng Du",
    "Hamed Hassani",
    "Paul Bogdan",
    "Dawn Song",
    "Sharon Li"
  ],
  "abstract": "Large language models require uncertainty quantification frameworks that account for interactive agent behavior rather than traditional single-turn question answering scenarios. Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents , and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process , a viewpoint that breaks down for interactive agents in an open world . In contrast, we propose a novel perspective, a conditional uncertainty reduction process , that explicitly models reducible uncertainty over an agent's trajectory by highlighting \"interactivity\" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.",
  "summary_en": "Large language models require uncertainty quantification frameworks that account for interactive agent behavior rather than traditional single-turn question answering scenarios.",
  "summary_zh": "大语言模型需要面向交互式智能体行为而非传统单轮问答场景的不确定性量化框架。",
  "hf_url": "https://huggingface.co/papers/2602.05073",
  "arxiv_url": "https://arxiv.org/abs/2602.05073",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05073",
  "github_url": "",
  "upvotes": 11,
  "fetched_at": "2026-02-19T05:42:58.135629+00:00"
}