{
  "date": "2026-02-06",
  "paper_id": "2602.02159",
  "title": "Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing",
  "authors": [
    "Lingkun Long",
    "Yushi Huang",
    "Shihao Bai",
    "Ruihao Gong",
    "Jun Zhang",
    "Ao Zhou",
    "Jianlei Yang"
  ],
  "abstract": "Focus-dLLM introduces a training-free attention sparsification framework that improves inference efficiency for long-context diffusion large language models by predicting unmasked regions and pruning redundant attention computations. Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks . To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency . Experimental results show that our method offers more than 29times lossless speedup under 32K context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM",
  "summary_en": "Focus-dLLM introduces a training-free attention sparsification framework that improves inference efficiency for long-context diffusion large language models by predicting unmasked regions and pruning redundant attention computations.",
  "summary_zh": "Focus-dLLM 提出了一种免训练的注意力稀疏化框架，通过预测未掩码区域并剪枝冗余的注意力计算，提升长上下文扩散大语言模型的推理效率。",
  "hf_url": "https://huggingface.co/papers/2602.02159",
  "arxiv_url": "https://arxiv.org/abs/2602.02159",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02159",
  "github_url": "https://github.com/Longxmas/Focus-dLLM",
  "upvotes": 1,
  "fetched_at": "2026-02-19T05:42:39.077808+00:00"
}