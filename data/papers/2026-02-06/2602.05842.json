{
  "date": "2026-02-06",
  "paper_id": "2602.05842",
  "title": "Reinforcement World Model Learning for LLM-based Agents",
  "authors": [
    "Xiao Yu",
    "Baolin Peng",
    "Ruize Xu",
    "Yelong Shen",
    "Pengcheng He",
    "Suman Nath",
    "Nikhil Singh",
    "Jiangfeng Gao",
    "Zhou Yu"
  ],
  "abstract": "Reinforcement World Model Learning enables LLM-based agents to better anticipate action consequences and adapt to environment dynamics through self-supervised training that aligns simulated and real-world state transitions in embedding space. Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards . Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space . Unlike next-state token prediction , which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and τ^2 Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards , our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and τ^2 Bench respectively, while matching the performance of expert-data training.",
  "summary_en": "Reinforcement World Model Learning enables LLM-based agents to better anticipate action consequences and adapt to environment dynamics through self-supervised training that aligns simulated and real-world state transitions in embedding space.",
  "summary_zh": "强化世界模型学习通过在嵌入空间中对齐模拟与真实世界状态转移的自监督训练，使基于LLM的智能体能够更好地预测动作后果并适应环境动态。",
  "hf_url": "https://huggingface.co/papers/2602.05842",
  "arxiv_url": "https://arxiv.org/abs/2602.05842",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05842",
  "github_url": "",
  "upvotes": 27,
  "fetched_at": "2026-02-19T05:43:15.196514+00:00"
}