{
  "date": "2026-02-09",
  "paper_id": "2602.03075",
  "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
  "authors": [
    "Junjie Huang",
    "Jiarui Qin",
    "Di Yin",
    "Weiwen Liu",
    "Yong Yu",
    "Xing Sun",
    "Weinan Zhang"
  ],
  "abstract": "ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop. Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training . However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training , utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT ( Reinforcement Learning -Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase , prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop , enabling continuous and self-reinforcing evolution of LLMs.",
  "summary_en": "ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.",
  "hf_url": "https://huggingface.co/papers/2602.03075",
  "arxiv_url": "https://arxiv.org/abs/2602.03075",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03075",
  "github_url": "",
  "upvotes": 6,
  "fetched_at": "2026-02-19T05:43:46.853806+00:00"
}