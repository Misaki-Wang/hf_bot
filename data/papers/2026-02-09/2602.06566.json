{
  "date": "2026-02-09",
  "paper_id": "2602.06566",
  "title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
  "authors": [
    "Niccolo Avogaro",
    "Nayanika Debnath",
    "Li Mi",
    "Thomas Frick",
    "Junling Wang",
    "Zexue He",
    "Hang Hua",
    "Konrad Schindler",
    "Mattia Rigotti"
  ],
  "abstract": "SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks. Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning , leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning . Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks , SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the V^* VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200times lower token budget .",
  "summary_en": "SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks.",
  "summary_zh": "SPARC是一种模块化框架，它将视觉感知与推理在视觉-语言模型中解耦，通过针对性的计算分配实现高效的测试时扩展，并提升视觉推理任务的性能。",
  "hf_url": "https://huggingface.co/papers/2602.06566",
  "arxiv_url": "https://arxiv.org/abs/2602.06566",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06566",
  "github_url": "",
  "upvotes": 3,
  "fetched_at": "2026-02-19T05:58:07.346370+00:00"
}