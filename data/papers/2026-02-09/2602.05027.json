{
  "date": "2026-02-09",
  "paper_id": "2602.05027",
  "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
  "authors": [
    "Georgii Aparin",
    "Tasnima Sadekova",
    "Alexey Rukhovich",
    "Assel Yermekova",
    "Laida Kushnareva",
    "Vadim Popov",
    "Kristian Kuznetsov",
    "Irina Piontkovskaya"
  ],
  "abstract": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity. Sparse Autoencoders (SAEs) are po wer ful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT , provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whisper ing) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper 's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception , indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.",
  "summary_en": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nSparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.",
  "hf_url": "https://huggingface.co/papers/2602.05027",
  "arxiv_url": "https://arxiv.org/abs/2602.05027",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05027",
  "github_url": "https://github.com/audiosae/audiosae_demo",
  "upvotes": 59,
  "fetched_at": "2026-02-19T05:43:58.197237+00:00"
}