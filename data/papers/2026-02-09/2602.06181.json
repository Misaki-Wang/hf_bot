{
  "date": "2026-02-09",
  "paper_id": "2602.06181",
  "title": "Uncertainty Drives Social Bias Changes in Quantized Large Language Models",
  "authors": [
    "Stanley Z. Hua",
    "Sanae Lotfi",
    "Irene Y. Chen"
  ],
  "abstract": "Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels. Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping , in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty , where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups , where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes . Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.",
  "summary_en": "Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nPost-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.",
  "hf_url": "https://huggingface.co/papers/2602.06181",
  "arxiv_url": "https://arxiv.org/abs/2602.06181",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06181",
  "github_url": "",
  "upvotes": 1,
  "fetched_at": "2026-02-19T05:44:17.227679+00:00"
}