{
  "date": "2026-02-09",
  "paper_id": "2602.06291",
  "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math",
  "authors": [
    "Guijin Son",
    "Donghun Yang",
    "Hitesh Laxmichand Patel",
    "Hyunwoo Ko",
    "Amit Agarwal",
    "Sunghee Ahn",
    "Kyong-Ha Lee",
    "Youngjae Yu"
  ],
  "abstract": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation. Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models , generative reward models , and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap , maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.",
  "summary_en": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nConsequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.",
  "hf_url": "https://huggingface.co/papers/2602.06291",
  "arxiv_url": "https://arxiv.org/abs/2602.06291",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06291",
  "github_url": "",
  "upvotes": 23,
  "fetched_at": "2026-02-19T05:44:18.793795+00:00"
}