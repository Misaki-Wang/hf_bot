{
  "date": "2026-02-16",
  "paper_id": "2602.13191",
  "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
  "authors": [
    "Sayan Deb Sarkar",
    "Rémi Pautrat",
    "Ondrej Miksik",
    "Marc Pollefeys",
    "Iro Armeni",
    "Mahdi Rad",
    "Mihai Dusmanu"
  ],
  "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
  "summary_en": "Current Video Language Models (VideoLMs) rely on keyframe sampling that misses temporal details and requires expensive full-image encoding for each frame. We propose leveraging video codec primitives, specifically motion vectors and residuals, through lightweight transformer encoders that aggregate these primitives and align their representations with image embeddings. This approach reduces time-to-first-token by up to 86% and token usage by up to 93% while maintaining or exceeding performance on 14 video understanding benchmarks spanning question answering, temporal reasoning, and spatial scene understanding.",
  "summary_zh": "现有的视频语言模型（VideoLMs）依赖关键帧采样，这会丢失时序细节，并且需要对每一帧进行昂贵的全图像编码。我们提出利用视频编解码器原语，特别是运动向量和残差，通过轻量级 transformer 编码器聚合这些原语，并将其表征与图像嵌入对齐。该方法将 time-to-first-token 降低高达86%，token 使用量减少高达93%，同时在涵盖问答、时序推理和空间场景理解的14个视频理解基准测试上保持或超越性能。",
  "hf_url": "https://huggingface.co/papers/2602.13191",
  "arxiv_url": "https://arxiv.org/abs/2602.13191",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13191",
  "github_url": "",
  "upvotes": 24,
  "fetched_at": "2026-02-17T08:53:17.327218+00:00"
}