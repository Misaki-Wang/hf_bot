{
  "date": "2026-02-25",
  "paper_id": "2602.16813",
  "title": "One-step Language Modeling via Continuous Denoising",
  "authors": [
    "Chanhyuk Lee",
    "Jaehoon Yoo",
    "Manan Agarwal",
    "Sheel Shah",
    "Jerry Huang",
    "Aditi Raghunathan",
    "Seunghoon Hong",
    "Nicholas M. Boffi",
    "Jinwoo Kim"
  ],
  "abstract": "Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization. Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities , we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings . We show that the model can be trained by predicting the clean data via a cross entropy objective , where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation . On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities , and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.",
  "summary_en": "Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.",
  "summary_zh": "基于流的语言模型在质量和速度上均优于离散扩散模型，其方法是对 one-hot 词元编码进行欧几里得去噪，并通过时间重参数化提升训练稳定性。",
  "hf_url": "https://huggingface.co/papers/2602.16813",
  "arxiv_url": "https://arxiv.org/abs/2602.16813",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16813",
  "github_url": "https://github.com/david3684/flm",
  "upvotes": 3,
  "fetched_at": "2026-02-26T01:56:57.322853+00:00"
}