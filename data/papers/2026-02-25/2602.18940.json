{
  "date": "2026-02-25",
  "paper_id": "2602.18940",
  "title": "DREAM: Deep Research Evaluation with Agentic Metrics",
  "authors": [
    "Elad Ben Avraham",
    "Changhao Li",
    "Ron Dorfman",
    "Roy Ganz",
    "Oren Nuriel",
    "Amir Dudai",
    "Aviad Aberdam",
    "Noah Flynn",
    "Elman Mansimov",
    "Adi Kalyanpur",
    "Ron Litman"
  ],
  "abstract": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm. Deep Research Agents generate analyst-grade reports , yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis , where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness . To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics ), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent , enabling temporally aware coverage , grounded verification , and systematic reasoning probes . Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
  "summary_en": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
  "summary_zh": "深度研究智能体能够生成分析师级别的报告，然而由于缺乏单一的真值（ground truth）以及研究质量的多维特性，对其进行评估仍然具有挑战性。近期的基准测试提出了不同的方法论，但它们存在\"合成幻觉\"（Mirage of Synthesis）问题：表面上的流畅性和引用对齐可能掩盖潜在的事实性和推理性缺陷。我们通过引入一个涵盖四个维度的分类体系来刻画这一差距，该体系揭示了一个关键的能力不匹配：静态评估器本质上缺乏评估时效性（temporal validity）和事实正确性所需的工具使用能力。为解决这一问题，我们提出了DREAM（Deep Research Evaluation with Agentic Metrics），该框架通过使评估本身智能体化（agentic）来实例化能力对等（capability parity）原则。DREAM通过一种评估协议来构建评估流程，该协议结合查询无关指标（query-agnostic metrics）与由工具调用智能体生成的自适应指标（adaptive metrics），从而实现时间感知覆盖（temporally aware coverage）、基于依据的验证（grounded verification）以及系统性推理探针（reasoning probes）。对照评估表明，DREAM对事实性和时效性衰减的敏感度显著高于现有基准，提供了一种可扩展的、无需参考（reference-free）的评估范式。",
  "hf_url": "https://huggingface.co/papers/2602.18940",
  "arxiv_url": "https://arxiv.org/abs/2602.18940",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18940",
  "github_url": "",
  "upvotes": 10,
  "fetched_at": "2026-02-26T01:57:01.849129+00:00"
}