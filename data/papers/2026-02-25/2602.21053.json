{
  "date": "2026-02-25",
  "paper_id": "2602.21053",
  "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
  "authors": [
    "Shimin Wen",
    "Zeyu Zhang",
    "Xingdou Bian",
    "Hongjie Zhu",
    "Lulu He",
    "Layi Shama",
    "Daji Ergu",
    "Ying Cai"
  ],
  "abstract": "A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks. Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms , making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection . This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection , then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning . Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
  "summary_en": "A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.",
  "summary_zh": "一种新颖的迭代式自我修正框架通过能力反思与记忆反思机制增强视觉语言模型的推理鲁棒性，在视觉理解基准测试中取得更优性能。",
  "hf_url": "https://huggingface.co/papers/2602.21053",
  "arxiv_url": "https://arxiv.org/abs/2602.21053",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21053",
  "github_url": "https://github.com/AIGeeksGroup/OCR-Agent",
  "upvotes": 1,
  "fetched_at": "2026-02-26T01:57:19.498395+00:00"
}