{
  "date": "2026-02-03",
  "paper_id": "2601.21968",
  "title": "OVD: On-policy Verbal Distillation",
  "authors": [
    "Jing Xiong",
    "Hui Shen",
    "Shansan Gong",
    "Yuxin Cheng",
    "Jianghan Shen",
    "Chaofan Tao",
    "Haochen Tan",
    "Haoli Bai",
    "Lifeng Shang",
    "Ngai Wong"
  ],
  "abstract": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models ; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models , which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning . We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models . OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment , allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io",
  "summary_en": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints.",
  "summary_zh": "策略内语言蒸馏（OVD）通过以离散语言评分进行轨迹匹配来替代token级概率匹配，实现从教师模型到学生模型的高效知识迁移，降低内存消耗，并支持无token对齐约束的自由探索。",
  "hf_url": "https://huggingface.co/papers/2601.21968",
  "arxiv_url": "https://arxiv.org/abs/2601.21968",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21968",
  "github_url": "",
  "upvotes": 2,
  "fetched_at": "2026-02-19T05:36:04.617166+00:00"
}