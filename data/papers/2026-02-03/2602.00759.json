{
  "date": "2026-02-03",
  "paper_id": "2602.00759",
  "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
  "authors": [
    "Zhipeng Chen",
    "Xiaobo Qin",
    "Wayne Xin Zhao",
    "Youbin Wu",
    "Ji-Rong Wen"
  ],
  "abstract": "Adaptive Ability Decomposing (A²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model. Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration , which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions . Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer , revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner 's exploration and exploitation abilities.",
  "summary_en": "Adaptive Ability Decomposing (A²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nAdaptive Ability Decomposing (A²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.",
  "hf_url": "https://huggingface.co/papers/2602.00759",
  "arxiv_url": "https://arxiv.org/abs/2602.00759",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00759",
  "github_url": "",
  "upvotes": 5,
  "fetched_at": "2026-02-19T05:36:30.170477+00:00"
}