{
  "date": "2026-02-03",
  "paper_id": "2602.02156",
  "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
  "authors": [
    "Wen-Jie Shu",
    "Xuerui Qiu",
    "Rui-Jie Zhu",
    "Harold Haodong Chen",
    "Yexin Liu",
    "Harry Yang"
  ],
  "abstract": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters. Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark . However, we argue that the feed-forward architecture , where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence . Loop-ViT iterates a weight-tied Hybrid Block , combining local convolutions and global attention , to form a latent chain of thought . Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy : the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
  "summary_en": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.",
  "summary_zh": "Loop-ViT引入了一种递归视觉Transformer架构，通过权重共享的递归和动态退出机制将推理深度与模型容量解耦，以更少的参数实现了更优的视觉推理性能。",
  "hf_url": "https://huggingface.co/papers/2602.02156",
  "arxiv_url": "https://arxiv.org/abs/2602.02156",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02156",
  "github_url": "https://github.com/WenjieShu/LoopViT",
  "upvotes": 12,
  "fetched_at": "2026-02-19T05:37:39.963904+00:00"
}