{
  "date": "2026-02-03",
  "paper_id": "2602.02383",
  "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
  "authors": [
    "Maksim Afanasyev",
    "Illarion Iov"
  ],
  "abstract": "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints. Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback ( RLHF ) for aligning Large Language Models ( LLMs ). Latest approaches have streamlined the alignment process by deriving implicit reward functions , yet they often suffer from a critical objective mismatch : optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood . This can lead to `` unlearning '', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and `` formatting collapse '' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability .",
  "summary_en": "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nSLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.",
  "hf_url": "https://huggingface.co/papers/2602.02383",
  "arxiv_url": "https://arxiv.org/abs/2602.02383",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02383",
  "github_url": "https://github.com/fpsigma/trl-slime",
  "upvotes": 29,
  "fetched_at": "2026-02-19T05:37:58.687613+00:00"
}