{
  "date": "2026-02-03",
  "paper_id": "2602.00986",
  "title": "Sparse Reward Subsystem in Large Language Models",
  "authors": [
    "Guowei Xu",
    "Mert Yuksekgonul",
    "James Zou"
  ],
  "abstract": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments , we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.",
  "summary_en": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments , we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.",
  "summary_zh": "本文识别出大语言模型（LLMs）隐藏状态中存在一个稀疏奖励子系统，并将其类比于人脑中的生物奖励子系统。我们证明该子系统包含价值神经元，这些神经元表征模型对状态价值的内部预期，并通过干预实验确立了这些神经元对推理的重要性。实验表明，这些价值神经元在多样化数据集、不同模型规模和架构上均具有鲁棒性；此外，它们在来自同一基础模型的不同数据集和微调模型之间表现出显著的可迁移性。通过检验价值预测与实际奖励出现分歧的案例，我们在奖励子系统中识别出编码奖励预测误差（RPE）的多巴胺神经元。当奖励高于预期时，这些神经元表现出高激活；当奖励低于预期时，则表现出低激活。",
  "hf_url": "https://huggingface.co/papers/2602.00986",
  "arxiv_url": "https://arxiv.org/abs/2602.00986",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00986",
  "github_url": "",
  "upvotes": 13,
  "fetched_at": "2026-02-19T05:36:34.126929+00:00"
}