{
  "date": "2026-02-03",
  "paper_id": "2602.00919",
  "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
  "authors": [
    "I. Apanasevich",
    "M. Artemyev",
    "R. Babakyan",
    "P. Fedotova",
    "D. Grankin",
    "E. Kupryashin",
    "A. Misailidi",
    "D. Nerus",
    "A. Nutalapati",
    "G. Sidorov",
    "I. Efremov",
    "M. Gerasyov",
    "D. Pikurov",
    "Y. Senchenko",
    "S. Davidenko",
    "D. Kulikov",
    "M. Sultankin",
    "K. Askarbek",
    "O. Shamanin",
    "D. Statovoy",
    "E. Zalyaev",
    "I. Zorin"
  ],
  "abstract": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning. We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding , (R0) multi-embodiment pretraining , (R1) embodiment-specific adaptation , and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction , out-of-distribution detection , and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
  "summary_en": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
  "summary_zh": "Green-VLA是一个用于真实世界机器人部署的五阶段视觉-语言-动作框架，通过多模态训练和强化学习实现跨不同机器人本体的泛化。",
  "hf_url": "https://huggingface.co/papers/2602.00919",
  "arxiv_url": "https://arxiv.org/abs/2602.00919",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00919",
  "github_url": "https://github.com/greenvla/GreenVLA",
  "upvotes": 280,
  "fetched_at": "2026-02-19T05:36:32.100057+00:00"
}