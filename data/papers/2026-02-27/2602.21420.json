{
  "date": "2026-02-27",
  "paper_id": "2602.21420",
  "title": "Overconfident Errors Need Stronger Correction: Asymmetric Confidence Penalties for Reinforcement Learning",
  "authors": [
    "Yuanda Xu",
    "Hejian Sang",
    "Zhengze Zhou",
    "Ran He",
    "Zhipeng Wang"
  ],
  "abstract": "Reinforcement learning with verifiable rewards suffers from reduced reasoning diversity due to uniform error penalization, which is addressed by a confidence-aware asymmetric error penalty method that dynamically modulates advantages based on rollout confidence. Reinforcement Learning with Verifiable Rewards (RLVR) has become the leading paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard RLVR algorithms suffer from a well-documented pathology: while they improve Pass@1 accuracy through sharpened sampling , they simultaneously narrow the model's reasoning boundary and reduce generation diversity . We identify a root cause that existing methods overlook: the uniform penalization of errors. Current approaches -- whether data-filtering methods that select prompts by difficulty, or advantage normalization schemes -- treat all incorrect rollouts within a group identically. We show that this uniformity allows overconfident errors (incorrect reasoning paths that the RL process has spuriously reinforced) to persist and monopolize probability mass, ultimately suppressing valid exploratory trajectories. To address this, we propose the Asymmetric Confidence-aware Error Penalty (ACE). ACE introduces a per-rollout confidence shift metric , c_i = log(pi_theta(y_i|x) / pi_ref(y_i|x)), to dynamically modulate negative advantages. Theoretically, we demonstrate that ACE's gradient can be decomposed into the gradient of a selective regularizer restricted to overconfident errors , plus a well-characterized residual that partially moderates the regularizer's strength. We conduct extensive experiments fine-tuning Qwen2.5-Math-7B, Qwen3-8B-Base, and Llama-3.1-8B-Instruct on the DAPO -Math-17K dataset using GRPO and DAPO within the VERL framework . Evaluated on MATH-500 and AIME 2025 , ACE composes seamlessly with existing methods and consistently improves the full Pass@k spectrum across all three model families and benchmarks.",
  "summary_en": "Reinforcement learning with verifiable rewards suffers from reduced reasoning diversity due to uniform error penalization, which is addressed by a confidence-aware asymmetric error penalty method that dynamically modulates advantages based on rollout confidence.",
  "summary_zh": "可验证奖励的强化学习因统一错误惩罚而面临推理多样性降低的问题，对此，一种置信度感知的不对称错误惩罚方法通过基于 rollout 置信度动态调节优势加以解决。",
  "hf_url": "https://huggingface.co/papers/2602.21420",
  "arxiv_url": "https://arxiv.org/abs/2602.21420",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21420",
  "github_url": "",
  "upvotes": 1,
  "fetched_at": "2026-02-28T01:46:40.445633+00:00"
}