{
  "date": "2026-02-04",
  "paper_id": "2602.03295",
  "title": "POP: Prefill-Only Pruning for Efficient Large Model Inference",
  "authors": [
    "Junhui He",
    "Zhihui Fu",
    "Jun Wang",
    "Qingan Li"
  ],
  "abstract": "Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy. Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stage s. By introducing a virtual gate mechanism , our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage . To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.",
  "summary_en": "Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nStage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.",
  "hf_url": "https://huggingface.co/papers/2602.03295",
  "arxiv_url": "https://arxiv.org/abs/2602.03295",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03295",
  "github_url": "",
  "upvotes": 4,
  "fetched_at": "2026-02-19T05:26:58.848956+00:00"
}