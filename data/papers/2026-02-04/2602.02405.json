{
  "date": "2026-02-04",
  "paper_id": "2602.02405",
  "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning",
  "authors": [
    "Ethan Mendes",
    "Jungsoo Park",
    "Alan Ritter"
  ],
  "abstract": "Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data. Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization .",
  "summary_en": "Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.",
  "summary_zh": "分布对齐模仿学习（DAIL）通过将专家解决方案转换为分布内轨迹，并使用对比学习聚焦专家方法，在仅需极少专家数据的情况下实现了大语言模型推理能力的显著提升。",
  "hf_url": "https://huggingface.co/papers/2602.02405",
  "arxiv_url": "https://arxiv.org/abs/2602.02405",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02405",
  "github_url": "https://github.com/ethanm88/DAIL",
  "upvotes": 2,
  "fetched_at": "2026-02-19T05:39:05.756339+00:00"
}