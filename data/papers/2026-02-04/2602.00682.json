{
  "date": "2026-02-04",
  "paper_id": "2602.00682",
  "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
  "authors": [
    "Yuecheng Li",
    "Hengwei Ju",
    "Zeyu Song",
    "Wei Yang",
    "Chi Lu",
    "Peng Jiang",
    "Kun Gai"
  ],
  "abstract": "A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning. Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning . However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation , which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.",
  "summary_en": "A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.",
  "summary_zh": "[DUMMY-TRANSLATION] 未配置真实翻译服务，以下保留英文原文。\nA novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.",
  "hf_url": "https://huggingface.co/papers/2602.00682",
  "arxiv_url": "https://arxiv.org/abs/2602.00682",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00682",
  "github_url": "https://github.com/6lyc/RecGOAT-LLM4Rec",
  "upvotes": 1,
  "fetched_at": "2026-02-19T05:38:40.207167+00:00"
}