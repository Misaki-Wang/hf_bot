{
  "date": "2026-02-04",
  "paper_id": "2602.02494",
  "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
  "authors": [
    "Dulhan Jayalath",
    "Oiwi Parker Jones"
  ],
  "abstract": "MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data. Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context . Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models . We find that models pre-trained with longer contexts learn representations that transfer better to word decoding . Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .",
  "summary_en": "MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.",
  "summary_zh": "MEG-XL通过2.5分钟MEG上下文的扩展预训练提升了脑到文本解码性能，显著优于使用更少上下文数据的先前模型。",
  "hf_url": "https://huggingface.co/papers/2602.02494",
  "arxiv_url": "https://arxiv.org/abs/2602.02494",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02494",
  "github_url": "https://github.com/neural-processing-lab/MEG-XL",
  "upvotes": 1,
  "fetched_at": "2026-02-19T05:39:11.346986+00:00"
}