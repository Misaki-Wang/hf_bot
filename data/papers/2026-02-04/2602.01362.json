{
  "date": "2026-02-04",
  "paper_id": "2602.01362",
  "title": "Balancing Understanding and Generation in Discrete Diffusion Models",
  "authors": [
    "Yue Liu",
    "Yuzhong Zhao",
    "Zheyong Xie",
    "Qixiang Ye",
    "Jianbin Jiao",
    "Yao Hu",
    "Shaosheng Cao",
    "Yunfan Liu"
  ],
  "abstract": "XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality. In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel . XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities . Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation ( FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model , XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM",
  "summary_en": "XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.",
  "summary_zh": "XDLM 通过平稳噪声核统一了掩码扩散语言模型与均匀噪声扩散语言模型，在语义理解和生成质量上均取得了性能提升。",
  "hf_url": "https://huggingface.co/papers/2602.01362",
  "arxiv_url": "https://arxiv.org/abs/2602.01362",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01362",
  "github_url": "https://github.com/MzeroMiko/XDLM",
  "upvotes": 14,
  "fetched_at": "2026-02-19T05:38:48.043281+00:00"
}