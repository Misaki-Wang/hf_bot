{
  "date": "2026-02-04",
  "paper_id": "2602.03139",
  "title": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis",
  "authors": [
    "Tianhe Wu",
    "Ruibin Li",
    "Lei Zhang",
    "Kede Ma"
  ],
  "abstract": "A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead. Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse , as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction ) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.",
  "summary_en": "A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.",
  "summary_zh": "本文提出了一种名为 DP-DMD 的新型蒸馏框架，通过分离蒸馏步骤的角色，使用 v-prediction 保持多样性，并利用标准 DMD loss 进行质量优化，在不增加额外计算开销的情况下，实现文本到图像生成中的样本多样性保持。",
  "hf_url": "https://huggingface.co/papers/2602.03139",
  "arxiv_url": "https://arxiv.org/abs/2602.03139",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03139",
  "github_url": "https://github.com/Multimedia-Analytics-Laboratory/dpdmd",
  "upvotes": 41,
  "fetched_at": "2026-02-19T05:26:55.664893+00:00"
}