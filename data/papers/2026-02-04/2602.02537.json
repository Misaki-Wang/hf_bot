{
  "date": "2026-02-04",
  "paper_id": "2602.02537",
  "title": "WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models",
  "authors": [
    "Runjie Zhou",
    "Youbo Shao",
    "Haoyu Lu",
    "Bowei Xing",
    "Tongtong Bai",
    "Yujie Chen",
    "Jie Zhao",
    "Lin Sui",
    "Haotian Yao",
    "Zijia Zhao",
    "Hao Yang",
    "Haoning Wu",
    "Zaida Zhou",
    "Jinguo Zhu",
    "Zhiqi Huang",
    "Yiping Bao",
    "Yangyang Liu",
    "Y. Charles",
    "Xinyu Zhou"
  ],
  "abstract": "WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts. We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning , WorldVQA decouples these capabilities to strictly measure \"what the model memorizes.\" The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality , thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.",
  "summary_en": "WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.",
  "summary_zh": "WorldVQA是一个用于评估多模态大语言模型视觉世界知识的基准，通过分离视觉知识检索与推理来衡量记忆的事实。",
  "hf_url": "https://huggingface.co/papers/2602.02537",
  "arxiv_url": "https://arxiv.org/abs/2602.02537",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02537",
  "github_url": "",
  "upvotes": 6,
  "fetched_at": "2026-02-19T05:39:13.542051+00:00"
}