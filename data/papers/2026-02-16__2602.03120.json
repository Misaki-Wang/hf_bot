{
  "date": "2026-02-16",
  "paper_id": "2602.03120",
  "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
  "authors": [
    "Yinggan Xu",
    "Risto Miikkulainen",
    "Xin Qiu"
  ],
  "abstract": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .",
  "summary_en": "Post-Training Quantization (PTQ) enables deploying Large Language Models (LLMs) on memory-constrained devices but renders models static and incompatible with standard fine-tuning paradigms, including Reinforcement Learning (RL), which require backpropagation through continuous, high-precision weights. This paper introduces Quantized Evolution Strategies (QES), which performs full-parameter fine-tuning directly in the discrete, quantized space by integrating accumulated error feedback to preserve gradient signals and utilizing stateless seed replay to reduce memory usage to inference levels. QES significantly outperforms state-of-the-art zeroth-order fine-tuning methods on arithmetic reasoning tasks, enabling direct fine-tuning and scaling of LLMs entirely within the quantized parameter space.",
  "summary_zh": "后训练量化（PTQ）使得大语言模型（LLMs）能够部署在内存受限设备上，但会使模型静态化，且与包括强化学习（RL）在内的标准微调范式不兼容，后者需要通过连续的高精度权重进行反向传播。本文提出量化进化策略（QES），该方法通过整合累积误差反馈以保留梯度信号，并利用无状态种子重放将内存占用降至推理水平，从而在离散量化空间中直接进行全参数微调。QES在算术推理任务上显著优于最先进的zeroth-order微调方法，使得完全在量化参数空间内直接微调和扩展LLMs成为可能。",
  "hf_url": "https://huggingface.co/papers/2602.03120",
  "arxiv_url": "https://arxiv.org/abs/2602.03120",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03120",
  "github_url": "https://github.com/dibbla/Quantized-Evolution-Strategies",
  "upvotes": 1,
  "fetched_at": "2026-02-17T08:52:32.803504+00:00"
}