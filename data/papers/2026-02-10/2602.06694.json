{
  "date": "2026-02-10",
  "paper_id": "2602.06694",
  "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
  "authors": [
    "Hyochan Chong",
    "Dongkyu Kim",
    "Changdong Kim",
    "Minseop Choi"
  ],
  "abstract": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment. Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization , achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
  "summary_en": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.",
  "summary_zh": "NanoQuant通过低秩二值分解和ADMM优化，实现大语言模型的高效后训练量化至二值及亚1比特级别，在达到最先进精度的同时降低消费级硬件部署的内存需求。",
  "hf_url": "https://huggingface.co/papers/2602.06694",
  "arxiv_url": "https://arxiv.org/abs/2602.06694",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06694",
  "github_url": "",
  "upvotes": 15,
  "fetched_at": "2026-02-19T06:02:16.508724+00:00"
}