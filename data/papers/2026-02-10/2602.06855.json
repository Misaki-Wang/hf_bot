{
  "date": "2026-02-10",
  "paper_id": "2602.06855",
  "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
  "authors": [
    "Alisia Lupidi",
    "Bhavul Gauri",
    "Thomas Simon Foster",
    "Bassel Al Omari",
    "Despoina Magka",
    "Alberto Pepe",
    "Alexis Audran-Reiss",
    "Muna Aghamelu",
    "Nicolas Baldwin",
    "Lucia Cipolina-Kun",
    "Jean-Christophe Gagnon-Audet",
    "Chee Hau Leow",
    "Sandra Lefdal",
    "Hossam Mossalam",
    "Abhinav Moudgil",
    "Saba Nazir",
    "Emanuel Tewolde",
    "Isabel Urrego",
    "Jordi Armengol Estape",
    "Amar Budhiraja",
    "Gaurav Chaurasia",
    "Abhishek Charnalia"
  ],
  "abstract": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancement. LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark ), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds . Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
  "summary_en": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancement.",
  "summary_zh": "AIRS-Bench提出了一个全面的基准测试套件，用于评估跨多个科学领域的LLM智能体，揭示当前局限性并提供开源资源以推动发展。",
  "hf_url": "https://huggingface.co/papers/2602.06855",
  "arxiv_url": "https://arxiv.org/abs/2602.06855",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06855",
  "github_url": "https://github.com/facebookresearch/airs-bench",
  "upvotes": 70,
  "fetched_at": "2026-02-19T06:02:18.646878+00:00"
}