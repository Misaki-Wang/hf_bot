{
  "date": "2026-02-10",
  "paper_id": "2602.03784",
  "title": "Context Compression via Explicit Information Transmission",
  "authors": [
    "Jiangnan Ye",
    "Hanqi Yan",
    "Zhenyi Shen",
    "Heng Chang",
    "Ye Mao",
    "Yulan He"
  ],
  "abstract": "ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms. Long-context inference with Large Language Models ( LLMs ) is costly due to quadratic attention and growing key-value caches , motivating context compression. In this work, we study soft context compression , where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission ), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states . This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors , mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
  "summary_en": "ComprExIT introduces a novel approach to long-context inference in LLMs by using explicit information transmission over frozen hidden states, improving compression efficiency through depth-wise and width-wise transmission mechanisms.",
  "summary_zh": "ComprExIT 提出了一种用于大语言模型长上下文推理的新方法，该方法通过冻结隐状态上的显式信息传输，并借助深度与宽度方向的传输机制来提升压缩效率。",
  "hf_url": "https://huggingface.co/papers/2602.03784",
  "arxiv_url": "https://arxiv.org/abs/2602.03784",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03784",
  "github_url": "",
  "upvotes": 14,
  "fetched_at": "2026-02-19T06:01:53.843519+00:00"
}