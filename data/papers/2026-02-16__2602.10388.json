{
  "date": "2026-02-16",
  "paper_id": "2602.10388",
  "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
  "authors": [
    "Zhongzhi Li",
    "Xuansheng Wu",
    "Yijiang Li",
    "Lijie Hu",
    "Ninghao Liu"
  ],
  "abstract": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures. The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance . In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following , toxicity detection , reward modeling , and behavior steering . Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer . Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
  "summary_en": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.",
  "summary_zh": "Feature Activation Coverage 在可解释的特征空间中衡量数据多样性，并实现多样性驱动的数据合成，从而提升跨多种语言模型架构的下游性能。",
  "hf_url": "https://huggingface.co/papers/2602.10388",
  "arxiv_url": "https://arxiv.org/abs/2602.10388",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10388",
  "github_url": "https://github.com/Zhongzhi660/FAC-Synthesis",
  "upvotes": 202,
  "fetched_at": "2026-02-17T08:52:43.722760+00:00"
}