{
  "date": "2026-02-19",
  "paper_id": "2602.16301",
  "title": "Multi-agent cooperation through in-context co-player inference",
  "authors": [
    "Marissa A. Weis",
    "Maciej Wołczyk",
    "Rajai Nasser",
    "Rif A. Saurous",
    "Blaise Agüera y Arcas",
    "João Sacramento",
    "Alexander Meulemans"
  ],
  "abstract": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation. Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning . Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescale s and \" meta-learners \" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior . Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behavior s.",
  "summary_en": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.",
  "summary_zh": "序列模型通过上下文学习，无需硬编码假设或时间尺度分离，即可在多智能体强化学习中实现合作行为的涌现。",
  "hf_url": "https://huggingface.co/papers/2602.16301",
  "arxiv_url": "https://arxiv.org/abs/2602.16301",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16301",
  "github_url": "",
  "upvotes": 5,
  "fetched_at": "2026-02-19T06:47:33.270961+00:00"
}