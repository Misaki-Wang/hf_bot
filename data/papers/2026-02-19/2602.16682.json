{
  "date": "2026-02-19",
  "paper_id": "2602.16682",
  "title": "Learning Situated Awareness in the Real World",
  "authors": [
    "Chuhan Li",
    "Ruilin Han",
    "Joy Hsu",
    "Yongyuan Liang",
    "Rajiv Dhawan",
    "Jiajun Wu",
    "Ming-Hsuan Yang",
    "Xin Eric Wang"
  ],
  "abstract": "SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks. A core aspect of human perception is situated awareness , the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench ( Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos . SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs . It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos , they often fail to infer a coherent camera geometry , leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
  "summary_en": "SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.",
  "summary_zh": "SAW-Bench 提出了一个新的基准，通过包含人工标注问答对的真实世界视频数据集，评估多模态基础模型中的第一人称情境感知，聚焦于以观察者为中心的空间推理任务。",
  "hf_url": "https://huggingface.co/papers/2602.16682",
  "arxiv_url": "https://arxiv.org/abs/2602.16682",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16682",
  "github_url": "",
  "upvotes": 3,
  "fetched_at": "2026-02-19T06:47:39.228337+00:00"
}