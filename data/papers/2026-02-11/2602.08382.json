{
  "date": "2026-02-11",
  "paper_id": "2602.08382",
  "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
  "authors": [
    "Zhuoen Chen",
    "Dongfang Li",
    "Meishan Zhang",
    "Baotian Hu",
    "Min Zhang"
  ],
  "abstract": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches. Large Language Models (LLMs) face significant challenges in long-context processing , including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall , rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor . A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning , while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA , extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent .",
  "summary_en": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.",
  "summary_zh": "一种受认知启发的长上下文语言建模框架采用分块压缩和选择性记忆召回，以提升效率和性能，优于传统方法。",
  "hf_url": "https://huggingface.co/papers/2602.08382",
  "arxiv_url": "https://arxiv.org/abs/2602.08382",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08382",
  "github_url": "",
  "upvotes": 10,
  "fetched_at": "2026-02-19T06:21:21.015506+00:00"
}