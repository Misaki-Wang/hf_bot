{
  "date": "2026-02-11",
  "paper_id": "2602.05400",
  "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
  "authors": [
    "Shaobo Wang",
    "Xuan Ouyang",
    "Tianyi Xu",
    "Yuzheng Hu",
    "Jialin Liu",
    "Guo Chen",
    "Tianyu Zhang",
    "Junhao Zheng",
    "Kexin Yang",
    "Xingzhang Ren",
    "Dayiheng Liu",
    "Linfeng Zhang"
  ],
  "abstract": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead. As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space . OPUS scores candidates by projecting their effective updates , shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia , OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
  "summary_en": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.",
  "summary_zh": "OPUS是一种动态数据选择框架，通过在稳定的代理派生目标空间中基于优化器诱导的更新投影对数据候选进行评分来提升预训练效率，从而在降低计算开销的同时实现更优性能。",
  "hf_url": "https://huggingface.co/papers/2602.05400",
  "arxiv_url": "https://arxiv.org/abs/2602.05400",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05400",
  "github_url": "",
  "upvotes": 316,
  "fetched_at": "2026-02-19T06:20:41.061634+00:00"
}