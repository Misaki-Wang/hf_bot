{
  "date": "2026-02-11",
  "paper_id": "2602.04208",
  "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
  "authors": [
    "Hyeonbeom Choi",
    "Daechul Ahn",
    "Youhan Lee",
    "Taewook Kang",
    "Seongwon Cho",
    "Jonghyun Choi"
  ],
  "abstract": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes. Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity , where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on ' self-uncertainty ', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
  "summary_en": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.",
  "summary_zh": "SCALE是一种针对视觉-语言-动作模型的新型推理策略，它基于自不确定性联合调节视觉感知与动作，无需额外训练或多次前向传播即可提升鲁棒性。",
  "hf_url": "https://huggingface.co/papers/2602.04208",
  "arxiv_url": "https://arxiv.org/abs/2602.04208",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04208",
  "github_url": "",
  "upvotes": 19,
  "fetched_at": "2026-02-19T06:20:30.145387+00:00"
}