{
  "date": "2026-02-11",
  "paper_id": "2602.05892",
  "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
  "authors": [
    "Han Li",
    "Letian Zhu",
    "Bohan Zhang",
    "Rili Feng",
    "Jiaming Wang",
    "Yue Pan",
    "Earl T. Barr",
    "Sarro Federica",
    "Zhaoyang Chu",
    "He Ye"
  ],
  "abstract": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information. LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents . ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts . We further implement an automated evaluation framework that tracks agent trajectories and measures context recall , precision, and efficiency throughout issue resolution . Using ContextBench, we evaluate four frontier LLMs and five coding agents . Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents ), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.",
  "summary_en": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information.",
  "summary_zh": "ContextBench 通过详细的过程分析评估 coding agents 的上下文检索，揭示先进的 agent 设计在上下文使用方面的提升有限，同时突显了已探索信息与已利用信息之间的差距。",
  "hf_url": "https://huggingface.co/papers/2602.05892",
  "arxiv_url": "https://arxiv.org/abs/2602.05892",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05892",
  "github_url": "https://github.com/EuniAI/ContextBench",
  "upvotes": 3,
  "fetched_at": "2026-02-19T06:20:45.438477+00:00"
}