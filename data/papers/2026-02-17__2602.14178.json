{
  "date": "2026-02-17",
  "paper_id": "2602.14178",
  "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model",
  "authors": [
    "Shaobin Zhuang",
    "Yuang Ai",
    "Jiaming Han",
    "Weijia Mao",
    "Xiaohui Li",
    "Fangyikang Wang",
    "Xiao Wang",
    "Yan Li",
    "Shanchuan Lin",
    "Kun Xu",
    "Zhenheng Yang",
    "Huaibo Huang",
    "Xiangyu Yue",
    "Hao Chen",
    "Yali Wang"
  ],
  "abstract": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements. Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function . SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss . We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding , image generation ( DPG Score : UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing ( GEdit Overall Score : UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
  "summary_en": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.",
  "summary_zh": "UniWeTok引入了一种统一离散tokenizer，其采用大规模二进制码本和新颖的训练技术，在图像生成和多模态任务中实现了卓越性能，同时降低了计算需求。",
  "hf_url": "https://huggingface.co/papers/2602.14178",
  "arxiv_url": "https://arxiv.org/abs/2602.14178",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14178",
  "github_url": "https://github.com/shallowdream204/BitDance",
  "upvotes": 6,
  "fetched_at": "2026-02-17T15:57:55.097541+00:00"
}