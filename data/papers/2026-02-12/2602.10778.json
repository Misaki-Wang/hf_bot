{
  "date": "2026-02-12",
  "paper_id": "2602.10778",
  "title": "GoodVibe: Security-by-Vibe for LLM-Based Code Generation",
  "authors": [
    "Maximilian Thang",
    "Lichao Wu",
    "Sasha Behrouzi",
    "Mohamadreza Rostami",
    "Jona te Lintelo",
    "Stjepan Picek",
    "Ahmad-Reza Sadeghi"
  ],
  "abstract": "GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs. Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations , which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control. We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering , enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters , and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.",
  "summary_en": "GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.",
  "summary_zh": "GoodVibe是一种神经元级框架，通过对安全相关神经元进行针对性微调来增强代码语言模型的安全性，同时保持模型效用并显著降低训练成本。",
  "hf_url": "https://huggingface.co/papers/2602.10778",
  "arxiv_url": "https://arxiv.org/abs/2602.10778",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10778",
  "github_url": "",
  "upvotes": 2,
  "fetched_at": "2026-02-19T06:31:55.008381+00:00"
}