{
  "date": "2026-02-12",
  "paper_id": "2602.10224",
  "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
  "authors": [
    "Shiting Huang",
    "Zecheng Li",
    "Yu Zeng",
    "Qingnan Ren",
    "Zhen Fang",
    "Qisheng Su",
    "Kou Shi",
    "Lin Chen",
    "Zehui Chen",
    "Feng Zhao"
  ],
  "abstract": "Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience . Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory . Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience . The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood , which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.",
  "summary_en": "Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.",
  "summary_zh": "元经验学习通过对比轨迹分析与语言建模的奖励信号，将自蒸馏错误表征纳入参数化记忆，从而增强大语言模型的推理能力。",
  "hf_url": "https://huggingface.co/papers/2602.10224",
  "arxiv_url": "https://arxiv.org/abs/2602.10224",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10224",
  "github_url": "",
  "upvotes": 19,
  "fetched_at": "2026-02-19T06:31:30.281131+00:00"
}