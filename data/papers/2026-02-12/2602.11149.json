{
  "date": "2026-02-12",
  "paper_id": "2602.11149",
  "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
  "authors": [
    "Dawid J. Kopiczko",
    "Sagar Vaze",
    "Tijmen Blankevoort",
    "Yuki M. Asano"
  ],
  "abstract": "Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration. Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models . Standard machine learning intuition suggests that training with more unique training samples yields better generalization . Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME '24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting . We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization , a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization , as a new open problem for the community in understanding the training dynamics of large language models.",
  "summary_en": "Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.",
  "summary_zh": "在较小数据集上使用重复样本训练推理语言模型，其性能优于在更大数据集上进行单轮训练，且 token 准确率可作为确定最优训练时长的可靠指标。",
  "hf_url": "https://huggingface.co/papers/2602.11149",
  "arxiv_url": "https://arxiv.org/abs/2602.11149",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11149",
  "github_url": "https://github.com/dkopi/data-repetition",
  "upvotes": 12,
  "fetched_at": "2026-02-19T06:32:17.427661+00:00"
}