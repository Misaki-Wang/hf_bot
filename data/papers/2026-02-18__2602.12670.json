{
  "date": "2026-02-18",
  "paper_id": "2602.12670",
  "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
  "authors": [
    "Xiangyi Li",
    "Wenbo Chen",
    "Yimin Liu",
    "Shenghan Zheng",
    "Xiaokun Chen",
    "Yifeng He",
    "Yubo Li",
    "Bingran You",
    "Haotian Shen",
    "Jiankai Sun",
    "Shuyi Wang",
    "Qunhong Zeng",
    "Di Wang",
    "Xuandong Zhao",
    "Yuanli Wang",
    "Roey Ben Chaim",
    "Zonglin Di",
    "Yipeng Gao",
    "Junwei He",
    "Yizhuo He",
    "Liqiang Jing",
    "Luyang Kong"
  ],
  "abstract": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions. Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench , a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills , and self-generated Skills . We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
  "summary_en": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.",
  "summary_zh": "SkillsBench在86项任务中评估了智能体技能，发现精选技能虽能显著提升性能但效果不一，而自生成技能则毫无助益，这表明尽管模型能从精选版本中获益，却难以创建有用的程序性知识。",
  "hf_url": "https://huggingface.co/papers/2602.12670",
  "arxiv_url": "https://arxiv.org/abs/2602.12670",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12670",
  "github_url": "https://github.com/benchflow-ai/skillsbench",
  "upvotes": 26,
  "fetched_at": "2026-02-18T14:10:21.303859+00:00"
}