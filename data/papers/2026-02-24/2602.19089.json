{
  "date": "2026-02-24",
  "paper_id": "2602.19089",
  "title": "Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling",
  "authors": [
    "Qi Sun",
    "Can Wang",
    "Jiaxiang Shang",
    "Yingchun Liu",
    "Jing Liao"
  ],
  "abstract": "Ani3DHuman framework combines kinematics-based animation with video diffusion priors to generate photorealistic 3D human animations by addressing challenges in non-rigid motion synthesis and identity preservation. Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non- rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors . We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion . Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion . However, this restoration task, based on diffusion sampling , is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality ) with self-guidance (for identity fidelity ). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \\MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman.",
  "summary_en": "Ani3DHuman framework combines kinematics-based animation with video diffusion priors to generate photorealistic 3D human animations by addressing challenges in non-rigid motion synthesis and identity preservation.",
  "summary_zh": "Ani3DHuman框架结合基于运动学的动画与视频扩散先验，通过解决非刚性运动合成和身份保持的挑战，生成真实感3D人体动画。",
  "hf_url": "https://huggingface.co/papers/2602.19089",
  "arxiv_url": "https://arxiv.org/abs/2602.19089",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19089",
  "github_url": "https://github.com/qiisun/ani3dhuman",
  "upvotes": 0,
  "fetched_at": "2026-02-25T02:01:28.149662+00:00"
}