{
  "date": "2026-02-24",
  "paper_id": "2602.20161",
  "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
  "authors": [
    "Abdelrahman Shaker",
    "Ahmed Heakl",
    "Jaseel Muhammad",
    "Ritesh Thawkar",
    "Omkar Thawakar",
    "Senmao Li",
    "Hisham Cholakkal",
    "Ian Reid",
    "Eric P. Xing",
    "Salman Khan",
    "Fahad Shahbaz Khan"
  ],
  "abstract": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology. Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices . We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment . This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding , Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices . We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
  "summary_en": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.",
  "summary_zh": "一种名为 Mobile-O 的紧凑视觉-语言-扩散模型通过专门的架构设计和优化的训练方法，在移动设备上实现了高效的统一多模态理解与生成。",
  "hf_url": "https://huggingface.co/papers/2602.20161",
  "arxiv_url": "https://arxiv.org/abs/2602.20161",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20161",
  "github_url": "https://github.com/Amshaker/Mobile-O",
  "upvotes": 18,
  "fetched_at": "2026-02-25T02:01:41.067874+00:00"
}