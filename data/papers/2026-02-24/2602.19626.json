{
  "date": "2026-02-24",
  "paper_id": "2602.19626",
  "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
  "authors": [
    "Roberto Tacconelli"
  ],
  "abstract": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder . Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs. On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
  "summary_en": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder . Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs. On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
  "summary_zh": "我们提出Nacrith，一种无损压缩系统，其将1.35亿参数Transformer语言模型（SmolLM2-135M）与轻量级在线预测器集合及32位算术编码器相结合。除基础的LLM加算术编码范式外，Nacrith引入以下贡献：（1）CDF精度从2^16提升至2^24，消除大词汇表中由最小概率下限导致的大约75%量化开销；（2）用于快速局部预测的token级N-gram模型；（3）通过在线梯度下降修正每篇文档LLM误差的自适应对数空间偏置头；（4）基于置信度的LLM跳过机制，用于加速高可预测性token；（5）混合二进制格式（NC06），将神经压缩扩展至任意二进制文件——据我们所知，这在基于LLM的压缩器中尚属首次；（6）llama.cpp推理后端，单token解码速度比PyTorch快约7倍；（7）最多8个worker的并行多GPU压缩；（8）原生KV缓存滑动窗口，将每步滑动开销降低约37倍。该系统每worker仅需约500 MB GGUF权重和约1.2 GB显存，可在消费级GPU上运行。在alice29.txt（Canterbury语料库，152 KB）上，Nacrith达到0.918 bits per byte（bpb），优于gzip达3.1倍、bzip2达2.5倍、CMIX v21达44%、ts_zip达20%，且压缩结果低于0阶、1阶和2阶字节级香农熵界限。在enwik8（100 MB）上，Nacrith达到0.9389 bpb（11.74%），超过ts_zip（约1.11 bpb）15%、FineZip（1.024 bpb）8%，尽管其使用小60倍的模型且未经微调。在模型训练截止日期之后发布的文档上进行分布外评估，证实这些提升并非记忆效应，在未见文本上达到0.723 bpb。",
  "hf_url": "https://huggingface.co/papers/2602.19626",
  "arxiv_url": "https://arxiv.org/abs/2602.19626",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19626",
  "github_url": "https://github.com/robtacconelli/Nacrith-GPU",
  "upvotes": 2,
  "fetched_at": "2026-02-25T02:01:32.760668+00:00"
}