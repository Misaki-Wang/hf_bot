{
  "date": "2026-02-02",
  "paper_id": "2601.21709",
  "title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis",
  "authors": [
    "Qingyue Yang",
    "Jie Wang",
    "Xing Li",
    "Yinqi Bai",
    "Xialiang Tong",
    "Huiling Zhen",
    "Jianye Hao",
    "Mingxuan Yuan",
    "Bin Li"
  ],
  "abstract": "Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity. Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis ( TAPPA ), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings ( RoPE ). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM- TAPPA .",
  "summary_en": "Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity.",
  "summary_zh": "时序注意力模式可预测性分析（TAPPA）通过从时间视角分析大语言模型的数学公式，基于查询自相似性区分可预测与不可预测的模式，为理解注意力模式提供了统一框架。",
  "hf_url": "https://huggingface.co/papers/2601.21709",
  "arxiv_url": "https://arxiv.org/abs/2601.21709",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21709",
  "github_url": "",
  "upvotes": 2,
  "fetched_at": "2026-02-19T05:34:45.050898+00:00"
}