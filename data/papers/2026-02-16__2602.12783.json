{
  "date": "2026-02-16",
  "paper_id": "2602.12783",
  "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise",
  "authors": [
    "Yuejie Li",
    "Ke Yang",
    "Yueying Hua",
    "Berlin Chen",
    "Jianhao Nie",
    "Yueping He",
    "Caixin Kang"
  ],
  "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.",
  "summary_en": "SQuTR is a robustness benchmark for spoken query retrieval that addresses limitations of existing datasets by aggregating 37,317 unique queries from six English and Chinese text retrieval datasets, synthesized using voice profiles from 200 real speakers and mixed with 17 categories of real-world environmental noise under controlled SNR levels. The benchmark provides a unified evaluation protocol for assessing both cascaded and end-to-end retrieval systems across conditions ranging from quiet to highly noisy environments. Experimental results demonstrate that retrieval performance degrades substantially as noise increases, with significant variation across different systems, and even large-scale models struggle under extreme noise, indicating that robustness remains a critical bottleneck in spoken query to text retrieval.",
  "summary_zh": "SQuTR 是一个面向口语查询检索的鲁棒性基准测试，它通过从六个英文和中文文本检索数据集中聚合 37,317 条独特查询，使用 200 位真实说话人的语音特征合成，并在受控 SNR 水平下混入 17 类真实环境噪声，解决了现有数据集的局限性。该基准提供了统一的评估协议，用于在从安静到高噪声环境的多种条件下评估级联（cascaded）和端到端（end-to-end）检索系统。实验结果表明，随着噪声增加，检索性能显著下降，不同系统间存在显著差异，且即使大规模模型在极端噪声下也表现不佳，这表明鲁棒性仍然是口语查询到文本检索中的关键瓶颈。",
  "hf_url": "https://huggingface.co/papers/2602.12783",
  "arxiv_url": "https://arxiv.org/abs/2602.12783",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12783",
  "github_url": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval",
  "upvotes": 134,
  "fetched_at": "2026-02-17T09:52:23.869416+00:00"
}