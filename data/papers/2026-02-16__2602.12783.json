{
  "date": "2026-02-16",
  "paper_id": "2602.12783",
  "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise",
  "authors": [
    "Yuejie Li",
    "Ke Yang",
    "Yueying Hua",
    "Berlin Chen",
    "Jianhao Nie",
    "Yueping He",
    "Caixin Kang"
  ],
  "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.",
  "summary_en": "Existing spoken query retrieval evaluation datasets are limited to simple queries under constrained noise conditions, inadequately assessing robustness under complex acoustic perturbations. We introduce SQuTR, a benchmark aggregating 37,317 queries from six English and Chinese text retrieval datasets, synthesized using 200 real speaker voice profiles and mixed with 17 categories of real-world noise at controlled SNR levels. Experimental results show that retrieval performance degrades substantially with increasing noise across both cascaded and end-to-end systems, with large-scale models struggling under extreme conditions, indicating robustness remains a critical bottleneck. SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis to advance research on robustness in spoken query to text retrieval.",
  "summary_zh": "现有的语音查询检索评估数据集局限于约束噪声条件下的简单查询，无法充分评估复杂声学扰动下的鲁棒性。我们提出了 SQuTR，该基准聚合了来自六个英文和中文文本检索数据集的 37,317 个查询，使用 200 个真实说话人语音特征合成，并与 17 类真实世界噪声在受控 SNR 水平下混合。实验结果表明，在级联和端到端系统中，检索性能随噪声增加而显著下降，大规模模型在极端条件下表现不佳，表明鲁棒性仍然是一个关键瓶颈。SQuTR 提供了一个可复现的测试平台，用于基准测试和诊断分析，以推进语音查询到文本检索鲁棒性的研究。",
  "hf_url": "https://huggingface.co/papers/2602.12783",
  "arxiv_url": "https://arxiv.org/abs/2602.12783",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12783",
  "github_url": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval",
  "upvotes": 134,
  "fetched_at": "2026-02-17T08:53:09.405234+00:00"
}