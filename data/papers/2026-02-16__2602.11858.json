{
  "date": "2026-02-16",
  "paper_id": "2602.11858",
  "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
  "authors": [
    "Lai Wei",
    "Liangbo He",
    "Jun Lan",
    "Lingzhong Dong",
    "Yutong Cai",
    "Siyuan Li",
    "Huijia Zhu",
    "Weiqiang Wang",
    "Linghe Kong",
    "Yue Wang",
    "Zhuosheng Zhang",
    "Weiran Huang"
  ],
  "abstract": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks. Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception , where decisive evidence is small and easily overwhelmed by global context. Recent \" Thinking-with-Images \" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation , which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench , a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents . We further discuss when \" Thinking-with-Images \" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
  "summary_en": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.",
  "summary_zh": "Region-to-Image Distillation 通过训练模型在推理过程中内部执行迭代缩放，使 MLLM 能够实现细粒度视觉感知，无需重复工具调用和视觉重新编码，同时在多个基准测试中保持高性能。",
  "hf_url": "https://huggingface.co/papers/2602.11858",
  "arxiv_url": "https://arxiv.org/abs/2602.11858",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11858",
  "github_url": "https://github.com/inclusionAI/Zooming-without-Zooming",
  "upvotes": 51,
  "fetched_at": "2026-02-17T08:52:52.455693+00:00"
}