{
  "date": "2026-02-16",
  "paper_id": "2602.12506",
  "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
  "authors": [
    "Rosie Zhao",
    "Anshul Shah",
    "Xiaoyu Zhu",
    "Xinke Deng",
    "Zhongyu Jiang",
    "Yang Yang",
    "Joerg Liebelt",
    "Arnab Mondal"
  ],
  "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
  "summary_en": "RL fine-tuning improves VLMs on visual reasoning benchmarks but leaves them vulnerable to weak visual grounding and hallucinations, where simple textual perturbations cause robustness drops that entropy-based metrics trace to reshaped uncertainty and probability mass, particularly when CoT consistency is evaluated. Analysis of RL dynamics reveals an accuracy-faithfulness trade-off: fine-tuning boosts accuracy while eroding CoT reliability and robustness to contextual shifts, and although adversarial augmentation improves robustness, it does not prevent faithfulness drift. Faithfulness-aware rewards restore alignment between answers and reasoning but risk training collapse when paired with augmentation, leaving robustness elusive. These findings highlight the limitations of accuracy-only evaluations and motivate protocols that jointly emphasize correctness, robustness, and faithfulness in visually grounded reasoning.",
  "summary_zh": "RL微调提升了VLM在视觉推理基准上的表现，但使其易受视觉接地薄弱和幻觉的影响，其中简单的文本扰动会导致鲁棒性下降，而基于熵的指标将这种下降追溯至不确定性和概率质量的重塑，特别是在评估CoT一致性时。对RL动态的分析揭示了准确性与忠实度之间的权衡：微调提升了准确性，同时侵蚀了CoT的可靠性以及对上下文偏移的鲁棒性；尽管对抗增强提升了鲁棒性，但无法阻止忠实度漂移。忠实度感知奖励恢复了答案与推理之间的对齐，但在与增强方法结合时存在训练崩溃的风险，使得鲁棒性难以实现。这些发现凸显了仅关注准确性评估的局限性，并推动建立能够在视觉接地推理中联合强调正确性、鲁棒性和忠实度的评估协议。",
  "hf_url": "https://huggingface.co/papers/2602.12506",
  "arxiv_url": "https://arxiv.org/abs/2602.12506",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12506",
  "github_url": "",
  "upvotes": 3,
  "fetched_at": "2026-02-17T09:52:17.485935+00:00"
}