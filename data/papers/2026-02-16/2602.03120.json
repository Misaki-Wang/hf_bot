{
  "date": "2026-02-16",
  "paper_id": "2602.03120",
  "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
  "authors": [
    "Yinggan Xu",
    "Risto Miikkulainen",
    "Xin Qiu"
  ],
  "abstract": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .",
  "summary_en": "Post-Training Quantization (PTQ) enables deploying Large Language Models (LLMs) on memory-constrained devices but renders models static and incompatible with standard fine-tuning methods that require backpropagation through discrete, non-differentiable parameter spaces. This paper introduces Quantized Evolution Strategies (QES), which performs full-parameter fine-tuning directly in quantized space using accumulated error feedback to preserve high-precision gradient signals and stateless seed replay to reduce memory usage. QES significantly outperforms state-of-the-art zeroth-order methods on arithmetic reasoning tasks, enabling direct fine-tuning of quantized models and opening possibilities for scaling LLMs entirely in quantized space.",
  "summary_zh": "后训练量化（PTQ）使得大型语言模型（LLMs）能够部署于内存受限设备，但会使模型静态化，并与需要通过离散、不可微参数空间进行反向传播的标准微调方法不兼容。本文提出量化进化策略（QES），该方法直接在量化空间中进行全参数微调，利用累积误差反馈保留高精度梯度信号，并使用无状态种子重放减少内存使用。QES在算术推理任务上显著优于最先进的zeroth-order方法，实现了对量化模型的直接微调，并为完全在量化空间中扩展LLMs开辟了可能性。",
  "hf_url": "https://huggingface.co/papers/2602.03120",
  "arxiv_url": "https://arxiv.org/abs/2602.03120",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03120",
  "github_url": "https://github.com/dibbla/Quantized-Evolution-Strategies",
  "upvotes": 1,
  "fetched_at": "2026-02-17T09:51:55.095923+00:00"
}