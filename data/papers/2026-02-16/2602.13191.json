{
  "date": "2026-02-16",
  "paper_id": "2602.13191",
  "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
  "authors": [
    "Sayan Deb Sarkar",
    "Rémi Pautrat",
    "Ondrej Miksik",
    "Marc Pollefeys",
    "Iro Armeni",
    "Mahdi Rad",
    "Mihai Dusmanu"
  ],
  "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
  "summary_en": "Current Video Language Models (VideoLMs) rely on keyframe sampling that misses temporal details and incurs high computational costs from full-image processing. We propose leveraging video codec primitives (motion vectors and residuals) to encode sparsity without full-image encoding, using lightweight transformer encoders that aggregate these primitives and align them with image embeddings through pre-training. This approach reduces time-to-first-token by up to 86% and token usage by up to 93% while maintaining or exceeding performance on 14 video understanding benchmarks spanning question answering, temporal reasoning, and spatial understanding.",
  "summary_zh": "现有的视频语言模型（VideoLMs）依赖关键帧采样，这会丢失时序细节，并因全图处理而产生高昂的计算成本。我们提出利用视频编解码原语（运动向量和残差）来编码稀疏性，无需完整图像编码，并使用轻量级 transformer 编码器聚合这些原语，通过预训练将其与图像嵌入对齐。该方法将 time-to-first-token 降低多达 86%，token 使用量减少多达 93%，同时在涵盖问答、时序推理和空间理解的 14 个视频理解基准测试上保持或超越性能。",
  "hf_url": "https://huggingface.co/papers/2602.13191",
  "arxiv_url": "https://arxiv.org/abs/2602.13191",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13191",
  "github_url": "",
  "upvotes": 25,
  "fetched_at": "2026-02-17T09:52:30.980694+00:00"
}