{
  "date": "2026-02-16",
  "paper_id": "2602.12395",
  "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
  "authors": [
    "Xirui Li",
    "Ming Li",
    "Tianyi Zhou"
  ],
  "abstract": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
  "summary_en": "While reinforcement learning (RL) with verifiable rewards is widely used to enhance visual reasoning in vision-language models, end-to-end benchmarks obscure what specific capabilities improve beyond supervised fine-tuning initialization. To disentangle these effects, the authors propose a Frankenstein-style framework combining causal probing, parameter comparison, and model merging to analyze functional localization, update characterization, and transferability. Their findings demonstrate that RL induces consistent inference-time shifts primarily in mid-to-late transformer layers, with these refinements proving both transferable via merging and necessary via freezing for RL gains. These results suggest RL systematically refines mid-to-late computation to improve vision-to-reasoning alignment rather than uniformly enhancing visual perception, highlighting limitations of benchmark-only evaluation for assessing multimodal reasoning.",
  "summary_zh": "虽然基于可验证奖励的强化学习（RL）被广泛用于增强视觉语言模型中的视觉推理能力，但端到端基准测试掩盖了相较于监督微调（SFT）初始化具体哪些能力得到了提升。为了解耦这些效应，作者提出了一种 Frankenstein-style 框架，结合因果探测（causal probing）、参数比较和模型合并（model merging），以分析功能定位（functional localization）、更新特征刻画（update characterization）和可迁移性（transferability）。他们的研究发现，RL 主要在中后层 Transformer 层引发一致的推理时偏移（inference-time shifts），这些优化既可通过 merging 实现迁移，也可通过 freezing 验证其对 RL 收益的必要性。这些结果表明，RL 系统地优化中后层计算以改进 vision-to-reasoning alignment，而非均匀地增强视觉感知，凸显了仅依赖基准测试评估多模态推理的局限性。",
  "hf_url": "https://huggingface.co/papers/2602.12395",
  "arxiv_url": "https://arxiv.org/abs/2602.12395",
  "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12395",
  "github_url": "https://github.com/tianyi-lab/Frankenstein",
  "upvotes": 13,
  "fetched_at": "2026-02-17T09:52:15.581155+00:00"
}