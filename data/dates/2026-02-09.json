{
  "date": "2026-02-09",
  "count": 43,
  "papers": [
    {
      "date": "2026-02-09",
      "paper_id": "2602.06717",
      "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare",
      "authors": [
        "Daniil Plyusov",
        "Alexey Gorbatovski",
        "Boris Shaposhnikov",
        "Viacheslav Sinii",
        "Alexey Malakhov",
        "Daniil Gavrilov"
      ],
      "abstract": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost. Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates . In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss , that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO , DAPO , and CISPO . On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 ( GRPO ), 69.3 rightarrow 72.5 ( DAPO ), and 73.2 rightarrow 76.8 ( CISPO ), while preserving or improving pass@1, without increasing group size or computational cost.",
      "summary_en": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.",
      "summary_zh": "使用组采样的RLVR方法存在偏向高概率轨迹而遗漏稀有正确轨迹的问题；难度感知优势缩放技术能够在不增加计算成本的前提下提升基准测试性能。",
      "hf_url": "https://huggingface.co/papers/2602.06717",
      "arxiv_url": "https://arxiv.org/abs/2602.06717",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06717",
      "github_url": "",
      "upvotes": 71,
      "fetched_at": "2026-02-19T05:58:16.710161+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06570",
      "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
      "authors": [
        "Baichuan-M3 Team",
        "Chengfeng Dou",
        "Fan Yang",
        "Fei Li",
        "Jiyuan Jia",
        "Qiang Ju",
        "Shuai Wang",
        "Tianpeng Li",
        "Xiangrong Zeng",
        "Yijie Zhou",
        "Hongda Zhang",
        "Jinyang Tai",
        "Linzhuang Sun",
        "Peidong Guo",
        "Yichuan Mo",
        "Xiaochuan Wang",
        "Hengfu Cui",
        "Zhishou Zhang"
      ],
      "abstract": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression. We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench , the newly introduced HealthBench-Hallu and ScanBench , significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.",
      "summary_en": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.",
      "summary_zh": "Baichuan-M3是一款面向临床决策支持的医学增强大语言模型，具备主动信息收集、长程推理和幻觉抑制能力。",
      "hf_url": "https://huggingface.co/papers/2602.06570",
      "arxiv_url": "https://arxiv.org/abs/2602.06570",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06570",
      "github_url": "https://github.com/baichuan-inc/Baichuan-M3-235B",
      "upvotes": 59,
      "fetched_at": "2026-02-19T05:58:09.945123+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05027",
      "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
      "authors": [
        "Georgii Aparin",
        "Tasnima Sadekova",
        "Alexey Rukhovich",
        "Assel Yermekova",
        "Laida Kushnareva",
        "Vadim Popov",
        "Kristian Kuznetsov",
        "Irina Piontkovskaya"
      ],
      "abstract": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity. Sparse Autoencoders (SAEs) are po wer ful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT , provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whisper ing) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper 's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception , indicating alignment with human neural processing. The code and checkpoints are available at https://github.com/audiosae/audiosae_demo.",
      "summary_en": "Sparse Autoencoders trained on Whisper and HuBERT models demonstrate stable feature extraction and effective disentanglement of acoustic and semantic information, showing practical applications in audio processing and correlation with human neural activity.",
      "summary_zh": "在 Whisper 和 HuBERT 模型上训练的稀疏自编码器展现出稳定的特征提取能力，有效解耦声学信息与语义信息，在音频处理中具有实际应用价值，并与人类神经活动存在相关性。",
      "hf_url": "https://huggingface.co/papers/2602.05027",
      "arxiv_url": "https://arxiv.org/abs/2602.05027",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05027",
      "github_url": "https://github.com/audiosae/audiosae_demo",
      "upvotes": 59,
      "fetched_at": "2026-02-19T05:43:58.197237+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05843",
      "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
      "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Qiushi Sun",
        "Jinyang Wu",
        "Zixian Huang",
        "Muye Huang",
        "Jingyang Gong",
        "Zichen Ding",
        "Kanzhi Cheng",
        "Yian Wang",
        "Xinyu Che",
        "Zeyi Sun",
        "Jian Zhang",
        "Zhangyue Yin",
        "Haoran Luo",
        "Xuanjing Huang",
        "Ben Kao",
        "Jun Liu",
        "Qika Lin"
      ],
      "abstract": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws. The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena , which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
      "summary_en": "OdysseyArena presents a new framework for evaluating large language models on long-horizon, inductive agent tasks that emphasize autonomous discovery of environmental transition laws.",
      "summary_zh": "OdysseyArena提出了一种新框架，用于评估大语言模型在长程归纳式智能体任务上的表现，这类任务强调对环境转移规律的自主发现。",
      "hf_url": "https://huggingface.co/papers/2602.05843",
      "arxiv_url": "https://arxiv.org/abs/2602.05843",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05843",
      "github_url": "https://github.com/xufangzhi/Odyssey-Arena",
      "upvotes": 57,
      "fetched_at": "2026-02-19T05:44:04.128535+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03392",
      "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models",
      "authors": [
        "Shumin Wang",
        "Yuexiang Xie",
        "Wenhao Zhang",
        "Yuchang Sun",
        "Yanxi Chen",
        "Yaliang Li",
        "Yanyong Zhang"
      ],
      "abstract": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis. Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning ( RFT ), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update . This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization ( GRPO ). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy -based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.",
      "summary_en": "The paper establishes a theoretical framework for analyzing entropy dynamics in reinforcement fine-tuning of large language models, deriving expressions for entropy change and proposing entropy control methods based on discriminant analysis.",
      "summary_zh": "本文建立了大语言模型强化微调中熵动态分析的理论框架，推导了熵变化的表达式，并基于判别分析提出了熵控制方法。",
      "hf_url": "https://huggingface.co/papers/2602.03392",
      "arxiv_url": "https://arxiv.org/abs/2602.03392",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03392",
      "github_url": "https://github.com/agentscope-ai/Trinity-RFT",
      "upvotes": 53,
      "fetched_at": "2026-02-19T05:43:48.268160+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2601.18415",
      "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
      "authors": [
        "Ivan Bondarenko",
        "Daniil Grebenkin",
        "Oleg Sedukhin",
        "Mikhail Klementev",
        "Roman Derunets",
        "Lyudmila Budneva"
      ],
      "abstract": "A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian speech recognition. This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2 , false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper . The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality . The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to Whisper X and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
      "summary_en": "A three-component speech-to-text system combines Wav2Vec2, AST, and Whisper models with curriculum learning and uncertainty modeling to improve transcription accuracy and reduce hallucinations in Russian speech recognition.",
      "summary_zh": "一种三组件语音转文本系统结合Wav2Vec2、AST和Whisper模型，通过课程学习与不确定性建模，提升俄语语音识别的转录准确率并减少幻觉。",
      "hf_url": "https://huggingface.co/papers/2601.18415",
      "arxiv_url": "https://arxiv.org/abs/2601.18415",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.18415",
      "github_url": "https://github.com/bond005/pisets",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:43:39.785703+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.01734",
      "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration",
      "authors": [
        "Lianhai Ren",
        "Yucheng Ding",
        "Xiao Liu",
        "Qianxiao Li",
        "Peng Cheng",
        "Yeyun Gong"
      ],
      "abstract": "Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions. Training instability remains a critical challenge in large language model (LLM) pretraining , often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via μP, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm ), and (2) increasing alignment between adjacent layer Jacobian s. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.",
      "summary_en": "Training instability in large language models is linked to weight matrix stable rank decline and Jacobian alignment, which MSign addresses through matrix sign operations to prevent gradient explosions.",
      "summary_zh": "大语言模型的训练不稳定性与权重矩阵稳定秩下降及 Jacobian 对齐相关，MSign 通过矩阵符号运算解决该问题以防止梯度爆炸。",
      "hf_url": "https://huggingface.co/papers/2602.01734",
      "arxiv_url": "https://arxiv.org/abs/2602.01734",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01734",
      "github_url": "",
      "upvotes": 32,
      "fetched_at": "2026-02-19T05:43:44.153517+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06949",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "authors": [
        "Shenyuan Gao",
        "William Liang",
        "Kaiyuan Zheng",
        "Ayaan Malik",
        "Seonghyeon Ye",
        "Sihyun Yu",
        "Wei-Cheng Tseng",
        "Yuzhu Dong",
        "Kaichun Mo",
        "Chen-Hsuan Lin",
        "Qianli Ma",
        "Seungjun Nah",
        "Loic Magne",
        "Jiannan Xiang",
        "Yuqi Xie",
        "Ruijie Zheng",
        "Dantong Niu",
        "You Liang Tan",
        "K. R. Zentner",
        "George Kurian",
        "Suneel Indupuru",
        "Pooya Jannaty"
      ],
      "abstract": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation. Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels . As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels , we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world model s, including live teleoperation , policy evaluation , and model-based planning . Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world model s.",
      "summary_en": "DreamDojo is a foundation world model trained on 44k hours of egocentric human videos that enables efficient simulation of dexterous robotic tasks through continuous latent actions and real-time distillation.",
      "summary_zh": "DreamDojo是一种在44k小时第一人称人类视频上训练的基础世界模型，通过连续潜在动作和实时蒸馏，实现灵巧机器人任务的高效模拟。",
      "hf_url": "https://huggingface.co/papers/2602.06949",
      "arxiv_url": "https://arxiv.org/abs/2602.06949",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06949",
      "github_url": "",
      "upvotes": 30,
      "fetched_at": "2026-02-19T05:58:28.023980+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06130",
      "title": "Self-Improving World Modelling with Latent Actions",
      "authors": [
        "Yifu Qiu",
        "Zheng Zhao",
        "Waylon Li",
        "Yftah Ziser",
        "Anna Korhonen",
        "Shay B. Cohen",
        "Edoardo M. Ponti"
      ],
      "abstract": "SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks. Internal modelling of the world -- predicting transitions between previous states X and next states Y under actions Z -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) P_θ(Y|X,Z) and an Inverse Dynamics Modelling (IDM) Q_φ(Z|X,Y). SWIRL iterates two phases: (1) Variational Information Maximisation , which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation , which updates the IDM to explain observed transitions, effectively performing coordinate ascent . Both models are trained with reinforcement learning (specifically, GRPO ) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.",
      "summary_en": "SWIRL is a self-improvement framework that learns world models from state-only sequences by alternating between forward and inverse dynamics modeling with variational information maximization and ELBO maximization, achieving improved performance on various reasoning and planning benchmarks.",
      "summary_zh": "SWIRL是一种自改进框架，通过在前向与逆向动力学建模之间交替，并采用变分信息最大化与ELBO最大化，从仅状态序列中学习世界模型，在多种推理与规划基准测试中取得了性能提升。",
      "hf_url": "https://huggingface.co/papers/2602.06130",
      "arxiv_url": "https://arxiv.org/abs/2602.06130",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06130",
      "github_url": "",
      "upvotes": 29,
      "fetched_at": "2026-02-19T05:44:13.066392+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06291",
      "title": "Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math",
      "authors": [
        "Guijin Son",
        "Donghun Yang",
        "Hitesh Laxmichand Patel",
        "Hyunwoo Ko",
        "Amit Agarwal",
        "Sunghee Ahn",
        "Kyong-Ha Lee",
        "Youngjae Yu"
      ],
      "abstract": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation. Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose Consequence-Based Utility, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models , generative reward models , and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap , maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.",
      "summary_en": "Consequence-Based Utility evaluates mathematical solutions by testing their effectiveness as exemplars for related problems, outperforming reward models and LLM judges in ranking quality and correct-wrong separation.",
      "summary_zh": "基于后果的效用通过测试数学解决方案作为相关问题范例的有效性来评估这些方案，在排序质量和对错区分方面优于奖励模型和LLM评判器。",
      "hf_url": "https://huggingface.co/papers/2602.06291",
      "arxiv_url": "https://arxiv.org/abs/2602.06291",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06291",
      "github_url": "",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:44:18.793795+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06079",
      "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers",
      "authors": [
        "Liangyu Wang",
        "Siqi Zhang",
        "Junjie Wang",
        "Yiming Dong",
        "Bo Zheng",
        "Zihan Qiu",
        "Shengkun Tang",
        "Di Wang",
        "Rui Men",
        "Dayiheng Liu"
      ],
      "abstract": "Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.",
      "summary_en": "Canzona presents a unified asynchronous framework that addresses the conflict between matrix-based optimizers and distributed tensor fragmentation in LLM training, improving efficiency and reducing latency.",
      "summary_zh": "Canzona提出了一种统一的异步框架，解决了基于矩阵的优化器与分布式张量碎片化在LLM训练中的冲突，提升了效率并降低了延迟。",
      "hf_url": "https://huggingface.co/papers/2602.06079",
      "arxiv_url": "https://arxiv.org/abs/2602.06079",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06079",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:44:09.888939+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05940",
      "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
      "authors": [
        "Junxiao Liu",
        "Zhijun Wang",
        "Yixiao Li",
        "Zhejian Lai",
        "Liqian Huang",
        "Xin Huang",
        "Xue Han",
        "Junlan Feng",
        "Shujian Huang"
      ],
      "abstract": "TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages. Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning . To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning . Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH , our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200 .",
      "summary_en": "TRIT framework improves multilingual reasoning by jointly training translation and reasoning components, enhancing question understanding and response generation across languages.",
      "summary_zh": "TRIT框架通过联合训练翻译和推理组件，提升多语言推理能力，并增强跨语言的问题理解与回答生成。",
      "hf_url": "https://huggingface.co/papers/2602.05940",
      "arxiv_url": "https://arxiv.org/abs/2602.05940",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05940",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:44:07.148907+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06391",
      "title": "POINTS-GUI-G: GUI-Grounding Journey",
      "authors": [
        "Zhongyin Zhao",
        "Yuan Liu",
        "Yikun Liu",
        "Haicheng Wang",
        "Le Tian",
        "Xiao Zhou",
        "Yangxiu You",
        "Zilin Yu",
        "Yang Yu",
        "Jie Zhou"
      ],
      "abstract": "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.",
      "summary_en": "GUI agents for automated digital tasks rely on vision-language models with enhanced grounding capabilities, achieved through refined data engineering, improved training strategies, and reinforcement learning with verifiable rewards.",
      "summary_zh": "用于自动化数字任务的GUI智能体依赖具备增强grounding能力的视觉语言模型，这些能力通过精细化的数据工程、改进的训练策略以及可验证奖励的强化学习来实现。",
      "hf_url": "https://huggingface.co/papers/2602.06391",
      "arxiv_url": "https://arxiv.org/abs/2602.06391",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06391",
      "github_url": "https://github.com/Tencent/POINTS-GUI",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:44:20.305138+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05281",
      "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
      "authors": [
        "Pengyi Li",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov",
        "Ivan Oseledets"
      ],
      "abstract": "A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation , our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks . Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse . Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32 , highlighting its superior capability in generating diverse correct reasoning paths.",
      "summary_en": "A novel reinforcement learning approach called ARM addresses entropy collapse in LLM reasoning by equilibrating confidence levels across correct responses through dynamic reward shaping.",
      "summary_zh": "一种名为ARM的新型强化学习方法通过动态奖励塑造均衡各正确回答的置信度水平，以解决LLM推理中的熵坍缩问题。",
      "hf_url": "https://huggingface.co/papers/2602.05281",
      "arxiv_url": "https://arxiv.org/abs/2602.05281",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05281",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:43:59.679234+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06075",
      "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments",
      "authors": [
        "Guangyi Liu",
        "Pengxiang Zhao",
        "Yaozhen Liang",
        "Qinyi Luo",
        "Shunye Tang",
        "Yuxiang Chai",
        "Weifeng Lin",
        "Han Xiao",
        "WenHao Wang",
        "Siheng Chen",
        "Zhengxi Lu",
        "Gao Wu",
        "Hao Wang",
        "Liang Liu",
        "Yong Liu"
      ],
      "abstract": "A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights. Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation . Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention ; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics ; and (4) RQ-driven assessment of 11 state-of-the-art agents . Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes , and synthesize 5 actionable design implications . All resources including code, benchmark, and evaluation results will be \\textit{fully open-sourced and continuously maintained} at https://lgy0404.github.io/MemGUI-Bench/.",
      "summary_en": "A comprehensive memory-focused benchmark for mobile GUI agents reveals significant memory capability gaps and provides systematic evaluation methods and design insights.",
      "summary_zh": "一项全面的面向移动GUI智能体的记忆基准测试揭示了显著的记忆能力差距，并提供了系统性评估方法和设计见解。",
      "hf_url": "https://huggingface.co/papers/2602.06075",
      "arxiv_url": "https://arxiv.org/abs/2602.06075",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06075",
      "github_url": "https://github.com/lgy0404/MemGUI-Bench",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:44:08.554813+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06960",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "authors": [
        "Yuchen Yan",
        "Liang Jiang",
        "Jin Jiang",
        "Shuaicheng Li",
        "Zujie Wen",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Jian Shao",
        "Yueting Zhuang",
        "Yongliang Shen"
      ],
      "abstract": "InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models. Large reasoning models achieve strong performance by scaling inference-time chain-of-thought , but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization . InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning , enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "summary_en": "InftyThink+ uses reinforcement learning to optimize iterative reasoning processes, improving accuracy and efficiency in large language models.",
      "summary_zh": "InftyThink+采用强化学习优化迭代推理过程，提升大语言模型的准确性和效率。",
      "hf_url": "https://huggingface.co/papers/2602.06960",
      "arxiv_url": "https://arxiv.org/abs/2602.06960",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06960",
      "github_url": "https://github.com/ZJU-REAL/InftyThink-Plus",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:58:29.908523+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06139",
      "title": "EgoAVU: Egocentric Audio-Visual Understanding",
      "authors": [
        "Ashish Seth",
        "Xinhao Mei",
        "Changsheng Zhao",
        "Varun Nagaraja",
        "Ernie Chang",
        "Gregory P. Meyer",
        "Gael Le Lan",
        "Yunyang Xiong",
        "Vikas Chandra",
        "Yangyang Shi",
        "Dinesh Manocha",
        "Zhipeng Cai"
      ],
      "abstract": "Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning. Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations , questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling . Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct , a large-scale training dataset of 3M samples, and EgoAVU-Bench , a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench . Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion , achieving up to 28% relative performance gain. Code will be released to the community.",
      "summary_en": "Multi-modal large language models struggle to jointly understand audio and visual signals in egocentric videos, but a new scalable data engine and dataset significantly improve their performance through targeted fine-tuning.",
      "summary_zh": "多模态大语言模型难以联合理解第一人称视角视频中的音频和视觉信号，但一种新的可扩展数据引擎和数据集通过针对性微调显著提升了其性能。",
      "hf_url": "https://huggingface.co/papers/2602.06139",
      "arxiv_url": "https://arxiv.org/abs/2602.06139",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06139",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:44:14.728820+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05847",
      "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
      "authors": [
        "Zhangquan Chen",
        "Jiale Tao",
        "Ruihuang Li",
        "Yihao Hu",
        "Ruitao Chen",
        "Zhantao Yang",
        "Xinlei Yu",
        "Haodong Jing",
        "Manyuan Zhang",
        "Shuai Shao",
        "Biao Wang",
        "Qinglin Lu",
        "Ruqi Huang"
      ],
      "abstract": "OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning. While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning . OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
      "summary_en": "OmniVideo-R1 enhances audio-visual understanding through reinforced frameworks that integrate self-supervised and contrastive learning for multimodal reasoning.",
      "summary_zh": "OmniVideo-R1通过融合自监督与对比学习的强化框架，增强音视频理解以实现多模态推理。",
      "hf_url": "https://huggingface.co/papers/2602.05847",
      "arxiv_url": "https://arxiv.org/abs/2602.05847",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05847",
      "github_url": "",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:44:05.557235+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04649",
      "title": "Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models",
      "authors": [
        "Binghai Wang",
        "Yantao Liu",
        "Yuxuan Liu",
        "Tianyi Tang",
        "Shenzhi Wang",
        "Chang Gao",
        "Chujie Zheng",
        "Yichang Zhang",
        "Le Yu",
        "Shixuan Liu",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Bowen Yu",
        "Fei Huang",
        "Junyang Lin"
      ],
      "abstract": "Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF. Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy , which undermines their ability to generalize during RLHF . We introduce Rationale Consistency , a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment , while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF , our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks . Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.",
      "summary_en": "Generative Reward Models suffer from deceptive alignment due to outcome accuracy prioritization, but rationale consistency metrics and hybrid training signals improve performance and generalization in RLHF.",
      "summary_zh": "生成式奖励模型因优先优化结果准确性而遭受欺骗性对齐，但理由一致性指标与混合训练信号可改善RLHF中的性能与泛化能力。",
      "hf_url": "https://huggingface.co/papers/2602.04649",
      "arxiv_url": "https://arxiv.org/abs/2602.04649",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04649",
      "github_url": "https://github.com/QwenLM/RationaleRM",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:43:54.074362+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06176",
      "title": "Large Language Model Reasoning Failures",
      "authors": [
        "Peiyang Song",
        "Pengrui Han",
        "Noah Goodman"
      ],
      "abstract": "Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies. Large Language Models (LLMs) have exhibited remarkable reasoning capabilities , achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities . We additionally release a comprehensive collection of research works on LLM reasoning failures , as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.",
      "summary_en": "Large language models exhibit significant reasoning failures that can be categorized into embodied and non-embodied types, with fundamental, application-specific, and robustness-related subtypes, requiring systematic analysis and mitigation strategies.",
      "summary_zh": "大语言模型表现出显著的推理失败，可分为具身与非具身类型，并包含基础型、应用特定型及鲁棒性相关型等子类型，需要系统性的分析与缓解策略。",
      "hf_url": "https://huggingface.co/papers/2602.06176",
      "arxiv_url": "https://arxiv.org/abs/2602.06176",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06176",
      "github_url": "https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures",
      "upvotes": 11,
      "fetched_at": "2026-02-19T05:44:15.943778+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05711",
      "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
      "authors": [
        "Jingze Shi",
        "Zhangyang Peng",
        "Yizhang Zhu",
        "Yifan Wu",
        "Guang Liu",
        "Yuyu Luo"
      ],
      "abstract": "OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms. Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts , enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access . To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
      "summary_en": "OmniMoE presents a system-algorithm co-designed framework that achieves fine-grained expert specialization in Mixture-of-Experts architectures through vector-level atomic experts and optimized routing and scheduling mechanisms.",
      "summary_zh": "OmniMoE提出了一种系统-算法协同设计框架，通过向量级原子专家以及优化的路由与调度机制，在混合专家架构中实现了细粒度的专家特化。",
      "hf_url": "https://huggingface.co/papers/2602.05711",
      "arxiv_url": "https://arxiv.org/abs/2602.05711",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05711",
      "github_url": "https://github.com/flash-algo/omni-moe",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:44:02.652511+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.02581",
      "title": "QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals",
      "authors": [
        "Nan Zhang",
        "Eugene Kwek",
        "Yusen Zhang",
        "Muyu Pan",
        "Suhang Wang",
        "Prasenjit Mitra",
        "Rui Zhang"
      ],
      "abstract": "QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation. Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning , we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term \"protecting both ends\". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning ) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning , which greatly enhances its applicability.",
      "summary_en": "QuantLRM uses weight update magnitude signals from fine-tuning to improve quantization of Large Reasoning Models, achieving better performance than traditional methods through channel importance estimation.",
      "summary_zh": "QuantLRM利用微调过程中的权重更新幅度信号来改进大型推理模型的量化，通过通道重要性估计实现了优于传统方法的性能。",
      "hf_url": "https://huggingface.co/papers/2602.02581",
      "arxiv_url": "https://arxiv.org/abs/2602.02581",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02581",
      "github_url": "https://github.com/psunlpgroup/QuantLRM",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:43:45.416376+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04837",
      "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
      "authors": [
        "Zhaotian Weng",
        "Antonis Antoniades",
        "Deepak Nathani",
        "Zhen Zhang",
        "Xiao Pu",
        "Xin Eric Wang"
      ],
      "abstract": "Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness. Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit , enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks , where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified , 88.3% vs. 68.3% on Polyglot ) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods .",
      "summary_en": "Group-Evolving Agents enable open-ended self-improvement by treating groups of agents as evolutionary units, allowing efficient experience sharing and reuse to enhance coding performance and robustness.",
      "summary_zh": "群体进化智能体将智能体群体视为进化单元，通过高效的经验共享与复用提升代码性能与鲁棒性，实现开放式自我改进。",
      "hf_url": "https://huggingface.co/papers/2602.04837",
      "arxiv_url": "https://arxiv.org/abs/2602.04837",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04837",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:43:56.641430+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06669",
      "title": "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data",
      "authors": [
        "Lucie Termignon",
        "Simonas Zilinskas",
        "Hadrien Pélissier",
        "Aurélien Barrot",
        "Nicolas Chesnais",
        "Elie Gavoty"
      ],
      "abstract": "Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.",
      "summary_en": "Compar:IA is an open-source platform that collects large-scale human preference data for multilingual language model training and evaluation, featuring a blind pairwise comparison interface and releasing three datasets under open licenses.",
      "summary_zh": "Compar:IA 是一个开源平台，收集用于多语言语言模型训练与评估的大规模人类偏好数据，具备盲测成对比较界面，并以开放许可发布三个数据集。",
      "hf_url": "https://huggingface.co/papers/2602.06669",
      "arxiv_url": "https://arxiv.org/abs/2602.06669",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06669",
      "github_url": "https://github.com/betagouv/ComparIA",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:58:14.493864+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.05367",
      "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
      "authors": [
        "Youngcheon You",
        "Banseok Lee",
        "Minseop Choi",
        "Seonyoung Kim",
        "Hyochan Chong",
        "Changdong Kim",
        "Youngmin Kim",
        "Dongkyu Kim"
      ],
      "abstract": "Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs. Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary (pm1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation : during quantization-aware training ( QAT ), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT , a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy . Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization ( VQ ) methods, and delivers a 4.49times inference speed-up over full-precision models on an RTX 4090 .",
      "summary_en": "Residual binarization framework RaBiT addresses feature co-adaptation in quantized LLMs through hierarchical path derivation and robust initialization, achieving superior accuracy-efficiency trade-offs.",
      "summary_zh": "残差二值化框架RaBiT通过分层路径推导与鲁棒初始化解决量化LLM中的特征协同适应问题，实现了更优的精度-效率权衡。",
      "hf_url": "https://huggingface.co/papers/2602.05367",
      "arxiv_url": "https://arxiv.org/abs/2602.05367",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05367",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:44:01.187355+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06869",
      "title": "Uncovering Cross-Objective Interference in Multi-Objective Alignment",
      "authors": [
        "Yining Lu",
        "Meng Jiang"
      ],
      "abstract": "Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals. We study a persistent failure mode in multi-objective alignment for large language models (LLMs): training improves performance on only a subset of objectives while causing others to degrade. We formalize this phenomenon as cross-objective interference and conduct the first systematic study across classic scalarization algorithms , showing that interference is pervasive and exhibits strong model dependence. To explain this phenomenon, we derive a local covariance law showing that an objective improves at first order when its reward exhibits positive covariance with the scalarized score. We extend this analysis to clipped surrogate objectives used in modern alignment, demonstrating that the covariance law remains valid under mild conditions despite clipping. Building on this analysis, we propose Covariance Targeted Weight Adaptation (CTWA), a plug-and-play method that maintains positive covariance between objective rewards and the training signal to effectively mitigate cross-objective interference . Finally, we complement these local improvement conditions with a global convergence analysis under the Polyak--Łojasiewicz condition , establishing when non-convex scalarized optimization achieves global convergence and how cross-objective interference depends on specific model geometric properties.",
      "summary_en": "Multi-objective alignment in LLMs suffers from cross-objective interference where improving performance on some objectives degrades others, with a covariance-based analysis and a proposed method to maintain positive correlations between rewards and training signals.",
      "summary_zh": "大语言模型的多目标对齐存在跨目标干扰，即提升某些目标性能会降低其他目标；该研究采用协方差分析，并提出一种保持奖励与训练信号正相关性的方法。",
      "hf_url": "https://huggingface.co/papers/2602.06869",
      "arxiv_url": "https://arxiv.org/abs/2602.06869",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06869",
      "github_url": "https://github.com/yining610/ctwa",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:58:23.230832+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06854",
      "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
      "authors": [
        "Mingqian Feng",
        "Xiaodong Liu",
        "Weiwei Yang",
        "Jialin Song",
        "Xuekai Zhu",
        "Chenliang Xu",
        "Jianfeng Gao"
      ],
      "abstract": "A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets. Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models , and jailbreak judges , our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT ( Supervised Fine-Tuning ) and DPO ( Direct Preference Optimization ) variants. For instance, SEMA performs an average 80.1% ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
      "summary_en": "A novel framework called SEMA is introduced that effectively trains multi-turn attackers for large language models without relying on existing strategies or external data, achieving state-of-the-art attack success rates while being compact, reproducible, and transferable across different models and datasets.",
      "summary_zh": "本文提出了一种名为 SEMA 的新框架，可在不依赖现有策略或外部数据的情况下有效训练面向大型语言模型的多轮攻击器，实现了最先进的攻击成功率，且具备紧凑、可复现、可跨不同模型和数据集迁移的特性。",
      "hf_url": "https://huggingface.co/papers/2602.06854",
      "arxiv_url": "https://arxiv.org/abs/2602.06854",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06854",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:58:20.732702+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03075",
      "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
      "authors": [
        "Junjie Huang",
        "Jiarui Qin",
        "Di Yin",
        "Weiwen Liu",
        "Yong Yu",
        "Xing Sun",
        "Weinan Zhang"
      ],
      "abstract": "ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop. Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training . However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training , utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT ( Reinforcement Learning -Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase , prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop , enabling continuous and self-reinforcing evolution of LLMs.",
      "summary_en": "ReMiT introduces a bidirectional training approach where reinforcement learning-guided mid-training token reweighting improves large language model pre-training and post-training performance through an iterative feedback loop.",
      "summary_zh": "ReMiT提出了一种双向训练方法，其中强化学习引导的训练中期token重加权通过迭代反馈循环提升大语言模型的预训练与后训练性能。",
      "hf_url": "https://huggingface.co/papers/2602.03075",
      "arxiv_url": "https://arxiv.org/abs/2602.03075",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03075",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:43:46.853806+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06663",
      "title": "PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks",
      "authors": [
        "Junxian Li",
        "Kai Liu",
        "Leyang Chen",
        "Weida Wang",
        "Zhixin Wang",
        "Jiaqi Xu",
        "Fan Li",
        "Renjing Pei",
        "Linghe Kong",
        "Yulun Zhang"
      ],
      "abstract": "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.",
      "summary_en": "PlanViz benchmark evaluates unified multimodal models' capabilities in computer-use planning tasks through route planning, work diagramming, and web&UI displaying sub-tasks with a task-adaptive scoring system.",
      "summary_zh": "PlanViz基准测试通过路径规划、工作图表绘制和网页与UI展示子任务，采用任务自适应评分系统，评估统一多模态模型在计算机使用规划任务中的能力。",
      "hf_url": "https://huggingface.co/papers/2602.06663",
      "arxiv_url": "https://arxiv.org/abs/2602.06663",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06663",
      "github_url": "https://github.com/lijunxian111/PlanViz",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:58:11.830421+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06554",
      "title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
      "authors": [
        "Tianyi Hu",
        "Qingxu Fu",
        "Yanxi Chen",
        "Zhaoyang Liu",
        "Bolin Ding"
      ],
      "abstract": "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates. Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies. In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/ multi-turn scenarios . We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO 's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios . To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems . Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction . Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.",
      "summary_en": "SeeUPO is a critic-free reinforcement learning method that ensures convergence guarantees in multi-turn agent interactions by modeling sequential decision-making as multi-agent bandit problems and using backward induction for policy updates.",
      "summary_zh": "SeeUPO是一种无critic的强化学习方法，通过将序列决策建模为多智能体bandit问题并使用逆向归纳进行策略更新，确保多轮智能体交互的收敛性保证。",
      "hf_url": "https://huggingface.co/papers/2602.06554",
      "arxiv_url": "https://arxiv.org/abs/2602.06554",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06554",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:44:22.936836+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06883",
      "title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
      "authors": [
        "Ambroise Odonnat",
        "Laetitia Chapel",
        "Romain Tavenard",
        "Ievgen Redko"
      ],
      "abstract": "Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial. The smoothness of the transformer architecture has been extensively studied in the context of generalization , training stability , and adversarial robustness . However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity . Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness . We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance . Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit- plasticity .",
      "summary_en": "Vision transformer components exhibit varying plasticity levels that correlate with finetuning performance, challenging the assumption that smoothness is always beneficial.",
      "summary_zh": "Vision Transformer组件表现出与微调性能相关的不同可塑性水平，挑战了平滑性总是有益的这一假设。",
      "hf_url": "https://huggingface.co/papers/2602.06883",
      "arxiv_url": "https://arxiv.org/abs/2602.06883",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06883",
      "github_url": "https://github.com/ambroiseodt/vit-plasticity",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:58:25.427610+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06471",
      "title": "Revisiting the Shape Convention of Transformer Language Models",
      "authors": [
        "Feng-Ting Liao",
        "Meng-Hsi Chen",
        "Guan-Ting Yi",
        "Da-shan Shiu"
      ],
      "abstract": "Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling. Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP , allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLP s offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer , challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub- MLP s connected by residual pathways . We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models.",
      "summary_en": "Replacing conventional feed-forward networks with hourglass-shaped MLPs in Transformers improves model efficiency and performance by enabling better parameter utilization and competitive scaling.",
      "summary_zh": "在Transformer中用沙漏形MLP替换传统前馈网络，通过实现更优的参数利用和具有竞争力的扩展性，提升了模型效率与性能。",
      "hf_url": "https://huggingface.co/papers/2602.06471",
      "arxiv_url": "https://arxiv.org/abs/2602.06471",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06471",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:44:21.655739+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03548",
      "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue",
      "authors": [
        "Yuqin Dai",
        "Ning Gao",
        "Wei Zhang",
        "Jie Wang",
        "Zichen Luo",
        "Jinpeng Wang",
        "Yujie Wang",
        "Ruiyuan Wu",
        "Chaozheng Wang"
      ],
      "abstract": "SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models. Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues , as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.",
      "summary_en": "SEAD framework enables service dialogue agents to learn effective strategies through self-evolving user modeling components, achieving superior task completion and dialogue efficiency compared to existing foundation and commercial models.",
      "summary_zh": "SEAD框架使服务对话智能体能够通过自进化用户建模组件学习有效策略，在任务完成和对话效率方面优于现有的基础模型和商业模型。",
      "hf_url": "https://huggingface.co/papers/2602.03548",
      "arxiv_url": "https://arxiv.org/abs/2602.03548",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03548",
      "github_url": "https://github.com/Da1yuqin/SEAD",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:43:49.821404+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06566",
      "title": "SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs",
      "authors": [
        "Niccolo Avogaro",
        "Nayanika Debnath",
        "Li Mi",
        "Thomas Frick",
        "Junling Wang",
        "Zexue He",
        "Hang Hua",
        "Konrad Schindler",
        "Mattia Rigotti"
      ],
      "abstract": "SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks. Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning , leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning . Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks , SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the V^* VQA benchmark by 6.7 percentage points, and it surpasses \"thinking with images\" by 4.6 points on a challenging OOD task despite requiring a 200times lower token budget .",
      "summary_en": "SPARC is a modular framework that decouples visual perception from reasoning in vision-language models, enabling efficient test-time scaling through targeted compute allocation and improved performance on visual reasoning tasks.",
      "summary_zh": "SPARC是一种模块化框架，它将视觉感知与推理在视觉-语言模型中解耦，通过针对性的计算分配实现高效的测试时扩展，并提升视觉推理任务的性能。",
      "hf_url": "https://huggingface.co/papers/2602.06566",
      "arxiv_url": "https://arxiv.org/abs/2602.06566",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06566",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:58:07.346370+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06964",
      "title": "Learning a Generative Meta-Model of LLM Activations",
      "authors": [
        "Grace Luo",
        "Jiahai Feng",
        "Trevor Darrell",
        "Alec Radford",
        "Jacob Steinhardt"
      ],
      "abstract": "Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions. Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity . We explore this direction by training diffusion models on one billion residual stream activations , creating \" meta-models \" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.",
      "summary_en": "Training diffusion models on neural network activations creates meta-models that learn internal state distributions and improve intervention fidelity without restrictive structural assumptions.",
      "summary_zh": "在神经网络激活上训练扩散模型可构建学习内部状态分布的元模型，无需限制性结构假设即可提升干预保真度。",
      "hf_url": "https://huggingface.co/papers/2602.06964",
      "arxiv_url": "https://arxiv.org/abs/2602.06964",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06964",
      "github_url": "https://github.com/g-luo/generative_latent_prior",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:58:32.272125+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06724",
      "title": "Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion",
      "authors": [
        "Tian Lan",
        "Felix Henry",
        "Bin Zhu",
        "Qianghuai Jia",
        "Junyang Ren",
        "Qihang Pu",
        "Haijun Li",
        "Longyue Wang",
        "Zhao Xu",
        "Weihua Luo"
      ],
      "abstract": "Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management. Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states , including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce Table-as-Search (TaS), a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states : filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search , Wide Search , and the challenging DeepWide Search . Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.",
      "summary_en": "Table-as-Search framework reformulates information seeking tasks as table completion problems, improving long-horizon search robustness through structured state management.",
      "summary_zh": "Table-as-Search框架将信息检索任务重新表述为表格补全问题，通过结构化状态管理提升长程搜索鲁棒性。",
      "hf_url": "https://huggingface.co/papers/2602.06724",
      "arxiv_url": "https://arxiv.org/abs/2602.06724",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06724",
      "github_url": "https://github.com/AIDC-AI/Marco-DeepResearch",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:58:18.882352+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04811",
      "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
      "authors": [
        "Jiarui Yuan",
        "Tailin Jin",
        "Weize Chen",
        "Zeyuan Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches. True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox , where training with reference documentation inhibits retention, requiring \" Closed-Book Training \" to force knowledge compression into weights; (2) the RL Gap , where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients ; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT , but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization . Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
      "summary_en": "SE-Bench presents a diagnostic environment that obscures NumPy's API to evaluate agents' ability to internally store and utilize novel knowledge without external documentation, revealing challenges in knowledge retention and internalization through different training approaches.",
      "summary_zh": "SE-Bench 构建了一个隐藏 NumPy API 的诊断环境，用于评估智能体在无外部文档时内部存储和利用新知识的能力，揭示了不同训练方法在知识保持与内化方面存在的挑战。",
      "hf_url": "https://huggingface.co/papers/2602.04811",
      "arxiv_url": "https://arxiv.org/abs/2602.04811",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04811",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:43:55.279809+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.01064",
      "title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs",
      "authors": [
        "Ruihan Jin",
        "Pengpeng Shao",
        "Zhengqi Wen",
        "Jinyang Wu",
        "Mingkuan Feng",
        "Shuo Yang",
        "Chu Yuan Zhang",
        "Jianhua Tao"
      ],
      "abstract": "Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes. Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models . In this paper, we introduce the concept of Knowledge Purification , which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification , we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts . Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.",
      "summary_en": "Knowledge purification techniques consolidate rationales from multiple teacher LLMs to reduce conflicts and improve efficiency in distillation processes.",
      "summary_zh": "知识净化技术整合来自多个教师LLM的推理依据，以减少蒸馏过程中的冲突并提高效率。",
      "hf_url": "https://huggingface.co/papers/2602.01064",
      "arxiv_url": "https://arxiv.org/abs/2602.01064",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01064",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:43:42.724279+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06181",
      "title": "Uncertainty Drives Social Bias Changes in Quantized Large Language Models",
      "authors": [
        "Stanley Z. Hua",
        "Sanae Lotfi",
        "Irene Y. Chen"
      ],
      "abstract": "Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels. Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on PostTrainingBiasBench, a unified benchmark of 13 closed- and open-ended bias datasets. We identify a phenomenon we term quantization-induced masked bias flipping , in which up to 21% of responses flip between biased and unbiased states after quantization, despite showing no change in aggregate bias scores. These flips are strongly driven by model uncertainty , where the responses with high uncertainty are 3-11x more likely to change than the confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups , where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes . Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, requiring crucial post-quantization evaluation and interventions to ensure reliability in practice.",
      "summary_en": "Post-training quantization of large language models causes significant changes in social biases that aggregate metrics fail to detect, with quantization-induced masked bias flipping occurring more frequently in uncertain responses and stronger quantization levels.",
      "summary_zh": "大语言模型的训练后量化会导致聚合指标无法检测到的社会偏见显著变化，且量化引发的掩蔽偏见翻转在不确定的回复和更强的量化级别中出现得更频繁。",
      "hf_url": "https://huggingface.co/papers/2602.06181",
      "arxiv_url": "https://arxiv.org/abs/2602.06181",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06181",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:44:17.227679+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.06129",
      "title": "Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction",
      "authors": [
        "Olaf Yunus Laitinen Imanov",
        "Derya Umut Kulali",
        "Taner Yilmaz"
      ],
      "abstract": "A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications. Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer ; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation ; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.",
      "summary_en": "A diffusion-transformer framework integrates spatio-temporal urban data to predict building-level climate risks while incorporating transportation network structures for emergency response applications.",
      "summary_zh": "一种diffusion-transformer框架整合时空城市数据以预测建筑级气候风险，同时融合交通网络结构以支持应急响应应用。",
      "hf_url": "https://huggingface.co/papers/2602.06129",
      "arxiv_url": "https://arxiv.org/abs/2602.06129",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06129",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:44:11.315087+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.04454",
      "title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search",
      "authors": [
        "Tianming Liang",
        "Qirui Du",
        "Jian-Fang Hu",
        "Haichao Jiang",
        "Zicheng Lin",
        "Wei-Shi Zheng"
      ],
      "abstract": "Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks. Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search , Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation . Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.",
      "summary_en": "Seg-ReSearch introduces a novel segmentation approach that combines interleaved reasoning with external search to overcome limitations of frozen MLLM knowledge, using hierarchical reward design for training and demonstrating superior performance on video object segmentation benchmarks.",
      "summary_zh": "Seg-ReSearch 提出了一种结合交错推理与外部搜索的新颖分割方法，以克服冻结 MLLM 知识的局限性，采用分层奖励设计进行训练，并在视频目标分割基准测试中展现出优越性能。",
      "hf_url": "https://huggingface.co/papers/2602.04454",
      "arxiv_url": "https://arxiv.org/abs/2602.04454",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04454",
      "github_url": "https://github.com/iSEE-Laboratory/Seg-ReSearch",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:43:52.748358+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2602.03998",
      "title": "AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology",
      "authors": [
        "Ahmed Alagha",
        "Christopher Leclerc",
        "Yousef Kotp",
        "Omar Metwally",
        "Calvin Moras",
        "Peter Rentopoulos",
        "Ghodsiyeh Rostami",
        "Bich Ngoc Nguyen",
        "Jumanah Baig",
        "Abdelhakim Khellaf",
        "Vincent Quoc-Huy Trinh",
        "Rabeb Mizouni",
        "Hadi Otrok",
        "Jamal Bentahar",
        "Mahdi S. Hosseini"
      ],
      "abstract": "AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead. Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction , is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection , or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model . The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning , matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.",
      "summary_en": "AtlasPatch is an efficient and scalable whole-slide image preprocessing framework that uses fine-tuned Segment-Anything model for accurate tissue detection and high-throughput patch extraction with reduced computational overhead.",
      "summary_zh": "AtlasPatch 是一种高效且可扩展的全切片图像预处理框架，使用微调后的 Segment-Anything 模型实现精确的组织检测和高通量 patch 提取，同时降低计算开销。",
      "hf_url": "https://huggingface.co/papers/2602.03998",
      "arxiv_url": "https://arxiv.org/abs/2602.03998",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03998",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:43:51.410332+00:00"
    },
    {
      "date": "2026-02-09",
      "paper_id": "2601.23039",
      "title": "Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference",
      "authors": [
        "Yizhi Liu"
      ],
      "abstract": "Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training. Differentiable matching layers and residual connection paradigms, often implemented via entropy-regularized Optimal Transport (OT), serve as critical mechanisms in structural prediction and architectural scaling. However, recovering discrete permutations or maintaining identity mappings via annealing εto 0 is notoriously unstable. In this work, we identify a fundamental mechanism for this failure: Premature Mode Collapse . By analyzing the non-normal dynamics of the Sinkhorn fixed-point map , we reveal a theoretical thermodynamic speed limit: standard exponential cooling outpaces the contraction rate of the inference operator, which degrades as O(1/ε). To address this, we propose Efficient Piecewise Hybrid Adaptive Stability Control (EPH-ASC), an adaptive scheduling algorithm that monitors the stability of the inference process. We demonstrate that EPH-ASC is essential for stabilizing Manifold-Constrained Hyper-Connections (mHC) during large-scale training on the FineWeb-Edu dataset, effectively preventing late-stage gradient explosions by enforcing a linear stability law.",
      "summary_en": "Researchers identify and address premature mode collapse in optimal transport-based structural prediction models through an adaptive stability control algorithm that prevents gradient explosions during large-scale training.",
      "summary_zh": "研究人员通过防止大规模训练梯度爆炸的自适应稳定性控制算法，识别并解决了基于最优传输的结构预测模型中的过早模式崩溃问题。",
      "hf_url": "https://huggingface.co/papers/2601.23039",
      "arxiv_url": "https://arxiv.org/abs/2601.23039",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23039",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:43:41.193780+00:00"
    }
  ]
}