{
  "date": "2026-02-11",
  "count": 57,
  "papers": [
    {
      "date": "2026-02-11",
      "paper_id": "2602.05400",
      "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
      "authors": [
        "Shaobo Wang",
        "Xuan Ouyang",
        "Tianyi Xu",
        "Yuzheng Hu",
        "Jialin Liu",
        "Guo Chen",
        "Tianyu Zhang",
        "Junhao Zheng",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
      ],
      "abstract": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead. As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space . OPUS scores candidates by projecting their effective updates , shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia , OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
      "summary_en": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.",
      "summary_zh": "OPUS是一种动态数据选择框架，通过在稳定的代理派生目标空间中基于优化器诱导的更新投影对数据候选进行评分来提升预训练效率，从而在降低计算开销的同时实现更优性能。",
      "hf_url": "https://huggingface.co/papers/2602.05400",
      "arxiv_url": "https://arxiv.org/abs/2602.05400",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05400",
      "github_url": "",
      "upvotes": 316,
      "fetched_at": "2026-02-19T06:20:41.061634+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09856",
      "title": "Code2World: A GUI World Model via Renderable Code Generation",
      "authors": [
        "Yuhao Zheng",
        "Li'an Zhong",
        "Yi Wang",
        "Rui Dai",
        "Kaikui Liu",
        "Xiangxiang Chu",
        "Linyuan Lv",
        "Philip Torr",
        "Kevin Qinghong Lin"
      ],
      "abstract": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.",
      "summary_en": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.",
      "summary_zh": "Code2World通过可渲染代码生成使自主GUI智能体能够预测下一视觉状态，实现高视觉保真度与结构可控性，同时提升导航性能。",
      "hf_url": "https://huggingface.co/papers/2602.09856",
      "arxiv_url": "https://arxiv.org/abs/2602.09856",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09856",
      "github_url": "https://github.com/AMAP-ML/Code2World",
      "upvotes": 189,
      "fetched_at": "2026-02-19T06:22:03.257257+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09082",
      "title": "UI-Venus-1.5 Technical Report",
      "authors": [
        "Veuns-Team",
        "Changlong Gao",
        "Zhangxuan Gu",
        "Yulin Liu",
        "Xinyu Qiu",
        "Shuheng Shen",
        "Yue Wen",
        "Tianyu Xia",
        "Zhenyu Xu",
        "Zhengwen Zeng",
        "Beitong Zhou",
        "Xingran Zhou",
        "Weizhi Chen",
        "Sunhao Dai",
        "Jingya Dou",
        "Yichen Gong",
        "Yuan Guo",
        "Zhenlin Guo",
        "Feng Li",
        "Qian Li",
        "Jinzhen Lin",
        "Yuqi Zhou"
      ],
      "abstract": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.",
      "summary_en": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.",
      "summary_zh": "UI-Venus-1.5是一个通过中期训练阶段、在线强化学习和模型合并技术提升性能的统一GUI智能体。",
      "hf_url": "https://huggingface.co/papers/2602.09082",
      "arxiv_url": "https://arxiv.org/abs/2602.09082",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09082",
      "github_url": "https://github.com/inclusionAI/UI-Venus",
      "upvotes": 149,
      "fetched_at": "2026-02-19T06:21:38.863993+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10063",
      "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
      "authors": [
        "Tianyi Jiang",
        "Arctanx An",
        "Hengyi Feng",
        "Naixin Zhai",
        "Haodong Li",
        "Xiaomin Yu",
        "Jiahui Liu",
        "Hanwen Du",
        "Shuo Zhang",
        "Zhi Yang",
        "Jie Huang",
        "Yuhua Li",
        "Yongxin Ni",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "abstract": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches. Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a com mon trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset ( CoM ), a training-free agentic framework that enables step-level adaptive mindset orchestration . CoM de com poses reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency . Our code is publicly available at https://github. com /QuantaAlpha/chain-of-mindset{https://github. com /QuantaAlpha/chain-of-mindset}.",
      "summary_en": "A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.",
      "summary_zh": "一种名为 Chain of Mindset 的新型免训练框架通过整合空间、收敛、发散和算法推理方法，实现了面向大语言模型的步骤级别自适应思维编排。",
      "hf_url": "https://huggingface.co/papers/2602.10063",
      "arxiv_url": "https://arxiv.org/abs/2602.10063",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10063",
      "github_url": "https://github.com/QuantaAlpha/chain-of-mindset",
      "upvotes": 70,
      "fetched_at": "2026-02-19T06:22:07.821598+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08234",
      "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
      "authors": [
        "Peng Xia",
        "Jianwen Chen",
        "Hanyang Wang",
        "Jiaqi Liu",
        "Kaide Zeng",
        "Yu Wang",
        "Siwei Han",
        "Yiyang Zhou",
        "Xujiang Zhao",
        "Haifeng Chen",
        "Zeyu Zheng",
        "Cihang Xie",
        "Huaxiu Yao"
      ],
      "abstract": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead. Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution . Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank , an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning . These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
      "summary_en": "SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.",
      "summary_zh": "SkillRL使LLM智能体通过层次化技能发现与递归策略演化实现改进，在复杂任务上取得更优性能的同时降低计算开销。",
      "hf_url": "https://huggingface.co/papers/2602.08234",
      "arxiv_url": "https://arxiv.org/abs/2602.08234",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08234",
      "github_url": "https://github.com/aiming-lab/SkillRL",
      "upvotes": 65,
      "fetched_at": "2026-02-19T06:21:16.803051+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09443",
      "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads",
      "authors": [
        "Yun Luo",
        "Futing Wang",
        "Qianjia Cheng",
        "Fangchen Yu",
        "Haodi Lei",
        "Jianhao Yan",
        "Chenxi Li",
        "Jiacheng Chen",
        "Yufeng Zhao",
        "Haiyuan Wan",
        "Yuchen Zhang",
        "Shenghe Zheng",
        "Junchi Yao",
        "Qingyang Zhang",
        "Haonan He",
        "Wenxuan Zeng",
        "Li Sheng",
        "Chengxing Xie",
        "Yuxin Zuo",
        "Yizhuo Li",
        "Yulun Wu",
        "Rui Huang"
      ],
      "abstract": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception. The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning . Our method harmonizes Curriculum Reinforcement Learning , which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation , enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro . Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.",
      "summary_en": "Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.",
      "summary_zh": "面向物理的视觉-语言模型利用课程强化学习和智能体增强，在通过多模态感知保持物理一致性的同时，实现了最先进的科学推理性能。",
      "hf_url": "https://huggingface.co/papers/2602.09443",
      "arxiv_url": "https://arxiv.org/abs/2602.09443",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09443",
      "github_url": "https://github.com/PRIME-RL/P1-VL",
      "upvotes": 57,
      "fetched_at": "2026-02-19T06:21:51.230976+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10090",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "authors": [
        "Zhaoyang Wang",
        "Canwen Xu",
        "Boyi Liu",
        "Yite Wang",
        "Siwei Han",
        "Zhewei Yao",
        "Huaxiu Yao",
        "Yuxiong He"
      ],
      "abstract": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches. Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents . Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization . The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
      "summary_en": "Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.",
      "summary_zh": "在合成环境中通过代码驱动模拟和数据库支持状态转换训练的大型语言模型智能体，相比传统的针对特定基准的方法，展现出更优的分布外泛化能力。",
      "hf_url": "https://huggingface.co/papers/2602.10090",
      "arxiv_url": "https://arxiv.org/abs/2602.10090",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10090",
      "github_url": "https://github.com/Snowflake-Labs/agent-world-model",
      "upvotes": 49,
      "fetched_at": "2026-02-19T06:22:10.439986+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08426",
      "title": "Prism: Spectral-Aware Block-Sparse Attention",
      "authors": [
        "Xinghao Wang",
        "Pengyu Wang",
        "Xiaoran Liu",
        "Fangxu Liu",
        "Jason Chu",
        "Kai Song",
        "Xipeng Qiu"
      ],
      "abstract": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration. Block-sparse attention is promising for accelerating long-context LLM pre-filling , yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings ( RoPE ). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration , Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.",
      "summary_en": "Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.",
      "summary_zh": "Prism 采用频谱感知方法解决长上下文 LLM 预填充中块稀疏注意力的低效问题，该方法通过基于能量的温度校准提高块选择准确性。",
      "hf_url": "https://huggingface.co/papers/2602.08426",
      "arxiv_url": "https://arxiv.org/abs/2602.08426",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08426",
      "github_url": "https://github.com/xinghaow99/prism",
      "upvotes": 35,
      "fetched_at": "2026-02-19T06:21:23.446259+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07035",
      "title": "DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents",
      "authors": [
        "Jiahao Zhao",
        "Shaoxuan Xu",
        "Zhongxiang Sun",
        "Fengqi Zhu",
        "Jingyang Ou",
        "Yuling Shi",
        "Chongxuan Li",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "abstract": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm. Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge : the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm . Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge , we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct . P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C",
      "summary_en": "Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.",
      "summary_zh": "扩散大语言模型针对搜索智能体优化，通过增强推理能力并借助并行推理范式降低延迟。",
      "hf_url": "https://huggingface.co/papers/2602.07035",
      "arxiv_url": "https://arxiv.org/abs/2602.07035",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07035",
      "github_url": "https://github.com/bubble65/DLLM-Searcher",
      "upvotes": 30,
      "fetched_at": "2026-02-19T06:20:54.641223+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09084",
      "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling",
      "authors": [
        "Ruijie Ye",
        "Jiayi Zhang",
        "Zhuoxin Liu",
        "Zihao Zhu",
        "Siyuan Yang",
        "Li Li",
        "Tianfu Fu",
        "Franck Dernoncourt",
        "Yue Zhao",
        "Jiacheng Zhu",
        "Ryan Rossi",
        "Wenhao Chai",
        "Zhengzhong Tu"
      ],
      "abstract": "Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution. We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing . Agent Banana introduces two key mechanisms: (1) Context Folding , which compresses long interaction histories into structured memory for stable long-horizon control ; and (2) Image Layer Decomposition , which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs . To support rigorous evaluation, we build HDD-Bench , a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench , Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.",
      "summary_en": "Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.",
      "summary_zh": "Agent Banana 通过具有上下文折叠和图像层分解的分层框架，解决了基于指令的图像编辑中的挑战，实现了超高分辨率下的高保真多轮编辑。",
      "hf_url": "https://huggingface.co/papers/2602.09084",
      "arxiv_url": "https://arxiv.org/abs/2602.09084",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09084",
      "github_url": "https://github.com/taco-group/agent-banana",
      "upvotes": 27,
      "fetched_at": "2026-02-19T06:21:40.767811+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10104",
      "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
      "authors": [
        "Yuxin Jiang",
        "Yuchao Gu",
        "Ivor W. Tsang",
        "Mike Zheng Shou"
      ],
      "abstract": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models. Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce SeqΔ-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder . Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
      "summary_en": "Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.",
      "summary_zh": "序列级控制-效果对齐支持视频世界模型学习结构化潜在动作空间，实现零样本动作迁移。",
      "hf_url": "https://huggingface.co/papers/2602.10104",
      "arxiv_url": "https://arxiv.org/abs/2602.10104",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10104",
      "github_url": "https://github.com/showlab/Olaf-World",
      "upvotes": 26,
      "fetched_at": "2026-02-19T06:22:20.128449+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08847",
      "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems",
      "authors": [
        "Lang Feng",
        "Longtao Zheng",
        "Shuo He",
        "Fuxiang Zhang",
        "Bo An"
      ],
      "abstract": "Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability. Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems . We show that under GRPO-style optimization , a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability . Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems . Dr. MAS uses an agent-wise remedy : normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems , supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.",
      "summary_en": "Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.",
      "summary_zh": "多智能体大语言模型系统在强化学习中因全局归一化不匹配而面临训练不稳定问题，Dr. MAS通过智能体特定的优势归一化与增强的训练稳定性解决该问题。",
      "hf_url": "https://huggingface.co/papers/2602.08847",
      "arxiv_url": "https://arxiv.org/abs/2602.08847",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08847",
      "github_url": "https://github.com/langfengQ/DrMAS",
      "upvotes": 24,
      "fetched_at": "2026-02-19T06:21:29.488458+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07422",
      "title": "Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model",
      "authors": [
        "Tianyi Wu",
        "Mingzhe Du",
        "Yue Liu",
        "Chengran Yang",
        "Terry Yue Zhuo",
        "Jiaheng Zhang",
        "See-Kiong Ng"
      ],
      "abstract": "SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling. Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation . SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision . Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.",
      "summary_en": "SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling.",
      "summary_zh": "SecCoderX 利用在线强化学习对齐大语言模型，在保留功能性的同时实现安全代码生成，并通过集成漏洞检测与奖励建模解决功能与安全性的权衡问题。",
      "hf_url": "https://huggingface.co/papers/2602.07422",
      "arxiv_url": "https://arxiv.org/abs/2602.07422",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07422",
      "github_url": "https://github.com/AndrewWTY/SecCoderX",
      "upvotes": 21,
      "fetched_at": "2026-02-19T06:21:03.217258+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.00268",
      "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation",
      "authors": [
        "Ariel Shaulov",
        "Eitan Shaar",
        "Amit Edenzon",
        "Lior Wolf"
      ],
      "abstract": "Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency. Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift , where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation . Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space .",
      "summary_en": "Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.",
      "summary_zh": "自回归视频生成因潜在条件token的误差累积而存在时间漂移，通过在推理过程中识别并移除不稳定token以提升长程一致性。",
      "hf_url": "https://huggingface.co/papers/2602.00268",
      "arxiv_url": "https://arxiv.org/abs/2602.00268",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00268",
      "github_url": "https://github.com/arielshaulov/TokenTrim",
      "upvotes": 21,
      "fetched_at": "2026-02-19T06:20:19.280811+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07022",
      "title": "Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss",
      "authors": [
        "Yucheng Zhou",
        "Hao Li",
        "Jianbing Shen"
      ],
      "abstract": "Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory. Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion loss es. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss , highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss , demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition error s and leads to a stable condition distribution . Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution , effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.",
      "summary_en": "Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.",
      "summary_zh": "采用扩散损失的自回归模型通过块去噪优化以及基于最优传输理论的条件细化，有效缓解条件误差，性能优于传统扩散模型。",
      "hf_url": "https://huggingface.co/papers/2602.07022",
      "arxiv_url": "https://arxiv.org/abs/2602.07022",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07022",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T06:20:52.223477+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04208",
      "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
      "authors": [
        "Hyeonbeom Choi",
        "Daechul Ahn",
        "Youhan Lee",
        "Taewook Kang",
        "Seongwon Cho",
        "Jonghyun Choi"
      ],
      "abstract": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes. Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity , where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on ' self-uncertainty ', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
      "summary_en": "SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.",
      "summary_zh": "SCALE是一种针对视觉-语言-动作模型的新型推理策略，它基于自不确定性联合调节视觉感知与动作，无需额外训练或多次前向传播即可提升鲁棒性。",
      "hf_url": "https://huggingface.co/papers/2602.04208",
      "arxiv_url": "https://arxiv.org/abs/2602.04208",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04208",
      "github_url": "",
      "upvotes": 19,
      "fetched_at": "2026-02-19T06:20:30.145387+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.00462",
      "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
      "authors": [
        "Benno Krojer",
        "Shravan Nayak",
        "Oscar Mañas",
        "Vaibhav Adlakha",
        "Desmond Elliott",
        "Siva Reddy",
        "Marius Mosbach"
      ],
      "abstract": "LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought. Transforming a large language model ( LLM ) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM . Intriguingly, this mapping can be as simple as a shallow MLP transformation . To understand why LLM s can so readily process visual tokens , we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens . With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations .",
      "summary_en": "LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.",
      "summary_zh": "LatentLens通过将视觉-语言模型中的视觉token表示与上下文文本表示进行比较，实现了对其的解释，并揭示视觉token比之前认为的更具可解释性。",
      "hf_url": "https://huggingface.co/papers/2602.00462",
      "arxiv_url": "https://arxiv.org/abs/2602.00462",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00462",
      "github_url": "https://github.com/McGill-NLP/latentlens",
      "upvotes": 18,
      "fetched_at": "2026-02-19T06:20:21.376631+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10098",
      "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
      "authors": [
        "Jingwen Sun",
        "Wenyao Zhang",
        "Zekun Qi",
        "Shaojie Ren",
        "Zezhi Liu",
        "Hanxin Zhu",
        "Guangzhong Sun",
        "Xin Jin",
        "Zhibo Chen"
      ],
      "abstract": "VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks. Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions , making them vulnerable to appearance bias , nuisance motion , and information leakage . We introduce VLA- JEPA , a JEPA -style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames , while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA- JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes . This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA- JEPA achieves consistent gains in generalization and robustness over existing methods.",
      "summary_en": "VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.",
      "summary_zh": "VLA-JEPA是一种JEPA风格的预训练框架，利用潜在空间中的无泄漏状态预测改进视觉-语言-动作策略学习，提升操作任务的泛化性和鲁棒性。",
      "hf_url": "https://huggingface.co/papers/2602.10098",
      "arxiv_url": "https://arxiv.org/abs/2602.10098",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10098",
      "github_url": "https://github.com/ginwind/VLA-JEPA",
      "upvotes": 17,
      "fetched_at": "2026-02-19T06:22:12.746685+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09849",
      "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
      "authors": [
        "Yucheng Hu",
        "Jianke Zhang",
        "Yuanfei Luo",
        "Yanjiang Guo",
        "Xiaoyu Chen",
        "Xinshu Sun",
        "Kun Feng",
        "Qingzhou Lu",
        "Sheng Chen",
        "Yangang Zhang",
        "Wei Li",
        "Jianyu Chen"
      ],
      "abstract": "BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks. Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation , leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning , visual forecasting , and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning .",
      "summary_en": "BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.",
      "summary_zh": "BagelVLA是一种统一的Vision-Language-Action模型，通过残差流引导整合语言规划、视觉预测和动作生成，以改进操作任务。",
      "hf_url": "https://huggingface.co/papers/2602.09849",
      "arxiv_url": "https://arxiv.org/abs/2602.09849",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09849",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T06:22:00.781887+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09000",
      "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
      "authors": [
        "Ali Hatamizadeh",
        "Shrimai Prabhumoye",
        "Igor Gitman",
        "Ximing Lu",
        "Seungju Han",
        "Wei Ping",
        "Yejin Choi",
        "Jan Kautz"
      ],
      "abstract": "Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks. Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization . We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements , training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse . These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning .",
      "summary_en": "Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks.",
      "summary_zh": "迭代式群体相对策略优化通过结合探索性起草与精细化迭代的两阶段过程，增强大语言模型的数学推理能力，并在竞争性基准测试中取得最先进的结果。",
      "hf_url": "https://huggingface.co/papers/2602.09000",
      "arxiv_url": "https://arxiv.org/abs/2602.09000",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09000",
      "github_url": "",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:21:31.787398+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.01244",
      "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments",
      "authors": [
        "Siwei Wu",
        "Yizhi Li",
        "Yuyang Song",
        "Wei Zhang",
        "Yang Wang",
        "Riza Batista-Navarro",
        "Xian Yang",
        "Mingjie Tang",
        "Bryan Dai",
        "Jian Yang",
        "Chenghua Lin"
      ],
      "abstract": "A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains. Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\emph{ Executability }, since each instance requires a suitable and often distinct Docker environment; and \\emph{ Verifiability }, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances , and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.",
      "summary_en": "A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.",
      "summary_zh": "名为 TerminalTraj 的可扩展流程通过筛选代码仓库、生成 Docker 对齐的任务实例并在多领域合成可执行智能体轨迹，解决了为训练智能体模型创建高质量终端轨迹的挑战。",
      "hf_url": "https://huggingface.co/papers/2602.01244",
      "arxiv_url": "https://arxiv.org/abs/2602.01244",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01244",
      "github_url": "https://github.com/multimodal-art-projection/TerminalTraj",
      "upvotes": 15,
      "fetched_at": "2026-02-19T06:20:23.586349+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10102",
      "title": "VideoWorld 2: Learning Transferable Knowledge from Real-world Videos",
      "authors": [
        "Zhongwei Ren",
        "Yunchao Wei",
        "Xiao Yu",
        "Guixun Luo",
        "Yao Zhao",
        "Bingyi Kang",
        "Jiashi Feng",
        "Xiaojie Jin"
      ],
      "abstract": "VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications. Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance : a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning . We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset , which substantially improves task performance on CALVIN . This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.",
      "summary_en": "VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.",
      "summary_zh": "VideoWorld 2 通过动态增强的潜在动力学模型，将动作动态与视觉外观解耦，实现了从原始视频中进行可迁移知识学习，并在真实世界应用中提升了任务性能与长程推理能力。",
      "hf_url": "https://huggingface.co/papers/2602.10102",
      "arxiv_url": "https://arxiv.org/abs/2602.10102",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10102",
      "github_url": "https://github.com/ByteDance-Seed/VideoWorld",
      "upvotes": 14,
      "fetched_at": "2026-02-19T06:22:17.746149+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09439",
      "title": "Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning",
      "authors": [
        "Xu Ma",
        "Yitian Zhang",
        "Qihua Dong",
        "Yun Fu"
      ],
      "abstract": "A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles. High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning . Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment , or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning . Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment , visual fidelity , and prompt quality , with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning -level quality. Across a diverse set of pretrained diffusion and autoregressive models , fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.",
      "summary_en": "A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.",
      "summary_zh": "本文介绍了一个大规模、高质量、完全开源的文本到图像微调数据集，包含超过600万文本-图像对，并针对多种任务组合和视觉风格进行了严格的对齐与质量过滤。",
      "hf_url": "https://huggingface.co/papers/2602.09439",
      "arxiv_url": "https://arxiv.org/abs/2602.09439",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09439",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:21:49.147618+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.06820",
      "title": "ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training",
      "authors": [
        "Dunwei Tu",
        "Hongyan Hao",
        "Hansi Yang",
        "Yihao Chen",
        "Yi-Kai Zhang",
        "Zhikang Xia",
        "Yu Yang",
        "Yueqing Sun",
        "Xingchen Liu",
        "Furao Shen",
        "Qi Gu",
        "Hui Su",
        "Xunliang Cai"
      ],
      "abstract": "ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion. Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration . However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing , and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification . By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as τ^2-Bench and VitaBench , highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.",
      "summary_en": "ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.",
      "summary_zh": "ScaleEnv框架从零开始生成交互式环境，通过多样化域缩放与验证任务完成来提升智能体泛化能力。",
      "hf_url": "https://huggingface.co/papers/2602.06820",
      "arxiv_url": "https://arxiv.org/abs/2602.06820",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06820",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T06:20:50.291355+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09017",
      "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
      "authors": [
        "Zichen Jeff Cui",
        "Omar Rayyan",
        "Haritheja Etukuru",
        "Bowen Tan",
        "Zavier Andrianarivo",
        "Zicheng Teng",
        "Yihang Zhou",
        "Krish Mehta",
        "Nicholas Wojno",
        "Kevin Yuanbo Wu",
        "Manan H Anjaria",
        "Ziyuan Wu",
        "Manrong Mao",
        "Guangxun Zhang",
        "Binit Shah",
        "Yejin Kim",
        "Soumith Chintala",
        "Lerrel Pinto",
        "Nur Muhammad Mahi Shafiullah"
      ],
      "abstract": "Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles. The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym , a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data , and outperforms large, state-of-the-art VLAs in zero-shot evaluation s by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/",
      "summary_en": "Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles.",
      "summary_zh": "接触锚定策略以物理接触点替代语言条件，并采用模块化效用模型实现鲁棒操控，通过 real-to-sim 迭代循环，以极少演示数据达成卓越的零样本性能。",
      "hf_url": "https://huggingface.co/papers/2602.09017",
      "arxiv_url": "https://arxiv.org/abs/2602.09017",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09017",
      "github_url": "https://github.com/jeffacce/cap-policy",
      "upvotes": 12,
      "fetched_at": "2026-02-19T06:21:34.406799+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09276",
      "title": "Effective Reasoning Chains Reduce Intrinsic Dimensionality",
      "authors": [
        "Archiki Prasad",
        "Mandar Joshi",
        "Kenton Lee",
        "Mohit Bansal",
        "Peter Shaw"
      ],
      "abstract": "Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds. Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies , we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.",
      "summary_en": "Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds.",
      "summary_zh": "有效的思维链推理策略可降低内在维度，从而在达到给定准确率阈值时只需更少的模型参数，实现更好的泛化。",
      "hf_url": "https://huggingface.co/papers/2602.09276",
      "arxiv_url": "https://arxiv.org/abs/2602.09276",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09276",
      "github_url": "",
      "upvotes": 11,
      "fetched_at": "2026-02-19T06:21:46.907823+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08382",
      "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
      "authors": [
        "Zhuoen Chen",
        "Dongfang Li",
        "Meishan Zhang",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches. Large Language Models (LLMs) face significant challenges in long-context processing , including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall , rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor . A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning , while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA , extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent .",
      "summary_en": "A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.",
      "summary_zh": "一种受认知启发的长上下文语言建模框架采用分块压缩和选择性记忆召回，以提升效率和性能，优于传统方法。",
      "hf_url": "https://huggingface.co/papers/2602.08382",
      "arxiv_url": "https://arxiv.org/abs/2602.08382",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08382",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T06:21:21.015506+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07276",
      "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs",
      "authors": [
        "Pengrui Han",
        "Xueqiang Xu",
        "Keyang Xuan",
        "Peiyang Song",
        "Siru Ouyang",
        "Runchu Tian",
        "Yuqing Jiang",
        "Cheng Qian",
        "Pengcheng Jiang",
        "Jiashuo Sun",
        "Junxia Cui",
        "Ming Zhong",
        "Ge Liu",
        "Jiawei Han",
        "Jiaxuan You"
      ],
      "abstract": "STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.",
      "summary_en": "STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.",
      "summary_zh": "STEER2ADAPT是一种轻量级框架，通过从可复用的语义先验子空间中组合引导向量来适配大型语言模型，借助基向量的线性组合实现高效且灵活的任务适配。",
      "hf_url": "https://huggingface.co/papers/2602.07276",
      "arxiv_url": "https://arxiv.org/abs/2602.07276",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07276",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T06:20:59.095957+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08025",
      "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models",
      "authors": [
        "Yixuan Ye",
        "Xuanyu Lu",
        "Yuxin Jiang",
        "Yuchao Gu",
        "Rui Zhao",
        "Qiwei Liang",
        "Jiachun Pan",
        "Fengda Zhang",
        "Weijia Wu",
        "Alex Jinpeng Wang"
      ],
      "abstract": "MIND is introduced as the first open-domain closed-loop benchmark for evaluating memory consistency and action control in world models, featuring high-quality videos and diverse action spaces to assess temporal stability and contextual coherence. World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models . MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control , capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline . Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models , including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/",
      "summary_en": "MIND is introduced as the first open-domain closed-loop benchmark for evaluating memory consistency and action control in world models, featuring high-quality videos and diverse action spaces to assess temporal stability and contextual coherence.",
      "summary_zh": "MIND是首个用于评估世界模型记忆一致性与动作控制的开放域闭环基准，具有高质量视频与多样化动作空间，用于评估时间稳定性与上下文连贯性。",
      "hf_url": "https://huggingface.co/papers/2602.08025",
      "arxiv_url": "https://arxiv.org/abs/2602.08025",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08025",
      "github_url": "https://github.com/CSU-JPG/MIND",
      "upvotes": 9,
      "fetched_at": "2026-02-19T06:21:14.828419+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09823",
      "title": "Covo-Audio Technical Report",
      "authors": [
        "Wenfu Wang",
        "Chenxing Li",
        "Liqiang Zhang",
        "Yiyang Zhao",
        "Yuxiang Zou",
        "Hanzhao Li",
        "Mingyu Cui",
        "Hao Zhang",
        "Kun Wei",
        "Le Xu",
        "Zikang Huang",
        "Jiajun Xu",
        "Jiliang Hu",
        "Xiang He",
        "Zeyu Xie",
        "Jiawen Kang",
        "Youjun Chen",
        "Meng Yu",
        "Dong Yu",
        "Rilin Chen",
        "Linlin Di",
        "Shulin Feng"
      ],
      "abstract": "Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques. In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training , Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling , spoken dialogue , speech understanding , audio understanding , and full-duplex voice interaction . Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant , demonstrates strong spoken conversational abilities , including understanding, contextual reasoning , instruction following , and generating contextually appropriate and empathetic responses , validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model , achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALM s for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering , enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning , and suggest a scalable path toward more capable and versatile LALMs.",
      "summary_en": "Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques.",
      "summary_zh": "Covo-Audio是一个7B参数的端到端大型音频语言模型，可处理连续音频输入并生成音频输出，通过大规模预训练和后训练技术，在语音-文本建模、口语对话和全双工语音交互任务上实现了最先进的性能。",
      "hf_url": "https://huggingface.co/papers/2602.09823",
      "arxiv_url": "https://arxiv.org/abs/2602.09823",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09823",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:21:58.759959+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09268",
      "title": "Rethinking Global Text Conditioning in Diffusion Transformers",
      "authors": [
        "Nikita Starodubcev",
        "Daniil Pakhomov",
        "Zongze Wu",
        "Ilya Drobyshevskiy",
        "Yuchen Liu",
        "Zhonghao Wang",
        "Yuqian Zhou",
        "Zhe Lin",
        "Dmitry Baranchuk"
      ],
      "abstract": "Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms. Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding . Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing .",
      "summary_en": "Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.",
      "summary_zh": "扩散Transformer中，基于调制的文本条件化在作为可控生成引导而非仅作为注意力机制使用时具有性能优势。",
      "hf_url": "https://huggingface.co/papers/2602.09268",
      "arxiv_url": "https://arxiv.org/abs/2602.09268",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09268",
      "github_url": "https://github.com/quickjkee/modulation-guidance",
      "upvotes": 8,
      "fetched_at": "2026-02-19T06:21:44.885507+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10116",
      "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
      "authors": [
        "Hongchi Xia",
        "Xuan Li",
        "Zhaoshuo Li",
        "Qianli Ma",
        "Jiashu Xu",
        "Ming-Yu Liu",
        "Yin Cui",
        "Tsung-Yi Lin",
        "Wei-Chiu Ma",
        "Shenlong Wang",
        "Shuran Song",
        "Fangyin Wei"
      ],
      "abstract": "SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability. Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility , visual realism , and physical stability . Through iterative reasoning and adaptive tool selection , it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training . Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI . Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
      "summary_en": "SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.",
      "summary_zh": "SAGE是一种智能体框架，通过结合布局与物体组合生成器以及用于语义合理性和物理稳定性的评估评判器，为具身智能自动生成可直接用于仿真的3D环境。",
      "hf_url": "https://huggingface.co/papers/2602.10116",
      "arxiv_url": "https://arxiv.org/abs/2602.10116",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10116",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-19T06:22:22.475854+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09662",
      "title": "TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution",
      "authors": [
        "Deyang Jiang",
        "Jing Huang",
        "Xuanle Zhao",
        "Lei Chen",
        "Liming Zheng",
        "Fanfan Liu",
        "Haibo Qiu",
        "Peng Shi",
        "Zhixiong Zeng"
      ],
      "abstract": "TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification. Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning , which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning . In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (i.e., trajectory difficulty) and breadth (i.e., trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.",
      "summary_en": "TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.",
      "summary_zh": "TreeCUA通过树状结构轨迹组织与多智能体协作实现GUI自动化的高效扩展，并通过自适应探索与轨迹验证提升GUI规划能力。",
      "hf_url": "https://huggingface.co/papers/2602.09662",
      "arxiv_url": "https://arxiv.org/abs/2602.09662",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09662",
      "github_url": "https://github.com/UITron-hub/TreeCUA",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:21:56.177494+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07839",
      "title": "TodoEvolve: Learning to Architect Agent Planning Systems",
      "authors": [
        "Jiaxi Liu",
        "Yanzuo Jiang",
        "Guibin Zhang",
        "Zihan Zhang",
        "Heng Chang",
        "Zhenfei Yin",
        "Qibing Ren",
        "Junchi Yan"
      ],
      "abstract": "TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization. Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory , a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory , we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization ( IGPO ), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.",
      "summary_en": "TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.",
      "summary_zh": "TodoEvolve通过模块化设计空间与多目标强化学习优化，实现任务特定规划架构的自主合成与修订。",
      "hf_url": "https://huggingface.co/papers/2602.07839",
      "arxiv_url": "https://arxiv.org/abs/2602.07839",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07839",
      "github_url": "https://github.com/EcthelionLiu/TodoEvolve",
      "upvotes": 6,
      "fetched_at": "2026-02-19T06:21:09.846009+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09153",
      "title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes",
      "authors": [
        "Nicholas Pfaff",
        "Thomas Cohn",
        "Sergey Zakharov",
        "Rick Cory",
        "Russ Tedrake"
      ],
      "abstract": "SceneSmith is a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts through multiple stages involving VLM agents and integrated asset generation techniques. Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation . We introduce SceneSmith , a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts . SceneSmith constructs scenes through successive stagesx2013from architectural layout to furniture placement to small object populationx2013each implemented as an interaction among VLM agents : designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation . SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.",
      "summary_en": "SceneSmith is a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts through multiple stages involving VLM agents and integrated asset generation techniques.",
      "summary_zh": "SceneSmith是一种分层智能体框架，通过包含VLM智能体与集成资产生成技术的多阶段流程，基于自然语言提示生成可直接用于仿真的室内环境。",
      "hf_url": "https://huggingface.co/papers/2602.09153",
      "arxiv_url": "https://arxiv.org/abs/2602.09153",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09153",
      "github_url": "https://github.com/nepfaff/scenesmith",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:43.069531+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09024",
      "title": "Autoregressive Image Generation with Masked Bit Modeling",
      "authors": [
        "Qihang Yu",
        "Qihao Liu",
        "Ju He",
        "Xinyang Zhang",
        "Yang Liu",
        "Liang-Chieh Chen",
        "Xi Chen"
      ],
      "abstract": "Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs. This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook size s. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256 , outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/",
      "summary_en": "Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.",
      "summary_zh": "离散tokenizer在适当扩展时可匹敌甚至超越连续方法，且一种新的掩码Bit自回归建模方法以更低的计算成本达到了最优结果。",
      "hf_url": "https://huggingface.co/papers/2602.09024",
      "arxiv_url": "https://arxiv.org/abs/2602.09024",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09024",
      "github_url": "https://github.com/amazon-far/BAR",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:36.512548+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08344",
      "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration",
      "authors": [
        "Qi Guo",
        "Jianing Wang",
        "Deyang Kong",
        "Xiangyu Xi",
        "Jianfei Zhang",
        "Yi Lu",
        "Jingang Wang",
        "Wei Wang",
        "Shikun Zhang",
        "Wei Ye"
      ],
      "abstract": "Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery. Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking , aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase , with limited attention to the path exploration stage . In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards ( RLVR ) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.",
      "summary_en": "Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.",
      "summary_zh": "可验证奖励的强化学习通过大纲引导的路径探索来增强大型推理模型的并行思考，从而减少信息冗余并提升解的发现。",
      "hf_url": "https://huggingface.co/papers/2602.08344",
      "arxiv_url": "https://arxiv.org/abs/2602.08344",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08344",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:18.746955+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07755",
      "title": "Learning to Continually Learn via Meta-learning Agentic Memory Designs",
      "authors": [
        "Yiming Xiong",
        "Shengran Hu",
        "Jeff Clune"
      ],
      "abstract": "ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains. The statelessness of foundation models bottlenecks agentic systems ' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems ), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms . Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.",
      "summary_en": "ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.",
      "summary_zh": "ALMA是一个利用元学习自动发现智能体系统记忆设计的框架，能够在不同领域实现无需人工工程的持续学习。",
      "hf_url": "https://huggingface.co/papers/2602.07755",
      "arxiv_url": "https://arxiv.org/abs/2602.07755",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07755",
      "github_url": "https://github.com/zksha/alma",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:21:07.803056+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07153",
      "title": "ANCHOR: Branch-Point Data Generation for GUI Agents",
      "authors": [
        "Jinbiao Wei",
        "Yilun Zhao",
        "Kangqi Ni",
        "Arman Cohan"
      ],
      "abstract": "A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants. End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations . Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.",
      "summary_en": "A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.",
      "summary_zh": "名为Anchor的轨迹扩展框架通过识别分支点并生成基于状态的任务变体新轨迹，从种子演示中引导可扩展的桌面监督。",
      "hf_url": "https://huggingface.co/papers/2602.07153",
      "arxiv_url": "https://arxiv.org/abs/2602.07153",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07153",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T06:20:56.641233+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05085",
      "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories",
      "authors": [
        "Sidi Lu",
        "Zhenwen Liang",
        "Dongyang Ma",
        "Yan Wang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Locas, a locally-supported parametric memory mechanism, enables flexible integration with transformer models for continual learning while minimizing catastrophic forgetting through principled initialization techniques. In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformer s, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning . We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning . Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation . Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.",
      "summary_en": "Locas, a locally-supported parametric memory mechanism, enables flexible integration with transformer models for continual learning while minimizing catastrophic forgetting through principled initialization techniques.",
      "summary_zh": "Locas是一种局部支持的参数化记忆机制，可与Transformer模型灵活集成以进行持续学习，同时通过原则性初始化技术最小化灾难性遗忘。",
      "hf_url": "https://huggingface.co/papers/2602.05085",
      "arxiv_url": "https://arxiv.org/abs/2602.05085",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05085",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-19T06:20:38.575779+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09591",
      "title": "On the Optimal Reasoning Length for RL-Trained Language Models",
      "authors": [
        "Daisuke Nohara",
        "Taishi Nakamura",
        "Rio Yokota"
      ],
      "abstract": "Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors. Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance . In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition , while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.",
      "summary_en": "Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.",
      "summary_zh": "强化学习训练的语言模型中的长度控制方法会影响推理性能和计算效率，最优输出长度能够平衡这些因素。",
      "hf_url": "https://huggingface.co/papers/2602.09591",
      "arxiv_url": "https://arxiv.org/abs/2602.09591",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09591",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:21:53.570598+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08503",
      "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
      "authors": [
        "Yi Ding",
        "Ziliang Qiu",
        "Bolian Li",
        "Ruqi Zhang"
      ],
      "abstract": "Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies. Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.",
      "summary_en": "Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.",
      "summary_zh": "Octopus 是一种 RL rollout 增强框架，通过合成样本生成与响应掩码策略，实现了视觉-语言模型的高效自校正学习。",
      "hf_url": "https://huggingface.co/papers/2602.08503",
      "arxiv_url": "https://arxiv.org/abs/2602.08503",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08503",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:21:25.252998+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.06161",
      "title": "Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding",
      "authors": [
        "Yanzheng Xiang",
        "Lan Wei",
        "Yizhen Yao",
        "Qinglin Zhu",
        "Hanqi Yan",
        "Chen Jin",
        "Philip Alexander Teare",
        "Dandan Zhang",
        "Lin Gui",
        "Amrutha Saseendran",
        "Yulan He"
      ],
      "abstract": "COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction. Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations , where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override : selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift , and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.",
      "summary_en": "COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.",
      "summary_zh": "COVER 通过实现缓存覆盖验证，使扩散语言模型能够进行高效并行解码，减少不必要的修正，并通过稳定的起草与注意力视图构建保持输出质量。",
      "hf_url": "https://huggingface.co/papers/2602.06161",
      "arxiv_url": "https://arxiv.org/abs/2602.06161",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.06161",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:47.889711+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05892",
      "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
      "authors": [
        "Han Li",
        "Letian Zhu",
        "Bohan Zhang",
        "Rili Feng",
        "Jiaming Wang",
        "Yue Pan",
        "Earl T. Barr",
        "Sarro Federica",
        "Zhaoyang Chu",
        "He Ye"
      ],
      "abstract": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information. LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents . ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts . We further implement an automated evaluation framework that tracks agent trajectories and measures context recall , precision, and efficiency throughout issue resolution . Using ContextBench, we evaluate four frontier LLMs and five coding agents . Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents ), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.",
      "summary_en": "ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information.",
      "summary_zh": "ContextBench 通过详细的过程分析评估 coding agents 的上下文检索，揭示先进的 agent 设计在上下文使用方面的提升有限，同时突显了已探索信息与已利用信息之间的差距。",
      "hf_url": "https://huggingface.co/papers/2602.05892",
      "arxiv_url": "https://arxiv.org/abs/2602.05892",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05892",
      "github_url": "https://github.com/EuniAI/ContextBench",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:45.438477+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.05435",
      "title": "Stable Velocity: A Variance Perspective on Flow Matching",
      "authors": [
        "Donglin Yang",
        "Yongxing Zhang",
        "Xin Yu",
        "Liang Hou",
        "Xin Tao",
        "Pengfei Wan",
        "Xiaojuan Qi",
        "Renjie Liao"
      ],
      "abstract": "Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed. While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
      "summary_en": "Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.",
      "summary_zh": "Stable Velocity 框架通过识别低方差区域并提出方差缩减技术，解决流匹配中的高方差训练问题，从而提升训练效率和采样速度。",
      "hf_url": "https://huggingface.co/papers/2602.05435",
      "arxiv_url": "https://arxiv.org/abs/2602.05435",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.05435",
      "github_url": "https://github.com/linYDTHU/StableVelocity",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:43.254130+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.02464",
      "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
      "authors": [
        "Or Shafran",
        "Shaked Ronen",
        "Omri Fahn",
        "Shauli Ravfogel",
        "Atticus Geiger",
        "Mor Geva"
      ],
      "abstract": "Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties. Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space . Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure . MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space , and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space . Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders . Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control , accounting for complex structures that isolated directions fail to capture.",
      "summary_en": "Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.",
      "summary_zh": "因子分析器混合模型（MFA）提供了一种可扩展的无监督方法，通过高斯区域及其协方差特性对局部几何结构进行建模，从而发现语言模型激活空间中的复杂非线性结构。",
      "hf_url": "https://huggingface.co/papers/2602.02464",
      "arxiv_url": "https://arxiv.org/abs/2602.02464",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02464",
      "github_url": "https://github.com/ordavid-s/decomposing-activations-local-geometry",
      "upvotes": 3,
      "fetched_at": "2026-02-19T06:20:27.956194+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.10099",
      "title": "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders",
      "authors": [
        "Amandeep Kumar",
        "Vishal M. Patel"
      ],
      "abstract": "Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling. Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders , rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation , RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF",
      "summary_en": "Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.",
      "summary_zh": "标准扩散Transformer中的几何干扰阻碍了表示编码器的收敛，通过结合Jacobi正则化的黎曼流匹配可解决该问题，从而在无需宽度缩放的情况下实现有效训练。",
      "hf_url": "https://huggingface.co/papers/2602.10099",
      "arxiv_url": "https://arxiv.org/abs/2602.10099",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10099",
      "github_url": "https://github.com/amandpkr/RJF",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:22:15.296928+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07398",
      "title": "AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management",
      "authors": [
        "Ruoyao Wen",
        "Hao Li",
        "Chaowei Xiao",
        "Ning Zhang"
      ],
      "abstract": "AgentSys defends against indirect prompt injection in LLM agents through hierarchical memory isolation and controlled data flow, significantly reducing attack success rates while maintaining performance. Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window , which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack. We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls , each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing . Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length. On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.",
      "summary_en": "AgentSys defends against indirect prompt injection in LLM agents through hierarchical memory isolation and controlled data flow, significantly reducing attack success rates while maintaining performance.",
      "summary_zh": "AgentSys通过分层内存隔离和受控数据流防御LLM智能体中的间接提示注入，显著降低攻击成功率的同时保持性能。",
      "hf_url": "https://huggingface.co/papers/2602.07398",
      "arxiv_url": "https://arxiv.org/abs/2602.07398",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07398",
      "github_url": "https://github.com/ruoyaow/agentsys-memory",
      "upvotes": 2,
      "fetched_at": "2026-02-19T06:21:01.099791+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.09924",
      "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations",
      "authors": [
        "William Lugoloobi",
        "Thomas Foster",
        "William Bankes",
        "Chris Russell"
      ],
      "abstract": "LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance. Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks , substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC , which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning . Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty",
      "summary_en": "LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.",
      "summary_zh": "LLM的内部表征可以预测问题难度，并实现高效推理路由，从而在保持性能的同时降低成本。",
      "hf_url": "https://huggingface.co/papers/2602.09924",
      "arxiv_url": "https://arxiv.org/abs/2602.09924",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09924",
      "github_url": "https://github.com/KabakaWilliam/llms_know_difficulty",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:22:05.328936+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.08519",
      "title": "Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering",
      "authors": [
        "Yunhui Liu",
        "Pengyu Qiu",
        "Yu Xing",
        "Yongchao Liu",
        "Peng Du",
        "Chuntao Hong",
        "Jiajun Zheng",
        "Tao Zheng",
        "Tieke He"
      ],
      "abstract": "PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols. Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data . Despite its significance in industrial applications such as fraud detection and user segmentation , a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high- homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily . Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).",
      "summary_en": "PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.",
      "summary_zh": "PyAGC 提出了一个面向属性图聚类的生产就绪基准与库，通过可扩展、内存高效的实现和全面的评估协议，解决了当前研究的局限性。",
      "hf_url": "https://huggingface.co/papers/2602.08519",
      "arxiv_url": "https://arxiv.org/abs/2602.08519",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08519",
      "github_url": "https://github.com/Cloudy1225/PyAGC",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:21:27.360331+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07918",
      "title": "CausalArmor: Efficient Indirect Prompt Injection Guardrails via Causal Attribution",
      "authors": [
        "Minbeom Kim",
        "Mihir Parmar",
        "Phillip Wallis",
        "Lesly Miculicich",
        "Kyomin Jung",
        "Krishnamurthy Dj Dvijotham",
        "Long T. Le",
        "Tomas Pfister"
      ],
      "abstract": "CausalArmor is a selective defense framework for AI agents that uses causal ablation to detect and mitigate Indirect Prompt Injection attacks by identifying dominant untrusted segments and applying targeted sanitization. AI agents equipped with tool-calling capabilities are susceptible to Indirect Prompt Injection (IPI) attacks. In this attack scenario, malicious commands hidden within untrusted content trick the agent into performing unauthorized actions. Existing defenses can reduce attack success but often suffer from the over-defense dilemma: they deploy expensive, always-on sanitization regardless of actual threat, thereby degrading utility and latency even in benign scenarios. We revisit IPI through a causal ablation perspective: a successful injection manifests as a dominance shift where the user request no longer provides decisive support for the agent's privileged action, while a particular untrusted segment, such as a retrieved document or tool output, provides disproportionate attributable influence. Based on this signature, we propose CausalArmor, a selective defense framework that (i) computes lightweight, leave-one-out ablation -based attribution s at privileged decision points , and (ii) triggers targeted sanitization only when an untrusted segment dominates the user intent. Additionally, CausalArmor employs retroactive Chain-of-Thought masking to prevent the agent from acting on ``poisoned'' reasoning traces. We present a theoretical analysis showing that sanitization based on attribution margins conditionally yields an exponentially small upper bound on the probability of selecting malicious actions. Experiments on AgentDojo and DoomArena demonstrate that CausalArmor matches the security of aggressive defenses while improving explainability and preserving utility and latency of AI agents.",
      "summary_en": "CausalArmor is a selective defense framework for AI agents that uses causal ablation to detect and mitigate Indirect Prompt Injection attacks by identifying dominant untrusted segments and applying targeted sanitization.",
      "summary_zh": "CausalArmor是一种面向AI智能体的选择性防御框架，利用因果消融技术检测和缓解间接提示注入攻击，通过识别主导性不可信片段并实施定向净化。",
      "hf_url": "https://huggingface.co/papers/2602.07918",
      "arxiv_url": "https://arxiv.org/abs/2602.07918",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07918",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:21:12.327747+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.07670",
      "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation",
      "authors": [
        "Jarrod Barnes"
      ],
      "abstract": "Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.",
      "summary_en": "Test-time training fails in verification-grounded tasks due to over-sharpening, while surprisal-guided selection improves performance by favoring diverse, low-confidence samples.",
      "summary_zh": "测试时训练因过度锐化在基于验证的任务中失效，而惊奇值引导的选择通过偏好多样化低置信样本提升性能。",
      "hf_url": "https://huggingface.co/papers/2602.07670",
      "arxiv_url": "https://arxiv.org/abs/2602.07670",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07670",
      "github_url": "https://github.com/jbarnes850/test-time-training",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:21:05.658756+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04908",
      "title": "Temporal Pair Consistency for Variance-Reduced Flow Matching",
      "authors": [
        "Chika Maduabuchi",
        "Jindong Wang"
      ],
      "abstract": "Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures. Continuous-time generative models , such as diffusion models , flow matching , and rectified flow , learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective . Instantiated within flow matching , TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow .",
      "summary_en": "Temporal Pair Consistency reduces variance in continuous-time generative models by coupling velocity predictions at paired timesteps, improving sample quality and efficiency without altering model architecture or training procedures.",
      "summary_zh": "时序配对一致性通过在配对时间步耦合速度预测来降低连续时间生成模型的方差，在不改变模型架构或训练流程的情况下提升样本质量与效率。",
      "hf_url": "https://huggingface.co/papers/2602.04908",
      "arxiv_url": "https://arxiv.org/abs/2602.04908",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04908",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:36.725890+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04802",
      "title": "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?",
      "authors": [
        "Qing'an Liu",
        "Juntong Feng",
        "Yuhao Wang",
        "Xinzhe Han",
        "Yujie Cheng",
        "Yue Zhu",
        "Haiwen Diao",
        "Yunzhi Zhuge",
        "Huchuan Lu"
      ],
      "abstract": "VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations. Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries . In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception , reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap : models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text . This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.",
      "summary_en": "VISTA-Bench evaluates vision-language models' ability to understand visualized text versus pure-text queries, revealing significant performance gaps and sensitivity to rendering variations.",
      "summary_zh": "VISTA-Bench评估视觉语言模型理解可视化文本与纯文本查询的能力，揭示出显著的性能差距以及对渲染变化的敏感性。",
      "hf_url": "https://huggingface.co/papers/2602.04802",
      "arxiv_url": "https://arxiv.org/abs/2602.04802",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04802",
      "github_url": "https://github.com/QingAnLiu/VISTA-Bench",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:34.778179+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.04521",
      "title": "C-ΔΘ: Circuit-Restricted Weight Arithmetic for Selective Refusal",
      "authors": [
        "Aditya Kasliwal",
        "Pratinav Seth",
        "Vinay Kumar Sankarapu"
      ],
      "abstract": "Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance. Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Δθ: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ΔθC supported only on that circuit (typically <5% of parameters). Applying ΔθC yields a drop-in edited checkpoint with no inference-time hooks , shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.",
      "summary_en": "Offline selective refusal in large language models is achieved through circuit-restricted weight updates that eliminate runtime intervention costs while maintaining performance.",
      "summary_zh": "大语言模型的离线选择性拒绝通过电路受限权重更新实现，在消除运行时干预成本的同时保持性能。",
      "hf_url": "https://huggingface.co/papers/2602.04521",
      "arxiv_url": "https://arxiv.org/abs/2602.04521",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04521",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:32.675564+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2602.01725",
      "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
      "authors": [
        "Yurun Chen",
        "Zeyi Liao",
        "Ping Yin",
        "Taotao Xie",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "abstract": "SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments. With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach , with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction : by using safety policies as the basis for risk prediction , SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization : translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning . Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
      "summary_en": "SafePred is a predictive guardrail framework for computer-using agents that uses risk prediction and decision optimization to prevent both immediate and delayed high-risk consequences in complex environments.",
      "summary_zh": "SafePred是一种面向计算机使用智能体的预测性护栏框架，利用风险预测和决策优化来防止复杂环境中的即时和延迟高风险后果。",
      "hf_url": "https://huggingface.co/papers/2602.01725",
      "arxiv_url": "https://arxiv.org/abs/2602.01725",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01725",
      "github_url": "https://github.com/YurunChen/SafePred",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:25.691666+00:00"
    },
    {
      "date": "2026-02-11",
      "paper_id": "2601.21235",
      "title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
      "authors": [
        "Alok Abhishek",
        "Tushar Bandopadhyay",
        "Lisa Erickson"
      ],
      "abstract": "Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics. Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior . This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias , fairness , ethics , and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk . The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility . Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling.",
      "summary_en": "Large language models exhibit varying levels of social risk across multiple dimensions, with significant differences in worst-case behavior that cannot be captured by traditional scalar evaluation metrics.",
      "summary_zh": "大语言模型在多个维度上表现出不同程度的社会风险，其最坏情况行为存在显著差异，而传统标量评估指标无法捕捉这些差异。",
      "hf_url": "https://huggingface.co/papers/2601.21235",
      "arxiv_url": "https://arxiv.org/abs/2601.21235",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21235",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T06:20:17.074995+00:00"
    }
  ]
}