{
  "date": "2026-02-25",
  "count": 32,
  "papers": [
    {
      "date": "2026-02-25",
      "paper_id": "2602.21193",
      "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
      "authors": [
        "Renjie Pi",
        "Grace Lam",
        "Mohammad Shoeybi",
        "Pooya Jannaty",
        "Bryan Catanzaro",
        "Wei Ping"
      ],
      "abstract": "Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests. Despite rapid recent progress in the terminal capabilities of large language models , the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents , making two key contributions: (1) Terminal-Task-Gen , a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning , long context training , and scaling behavior . Our pipeline yields Terminal-Corpus , a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal , a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0 : Nemotron-Terminal -8B improves from 2.5% to 13.0% Nemotron-Terminal -14B improves from 4.0% to 20.2%, and Nemotron-Terminal -32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/ nemotron-terminal .",
      "summary_en": "Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.",
      "summary_zh": "研究人员开发了合成任务生成流程，并分析数据策略以提升终端 agent 性能，构建了大规模数据集与模型，在基准测试中超越了规模更大的同类模型。",
      "hf_url": "https://huggingface.co/papers/2602.21193",
      "arxiv_url": "https://arxiv.org/abs/2602.21193",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21193",
      "github_url": "",
      "upvotes": 64,
      "fetched_at": "2026-02-26T01:57:21.980776+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.12192",
      "title": "Query-focused and Memory-aware Reranker for Long Context Processing",
      "authors": [
        "Yuqing Li",
        "Jiangnan Li",
        "Mo Yu",
        "Guoxuan Ding",
        "Zheng Lin",
        "Weiping Wang",
        "Jie Zhou"
      ],
      "abstract": "A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks. Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores , enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision . Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage . We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.",
      "summary_en": "A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.",
      "summary_zh": "轻量级重排序框架使用选定头的注意力分数估计段落-查询相关性，在多个领域和基准测试上取得了良好性能。",
      "hf_url": "https://huggingface.co/papers/2602.12192",
      "arxiv_url": "https://arxiv.org/abs/2602.12192",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12192",
      "github_url": "",
      "upvotes": 35,
      "fetched_at": "2026-02-26T01:56:52.921629+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20739",
      "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
      "authors": [
        "Shitian Zhao",
        "Shaoheng Lin",
        "Ming Li",
        "Haoquan Zhang",
        "Wenshuo Peng",
        "Kaipeng Zhang",
        "Chen Wei"
      ],
      "abstract": "PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies. Reinforcement learning for agentic multimodal models often suffers from interaction collapse , where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline , we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction , selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage . Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
      "summary_en": "PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.",
      "summary_zh": "PyVision-RL框架通过增强的强化学习技术和高效的视频处理策略，解决了多模态模型中的交互崩溃问题。",
      "hf_url": "https://huggingface.co/papers/2602.20739",
      "arxiv_url": "https://arxiv.org/abs/2602.20739",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20739",
      "github_url": "https://github.com/agents-x-project/PyVision-RL",
      "upvotes": 22,
      "fetched_at": "2026-02-26T01:57:11.212872+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21204",
      "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
      "authors": [
        "Junchen Liu",
        "Sven Elflein",
        "Or Litany",
        "Zan Gojcic",
        "Ruilong Li"
      ],
      "abstract": "Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency. Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications , admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity .",
      "summary_en": "Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.",
      "summary_zh": "测试时训练被重新诠释为学习到的线性注意力而非记忆，带来了架构简化和效率提升。",
      "hf_url": "https://huggingface.co/papers/2602.21204",
      "arxiv_url": "https://arxiv.org/abs/2602.21204",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21204",
      "github_url": "",
      "upvotes": 21,
      "fetched_at": "2026-02-26T01:57:28.371504+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21015",
      "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
      "authors": [
        "Yuhao Wu",
        "Maojia Song",
        "Yihuai Lan",
        "Lei Wang",
        "Zhiqiang Hu",
        "Yao Xiao",
        "Heng Zhou",
        "Weihua Zheng",
        "Dylan Raharja",
        "Soujanya Poria",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints. Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D , physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints . CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints , often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
      "summary_en": "Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.",
      "summary_zh": "现有视觉-语言模型缺乏理解物理结构与因果约束的能力，难以胜任复杂的交互式3D任务，正如CHAIN基准在评估受物理约束的结构化动作规划时所证明的那样。",
      "hf_url": "https://huggingface.co/papers/2602.21015",
      "arxiv_url": "https://arxiv.org/abs/2602.21015",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21015",
      "github_url": "https://github.com/Social-AI-Studio/CHAIN",
      "upvotes": 21,
      "fetched_at": "2026-02-26T01:57:16.704167+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21202",
      "title": "Multi-Vector Index Compression in Any Modality",
      "authors": [
        "Hanxiang Qin",
        "Alexander Martin",
        "Rohan Jha",
        "Chunsheng Zuo",
        "Reno Kriz",
        "Benjamin Van Durme"
      ],
      "abstract": "Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities. We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression : sequence resizing , memory tokens , hierarchical pooling , and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods ( sequence resizing and memory tokens ), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.",
      "summary_en": "Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.",
      "summary_zh": "用于后期交互检索任务中多向量文档表示压缩的注意力引导聚类方法，在文本、视觉文档和视频模态上相比其他压缩技术表现出更优性能。",
      "hf_url": "https://huggingface.co/papers/2602.21202",
      "arxiv_url": "https://arxiv.org/abs/2602.21202",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21202",
      "github_url": "https://github.com/hanxiangqin/omni-col-press",
      "upvotes": 18,
      "fetched_at": "2026-02-26T01:57:27.371290+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20951",
      "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
      "authors": [
        "Jaehyun Park",
        "Minyoung Ahn",
        "Minkyu Kim",
        "Jonghyun Lee",
        "Jae-Gil Lee",
        "Dongmin Park"
      ],
      "abstract": "ArtiAgent automates the creation of real-artifact image pairs using three agents for perception, synthesis, and curation in diffusion transformers. Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets . In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer , and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.",
      "summary_en": "ArtiAgent automates the creation of real-artifact image pairs using three agents for perception, synthesis, and curation in diffusion transformers.",
      "summary_zh": "ArtiAgent通过感知、合成与筛选三个智能体，在扩散Transformer中自动化创建真实-伪影图像对。",
      "hf_url": "https://huggingface.co/papers/2602.20951",
      "arxiv_url": "https://arxiv.org/abs/2602.20951",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20951",
      "github_url": "https://github.com/krafton-ai/ArtiAgent",
      "upvotes": 12,
      "fetched_at": "2026-02-26T01:57:16.081329+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.18940",
      "title": "DREAM: Deep Research Evaluation with Agentic Metrics",
      "authors": [
        "Elad Ben Avraham",
        "Changhao Li",
        "Ron Dorfman",
        "Roy Ganz",
        "Oren Nuriel",
        "Amir Dudai",
        "Aviad Aberdam",
        "Noah Flynn",
        "Elman Mansimov",
        "Adi Kalyanpur",
        "Ron Litman"
      ],
      "abstract": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm. Deep Research Agents generate analyst-grade reports , yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis , where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness . To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics ), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent , enabling temporally aware coverage , grounded verification , and systematic reasoning probes . Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
      "summary_en": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.",
      "summary_zh": "深度研究智能体能够生成分析师级别的报告，然而由于缺乏单一的真值（ground truth）以及研究质量的多维特性，对其进行评估仍然具有挑战性。近期的基准测试提出了不同的方法论，但它们存在\"合成幻觉\"（Mirage of Synthesis）问题：表面上的流畅性和引用对齐可能掩盖潜在的事实性和推理性缺陷。我们通过引入一个涵盖四个维度的分类体系来刻画这一差距，该体系揭示了一个关键的能力不匹配：静态评估器本质上缺乏评估时效性（temporal validity）和事实正确性所需的工具使用能力。为解决这一问题，我们提出了DREAM（Deep Research Evaluation with Agentic Metrics），该框架通过使评估本身智能体化（agentic）来实例化能力对等（capability parity）原则。DREAM通过一种评估协议来构建评估流程，该协议结合查询无关指标（query-agnostic metrics）与由工具调用智能体生成的自适应指标（adaptive metrics），从而实现时间感知覆盖（temporally aware coverage）、基于依据的验证（grounded verification）以及系统性推理探针（reasoning probes）。对照评估表明，DREAM对事实性和时效性衰减的敏感度显著高于现有基准，提供了一种可扩展的、无需参考（reference-free）的评估范式。",
      "hf_url": "https://huggingface.co/papers/2602.18940",
      "arxiv_url": "https://arxiv.org/abs/2602.18940",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18940",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-26T01:57:01.849129+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.14337",
      "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces",
      "authors": [
        "Yukang Feng",
        "Jianwen Sun",
        "Zelai Yang",
        "Jiaxin Ai",
        "Chuanhao Li",
        "Zizhen Li",
        "Fanrui Zhang",
        "Kang He",
        "Rui Ma",
        "Jifan Lin",
        "Jie Sun",
        "Yang Xiao",
        "Sizhuo Zhou",
        "Wenxiao Wu",
        "Yiming Liu",
        "Pengfei Liu",
        "Yu Qiao",
        "Shenglin Zhang",
        "Kaipeng Zhang"
      ],
      "abstract": "LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights. Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces , however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.",
      "summary_en": "LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.",
      "summary_zh": "LongCLI-Bench 评估 AI 智能体通过命令行界面完成复杂多步编程任务的能力，并提供详细的失败分析以及人机协作方面的洞察。",
      "hf_url": "https://huggingface.co/papers/2602.14337",
      "arxiv_url": "https://arxiv.org/abs/2602.14337",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.14337",
      "github_url": "https://github.com/finyorko/longcli-bench",
      "upvotes": 10,
      "fetched_at": "2026-02-26T01:56:54.435222+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20309",
      "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models",
      "authors": [
        "Jingxuan Zhang",
        "Yunta Hsieh",
        "Zhongwei Wang",
        "Haokun Lin",
        "Xin Wang",
        "Ziqi Wang",
        "Yingtie Lei",
        "Mi Zhang"
      ],
      "abstract": "QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency. Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching , a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing , a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer , and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency , providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.",
      "summary_en": "QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.",
      "summary_zh": "QuantVLA是一种面向视觉-语言-动作模型的训练后量化框架，通过选择性量化、注意力温度匹配和输出头平衡，在保持性能的同时减少内存占用和延迟，实现高效部署。",
      "hf_url": "https://huggingface.co/papers/2602.20309",
      "arxiv_url": "https://arxiv.org/abs/2602.20309",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20309",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-26T01:57:06.317546+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.16990",
      "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation",
      "authors": [
        "Yan Wang",
        "Yi Han",
        "Lingfei Qian",
        "Yueru He",
        "Xueqing Peng",
        "Dongji Feng",
        "Zhuohan Xie",
        "Vincent Jim Zhang",
        "Rosie Guo",
        "Fengran Mo",
        "Jimin Huang",
        "Yankai Chen",
        "Xue Liu",
        "Jian-Yun Nie"
      ],
      "abstract": "A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories. Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory , however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality . We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences , enabling diagnosis of whether an LLM follows rational analysis , mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.",
      "summary_en": "A new conversational financial recommendation benchmark evaluates large language models' ability to balance rational decision-making with user behavior alignment using multi-view references derived from real market data and human decision trajectories.",
      "summary_zh": "一个新的对话式金融推荐基准利用源自真实市场数据和人类决策轨迹的多视角参考，评估大语言模型平衡理性决策与用户行为对齐的能力。",
      "hf_url": "https://huggingface.co/papers/2602.16990",
      "arxiv_url": "https://arxiv.org/abs/2602.16990",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16990",
      "github_url": "https://github.com/The-FinAI/Conv-FinRe",
      "upvotes": 8,
      "fetched_at": "2026-02-26T01:56:59.403134+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.16745",
      "title": "PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency",
      "authors": [
        "Zhangyi Liu",
        "Huaizhi Qu",
        "Xiaowei Yin",
        "He Sun",
        "Yanjun Han",
        "Tianlong Chen",
        "Zhun Deng"
      ],
      "abstract": "Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy. Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories . However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate , a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime , where all questions are known in advance, we connect trajectory allocation to crowdsourcing , a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime , where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.",
      "summary_en": "Principled and efficient test-time self-consistency method improves model performance by optimizing trajectory allocation through an optimization framework that reduces sampling requirements while maintaining accuracy.",
      "summary_zh": "原则性且高效的测试时自一致性方法通过优化框架优化轨迹分配，在减少采样需求的同时保持准确性，从而提升模型性能。",
      "hf_url": "https://huggingface.co/papers/2602.16745",
      "arxiv_url": "https://arxiv.org/abs/2602.16745",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16745",
      "github_url": "https://github.com/ZDCSlab/PETS",
      "upvotes": 5,
      "fetched_at": "2026-02-26T01:56:55.900762+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21198",
      "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
      "authors": [
        "Yining Hong",
        "Huang Huang",
        "Manling Li",
        "Li Fei-Fei",
        "Jiajun Wu",
        "Yejin Choi"
      ],
      "abstract": "Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance. Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning , which integrates two modes of reflection: reflection-in-action , where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and reflection-on-action , which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection , allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action . Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
      "summary_en": "Reflective Test-Time Planning enhances robot decision-making by integrating multiple reflection mechanisms that enable learning from experience and improving long-horizon task performance.",
      "summary_zh": "Reflective Test-Time Planning 通过整合多种反思机制来增强机器人决策，这些机制支持从经验中学习并提升长程任务性能。",
      "hf_url": "https://huggingface.co/papers/2602.21198",
      "arxiv_url": "https://arxiv.org/abs/2602.21198",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21198",
      "github_url": "https://github.com/Reflective-Test-Time-Planning/Reflective-Test-Time-Planning",
      "upvotes": 4,
      "fetched_at": "2026-02-26T01:57:24.371612+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20945",
      "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
      "authors": [
        "Taiqiang Wu",
        "Zenan Zu",
        "Bo Zhou",
        "Ngai Wong"
      ],
      "abstract": "Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping. Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budget s ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement . After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping , and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse . Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization .",
      "summary_en": "Large language models benefit from scaled chain-of-thought reasoning through efficient training methods that balance trajectory length and accuracy using reinforcement learning with reward shaping.",
      "summary_zh": "大语言模型通过高效的训练方法从规模化思维链推理中获益，这些方法利用带奖励塑造的强化学习来平衡轨迹长度和准确性。",
      "hf_url": "https://huggingface.co/papers/2602.20945",
      "arxiv_url": "https://arxiv.org/abs/2602.20945",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20945",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-26T01:57:15.203855+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20731",
      "title": "Communication-Inspired Tokenization for Structured Image Representations",
      "authors": [
        "Aram Davtyan",
        "Yusuf Sahin",
        "Yasaman Haghighi",
        "Sebastian Stapf",
        "Pablo Acuaviva",
        "Alexandre Alahi",
        "Paolo Favaro"
      ],
      "abstract": "COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization. Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.",
      "summary_en": "COMiT framework learns structured discrete visual tokens through iterative encoding and flow-matching decoding, improving object-centric representation and compositional generalization.",
      "summary_zh": "COMiT 框架通过迭代编码和流匹配解码学习结构化离散视觉令牌，改进了以对象为中心的表征和组合泛化。",
      "hf_url": "https://huggingface.co/papers/2602.20731",
      "arxiv_url": "https://arxiv.org/abs/2602.20731",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20731",
      "github_url": "https://github.com/araachie/comit",
      "upvotes": 4,
      "fetched_at": "2026-02-26T01:57:10.127428+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.19633",
      "title": "TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents",
      "authors": [
        "Jongwon Jeong",
        "Jungtaek Kim",
        "Kangwook Lee"
      ],
      "abstract": "TAPE framework improves language model agent performance in complex environments through enhanced planning and constrained execution strategies. Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks , identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path . During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re- planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.",
      "summary_en": "TAPE framework improves language model agent performance in complex environments through enhanced planning and constrained execution strategies.",
      "summary_zh": "TAPE框架通过增强的规划与受约束的执行策略，提升语言模型智能体在复杂环境中的性能。",
      "hf_url": "https://huggingface.co/papers/2602.19633",
      "arxiv_url": "https://arxiv.org/abs/2602.19633",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19633",
      "github_url": "https://github.com/UW-Madison-Lee-Lab/TAPE",
      "upvotes": 4,
      "fetched_at": "2026-02-26T01:57:04.854243+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21196",
      "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
      "authors": [
        "Ravi Ghadia",
        "Maksim Abraham",
        "Sergei Vorobyov",
        "Max Ryabinin"
      ],
      "abstract": "UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed. Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism . The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses , enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading , can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention , breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8timesH100 node, improving upon prior methods by over 25%.",
      "summary_en": "UPipe enables efficient processing of long sequences in Transformer models through fine-grained chunking at the attention head level, significantly reducing activation memory usage while maintaining training speed.",
      "summary_zh": "UPipe 通过在注意力头级别进行细粒度分块，实现了 Transformer 模型中长序列的高效处理，在显著降低激活内存占用的同时保持了训练速度。",
      "hf_url": "https://huggingface.co/papers/2602.21196",
      "arxiv_url": "https://arxiv.org/abs/2602.21196",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21196",
      "github_url": "https://github.com/togethercomputer/Untied-Ulysses",
      "upvotes": 3,
      "fetched_at": "2026-02-26T01:57:23.083392+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21185",
      "title": "The Diffusion Duality, Chapter II: Ψ-Samplers and Efficient Curriculum",
      "authors": [
        "Justin Deschenaux",
        "Caglar Gulcehre",
        "Subham Sekhar Sahoo"
      ],
      "abstract": "Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling. Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID / IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2",
      "summary_en": "Discrete diffusion models with predictor-corrector samplers surpass traditional methods in generation quality and efficiency, challenging assumptions about masked diffusion's necessity in language modeling.",
      "summary_zh": "采用预测-校正采样器的离散扩散模型在生成质量和效率方面超越传统方法，对语言建模中掩码扩散必要性的假设提出挑战。",
      "hf_url": "https://huggingface.co/papers/2602.21185",
      "arxiv_url": "https://arxiv.org/abs/2602.21185",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21185",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-26T01:57:20.566629+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20424",
      "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say",
      "authors": [
        "Ved Sirdeshmukh",
        "Marc Wetter"
      ],
      "abstract": "AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.",
      "summary_en": "AI agents struggle to interpret implicitly specified real-world requests that require contextual reasoning beyond explicit instructions, as demonstrated by an evaluation framework using interactive YAML-defined worlds.",
      "summary_zh": "AI 智能体难以解读需要超出显式指令的上下文推理的隐式指定现实世界请求，正如使用交互式 YAML 定义世界的评估框架所展示的那样。",
      "hf_url": "https://huggingface.co/papers/2602.20424",
      "arxiv_url": "https://arxiv.org/abs/2602.20424",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20424",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-26T01:57:07.700262+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.18998",
      "title": "Benchmark Test-Time Scaling of General LLM Agents",
      "authors": [
        "Xiaochuan Li",
        "Ryan Ming",
        "Pranav Setlur",
        "Abhijay Paladugu",
        "Andy Tang",
        "Hao Kang",
        "Shuai Shao",
        "Rong Jin",
        "Chenyan Xiong"
      ],
      "abstract": "General AgentBench evaluates large language model agents across multiple domains and scaling methods, revealing performance degradation and fundamental limitations in sequential and parallel scaling approaches. LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment . We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling . Code is publicly available at https://github.com/cxcscmu/General-AgentBench.",
      "summary_en": "General AgentBench evaluates large language model agents across multiple domains and scaling methods, revealing performance degradation and fundamental limitations in sequential and parallel scaling approaches.",
      "summary_zh": "General AgentBench 在多个领域和扩展方法下对大型语言模型智能体进行了评估，揭示了顺序和并行扩展方法中的性能下降与根本性局限。",
      "hf_url": "https://huggingface.co/papers/2602.18998",
      "arxiv_url": "https://arxiv.org/abs/2602.18998",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18998",
      "github_url": "https://github.com/cxcscmu/General-AgentBench",
      "upvotes": 3,
      "fetched_at": "2026-02-26T01:57:02.704916+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.16932",
      "title": "RankEvolve: Automating the Discovery of Retrieval Algorithms via LLM-Driven Evolution",
      "authors": [
        "Jinming Nian",
        "Fangchen Li",
        "Dae Hoon Park",
        "Yi Fang"
      ],
      "abstract": "Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.",
      "summary_en": "Large language models guided by evaluators and evolutionary search can automatically discover improved lexical retrieval algorithms through program evolution techniques.",
      "summary_zh": "由评估器与进化搜索引导的大型语言模型可通过程序进化技术自动发现改进的词汇检索算法。",
      "hf_url": "https://huggingface.co/papers/2602.16932",
      "arxiv_url": "https://arxiv.org/abs/2602.16932",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16932",
      "github_url": "https://github.com/fangchenli/ranking-evolved",
      "upvotes": 3,
      "fetched_at": "2026-02-26T01:56:58.385878+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.16813",
      "title": "One-step Language Modeling via Continuous Denoising",
      "authors": [
        "Chanhyuk Lee",
        "Jaehoon Yoo",
        "Manan Agarwal",
        "Sheel Shah",
        "Jerry Huang",
        "Aditi Raghunathan",
        "Seunghoon Hong",
        "Nicholas M. Boffi",
        "Jinwoo Kim"
      ],
      "abstract": "Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization. Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities , we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings . We show that the model can be trained by predicting the clean data via a cross entropy objective , where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation . On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities , and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.",
      "summary_en": "Flow-based language models outperform discrete diffusion models in both quality and speed by using Euclidean denoising over one-hot token encodings with improved training stability through time reparameterization.",
      "summary_zh": "基于流的语言模型在质量和速度上均优于离散扩散模型，其方法是对 one-hot 词元编码进行欧几里得去噪，并通过时间重参数化提升训练稳定性。",
      "hf_url": "https://huggingface.co/papers/2602.16813",
      "arxiv_url": "https://arxiv.org/abs/2602.16813",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16813",
      "github_url": "https://github.com/david3684/flm",
      "upvotes": 3,
      "fetched_at": "2026-02-26T01:56:57.322853+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21201",
      "title": "Aletheia tackles FirstProof autonomously",
      "authors": [
        "Tony Feng",
        "Junehyuk Jung",
        "Sang-hyun Kim",
        "Carlo Pagano",
        "Sergei Gukov",
        "Chiang-Chiang Tsai",
        "David Woodruff",
        "Adel Javanmard",
        "Aryan Mokhtari",
        "Dawsen Hwang",
        "Yuri Chervonyi",
        "Jonathan N. Lee",
        "Garrett Bingham",
        "Trieu H. Trinh",
        "Vahab Mirrokni",
        "Quoc V. Le",
        "Thang Luong"
      ],
      "abstract": "",
      "summary_en": "",
      "summary_zh": "",
      "hf_url": "https://huggingface.co/papers/2602.21201",
      "arxiv_url": "https://arxiv.org/abs/2602.21201",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21201",
      "github_url": "https://github.com/google-deepmind/superhuman/tree/main/aletheia",
      "upvotes": 2,
      "fetched_at": "2026-02-26T01:57:25.860639+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20792",
      "title": "SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking",
      "authors": [
        "Muhammad Saif Ullah Khan",
        "Didier Stricker"
      ],
      "abstract": "A biomechanics-aware keypoint simulation framework and the first open dataset, SIMSPINE, provide anatomically consistent 3D spinal annotations for natural full-body motions, enabling data-driven learning of vertebral kinematics and improving spine motion estimation accuracy. Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling . Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors , monocular 3D pose lifting models , and multi-view reconstruction pipelines , establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.",
      "summary_en": "A biomechanics-aware keypoint simulation framework and the first open dataset, SIMSPINE, provide anatomically consistent 3D spinal annotations for natural full-body motions, enabling data-driven learning of vertebral kinematics and improving spine motion estimation accuracy.",
      "summary_zh": "一种生物力学感知的关键点模拟框架及首个开放数据集SIMSPINE，为自然全身运动提供了解剖学一致的三维脊柱标注，实现了椎体运动学的数据驱动学习，并提升了脊柱运动估计精度。",
      "hf_url": "https://huggingface.co/papers/2602.20792",
      "arxiv_url": "https://arxiv.org/abs/2602.20792",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20792",
      "github_url": "https://github.com/dfki-av/simspine",
      "upvotes": 2,
      "fetched_at": "2026-02-26T01:57:12.873560+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21053",
      "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
      "authors": [
        "Shimin Wen",
        "Zeyu Zhang",
        "Xingdou Bian",
        "Hongjie Zhu",
        "Lulu He",
        "Layi Shama",
        "Daji Ergu",
        "Ying Cai"
      ],
      "abstract": "A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks. Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms , making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection . This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection , then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning . Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
      "summary_en": "A novel iterative self-correction framework enhances vision-language models' reasoning robustness through capability reflection and memory reflection mechanisms, achieving superior performance on visual understanding benchmarks.",
      "summary_zh": "一种新颖的迭代式自我修正框架通过能力反思与记忆反思机制增强视觉语言模型的推理鲁棒性，在视觉理解基准测试中取得更优性能。",
      "hf_url": "https://huggingface.co/papers/2602.21053",
      "arxiv_url": "https://arxiv.org/abs/2602.21053",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21053",
      "github_url": "https://github.com/AIGeeksGroup/OCR-Agent",
      "upvotes": 1,
      "fetched_at": "2026-02-26T01:57:19.498395+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.21042",
      "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
      "authors": [
        "Bonan Liu",
        "Zeyu Zhang",
        "Bingbing Meng",
        "Han Wang",
        "Hanshuo Zhang",
        "Chengping Wang",
        "Daji Ergu",
        "Ying Cai"
      ],
      "abstract": "OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings. Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation ( Dynamic LoRA ) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training , achieving state-of-the-art accuracy with superior parameter efficiency , and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
      "summary_en": "OmniOCR presents a universal framework for ethnic minority scripts using Dynamic LoRA and sparsity regularization to achieve state-of-the-art accuracy with improved parameter efficiency in low-resource settings.",
      "summary_zh": "OmniOCR 提出了一种面向少数民族文字的通用框架，利用 Dynamic LoRA 和稀疏正则化，在低资源场景下实现了最先进的准确率，并提升了参数效率。",
      "hf_url": "https://huggingface.co/papers/2602.21042",
      "arxiv_url": "https://arxiv.org/abs/2602.21042",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.21042",
      "github_url": "https://github.com/AIGeeksGroup/OmniOCR",
      "upvotes": 1,
      "fetched_at": "2026-02-26T01:57:18.225293+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20743",
      "title": "Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization",
      "authors": [
        "Gabriel Loiseau",
        "Damien Sileo",
        "Damien Riquet",
        "Maxime Meyer",
        "Marc Tommasi"
      ],
      "abstract": "Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints. Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization , a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models , enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models , with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.",
      "summary_en": "Adaptive text anonymization framework automatically adjusts anonymization strategies based on privacy-utility requirements using prompt optimization for language models across diverse domains and constraints.",
      "summary_zh": "自适应文本匿名化框架基于隐私-效用需求，利用提示优化针对语言模型，跨不同领域和约束自动调整匿名化策略。",
      "hf_url": "https://huggingface.co/papers/2602.20743",
      "arxiv_url": "https://arxiv.org/abs/2602.20743",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20743",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-26T01:57:12.096564+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.19020",
      "title": "Learning to Detect Language Model Training Data via Active Reconstruction",
      "authors": [
        "Junjie Oscar Yin",
        "John X. Morris",
        "Vitaly Shmatikov",
        "Sewon Min",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks. Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce Active Data Reconstruction Attack (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are more reconstructible than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards . The resulting algorithms, ADRA and its adaptive variant ADRA+, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training , post-training , and distillation data , with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.",
      "summary_en": "Active Data Reconstruction Attack uses reinforcement learning to identify training data by measuring the reconstructibility of text from model behavior, outperforming existing membership inference attacks.",
      "summary_zh": "主动数据重建攻击利用强化学习，通过度量文本从模型行为中的可重建性来识别训练数据，性能优于现有的成员推理攻击。",
      "hf_url": "https://huggingface.co/papers/2602.19020",
      "arxiv_url": "https://arxiv.org/abs/2602.19020",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19020",
      "github_url": "https://github.com/oseyosey/MIA-RL",
      "upvotes": 1,
      "fetched_at": "2026-02-26T01:57:03.596155+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.18735",
      "title": "LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency",
      "authors": [
        "Weilong Yan",
        "Haipeng Li",
        "Hao Xu",
        "Nianjin Ye",
        "Yihao Ai",
        "Shuaicheng Liu",
        "Jingyu Hu"
      ],
      "abstract": "LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement. This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First, harnesses these powerful generative priors for completion through a complementary two-stage design : (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models . Third, we introduce Omni-Comp , a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at https://github.com/DavidYan2001/LaS-Comp{LaS-Comp}.",
      "summary_en": "LaS-Comp presents a zero-shot 3D shape completion method using 3D foundation models with a two-stage approach for faithful reconstruction and seamless boundary refinement.",
      "summary_zh": "LaS-Comp 提出了一种基于 3D 基础模型的零样本 3D 形状补全方法，采用两阶段策略实现忠实重建与无缝边界细化。",
      "hf_url": "https://huggingface.co/papers/2602.18735",
      "arxiv_url": "https://arxiv.org/abs/2602.18735",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18735",
      "github_url": "https://github.com/DavidYan2001/LaS-Comp",
      "upvotes": 1,
      "fetched_at": "2026-02-26T01:57:00.784954+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.16603",
      "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving",
      "authors": [
        "Chia-chi Hsieh",
        "Zan Zong",
        "Xinyang Chen",
        "Jianjiang Li",
        "Jidong Zhai",
        "Lijie Wen"
      ],
      "abstract": "FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems. The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase , where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge. In this paper, we propose FlowPrefill, a TTFT- goodput -optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency . To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption , which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling , which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6times compared to state-of-the-art systems while satisfying heterogeneous SLOs.",
      "summary_en": "FlowPrefill addresses head-of-line blocking in large language model serving by decoupling preemption granularity from scheduling frequency through operator-level preemption and event-driven scheduling, achieving up to 5.6x better goodput than existing systems.",
      "summary_zh": "FlowPrefill 通过算子级抢占和事件驱动调度解耦抢占粒度与调度频率，解决大语言模型服务中的队头阻塞问题，较现有系统 goodput 最高提升 5.6 倍。",
      "hf_url": "https://huggingface.co/papers/2602.16603",
      "arxiv_url": "https://arxiv.org/abs/2602.16603",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16603",
      "github_url": "https://github.com/HSIEHCHIACHI/FlowPrefill",
      "upvotes": 1,
      "fetched_at": "2026-02-26T01:56:55.131280+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20903",
      "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
      "authors": [
        "Hanshen Zhu",
        "Yuliang Liu",
        "Xuecheng Wu",
        "An-Lan Wang",
        "Hao Feng",
        "Dingkang Yang",
        "Chao Feng",
        "Can Huang",
        "Jingqun Tang",
        "Xiang Bai"
      ],
      "abstract": "TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity. Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation , where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies , creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker , a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
      "summary_en": "TextPecker enhances visual text rendering by addressing structural anomalies through a reinforcement learning approach that improves text-to-image generation quality and fidelity.",
      "summary_zh": "TextPecker 通过强化学习方法解决结构异常，从而增强视觉文本渲染，提升文本到图像生成的质量与保真度。",
      "hf_url": "https://huggingface.co/papers/2602.20903",
      "arxiv_url": "https://arxiv.org/abs/2602.20903",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20903",
      "github_url": "https://github.com/CIawevy/TextPecker",
      "upvotes": 0,
      "fetched_at": "2026-02-26T01:57:14.187928+00:00"
    },
    {
      "date": "2026-02-25",
      "paper_id": "2602.20540",
      "title": "Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization",
      "authors": [
        "Minseop Kim",
        "Takhyeong Kim",
        "Taekhyun Park",
        "Hanbyeol Park",
        "Hyerim Bae"
      ],
      "abstract": "A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals. Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text , which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning . The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness.",
      "summary_en": "A collaborative framework integrating generative artificial intelligence with machine learning improves container dwell time prediction by standardizing unstructured text data, leading to reduced rehandling operations in container terminals.",
      "summary_zh": "一种整合生成式人工智能与机器学习的协作框架，通过标准化非结构化文本数据改进了集装箱滞留时间预测，进而减少了集装箱码头的倒箱作业。",
      "hf_url": "https://huggingface.co/papers/2602.20540",
      "arxiv_url": "https://arxiv.org/abs/2602.20540",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20540",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-26T01:57:09.150844+00:00"
    }
  ]
}