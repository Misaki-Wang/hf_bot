{
  "date": "2026-02-16",
  "count": 32,
  "papers": [
    {
      "date": "2026-02-16",
      "paper_id": "2602.10388",
      "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
      "authors": [
        "Zhongzhi Li",
        "Xuansheng Wu",
        "Yijiang Li",
        "Lijie Hu",
        "Ninghao Liu"
      ],
      "abstract": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures. The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance . In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following , toxicity detection , reward modeling , and behavior steering . Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer . Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.",
      "summary_en": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.",
      "summary_zh": "Feature Activation Coverage 在可解释的特征空间中衡量数据多样性，并实现多样性驱动的数据合成，从而提升跨多种语言模型架构的下游性能。",
      "hf_url": "https://huggingface.co/papers/2602.10388",
      "arxiv_url": "https://arxiv.org/abs/2602.10388",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.10388",
      "github_url": "https://github.com/Zhongzhi660/FAC-Synthesis",
      "upvotes": 202,
      "fetched_at": "2026-02-17T08:52:43.722760+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12783",
      "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise",
      "authors": [
        "Yuejie Li",
        "Ke Yang",
        "Yueying Hua",
        "Berlin Chen",
        "Jianhao Nie",
        "Yueping He",
        "Caixin Kang"
      ],
      "abstract": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.",
      "summary_en": "Existing spoken query retrieval evaluation datasets are limited to simple queries under constrained noise conditions, inadequately assessing robustness under complex acoustic perturbations. We introduce SQuTR, a benchmark aggregating 37,317 queries from six English and Chinese text retrieval datasets, synthesized using 200 real speaker voice profiles and mixed with 17 categories of real-world noise at controlled SNR levels. Experimental results show that retrieval performance degrades substantially with increasing noise across both cascaded and end-to-end systems, with large-scale models struggling under extreme conditions, indicating robustness remains a critical bottleneck. SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis to advance research on robustness in spoken query to text retrieval.",
      "summary_zh": "现有的语音查询检索评估数据集局限于约束噪声条件下的简单查询，无法充分评估复杂声学扰动下的鲁棒性。我们提出了 SQuTR，该基准聚合了来自六个英文和中文文本检索数据集的 37,317 个查询，使用 200 个真实说话人语音特征合成，并与 17 类真实世界噪声在受控 SNR 水平下混合。实验结果表明，在级联和端到端系统中，检索性能随噪声增加而显著下降，大规模模型在极端条件下表现不佳，表明鲁棒性仍然是一个关键瓶颈。SQuTR 提供了一个可复现的测试平台，用于基准测试和诊断分析，以推进语音查询到文本检索鲁棒性的研究。",
      "hf_url": "https://huggingface.co/papers/2602.12783",
      "arxiv_url": "https://arxiv.org/abs/2602.12783",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12783",
      "github_url": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval",
      "upvotes": 134,
      "fetched_at": "2026-02-17T08:53:09.405234+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12705",
      "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs",
      "authors": [
        "Baorong Shi",
        "Bo Cui",
        "Boyuan Jiang",
        "Deli Yu",
        "Fang Qian",
        "Haihua Yang",
        "Huichao Wang",
        "Jiale Chen",
        "Jianfei Pan",
        "Jieqiong Cao",
        "Jinghao Lin",
        "Kai Wu",
        "Lin Yang",
        "Shengsheng Yao",
        "Tao Chen",
        "Xiaojun Xiao",
        "Xiaozhong Ji",
        "Xu Wang",
        "Yijun He",
        "Zhixiong Yang"
      ],
      "abstract": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "summary_en": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.",
      "summary_zh": "MedXIAOHE是一种医疗视觉-语言基础模型，通过entity-aware持续预训练、强化学习和工具增强的agentic训练来增强临床理解能力，实现可靠的诊断推理。",
      "hf_url": "https://huggingface.co/papers/2602.12705",
      "arxiv_url": "https://arxiv.org/abs/2602.12705",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12705",
      "github_url": "",
      "upvotes": 56,
      "fetched_at": "2026-02-17T08:53:07.745124+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11858",
      "title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
      "authors": [
        "Lai Wei",
        "Liangbo He",
        "Jun Lan",
        "Lingzhong Dong",
        "Yutong Cai",
        "Siyuan Li",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Linghe Kong",
        "Yue Wang",
        "Zhuosheng Zhang",
        "Weiran Huang"
      ],
      "abstract": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks. Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception , where decisive evidence is small and easily overwhelmed by global context. Recent \" Thinking-with-Images \" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation , which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench , a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents . We further discuss when \" Thinking-with-Images \" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.",
      "summary_en": "Region-to-Image Distillation enables fine-grained visual perception in MLLMs by training models to internally perform iterative zooming during inference, eliminating the need for repeated tool calls and visual re-encoding while maintaining high performance across multiple benchmarks.",
      "summary_zh": "Region-to-Image Distillation 通过训练模型在推理过程中内部执行迭代缩放，使 MLLM 能够实现细粒度视觉感知，无需重复工具调用和视觉重新编码，同时在多个基准测试中保持高性能。",
      "hf_url": "https://huggingface.co/papers/2602.11858",
      "arxiv_url": "https://arxiv.org/abs/2602.11858",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11858",
      "github_url": "https://github.com/inclusionAI/Zooming-without-Zooming",
      "upvotes": 51,
      "fetched_at": "2026-02-17T08:52:52.455693+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.08683",
      "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
      "authors": [
        "Feilong Tang",
        "Xiang An",
        "Yunyao Yan",
        "Yin Xie",
        "Bin Qin",
        "Kaicheng Yang",
        "Yifei Shen",
        "Yuanhan Zhang",
        "Chunyuan Li",
        "Shikun Feng",
        "Changrui Chen",
        "Huajie Tan",
        "Ming Hu",
        "Manyuan Zhang",
        "Bo Li",
        "Ziyong Feng",
        "Ziwei Liu",
        "Zongyuan Ge",
        "Jiankang Deng"
      ],
      "abstract": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy. Hypothesis. Artificial general intelligence is, at its core, a compression problem . Effective compression demands resonance : deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information , the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification , OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts , jointly capturing object permanence and motion dynamics . Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM , it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data . Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
      "summary_en": "Visual understanding can be improved by aligning architectures with information-theoretic principles of video compression, using sparsity-driven encoding that outperforms traditional approaches in efficiency and accuracy.",
      "summary_zh": "视觉理解可通过将架构与视频压缩的信息论原理对齐，并采用在效率和准确性上均优于传统方法的稀疏驱动编码来提升。",
      "hf_url": "https://huggingface.co/papers/2602.08683",
      "arxiv_url": "https://arxiv.org/abs/2602.08683",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.08683",
      "github_url": "https://github.com/EvolvingLMMs-Lab/OneVision-Encoder",
      "upvotes": 40,
      "fetched_at": "2026-02-17T08:52:39.133653+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13191",
      "title": "CoPE-VideoLM: Codec Primitives For Efficient Video Language Models",
      "authors": [
        "Sayan Deb Sarkar",
        "Rémi Pautrat",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Iro Armeni",
        "Mahdi Rad",
        "Mihai Dusmanu"
      ],
      "abstract": "Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to 86% and token usage by up to 93% compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on 14 diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.",
      "summary_en": "Current Video Language Models (VideoLMs) rely on keyframe sampling that misses temporal details and requires expensive full-image encoding for each frame. We propose leveraging video codec primitives, specifically motion vectors and residuals, through lightweight transformer encoders that aggregate these primitives and align their representations with image embeddings. This approach reduces time-to-first-token by up to 86% and token usage by up to 93% while maintaining or exceeding performance on 14 video understanding benchmarks spanning question answering, temporal reasoning, and spatial scene understanding.",
      "summary_zh": "现有的视频语言模型（VideoLMs）依赖关键帧采样，这会丢失时序细节，并且需要对每一帧进行昂贵的全图像编码。我们提出利用视频编解码器原语，特别是运动向量和残差，通过轻量级 transformer 编码器聚合这些原语，并将其表征与图像嵌入对齐。该方法将 time-to-first-token 降低高达86%，token 使用量减少高达93%，同时在涵盖问答、时序推理和空间场景理解的14个视频理解基准测试上保持或超越性能。",
      "hf_url": "https://huggingface.co/papers/2602.13191",
      "arxiv_url": "https://arxiv.org/abs/2602.13191",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13191",
      "github_url": "",
      "upvotes": 24,
      "fetched_at": "2026-02-17T08:53:17.327218+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12617",
      "title": "GeoAgent: Learning to Geolocate Everywhere with Reinforced Geographic Characteristics",
      "authors": [
        "Modi Jin",
        "Yiming Zhang",
        "Boyuan Sun",
        "Dingwen Zhang",
        "MingMing Cheng",
        "Qibin Hou"
      ],
      "abstract": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency. This paper presents GeoAgent, a model capable of reasoning closely with humans and deriving fine-grained address conclusions. Previous RL-based methods have achieved breakthroughs in performance and interpretability but still remain concerns because of their reliance on AI-generated chain-of-thought (CoT) data and training strategies, which conflict with geographic characteristics . To address these issues, we first introduce GeoSeek, a new geolocation dataset comprising CoT data annotated by geographic experts and professional players. We further thoroughly explore the inherent characteristics of geographic tasks and propose a geo-similarity reward and a consistency reward assessed by a consistency agent to assist training. This encourages the model to converge towards correct answers from a geographic perspective while ensuring the integrity and consistency of its reasoning process . Experimental results show that GeoAgent outperforms existing methods and a series of general VLLMs across multiple grains, while generating reasoning that closely aligns with humans.",
      "summary_en": "GeoAgent achieves superior geolocation reasoning performance through a specialized dataset and reward mechanisms that ensure geographic accuracy and reasoning consistency.",
      "summary_zh": "GeoAgent 通过可确保地理准确性和推理一致性的专门数据集与奖励机制，实现了卓越的地理定位推理性能。",
      "hf_url": "https://huggingface.co/papers/2602.12617",
      "arxiv_url": "https://arxiv.org/abs/2602.12617",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12617",
      "github_url": "https://github.com/HVision-NKU/GeoAgent",
      "upvotes": 19,
      "fetched_at": "2026-02-17T08:53:04.164869+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.09146",
      "title": "SemanticMoments: Training-Free Motion Similarity via Third Moment Features",
      "authors": [
        "Saar Huberman",
        "Kfir Goldberg",
        "Or Patashnik",
        "Sagie Benaim",
        "Ron Mokady"
      ],
      "abstract": "Temporal statistics in semantic feature space provide a scalable approach for motion-centric video understanding, outperforming existing RGB, flow, and text-supervised methods.",
      "summary_en": "Temporal statistics in semantic feature space provide a scalable approach for motion-centric video understanding, outperforming existing RGB, flow, and text-supervised methods.",
      "summary_zh": "语义特征空间中的时序统计为以运动为中心的视频理解提供了一种可扩展的方法，性能优于现有的RGB、flow和文本监督方法。",
      "hf_url": "https://huggingface.co/papers/2602.09146",
      "arxiv_url": "https://arxiv.org/abs/2602.09146",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09146",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-17T08:52:40.878130+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12395",
      "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis",
      "authors": [
        "Xirui Li",
        "Ming Li",
        "Tianyi Zhou"
      ],
      "abstract": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.",
      "summary_en": "The authors propose a Frankenstein-style analysis framework combining causal probing, parameter comparison, and model merging to disentangle what capabilities reinforcement learning (RL) with verifiable rewards improves in vision-language models beyond supervised fine-tuning. Their results demonstrate that RL induces a consistent inference-time shift primarily in mid-to-late layers, with these refinements proving both transferable via model merging and necessary via layer freezing for RL gains. These findings indicate that RL's contribution to visual reasoning is not a uniform enhancement of visual perception, but rather a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance.",
      "summary_zh": "作者提出了一种Frankenstein-style分析框架，结合causal probing、parameter comparison与model merging，以厘清相较于supervised fine-tuning，reinforcement learning (RL) with verifiable rewards在vision-language models中提升了哪些能力。结果表明，RL主要在mid-to-late layers诱导了一致的inference-time shift，且证明这些refinement既可通过model merging迁移，又需通过layer freezing方能实现RL gains。这些发现表明，RL对visual reasoning的贡献并非visual perception的uniform enhancement，而是对mid-to-late transformer computation的systematic refinement，从而提升vision-to-reasoning alignment与reasoning performance。",
      "hf_url": "https://huggingface.co/papers/2602.12395",
      "arxiv_url": "https://arxiv.org/abs/2602.12395",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12395",
      "github_url": "https://github.com/tianyi-lab/Frankenstein",
      "upvotes": 13,
      "fetched_at": "2026-02-17T08:52:57.355724+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11865",
      "title": "Intelligent AI Delegation",
      "authors": [
        "Nenad Tomašev",
        "Matija Franklin",
        "Simon Osindero"
      ],
      "abstract": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms. AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web .",
      "summary_en": "AI agents require adaptive frameworks for task decomposition and delegation that can dynamically respond to environmental changes and handle unexpected failures through structured authority transfer and trust mechanisms.",
      "summary_zh": "AI智能体需要用于任务分解与委托的自适应框架，以动态响应环境变化，并通过结构化权限转移和信任机制处理意外故障。",
      "hf_url": "https://huggingface.co/papers/2602.11865",
      "arxiv_url": "https://arxiv.org/abs/2602.11865",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11865",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-17T08:52:53.587282+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11236",
      "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
      "authors": [
        "Yandan Yang",
        "Shuang Zeng",
        "Tong Lin",
        "Xinyuan Chang",
        "Dekang Qi",
        "Junjin Xiao",
        "Haoyun Liu",
        "Ronghan Chen",
        "Yuzhi Chen",
        "Dongjie Huo",
        "Feng Xiong",
        "Xing Wei",
        "Zhiheng Ma",
        "Mu Xu"
      ],
      "abstract": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability. Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies , enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis : effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit , enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
      "summary_en": "ABot-M0 presents a unified framework for embodied agent development that standardizes diverse robotic data and employs action manifold learning to improve prediction efficiency and stability.",
      "summary_zh": "ABot-M0 提出了一个用于具身智能体开发的统一框架，该框架对多样化的机器人数据进行标准化，并采用动作流形学习以提升预测效率和稳定性。",
      "hf_url": "https://huggingface.co/papers/2602.11236",
      "arxiv_url": "https://arxiv.org/abs/2602.11236",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11236",
      "github_url": "https://github.com/amap-cvlab/ABot-Manipulation",
      "upvotes": 10,
      "fetched_at": "2026-02-17T08:52:44.643648+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12628",
      "title": "RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
      "authors": [
        "Liangzhi Shi",
        "Shuaihang Chen",
        "Feng Gao",
        "Yinuo Chen",
        "Kang Chen",
        "Tonghe Zhang",
        "Hongzhi Zhang",
        "Weinan Zhang",
        "Chao Yu",
        "Yu Wang"
      ],
      "abstract": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring. Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\textit{RL}-based sim-real \\textit{Co}-training (RL-Co) framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting . We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and π_{0.5}, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on π_{0.5}. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency , providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
      "summary_en": "Reinforcement learning-based sim-real co-training framework improves vision-language-action policy performance through interactive simulation and real-world data anchoring.",
      "summary_zh": "基于强化学习的 sim-real 协同训练框架通过交互式仿真与真实世界数据锚定提升 vision-language-action 策略性能。",
      "hf_url": "https://huggingface.co/papers/2602.12628",
      "arxiv_url": "https://arxiv.org/abs/2602.12628",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12628",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-17T08:53:05.838236+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13013",
      "title": "Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions",
      "authors": [
        "Yunheng Li",
        "Hengrui Zhang",
        "Meng-Hao Guo",
        "Wenzhao Gao",
        "Shaoyong Jia",
        "Shaohui Jiao",
        "Qibin Hou",
        "Ming-Ming Cheng"
      ],
      "abstract": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning. Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning , attribute-wise captioning , caption-based QA , and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.",
      "summary_en": "A large-scale dataset and model for fine-grained audiovisual understanding are introduced, demonstrating improved caption quality and reduced hallucinations through structured annotations and supervised fine-tuning.",
      "summary_zh": "本文提出了一个用于细粒度视听理解的大规模数据集与模型，通过结构化标注与监督微调，显著提升了描述质量并减少了幻觉。",
      "hf_url": "https://huggingface.co/papers/2602.13013",
      "arxiv_url": "https://arxiv.org/abs/2602.13013",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13013",
      "github_url": "https://github.com/ASID-Caption/ASID-Caption",
      "upvotes": 7,
      "fetched_at": "2026-02-17T08:53:13.779897+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.04163",
      "title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
      "authors": [
        "Junyu Chen",
        "Jungang Li",
        "Jing Xiong",
        "Wenjie Wang",
        "Qingyao Yang",
        "He Xiao",
        "Zhen Li",
        "Taiqiang Wu",
        "Mengzhao Chen",
        "Zhen Peng",
        "Chaofan Tao",
        "Long Shi",
        "Hongxia Yang",
        "Ngai Wong"
      ],
      "abstract": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference. Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients , and iteratively refines them using approximate second-order information while progressively compensating quantization error s to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry . Code: github.com/KingdalfGoodman/BPDQ.",
      "summary_en": "Bit-Plane Decomposition Quantization (BPDQ) improves low-bit quantization by using variable quantization grids derived from bit-planes and scalar coefficients, achieving better accuracy than traditional methods in resource-constrained LLM inference.",
      "summary_zh": "Bit-Plane Decomposition Quantization (BPDQ) 利用源自位平面和标量系数的可变量化网格改进低比特量化，在资源受限的LLM推理中实现了优于传统方法的精度。",
      "hf_url": "https://huggingface.co/papers/2602.04163",
      "arxiv_url": "https://arxiv.org/abs/2602.04163",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04163",
      "github_url": "",
      "upvotes": 6,
      "fetched_at": "2026-02-17T08:52:34.419222+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11715",
      "title": "DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels",
      "authors": [
        "Haolei Bai",
        "Lingcheng Kong",
        "Xueyi Chen",
        "Jianmian Wang",
        "Zhiqiang Tao",
        "Huan Wang"
      ],
      "abstract": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework. Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation . This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation , spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation .",
      "summary_en": "Diffusion large language models (dLLMs) for CUDA kernel generation achieve superior performance through a specialized dataset and reinforcement learning framework.",
      "summary_zh": "面向 CUDA 内核生成的扩散大语言模型 (dLLMs) 通过专门的数据集和强化学习框架实现了更优的性能。",
      "hf_url": "https://huggingface.co/papers/2602.11715",
      "arxiv_url": "https://arxiv.org/abs/2602.11715",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11715",
      "github_url": "https://github.com/deadlykitten4/DICE",
      "upvotes": 5,
      "fetched_at": "2026-02-17T08:52:47.569751+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12984",
      "title": "SciAgentGym: Benchmarking Multi-Step Scientific Tool-use in LLM Agents",
      "authors": [
        "Yujiong Shen",
        "Yajie Yang",
        "Zhiheng Xi",
        "Binze Hu",
        "Huayu Sha",
        "Jiazheng Zhang",
        "Qiyuan Peng",
        "Junlin Shang",
        "Jixuan Huang",
        "Yutao Fan",
        "Jingqi Tong",
        "Shihan Dou",
        "Ming Zhang",
        "Lei Bai",
        "Zhenfei Yin",
        "Tao Gui",
        "Xingjun Ma",
        "Qi Zhang",
        "Xuanjing Huang",
        "Yu-Gang Jiang"
      ],
      "abstract": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "summary_en": "SciAgentGym and SciAgentBench enable evaluation of scientific tool-use capabilities, while SciForge improves agent performance through dependency graph modeling of tool interactions.",
      "summary_zh": "SciAgentGym 和 SciAgentBench 支持对科学工具使用能力的评估，而 SciForge 则通过工具交互的依赖图建模来提升智能体性能。",
      "hf_url": "https://huggingface.co/papers/2602.12984",
      "arxiv_url": "https://arxiv.org/abs/2602.12984",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12984",
      "github_url": "https://github.com/CMarsRover/SciAgentGYM",
      "upvotes": 4,
      "fetched_at": "2026-02-17T08:53:12.250735+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12829",
      "title": "FLAC: Maximum Entropy RL via Kinetic Energy Regularized Bridge Matching",
      "authors": [
        "Lei Lv",
        "Yunfei Li",
        "Yu Luo",
        "Fuchun Sun",
        "Xiao Ma"
      ],
      "abstract": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized Schrödinger bridge formulation. Iterative generative policies, such as diffusion models and flow matching , offer superior expressivity for continuous control but complicate Maximum Entropy Reinforcement Learning because their action log-densities are not directly accessible. To address this, we propose Field Least-Energy Actor-Critic (FLAC), a likelihood-free framework that regulates policy stochasticity by penalizing the kinetic energy of the velocity field . Our key insight is to formulate policy optimization as a Generalized Schrödinger Bridge (GSB) problem relative to a high-entropy reference process (e.g., uniform). Under this view, the maximum-entropy principle emerges naturally as staying close to a high-entropy reference while optimizing return, without requiring explicit action densities. In this framework, kinetic energy serves as a physically grounded proxy for divergence from the reference: minimizing path-space energy bounds the deviation of the induced terminal action distribution. Building on this view, we derive an energy-regularized policy iteration scheme and a practical off-policy algorithm that automatically tunes the kinetic energy via a Lagrangian dual mechanism . Empirically, FLAC achieves superior or comparable performance on high-dimensional benchmarks relative to strong baselines, while avoiding explicit density estimation.",
      "summary_en": "Field Least-Energy Actor-Critic (FLAC) addresses challenges in maximum entropy reinforcement learning with iterative generative policies by using kinetic energy as a proxy for policy stochasticity regulation through a generalized Schrödinger bridge formulation.",
      "summary_zh": "Field Least-Energy Actor-Critic (FLAC) 通过广义Schrödinger bridge公式，利用动能作为策略随机性调节的代理，解决了最大熵强化学习中迭代生成策略所面临的挑战。",
      "hf_url": "https://huggingface.co/papers/2602.12829",
      "arxiv_url": "https://arxiv.org/abs/2602.12829",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12829",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T08:53:10.739560+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12684",
      "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
      "authors": [
        "Rui Cai",
        "Jun Guo",
        "Xinze He",
        "Piaopiao Jin",
        "Jie Li",
        "Bingxuan Lin",
        "Futeng Liu",
        "Wei Liu",
        "Fei Ma",
        "Kun Ma",
        "Feng Qiu",
        "Heng Qu",
        "Yifei Su",
        "Qiao Sun",
        "Dong Wang",
        "Donghao Wang",
        "Yunhong Wang",
        "Rujie Wu",
        "Diyun Xiang",
        "Yu Yang",
        "Hangjun Ye",
        "Yuan Zhang"
      ],
      "abstract": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "summary_en": "A vision-language-action model for robotics combines large-scale pretraining with specialized training techniques to enable real-time execution and high-performance manipulation tasks.",
      "summary_zh": "面向机器人领域的视觉-语言-动作模型结合大规模预训练与专门的训练技术，实现实时执行和高性能操作任务。",
      "hf_url": "https://huggingface.co/papers/2602.12684",
      "arxiv_url": "https://arxiv.org/abs/2602.12684",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12684",
      "github_url": "https://github.com/XiaomiRobotics/Xiaomi-Robotics-0",
      "upvotes": 3,
      "fetched_at": "2026-02-17T08:53:06.787066+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12506",
      "title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs",
      "authors": [
        "Rosie Zhao",
        "Anshul Shah",
        "Xiaoyu Zhu",
        "Xinke Deng",
        "Zhongyu Jiang",
        "Yang Yang",
        "Joerg Liebelt",
        "Arnab Mondal"
      ],
      "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations--misleading captions or incorrect chain-of-thought (CoT) traces--cause substantial drops in robustness and confidence, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. Entropy-based metrics further show that these perturbations reshape model uncertainty and probability mass on the correct option, exposing model-specific trends in miscalibration. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy-faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts. Although adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
      "summary_en": "Reinforcement learning (RL) fine-tuning improves vision language models (VLMs) on visual reasoning benchmarks but introduces vulnerabilities to textual perturbations such as misleading captions or incorrect chain-of-thought (CoT) traces, which entropy-based metrics show reshape model uncertainty and reduce robustness. The authors identify an accuracy-faithfulness trade-off wherein RL fine-tuning increases benchmark accuracy while eroding CoT reliability and robustness to contextual shifts. While adversarial augmentation improves robustness and faithfulness-aware rewards restore alignment between answers and reasoning, their combination risks training collapse onto shortcut strategies. These findings highlight the limitations of accuracy-only evaluations and motivate assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.",
      "summary_zh": "强化学习 (RL) 微调可提升视觉语言模型 (VLMs) 在视觉推理基准上的表现，但会引入对文本扰动的脆弱性，例如误导性标题或错误的思维链 (CoT) 轨迹；基于熵的指标显示，这类扰动会重塑模型不确定性并降低鲁棒性。作者识别出一种准确性与忠实度之间的权衡 (accuracy-faithfulness trade-off)：RL 微调在提高基准准确率的同时，会削弱 CoT 的可靠性以及对上下文变化的鲁棒性。尽管对抗增强 (adversarial augmentation) 能提升鲁棒性，且忠实度感知奖励 (faithfulness-aware rewards) 能恢复答案与推理的一致性，但两者结合可能导致训练崩溃 (training collapse)，使模型陷入捷径策略 (shortcut strategies)。这些发现凸显了仅关注准确率的评估局限，并推动了同时强调正确性、鲁棒性及视觉基础推理 (visually grounded reasoning) 忠实度的评估协议。",
      "hf_url": "https://huggingface.co/papers/2602.12506",
      "arxiv_url": "https://arxiv.org/abs/2602.12506",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12506",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-17T08:53:00.561543+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11757",
      "title": "Code2Worlds: Empowering Coding LLMs for 4D World Generation",
      "authors": [
        "Yi Zhang",
        "Yunshuang Wang",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement. Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation . First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration . Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code . Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: https://github.com/AIGeeksGroup/Code2Worlds. Website: https://aigeeksgroup.github.io/Code2Worlds.",
      "summary_en": "Code2Worlds enables 4D dynamic scene generation by formulating it as language-to-simulation code generation with a dual-stream architecture and physics-aware closed-loop refinement.",
      "summary_zh": "Code2Worlds通过将4D动态场景生成形式化为language-to-simulation代码生成，并采用dual-stream architecture与physics-aware closed-loop refinement，实现了4D动态场景生成。",
      "hf_url": "https://huggingface.co/papers/2602.11757",
      "arxiv_url": "https://arxiv.org/abs/2602.11757",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11757",
      "github_url": "https://github.com/AIGeeksGroup/Code2Worlds",
      "upvotes": 3,
      "fetched_at": "2026-02-17T08:52:49.061993+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12612",
      "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback",
      "authors": [
        "Sein Kim",
        "Sangwu Park",
        "Hongseok Kang",
        "Wonjoong Kim",
        "Jimin Seo",
        "Yeonjun In",
        "Kanghoon Yoon",
        "Chanyoung Park"
      ],
      "abstract": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.",
      "summary_en": "Traditional Neural Architecture Search (NAS) methods are constrained by fixed search spaces, while recent LLM-driven code evolution frameworks rely on scalar metrics that lack qualitative insights. We propose Self-EvolveRec, which establishes a directional feedback loop using a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative verification, along with a Diagnosis Tool - Model Co-Evolution strategy to dynamically adapt evaluation criteria as architectures evolve. Experiments show Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven baselines in recommendation performance and user satisfaction.",
      "summary_zh": "传统的神经架构搜索（NAS）方法受限于固定的搜索空间，而近期基于LLM的代码演化框架依赖缺乏定性洞察的标量指标。我们提出Self-EvolveRec，利用User Simulator进行定性评估，利用Model Diagnosis Tool进行定量验证，建立定向反馈循环，并采用Diagnosis Tool - Model Co-Evolution策略随架构演化动态调整评估标准。实验表明，Self-EvolveRec在推荐性能和用户满意度方面显著优于当前最优的NAS和基于LLM的基线方法。",
      "hf_url": "https://huggingface.co/papers/2602.12612",
      "arxiv_url": "https://arxiv.org/abs/2602.12612",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12612",
      "github_url": "https://github.com/Sein-Kim/self_evolverec",
      "upvotes": 2,
      "fetched_at": "2026-02-17T08:53:01.701981+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12221",
      "title": "Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching",
      "authors": [
        "Onkar Susladkar",
        "Tushar Prakash",
        "Gayatri Deshmukh",
        "Kiet A. Nguyen",
        "Jiaxun Zhang",
        "Adheesh Juvekar",
        "Tianshu Bao",
        "Lin Chai",
        "Sparsh Mittal",
        "Inderjit S Dhillon",
        "Ismini Lourentzou"
      ],
      "abstract": "UniDFlow is a unified discrete flow-matching framework that decouples understanding and generation through low-rank adapters and uses reference-based alignment to improve multimodal tasks without retraining. We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding , generation , and editing . It decouples understanding and generation via task-specific low-rank adapters , avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting , in-context image generation , reference-based editing , and compositional generation , despite no explicit task-specific training.",
      "summary_en": "UniDFlow is a unified discrete flow-matching framework that decouples understanding and generation through low-rank adapters and uses reference-based alignment to improve multimodal tasks without retraining.",
      "summary_zh": "UniDFlow是一个统一的离散流匹配框架，通过low-rank adapters解耦理解与生成，并利用reference-based alignment改进多模态任务，无需重新训练。",
      "hf_url": "https://huggingface.co/papers/2602.12221",
      "arxiv_url": "https://arxiv.org/abs/2602.12221",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12221",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-17T08:52:55.775250+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11910",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": [
        "Łukasz Staniszewski",
        "Katarzyna Zaleska",
        "Mateusz Modrzejewski",
        "Kamil Deja"
      ],
      "abstract": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering. Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts , such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Activation Addition and Sparse Autoencoders in these layers enables more precise control over the generated audio, indicating a direct benefit of the specialization phenomenon. By steering activations of the identified layers, we can alter specific musical elements with high precision, such as modulating tempo or changing a track's mood.",
      "summary_en": "Research reveals that specific attention layers in audio diffusion models control distinct musical concepts, enabling precise manipulation of audio features through activation steering.",
      "summary_zh": "研究揭示，音频扩散模型中的特定注意力层控制不同的音乐概念，从而通过 activation steering 实现对音频特征的精确操控。",
      "hf_url": "https://huggingface.co/papers/2602.11910",
      "arxiv_url": "https://arxiv.org/abs/2602.11910",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11910",
      "github_url": "https://github.com/luk-st/steer-audio",
      "upvotes": 2,
      "fetched_at": "2026-02-17T08:52:54.723127+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11769",
      "title": "Light4D: Training-Free Extreme Viewpoint 4D Video Relighting",
      "authors": [
        "Zhenghuang Wu",
        "Kang Chen",
        "Zeyu Zhang",
        "Hao Tang"
      ],
      "abstract": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms. Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance , a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity . Second, to reinforce temporal consistency , we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: https://github.com/AIGeeksGroup/Light4D. Website: https://aigeeksgroup.github.io/Light4D.",
      "summary_en": "Light4D enables consistent 4D video synthesis under target illumination through disentangled flow guidance and temporal consistent attention mechanisms.",
      "summary_zh": "Light4D通过解耦的光流引导和时序一致性注意力机制，实现了目标光照下的一致4D视频合成。",
      "hf_url": "https://huggingface.co/papers/2602.11769",
      "arxiv_url": "https://arxiv.org/abs/2602.11769",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11769",
      "github_url": "https://github.com/AIGeeksGroup/Light4D",
      "upvotes": 2,
      "fetched_at": "2026-02-17T08:52:50.604613+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13022",
      "title": "Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels",
      "authors": [
        "Julius Pesonen",
        "Stefan Rua",
        "Josef Taher",
        "Niko Koivumäki",
        "Xiaowei Yu",
        "Eija Honkavaara"
      ],
      "abstract": "Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.",
      "summary_en": "Mapping individual tree crowns from aerial imagery is challenging due to texture variations and overlapping crowns, yet essential for environmental monitoring. This study presents a method to train deep learning models for segmenting individual trees from RGB and multispectral images using pseudo-labels derived from aerial laser scanning (ALS) data, enhanced by the zero-shot instance segmentation model Segment Anything Model 2 (SAM 2). This approach generates domain-specific training annotations without manual labeling costs, producing segmentation models that outperform general-domain alternatives on this task.",
      "summary_zh": "从航空影像中识别单木树冠因纹理变化和树冠重叠而具有挑战性，但对环境监测至关重要。本研究提出了一种方法，利用从航空激光雷达（ALS）数据生成并经零样本实例分割模型 Segment Anything Model 2（SAM 2）增强的伪标签，训练深度学习模型以从 RGB 和多光谱图像中分割单木。该方法无需人工标注成本即可生成领域特定的训练标注，所得分割模型在此任务上的性能优于通用领域替代方案。",
      "hf_url": "https://huggingface.co/papers/2602.13022",
      "arxiv_url": "https://arxiv.org/abs/2602.13022",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13022",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:53:15.204632+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.12500",
      "title": "Favia: Forensic Agent for Vulnerability-fix Identification and Analysis",
      "authors": [
        "André Storhaug",
        "Jiamou Sun",
        "Jingyue Li"
      ],
      "abstract": "Favia is a forensic, agent-based framework that combines scalable candidate ranking with deep semantic reasoning to accurately identify vulnerability-fixing commits by leveraging LLM agents with specialized tools and environmental context. Identifying vulnerability-fixing commits corresponding to disclosed CVEs is essential for secure software maintenance but remains challenging at scale, as large repositories contain millions of commits of which only a small fraction address security issues. Existing automated approaches, including traditional machine learning techniques and recent large language model (LLM)-based methods, often suffer from poor precision-recall trade-offs . Frequently evaluated on randomly sampled commits, we uncover that they are substantially underestimating real-world difficulty, where candidate commits are already security-relevant and highly similar. We propose Favia, a forensic, agent-based framework for vulnerability-fix identification that combines scalable candidate ranking with deep and iterative semantic reasoning . Favia first employs an efficient ranking stage to narrow the search space of commits. Each commit is then rigorously evaluated using a ReAct-based LLM agent . By providing the agent with a pre-commit repository as environment, along with specialized tools, the agent tries to localize vulnerable components, navigates the codebase, and establishes causal alignment between code changes and vulnerability root causes. This evidence-driven process enables robust identification of indirect, multi-file, and non-trivial fixes that elude single-pass or similarity-based methods. We evaluate Favia on CVEVC, a large-scale dataset we made that comprises over 8 million commits from 3,708 real-world repositories, and show that it consistently outperforms state-of-the-art traditional and LLM-based baselines under realistic candidate selection, achieving the strongest precision-recall trade-offs and highest F1-scores.",
      "summary_en": "Favia is a forensic, agent-based framework that combines scalable candidate ranking with deep semantic reasoning to accurately identify vulnerability-fixing commits by leveraging LLM agents with specialized tools and environmental context.",
      "summary_zh": "Favia是一个取证的、基于智能体的框架，它结合可扩展的候选排序与深度语义推理，通过利用配备专用工具和环境上下文的LLM智能体，以准确识别修复漏洞的提交。",
      "hf_url": "https://huggingface.co/papers/2602.12500",
      "arxiv_url": "https://arxiv.org/abs/2602.12500",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12500",
      "github_url": "https://github.com/andstor/agentic-security-patch-classification-replication-package",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:52:59.251340+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.11609",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "authors": [
        "Yiming Gao",
        "Zhen Wang",
        "Jefferson Chen",
        "Mark Antkowiak",
        "Mengzhou Hu",
        "JungHo Kong",
        "Dexter Pratt",
        "Jieyuan Liu",
        "Enze Ma",
        "Zhiting Hu",
        "Eric P. Xing"
      ],
      "abstract": "scPilot presents a framework for omics-native reasoning where large language models directly analyze single-cell RNA-seq data through step-by-step reasoning processes, improving accuracy and interpretability in cell-type annotation and developmental trajectory reconstruction. We present scPilot, the first systematic framework to practice omics-native reasoning : a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation , developmental-trajectory reconstruction , and transcription-factor targeting , into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence. To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic . By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses. Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "summary_en": "scPilot presents a framework for omics-native reasoning where large language models directly analyze single-cell RNA-seq data through step-by-step reasoning processes, improving accuracy and interpretability in cell-type annotation and developmental trajectory reconstruction.",
      "summary_zh": "scPilot 提出了一个用于 omics-native reasoning 的框架，其中大型语言模型通过逐步推理过程直接分析 single-cell RNA-seq 数据，从而提高细胞类型注释和发育轨迹重建的准确性和可解释性。",
      "hf_url": "https://huggingface.co/papers/2602.11609",
      "arxiv_url": "https://arxiv.org/abs/2602.11609",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.11609",
      "github_url": "https://github.com/maitrix-org/scPilot",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:52:45.901752+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.09870",
      "title": "Steer2Edit: From Activation Steering to Component-Level Editing",
      "authors": [
        "Chung-En Sun",
        "Ge Yan",
        "Zimo Wang",
        "Tsui-Wei Weng"
      ],
      "abstract": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.",
      "summary_en": "Steer2Edit is a training-free framework that transforms inference-time steering vectors into selective rank-1 weight edits of individual attention heads and MLP neurons, avoiding the attribute-utility trade-offs caused by fixed global activation interventions. By redistributing behavioral influence across specific components rather than uniformly modifying internal states during generation, Steer2Edit achieves up to 17.2% safety improvement, 9.8% increased truthfulness, and 12.2% reduced reasoning length while preserving downstream performance and compatibility with optimized inference. This approach provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable parameter updates.",
      "summary_zh": "Steer2Edit 是一种无需训练的框架，它将推理时的 steering vectors 转化为针对单个 attention heads 和 MLP neurons 的选择性 rank-1 weight edits，从而避免了固定全局 activation interventions 所导致的属性-效用权衡。通过将行为影响重新分配到特定组件，而非在生成过程中均匀修改内部状态，Steer2Edit 在保持 downstream performance 和与 optimized inference 兼容性的同时，实现了高达 17.2% 的安全性提升、9.8% 的真实性提升以及 12.2% 的推理长度缩减。该方法通过将 steering signals 转化为可解释的 parameter updates，在 representation steering 与 weight editing 之间建立了原则性的桥梁。",
      "hf_url": "https://huggingface.co/papers/2602.09870",
      "arxiv_url": "https://arxiv.org/abs/2602.09870",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.09870",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:52:41.895447+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.07298",
      "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
      "authors": [
        "Benyu Zhang",
        "Qiang Zhang",
        "Jianpeng Cheng",
        "Hong-You Chen",
        "Qifei Wang",
        "Wei Sun",
        "Shen Li",
        "Jia Li",
        "Jiahao Wu",
        "Xiangjun Fan",
        "Hong Yan"
      ],
      "abstract": "A novel layered framework generates high-quality synthetic data for large language models in recommender systems, demonstrating superior performance and predictable scaling laws compared to traditional methods. Large Language Models (LLMs) represent a promising frontier for recommender systems , yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform (+130% on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.",
      "summary_en": "A novel layered framework generates high-quality synthetic data for large language models in recommender systems, demonstrating superior performance and predictable scaling laws compared to traditional methods.",
      "summary_zh": "一种新颖的分层框架能够为推荐系统中的大语言模型生成高质量合成数据，与传统方法相比具有更优的性能和可预测的缩放定律。",
      "hf_url": "https://huggingface.co/papers/2602.07298",
      "arxiv_url": "https://arxiv.org/abs/2602.07298",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.07298",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:52:37.232386+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.04315",
      "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
      "authors": [
        "Guoqing Ma",
        "Siheng Wang",
        "Zeyu Zhang",
        "Shan Yu",
        "Hao Tang"
      ],
      "abstract": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection. Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability , which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning ), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM ( Affordance Segmentation Module ) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
      "summary_en": "GeneralVLA is a hierarchical vision-language-action model that enables zero-shot robotic manipulation through knowledge-guided trajectory planning without requiring real-world data collection.",
      "summary_zh": "GeneralVLA是一种分层视觉-语言-动作模型，通过知识引导的轨迹规划实现零样本机器人操作，无需真实世界数据收集。",
      "hf_url": "https://huggingface.co/papers/2602.04315",
      "arxiv_url": "https://arxiv.org/abs/2602.04315",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.04315",
      "github_url": "https://github.com/AIGeeksGroup/GeneralVLA",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:52:35.360451+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.03120",
      "title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
      "authors": [
        "Yinggan Xu",
        "Risto Miikkulainen",
        "Xin Qiu"
      ],
      "abstract": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .",
      "summary_en": "Post-Training Quantization (PTQ) enables deploying Large Language Models (LLMs) on memory-constrained devices but renders models static and incompatible with standard fine-tuning paradigms, including Reinforcement Learning (RL), which require backpropagation through continuous, high-precision weights. This paper introduces Quantized Evolution Strategies (QES), which performs full-parameter fine-tuning directly in the discrete, quantized space by integrating accumulated error feedback to preserve gradient signals and utilizing stateless seed replay to reduce memory usage to inference levels. QES significantly outperforms state-of-the-art zeroth-order fine-tuning methods on arithmetic reasoning tasks, enabling direct fine-tuning and scaling of LLMs entirely within the quantized parameter space.",
      "summary_zh": "后训练量化（PTQ）使得大语言模型（LLMs）能够部署在内存受限设备上，但会使模型静态化，且与包括强化学习（RL）在内的标准微调范式不兼容，后者需要通过连续的高精度权重进行反向传播。本文提出量化进化策略（QES），该方法通过整合累积误差反馈以保留梯度信号，并利用无状态种子重放将内存占用降至推理水平，从而在离散量化空间中直接进行全参数微调。QES在算术推理任务上显著优于最先进的zeroth-order微调方法，使得完全在量化参数空间内直接微调和扩展LLMs成为可能。",
      "hf_url": "https://huggingface.co/papers/2602.03120",
      "arxiv_url": "https://arxiv.org/abs/2602.03120",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.03120",
      "github_url": "https://github.com/dibbla/Quantized-Evolution-Strategies",
      "upvotes": 1,
      "fetched_at": "2026-02-17T08:52:32.803504+00:00"
    },
    {
      "date": "2026-02-16",
      "paper_id": "2602.13139",
      "title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report",
      "authors": [
        "Mariia Fedorova",
        "Nikolay Arefyev",
        "Maja Buljan",
        "Jindřich Helcl",
        "Stephan Oepen",
        "Egil Rønningstad",
        "Yves Scherrer"
      ],
      "abstract": "OpenLID-v3 improves language identification accuracy for closely related languages and low-resource variants through enhanced training data, cluster merging, and noise detection mechanisms. Language identification (LID) is an essential step in building high-quality multilingual datasets from web data . Existing LID tools (such as OpenLID or GlotLID ) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages . In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters , and introducing a special label for marking noise. We call this extended system OpenLID -v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages . OpenLID -v3 is available on https://huggingface.co/HPLT/ OpenLID -v3.",
      "summary_en": "OpenLID-v3 improves language identification accuracy for closely related languages and low-resource variants through enhanced training data, cluster merging, and noise detection mechanisms.",
      "summary_zh": "OpenLID-v3 通过增强的训练数据、聚类合并及噪声检测机制，提升了对近缘语言及低资源变体的语言识别准确率。",
      "hf_url": "https://huggingface.co/papers/2602.13139",
      "arxiv_url": "https://arxiv.org/abs/2602.13139",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.13139",
      "github_url": "https://github.com/hplt-project/openlid",
      "upvotes": 0,
      "fetched_at": "2026-02-17T08:53:16.283694+00:00"
    }
  ]
}