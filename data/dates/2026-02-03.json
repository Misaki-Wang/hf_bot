{
  "date": "2026-02-03",
  "count": 73,
  "papers": [
    {
      "date": "2026-02-03",
      "paper_id": "2602.00919",
      "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
      "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin"
      ],
      "abstract": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning. We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding , (R0) multi-embodiment pretraining , (R1) embodiment-specific adaptation , and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction , out-of-distribution detection , and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
      "summary_en": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.",
      "summary_zh": "Green-VLA是一个用于真实世界机器人部署的五阶段视觉-语言-动作框架，通过多模态训练和强化学习实现跨不同机器人本体的泛化。",
      "hf_url": "https://huggingface.co/papers/2602.00919",
      "arxiv_url": "https://arxiv.org/abs/2602.00919",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00919",
      "github_url": "https://github.com/greenvla/GreenVLA",
      "upvotes": 280,
      "fetched_at": "2026-02-19T05:36:32.100057+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02276",
      "title": "Kimi K2.5: Visual Agentic Intelligence",
      "authors": [
        "Kimi Team",
        "Tongtong Bai",
        "Yifan Bai",
        "Yiping Bao",
        "S. H. Cai",
        "Yuan Cao",
        "Y. Charles",
        "H. S. Che",
        "Cheng Chen",
        "Guanduo Chen",
        "Huarong Chen",
        "Jia Chen",
        "Jiahao Chen",
        "Jianlong Chen",
        "Jun Chen",
        "Kefan Chen",
        "Liang Chen",
        "Ruijue Chen",
        "Xinhao Chen",
        "Yanru Chen",
        "Yanxu Chen",
        "Yicun Chen"
      ],
      "abstract": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution. We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training , zero-vision SFT , and joint text-vision reinforcement learning . Building on this multimodal foundation, K2.5 introduces Agent Swarm , a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.",
      "summary_en": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.",
      "summary_zh": "Kimi K2.5 是一款开源的多模态智能体模型，通过联合优化技术增强文本与视觉处理能力，并引入 Agent Swarm 实现并行任务执行。",
      "hf_url": "https://huggingface.co/papers/2602.02276",
      "arxiv_url": "https://arxiv.org/abs/2602.02276",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02276",
      "github_url": "https://github.com/MoonshotAI/Kimi-K2.5",
      "upvotes": 233,
      "fetched_at": "2026-02-19T05:37:47.945680+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22060",
      "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models",
      "authors": [
        "Wenxuan Huang",
        "Yu Zeng",
        "Qiuchen Wang",
        "Zhen Fang",
        "Shaosheng Cao",
        "Zheng Chu",
        "Qingyu Yin",
        "Shuang Chen",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Yao Hu",
        "Philip Torr",
        "Feng Zhao",
        "Wanli Ouyang"
      ],
      "abstract": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning. Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by `` reasoning-then-tool-call '' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.",
      "summary_en": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.",
      "summary_zh": "Vision-DeepResearch提出了一种多模态深度研究范式，通过冷启动监督与强化学习集成深度研究能力，实现多轮、多实体、多尺度的视觉与文本搜索。",
      "hf_url": "https://huggingface.co/papers/2601.22060",
      "arxiv_url": "https://arxiv.org/abs/2601.22060",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22060",
      "github_url": "https://github.com/Osilly/Vision-DeepResearch",
      "upvotes": 153,
      "fetched_at": "2026-02-19T05:36:06.346246+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02185",
      "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models",
      "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Zhen Fang",
        "Shuang Chen",
        "Yufan Shen",
        "Yishuo Cai",
        "Xiaoman Wang",
        "Zhenfei Yin",
        "Lin Chen",
        "Zehui Chen",
        "Shiting Huang",
        "Yiming Zhao",
        "Yao Hu",
        "Philip Torr",
        "Wanli Ouyang",
        "Shaosheng Cao"
      ],
      "abstract": "Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow. Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding . However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems . The code will be released in https://github.com/Osilly/ Vision-DeepResearch .",
      "summary_en": "Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.",
      "summary_zh": "Vision-DeepResearch基准通过引入真实评估条件并采用多轮裁剪搜索工作流改进视觉检索，解决了多模态模型视觉-文本搜索能力评估的局限性。",
      "hf_url": "https://huggingface.co/papers/2602.02185",
      "arxiv_url": "https://arxiv.org/abs/2602.02185",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02185",
      "github_url": "",
      "upvotes": 125,
      "fetched_at": "2026-02-19T05:37:42.232250+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02084",
      "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder",
      "authors": [
        "Jane Luo",
        "Chengyu Yin",
        "Xin Zhang",
        "Qingtao Li",
        "Steven Liu",
        "Yiming Huang",
        "Jie Wu",
        "Hao Liu",
        "Yangyu Huang",
        "Yu Kang",
        "Fangkai Yang",
        "Ying Xin",
        "Scarlett Li"
      ],
      "abstract": "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy. Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder , a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies ; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation . In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.",
      "summary_en": "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.",
      "summary_zh": "RPG-Encoder框架通过将代码编码为高保真Repository Planning Graph表示，将代码库理解与生成转化为统一的循环，从而提升理解与重建的准确性。",
      "hf_url": "https://huggingface.co/papers/2602.02084",
      "arxiv_url": "https://arxiv.org/abs/2602.02084",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02084",
      "github_url": "https://github.com/microsoft/RPG-ZeroRepo",
      "upvotes": 82,
      "fetched_at": "2026-02-19T05:37:33.706738+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02437",
      "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
      "authors": [
        "Dianyi Wang",
        "Chaofan Ma",
        "Feng Han",
        "Size Wu",
        "Wei Song",
        "Yibin Wang",
        "Zhixiong Zhang",
        "Tianhang Wang",
        "Siyuan Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
      ],
      "abstract": "UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks. Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm . We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation , mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction . Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE , KrisBench and UniREditBench , while maintaining superior general synthesis capabilities.",
      "summary_en": "UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.",
      "summary_zh": "UniReason通过双重推理范式整合文本到图像生成与图像编辑，利用世界知识增强规划并使用编辑进行视觉优化，在推理密集型基准测试中取得了更优性能。",
      "hf_url": "https://huggingface.co/papers/2602.02437",
      "arxiv_url": "https://arxiv.org/abs/2602.02437",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02437",
      "github_url": "",
      "upvotes": 76,
      "fetched_at": "2026-02-19T05:38:00.634902+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02361",
      "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
      "authors": [
        "Mouxiang Chen",
        "Lei Zhang",
        "Yunlong Feng",
        "Xuwu Wang",
        "Wenting Zhao",
        "Ruisheng Cao",
        "Jiaxi Yang",
        "Jiawei Chen",
        "Mingze Li",
        "Zeyao Ma",
        "Hao Ge",
        "Zongmeng Zhang",
        "Zeyu Cui",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Jianling Sun",
        "Junyang Lin",
        "Binyuan Hui"
      ],
      "abstract": "A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities. We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model . This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning . Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified . Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
      "summary_en": "A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.",
      "summary_zh": "一种可扩展框架，利用具备自验证和黑客攻击检测能力的高效构建智能体，从 GitHub pull requests 构建真实软件工程环境。",
      "hf_url": "https://huggingface.co/papers/2602.02361",
      "arxiv_url": "https://arxiv.org/abs/2602.02361",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02361",
      "github_url": "",
      "upvotes": 60,
      "fetched_at": "2026-02-19T05:37:57.044088+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01566",
      "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents",
      "authors": [
        "Chiwei Zhu",
        "Benfeng Xu",
        "Mingxuan Du",
        "Shaohan Wang",
        "Xiaorui Wang",
        "Zhendong Mao",
        "Yongdong Zhang"
      ],
      "abstract": "A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.",
      "summary_en": "A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.",
      "summary_zh": "基于文件系统的双智能体框架通过将持久化存储作为外部记忆，使大型语言模型智能体能够突破上下文窗口限制，执行扩展研究任务。",
      "hf_url": "https://huggingface.co/papers/2602.01566",
      "arxiv_url": "https://arxiv.org/abs/2602.01566",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01566",
      "github_url": "",
      "upvotes": 48,
      "fetched_at": "2026-02-19T05:36:57.934456+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02472",
      "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
      "authors": [
        "Qifan Yu",
        "Xinyu Ma",
        "Zhijian Zhuo",
        "Minrui Wang",
        "Deyi Liu",
        "Shiyi Zhan",
        "Yiyuan Ma",
        "Liang Xiang",
        "Xingyan Bin",
        "Di He"
      ],
      "abstract": "SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs. Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings , yet it remains a formidable challenge due to severe training instabilities . Empirically, we show that naive initialization at this stage disrupts activation statistics , triggering loss spikes , while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion . Our method achieves signal preservation via RMS-scale consistency , stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup . Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion .",
      "summary_en": "SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.",
      "summary_zh": "SPARKLING是一种用于深度学习模型中期宽度扩展的框架，通过保持信号保留并打破对称性来稳定训练并降低计算成本。",
      "hf_url": "https://huggingface.co/papers/2602.02472",
      "arxiv_url": "https://arxiv.org/abs/2602.02472",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02472",
      "github_url": "",
      "upvotes": 44,
      "fetched_at": "2026-02-19T05:38:04.314322+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02493",
      "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
      "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
      ],
      "abstract": "PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations. Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAE s in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion model s. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision . Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision , PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to- image generation with a GenEval score of 0.79. PixelGen requires no VAE s, no latent representations , and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.",
      "summary_en": "PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.",
      "summary_zh": "PixelGen 是一种像素空间扩散框架，利用 LPIPS 和基于 DINO 的损失函数进行感知监督，无需 VAE 或潜在表示即可生成高质量图像。",
      "hf_url": "https://huggingface.co/papers/2602.02493",
      "arxiv_url": "https://arxiv.org/abs/2602.02493",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02493",
      "github_url": "https://github.com/Zehong-Ma/PixelGen",
      "upvotes": 42,
      "fetched_at": "2026-02-19T05:38:11.506490+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01576",
      "title": "Generative Visual Code Mobile World Models",
      "authors": [
        "Woosung Koh",
        "Sungjun Han",
        "Segyu Lee",
        "Se-Young Yun",
        "Jamin Shin"
      ],
      "abstract": "Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches. Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity , while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation , where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework ( gWorld ) that automatically synthesizes code-based training data . In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.",
      "summary_en": "Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.",
      "summary_zh": "通过视觉-语言模型生成可渲染代码，改进了移动GUI智能体的视觉世界模型，与现有方法相比，以更小的模型尺寸实现了更优性能。",
      "hf_url": "https://huggingface.co/papers/2602.01576",
      "arxiv_url": "https://arxiv.org/abs/2602.01576",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01576",
      "github_url": "https://github.com/trillion-labs/gWorld",
      "upvotes": 41,
      "fetched_at": "2026-02-19T05:36:59.609126+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02338",
      "title": "Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs",
      "authors": [
        "Yu Liang",
        "Zhongjin Zhang",
        "Yuxuan Zhu",
        "Kerui Zhang",
        "Zhiluohan Guo",
        "Wenhang Zhou",
        "Zonqi Yang",
        "Kangle Wu",
        "Yabo Ni",
        "Anxiang Zeng",
        "Cong Fu",
        "Jianxin Wang",
        "Jiazhi Xia"
      ],
      "abstract": "ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability. Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems , but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction , and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling . To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability , without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.",
      "summary_en": "ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.",
      "summary_zh": "ReSID 提出了一种新颖的推荐原生框架，通过学**习预测性物品表示并优化量化以保留信息并保持序列可预测性，从而改进序列推荐。",
      "hf_url": "https://huggingface.co/papers/2602.02338",
      "arxiv_url": "https://arxiv.org/abs/2602.02338",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02338",
      "github_url": "https://github.com/FuCongResearchSquad/ReSID",
      "upvotes": 40,
      "fetched_at": "2026-02-19T05:37:51.565193+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02053",
      "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
      "authors": [
        "Pengyu Wang",
        "Benfeng Xu",
        "Licheng Zhang",
        "Shaohan Wang",
        "Mingxuan Du",
        "Chiwei Zhu",
        "Zhendong Mao"
      ],
      "abstract": "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types. Graph-based Retrieval-Augmented Generation ( GraphRAG ) organizes external knowledge as a hierarchical graph , enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge , failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents . To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia 's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization . Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.",
      "summary_en": "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.",
      "summary_zh": "WildGraphBench 使用 Wikipedia 的结构化内容，在真实场景中评估 GraphRAG 跨多种文档类型的多事实聚合与总结能力。",
      "hf_url": "https://huggingface.co/papers/2602.02053",
      "arxiv_url": "https://arxiv.org/abs/2602.02053",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02053",
      "github_url": "https://github.com/BstWPY/WildGraphBench",
      "upvotes": 40,
      "fetched_at": "2026-02-19T05:37:32.161403+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01058",
      "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
      "authors": [
        "Dylan Zhang",
        "Yufeng Xu",
        "Haojin Wang",
        "Qingzhi Chen",
        "Hao Peng"
      ],
      "abstract": "Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss. Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone. We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts. We propose PEAR ( Policy Evaluation -inspired Algorithm for Offline Learning Loss Re-weighting ), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected. We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek -distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.",
      "summary_en": "Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.",
      "summary_zh": "推理大语言模型的后训练可通过对SFT损失进行重要性采样重加权，纠正监督微调和强化学习阶段之间的分布不匹配，从而得到改进。",
      "hf_url": "https://huggingface.co/papers/2602.01058",
      "arxiv_url": "https://arxiv.org/abs/2602.01058",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01058",
      "github_url": "",
      "upvotes": 40,
      "fetched_at": "2026-02-19T05:36:35.802552+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02453",
      "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
      "authors": [
        "Andong Chen",
        "Wenxin Zhu",
        "Qiuyu Ding",
        "Yuchen Song",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance. Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure , while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure , embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning .",
      "summary_en": "Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.",
      "summary_zh": "Thinking with Comics 是一种有效的视觉推理方法，通过利用漫画结构桥接图像与视频，以提升多模态推理的效率与性能。",
      "hf_url": "https://huggingface.co/papers/2602.02453",
      "arxiv_url": "https://arxiv.org/abs/2602.02453",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02453",
      "github_url": "https://github.com/andongBlue/Think-with-Comics",
      "upvotes": 35,
      "fetched_at": "2026-02-19T05:38:02.172058+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01590",
      "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
      "authors": [
        "Shaohan Wang",
        "Benfeng Xu",
        "Licheng Zhang",
        "Mingxuan Du",
        "Chiwei Zhu",
        "Xiaorui Wang",
        "Zhendong Mao",
        "Yongdong Zhang"
      ],
      "abstract": "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.",
      "summary_en": "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.",
      "summary_zh": "Deep Research Agents展现了自主信息检索的能力，但在使用新的实时基准测试和综合评估框架对照专家级Wikipedia文章进行评估时，显示出显著差距。",
      "hf_url": "https://huggingface.co/papers/2602.01590",
      "arxiv_url": "https://arxiv.org/abs/2602.01590",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01590",
      "github_url": "https://github.com/WangShao2000/Wiki_Live_Challenge",
      "upvotes": 33,
      "fetched_at": "2026-02-19T05:37:01.181665+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02488",
      "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
      "authors": [
        "Yinjie Wang",
        "Tianbao Xie",
        "Ke Shen",
        "Mengdi Wang",
        "Ling Yang"
      ],
      "abstract": "RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training. We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization , amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals , while the reward model is jointly optimized via consistency feedback , which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench , respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL",
      "summary_en": "RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.",
      "summary_zh": "RLAnything通过动态模型优化和闭环反馈机制，增强面向LLM和智能体的强化学习，改进策略与奖励模型训练。",
      "hf_url": "https://huggingface.co/papers/2602.02488",
      "arxiv_url": "https://arxiv.org/abs/2602.02488",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02488",
      "github_url": "https://github.com/Gen-Verse/Open-AgentRL",
      "upvotes": 32,
      "fetched_at": "2026-02-19T05:38:09.586429+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02383",
      "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization",
      "authors": [
        "Maksim Afanasyev",
        "Illarion Iov"
      ],
      "abstract": "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints. Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback ( RLHF ) for aligning Large Language Models ( LLMs ). Latest approaches have streamlined the alignment process by deriving implicit reward functions , yet they often suffer from a critical objective mismatch : optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood . This can lead to `` unlearning '', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and `` formatting collapse '' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability .",
      "summary_en": "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.",
      "summary_zh": "SLIME是一种用于大型语言模型的新型无参考对齐目标，通过结合似然最大化、概率稳定化和双边界约束的三方面方法，将偏好学习与生成质量解耦。",
      "hf_url": "https://huggingface.co/papers/2602.02383",
      "arxiv_url": "https://arxiv.org/abs/2602.02383",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02383",
      "github_url": "https://github.com/fpsigma/trl-slime",
      "upvotes": 29,
      "fetched_at": "2026-02-19T05:37:58.687613+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01801",
      "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention",
      "authors": [
        "Dvir Samuel",
        "Issar Tzachor",
        "Matan Levy",
        "Micahel Green",
        "Gal Chechik",
        "Rami Ben-Ari"
      ],
      "abstract": "Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance. Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; Ann CA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor ( ANN ) matching; and Ann SA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN . Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.",
      "summary_en": "Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.",
      "summary_zh": "自回归视频扩散模型因KV缓存增长和注意力计算冗余面临效率挑战，TempCache、AnnCA和AnnSA技术通过降低计算需求并保持视觉质量与稳定性能解决这些问题。",
      "hf_url": "https://huggingface.co/papers/2602.01801",
      "arxiv_url": "https://arxiv.org/abs/2602.01801",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01801",
      "github_url": "",
      "upvotes": 28,
      "fetched_at": "2026-02-19T05:37:12.726742+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02214",
      "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
      "authors": [
        "Hongzhou Zhu",
        "Min Zhao",
        "Guande He",
        "Hang Su",
        "Chongxuan Li",
        "Jun Zhu"
      ],
      "abstract": "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance. To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention . However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation , which requires frame-level injectivity , where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution , which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}",
      "summary_en": "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.",
      "summary_zh": "一种新颖的因果强制方法通过使用AR教师模型进行ODE初始化，解决了将双向视频扩散模型蒸馏为自回归模型时存在的架构差距问题，显著提升了视频生成性能。",
      "hf_url": "https://huggingface.co/papers/2602.02214",
      "arxiv_url": "https://arxiv.org/abs/2602.02214",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02214",
      "github_url": "https://github.com/thu-ml/Causal-Forcing",
      "upvotes": 24,
      "fetched_at": "2026-02-19T05:37:44.312354+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01624",
      "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards",
      "authors": [
        "Minh-Quan Le",
        "Gaurav Mittal",
        "Cheng Zhao",
        "David Gu",
        "Dimitris Samaras",
        "Mei Chen"
      ],
      "abstract": "PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations. Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning .",
      "summary_en": "PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.",
      "summary_zh": "PISCES是一种无需标注的文本到视频生成方法，通过双重最优传输对齐奖励提升视觉质量与语义对齐，无需人类偏好标注。",
      "hf_url": "https://huggingface.co/papers/2602.01624",
      "arxiv_url": "https://arxiv.org/abs/2602.01624",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01624",
      "github_url": "",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:37:04.889547+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01395",
      "title": "Rethinking Selective Knowledge Distillation",
      "authors": [
        "Almog Tavor",
        "Itay Ebenspanger",
        "Neil Cnaan",
        "Mor Geva"
      ],
      "abstract": "Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements. Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation , which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation . Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.",
      "summary_en": "Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.",
      "summary_zh": "基于学生熵引导位置选择的自回归语言模型选择性知识蒸馏可提升准确率与效率，同时降低内存和存储需求。",
      "hf_url": "https://huggingface.co/papers/2602.01395",
      "arxiv_url": "https://arxiv.org/abs/2602.01395",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01395",
      "github_url": "https://github.com/almogtavor/SE-KD3x",
      "upvotes": 23,
      "fetched_at": "2026-02-19T05:36:46.441147+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01756",
      "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation",
      "authors": [
        "Jun He",
        "Junyan Ye",
        "Zilong Huang",
        "Dongzhi Jiang",
        "Chenjue Zhang",
        "Leqi Zhu",
        "Renrui Zhang",
        "Xiang Zhang",
        "Weijia Li"
      ],
      "abstract": "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.",
      "summary_en": "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.",
      "summary_zh": "Mind-Brush 提出了一个统一的 agentic 框架，用于文本到图像生成，该框架动态检索多模态证据并采用推理工具，以提升对隐式用户意图和复杂知识推理的理解。",
      "hf_url": "https://huggingface.co/papers/2602.01756",
      "arxiv_url": "https://arxiv.org/abs/2602.01756",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01756",
      "github_url": "https://github.com/PicoTrex/Mind-Brush",
      "upvotes": 22,
      "fetched_at": "2026-02-19T05:37:10.900845+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02486",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches. LLM-based deep research agents are largely built on the ReAct framework . This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning , reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning , achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "summary_en": "Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.",
      "summary_zh": "Re-TRAC是一种智能体框架，通过结构化状态表示实现跨轨迹探索与迭代反思，从而增强基于LLM的研究智能体，相较于传统ReAct方法能够实现更高效有效的问题求解。",
      "hf_url": "https://huggingface.co/papers/2602.02486",
      "arxiv_url": "https://arxiv.org/abs/2602.02486",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02486",
      "github_url": "https://github.com/microsoft/InfoAgent",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:38:07.551172+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02092",
      "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space",
      "authors": [
        "FSVideo Team",
        "Qingyu Chen",
        "Zhiyuan Fang",
        "Haibin Huang",
        "Xinwei Huang",
        "Tong Jin",
        "Minxuan Lin",
        "Bo Liu",
        "Celong Liu",
        "Chongyang Ma",
        "Xing Mei",
        "Xiaohui Shen",
        "Yaojie Shen",
        "Fuwen Tan",
        "Angtian Wang",
        "Xiao Yang",
        "Yiding Yang",
        "Jiamin Yuan",
        "Lingxi Zhang",
        "Yuxin Zhang"
      ],
      "abstract": "FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.",
      "summary_en": "FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.",
      "summary_zh": "FSVideo是一种基于transformer的快速图像到视频扩散框架，采用压缩视频自编码器、具备增强层记忆的扩散transformer架构和多分辨率生成策略，在显著减少计算时间的同时实现高性能。",
      "hf_url": "https://huggingface.co/papers/2602.02092",
      "arxiv_url": "https://arxiv.org/abs/2602.02092",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02092",
      "github_url": "",
      "upvotes": 18,
      "fetched_at": "2026-02-19T05:37:35.937436+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01479",
      "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
      "authors": [
        "Xueqing Peng",
        "Ruoyu Xiang",
        "Fan Zhang",
        "Mingzi Song",
        "Mingyang Jiang",
        "Yan Wang",
        "Lingfei Qian",
        "Taiki Hara",
        "Yuqing Guo",
        "Jimin Huang",
        "Junichi Tsujii",
        "Sophia Ananiadou"
      ],
      "abstract": "A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities. Japanese finance combines agglutinative, head-final linguistic structure , mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment , posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A , and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures . We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.",
      "summary_en": "A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities.",
      "summary_zh": "介绍了一个名为Ebisu的日本金融语言理解基准，包含两个专家标注任务，用于评估隐性承诺识别和层次化金融术语抽取，揭示了当前语言模型尽管具备先进能力，但仍面临持续性挑战。",
      "hf_url": "https://huggingface.co/papers/2602.01479",
      "arxiv_url": "https://arxiv.org/abs/2602.01479",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01479",
      "github_url": "",
      "upvotes": 17,
      "fetched_at": "2026-02-19T05:36:50.350950+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01851",
      "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing",
      "authors": [
        "Huanyu Zhang",
        "Xuehai Bai",
        "Chengzu Li",
        "Chen Liang",
        "Haochen Tian",
        "Haodong Li",
        "Ruichuan An",
        "Yifan Zhang",
        "Anna Korhonen",
        "Zhang Zhang",
        "Liang Wang",
        "Tieniu Tan"
      ],
      "abstract": "Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models. Recent generative models have achieved remarkable progress in image editing . However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding , morphological manipulation , and causal reasoning . Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following . We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.",
      "summary_en": "Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.",
      "summary_zh": "面向图像编辑的视觉指令基准提出了一个三级交互层次结构，用于评估生成模型的视觉指令遵循能力。",
      "hf_url": "https://huggingface.co/papers/2602.01851",
      "arxiv_url": "https://arxiv.org/abs/2602.01851",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01851",
      "github_url": "https://github.com/hwanyu112/VIBE-Benchmark",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:37:17.917457+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01541",
      "title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
      "authors": [
        "Boyi Li",
        "Yifan Shen",
        "Yuanzhe Liu",
        "Yifan Xu",
        "Jiateng Liu",
        "Xinzhuo Li",
        "Zhengyuan Li",
        "Jingyuan Zhu",
        "Yunhan Zhong",
        "Fangzhou Lan",
        "Jianguo Cao",
        "James M. Rehg",
        "Heng Ji",
        "Ismini Lourentzou",
        "Xu Cao"
      ],
      "abstract": "MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.",
      "summary_en": "MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.",
      "summary_zh": "配备认知超感知与潜在视觉意象预测的MLLMs通过集成的视觉和文本推理路径展现出增强的认知推理能力。",
      "hf_url": "https://huggingface.co/papers/2602.01541",
      "arxiv_url": "https://arxiv.org/abs/2602.01541",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01541",
      "github_url": "https://github.com/PediaMedAI/Cognition-MLLM",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:36:55.996785+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01335",
      "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
      "authors": [
        "Yu Xu",
        "Yuxin Zhang",
        "Juan Cao",
        "Lin Gao",
        "Chunyu Wang",
        "Oliver Deussen",
        "Tong-Yee Lee",
        "Fan Tang"
      ],
      "abstract": "Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory. A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \" creative essence \" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic , component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.",
      "summary_en": "Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.",
      "summary_zh": "视觉隐喻迁移使创造性AI系统能够通过基于认知理论的多智能体框架，从参考图像中分解抽象概念关系，并将其重新应用于新主体。",
      "hf_url": "https://huggingface.co/papers/2602.01335",
      "arxiv_url": "https://arxiv.org/abs/2602.01335",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01335",
      "github_url": "",
      "upvotes": 16,
      "fetched_at": "2026-02-19T05:36:42.893765+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01538",
      "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars",
      "authors": [
        "Youliang Zhang",
        "Zhengguang Zhou",
        "Zhentao Yu",
        "Ziyao Huang",
        "Teng Hu",
        "Sen Liang",
        "Guozhen Zhang",
        "Ziqiao Peng",
        "Shunkai Li",
        "Yi Chen",
        "Zixiang Zhou",
        "Yuan Zhou",
        "Qinglin Lu",
        "Xiu Li"
      ],
      "abstract": "A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.",
      "summary_en": "A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.",
      "summary_zh": "提出了一种名为 InteractAvatar 的双流框架，用于生成能够与环境物体交互的说话头像，通过解耦的感知与规划模块，解决了真实场景人物交互中的挑战。",
      "hf_url": "https://huggingface.co/papers/2602.01538",
      "arxiv_url": "https://arxiv.org/abs/2602.01538",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01538",
      "github_url": "https://github.com/angzong/InteractAvatar",
      "upvotes": 15,
      "fetched_at": "2026-02-19T05:36:54.161500+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01511",
      "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training",
      "authors": [
        "Ran Xu",
        "Tianci Liu",
        "Zihan Dong",
        "Tony You",
        "Ilgee Hong",
        "Carl Yang",
        "Linjun Zhang",
        "Tao Zhao",
        "Haoyu Wang"
      ],
      "abstract": "Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks. Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback . Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.",
      "summary_en": "Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.",
      "summary_zh": "Rubric-ARM框架通过强化学习联合优化评分标准生成与评判，以提升创意性和开放式任务中的回复质量评估。",
      "hf_url": "https://huggingface.co/papers/2602.01511",
      "arxiv_url": "https://arxiv.org/abs/2602.01511",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01511",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-19T05:36:52.236345+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02343",
      "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics",
      "authors": [
        "Ziwen Xu",
        "Chenyan Wu",
        "Hengyu Sun",
        "Haiwen Hong",
        "Mengru Wang",
        "Yunzhi Yao",
        "Longtao Huang",
        "Hui Xue",
        "Shumin Deng",
        "Zhixuan Chu",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "abstract": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach. Methods for controlling large language models (LLMs), including local weight fine-tuning , LoRA-based adaptation , and activation-based interventions , are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal , placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples . Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold . Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/ SPLIT .md.",
      "summary_en": "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.",
      "summary_zh": "大语言模型控制方法统一于动态权重更新框架，揭示偏好-效用权衡，并通过SPLIT方法实现改进的引导。",
      "hf_url": "https://huggingface.co/papers/2602.02343",
      "arxiv_url": "https://arxiv.org/abs/2602.02343",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02343",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:37:53.607267+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00986",
      "title": "Sparse Reward Subsystem in Large Language Models",
      "authors": [
        "Guowei Xu",
        "Mert Yuksekgonul",
        "James Zou"
      ],
      "abstract": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments , we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.",
      "summary_en": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments , we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.",
      "summary_zh": "本文识别出大语言模型（LLMs）隐藏状态中存在一个稀疏奖励子系统，并将其类比于人脑中的生物奖励子系统。我们证明该子系统包含价值神经元，这些神经元表征模型对状态价值的内部预期，并通过干预实验确立了这些神经元对推理的重要性。实验表明，这些价值神经元在多样化数据集、不同模型规模和架构上均具有鲁棒性；此外，它们在来自同一基础模型的不同数据集和微调模型之间表现出显著的可迁移性。通过检验价值预测与实际奖励出现分歧的案例，我们在奖励子系统中识别出编码奖励预测误差（RPE）的多巴胺神经元。当奖励高于预期时，这些神经元表现出高激活；当奖励低于预期时，则表现出低激活。",
      "hf_url": "https://huggingface.co/papers/2602.00986",
      "arxiv_url": "https://arxiv.org/abs/2602.00986",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00986",
      "github_url": "",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:36:34.126929+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.21123",
      "title": "CUA-Skill: Develop Skills for Computer Using Agent",
      "authors": [
        "Tianyi Chen",
        "Yinheng Li",
        "Michael Solodko",
        "Sen Wang",
        "Nan Jiang",
        "Tingyuan Cui",
        "Junheng Hao",
        "Jongwoo Ko",
        "Sara Abdali",
        "Suzhen Zheng",
        "Leon Xu",
        "Hao Fan",
        "Pashmina Cameron",
        "Justin Wagle",
        "Kazuhito Koishida"
      ],
      "abstract": "CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks. Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs . CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base , we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval , argument instantiation , and memory-aware failure recovery . Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena , CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.",
      "summary_en": "CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.",
      "summary_zh": "CUA-Skill 引入了一个大规模工程化计算机使用技能库，可提升智能体在基于Windows的任务上的性能和效率。",
      "hf_url": "https://huggingface.co/papers/2601.21123",
      "arxiv_url": "https://arxiv.org/abs/2601.21123",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21123",
      "github_url": "https://github.com/microsoft/cua_skill",
      "upvotes": 13,
      "fetched_at": "2026-02-19T05:36:00.778763+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02156",
      "title": "LoopViT: Scaling Visual ARC with Looped Transformers",
      "authors": [
        "Wen-Jie Shu",
        "Xuerui Qiu",
        "Rui-Jie Zhu",
        "Harold Haodong Chen",
        "Yexin Liu",
        "Harry Yang"
      ],
      "abstract": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters. Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark . However, we argue that the feed-forward architecture , where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence . Loop-ViT iterates a weight-tied Hybrid Block , combining local convolutions and global attention , to form a latent chain of thought . Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy : the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.",
      "summary_en": "Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.",
      "summary_zh": "Loop-ViT引入了一种递归视觉Transformer架构，通过权重共享的递归和动态退出机制将推理深度与模型容量解耦，以更少的参数实现了更优的视觉推理性能。",
      "hf_url": "https://huggingface.co/papers/2602.02156",
      "arxiv_url": "https://arxiv.org/abs/2602.02156",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02156",
      "github_url": "https://github.com/WenjieShu/LoopViT",
      "upvotes": 12,
      "fetched_at": "2026-02-19T05:37:39.963904+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02477",
      "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
      "authors": [
        "Xiao Liang",
        "Zhong-Zhi Li",
        "Zhenghao Lin",
        "Eric Hancheng Jiang",
        "Hengyuan Zhang",
        "Yelong Shen",
        "Kai-Wei Chang",
        "Ying Nian Wu",
        "Yeyun Gong",
        "Weizhu Chen"
      ],
      "abstract": "An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks. Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability . A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability , surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.",
      "summary_en": "An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.",
      "summary_zh": "一种端到端强化学习框架通过分治策略增强大语言模型的推理能力，该策略在挑战性基准测试中优于传统思维链推理。",
      "hf_url": "https://huggingface.co/papers/2602.02477",
      "arxiv_url": "https://arxiv.org/abs/2602.02477",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02477",
      "github_url": "https://github.com/MasterVito/DAC-RL",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:38:05.830960+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02227",
      "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation",
      "authors": [
        "Harold Haodong Chen",
        "Xinxiang Yin",
        "Wen-Jie Shu",
        "Hongfei Zhang",
        "Zixin Zhang",
        "Chenfei Liao",
        "Litao Guo",
        "Qifeng Chen",
        "Ying-Cong Chen"
      ],
      "abstract": "LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.",
      "summary_en": "LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.",
      "summary_zh": "LatentMorph通过四个轻量级组件将隐式潜在推理整合至文本到图像生成中，实现自适应自我优化，并提升效率与认知对齐度。",
      "hf_url": "https://huggingface.co/papers/2602.02227",
      "arxiv_url": "https://arxiv.org/abs/2602.02227",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02227",
      "github_url": "https://github.com/EnVision-Research/LatentMorph",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:37:45.878233+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.20613",
      "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
      "authors": [
        "Kaiyuan Chen",
        "Qimin Wu",
        "Taiyu Hou",
        "Tianhao Tang",
        "Xueyu Hu",
        "Yuchen Hou",
        "Bikun Li",
        "Chengming Qian",
        "Guoyin Wang",
        "Haolin Chen",
        "Haotong Tian",
        "Haoye Zhang",
        "Haoyu Bian",
        "Hongbing Pan",
        "Hongkang Zhang",
        "Hongyi Zhou",
        "Jiaqi Cai",
        "Jiewu Rao",
        "Jiyuan Ren",
        "Keduan Huang",
        "Lucia Zhu Huang",
        "Mingyu Yuan"
      ],
      "abstract": "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories. The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution , which assesses adherence to explicit and complex workflows; Latent Instruction , which requires agents to infer implicit instructions from attachments; and Iterative Refinement , which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities , enabling AI application teams to develop cutting-edge Agent products.",
      "summary_en": "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.",
      "summary_zh": "AgentIF-OneDay评估AI智能体通过自然语言指令处理多样化日常任务的能力，要求具备问题解决、附件理解和基于文件的输出能力，涵盖三个以用户为中心的类别。",
      "hf_url": "https://huggingface.co/papers/2601.20613",
      "arxiv_url": "https://arxiv.org/abs/2601.20613",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.20613",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-19T05:35:59.058238+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01675",
      "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios",
      "authors": [
        "Yuanzhe Shen",
        "Zisu Huang",
        "Zhengyuan Wang",
        "Muzhao Tian",
        "Zhengkang Guo",
        "Chenyang Zhang",
        "Shuaiyu Zhou",
        "Zengjie Hu",
        "Dailin Li",
        "Jingwen Xu",
        "Kaimin Wang",
        "Wenhao Liu",
        "Tianlong Li",
        "Fengpeng Yue",
        "Feng Hong",
        "Cao Liu",
        "Ke Zeng"
      ],
      "abstract": "TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues. As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions . To bridge this gap, we introduce TRIP-Bench, a long-horizon benchmark grounded in realistic travel-planning scenarios . TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation . It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose GTPO, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing . Applied to Qwen2.5-32B-Instruct , GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.",
      "summary_en": "TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.",
      "summary_zh": "TRIP-Bench提出了一个全面的旅行规划长期基准测试，用于评估LLM智能体在复杂多轮交互中的表现；GTPO则提供了一种在线强化学习方法，以增强扩展对话中的约束满足和鲁棒性。",
      "hf_url": "https://huggingface.co/papers/2602.01675",
      "arxiv_url": "https://arxiv.org/abs/2602.01675",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01675",
      "github_url": "",
      "upvotes": 9,
      "fetched_at": "2026-02-19T05:37:08.962987+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01382",
      "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation",
      "authors": [
        "Fu-Yun Wang",
        "Han Zhang",
        "Michael Gharbi",
        "Hongsheng Li",
        "Taesung Park"
      ],
      "abstract": "Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements. Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting , where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval , 0.98 on OCR accuracy , and 24.05 on PickScore . Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2times fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/ UniRL .",
      "summary_en": "Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.",
      "summary_zh": "面向文本到图像生成的流匹配模型通过强化学习框架得到增强，该框架引入语言模型进行提示词优化，解决了样本效率低下和提示词过拟合问题，在降低计算需求的同时实现了更优性能。",
      "hf_url": "https://huggingface.co/papers/2602.01382",
      "arxiv_url": "https://arxiv.org/abs/2602.01382",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01382",
      "github_url": "https://github.com/G-U-N/UniRL",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:36:44.632399+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01322",
      "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
      "authors": [
        "Panagiotis Koromilas",
        "Andreas D. Demou",
        "James Oldfield",
        "Yannis Panagakis",
        "Mihalis Nicolaou"
      ],
      "abstract": "PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability. Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms . However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10times larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. r = 0.82 for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition , largely independent of surface statistics.",
      "summary_en": "PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.",
      "summary_zh": "PolySAE 通过多项式解码扩展稀疏自编码器，在保持线性编码器以确保可解释性的同时，捕捉特征交互与组合结构。",
      "hf_url": "https://huggingface.co/papers/2602.01322",
      "arxiv_url": "https://arxiv.org/abs/2602.01322",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01322",
      "github_url": "",
      "upvotes": 8,
      "fetched_at": "2026-02-19T05:36:41.245119+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01660",
      "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
      "authors": [
        "Zhongyuan Peng",
        "Caijun Xu",
        "Changyi Xiao",
        "Shibo Hong",
        "Eli Zhang",
        "Stephen Huang",
        "Yixin Cao"
      ],
      "abstract": "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance. Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions . However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation ), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation , making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance , verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus , CoDiQ-Generator , and implementations to support related research.",
      "summary_en": "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.",
      "summary_zh": "一种名为CoDiQ的新颖框架通过测试时缩放实现竞赛级问题的可控难度生成，由此产生的语料库显著提升了大型推理模型的性能。",
      "hf_url": "https://huggingface.co/papers/2602.01660",
      "arxiv_url": "https://arxiv.org/abs/2602.01660",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01660",
      "github_url": "https://github.com/ALEX-nlp/CoDiQ",
      "upvotes": 7,
      "fetched_at": "2026-02-19T05:37:07.065405+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00269",
      "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models",
      "authors": [
        "Keisuke Kamahori",
        "Wei-Tzu Lee",
        "Atindra Jha",
        "Rohan Kadekodi",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Baris Kasikci"
      ],
      "abstract": "VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines. Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency , high throughput , and strong guarantees of streamability . Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.",
      "summary_en": "VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.",
      "summary_zh": "VoxServe是一个面向语音语言模型的统一服务系统，通过模型执行抽象、流式感知调度和异步推理流水线提升流式性能。",
      "hf_url": "https://huggingface.co/papers/2602.00269",
      "arxiv_url": "https://arxiv.org/abs/2602.00269",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00269",
      "github_url": "https://github.com/vox-serve/vox-serve",
      "upvotes": 6,
      "fetched_at": "2026-02-19T05:36:26.308271+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02110",
      "title": "An Empirical Study of World Model Quantization",
      "authors": [
        "Zhongqian Fu",
        "Tianyi Zhao",
        "Kai Han",
        "Hang Zhou",
        "Xinghao Chen",
        "Yunhe Wang"
      ],
      "abstract": "Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability. World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizon s up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts , activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor module s. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.",
      "summary_en": "Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.",
      "summary_zh": "世界模型中的训练后量化效应呈现出独特的失效模式，以及精度、位宽与规划性能之间的权衡，特别是在编码器-预测器模块不对称性和低比特 rollout 稳定性方面。",
      "hf_url": "https://huggingface.co/papers/2602.02110",
      "arxiv_url": "https://arxiv.org/abs/2602.02110",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02110",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:37:38.177263+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02039",
      "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
      "authors": [
        "Wei Liu",
        "Peijie Yu",
        "Michele Orini",
        "Yali Du",
        "Yulan He"
      ],
      "abstract": "Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries. The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence , distinguishing it from executional intelligence , which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.",
      "summary_en": "Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.",
      "summary_zh": "智能体大语言模型需要探究智能来实现自主数据分析，Deep Data Research 基准通过评估其在没有显式查询的情况下从数据库中提取洞察的能力验证了这一点。",
      "hf_url": "https://huggingface.co/papers/2602.02039",
      "arxiv_url": "https://arxiv.org/abs/2602.02039",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02039",
      "github_url": "https://github.com/thinkwee/DDR_Bench",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:37:30.291928+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01984",
      "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling",
      "authors": [
        "Minyoung Lee",
        "Yeji Park",
        "Dongjun Hwang",
        "Yejin Kim",
        "Seong Joon Oh",
        "Junsuk Choe"
      ],
      "abstract": "Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance. Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage , where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage . To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens . This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions . Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis , MuirBench , MIRB , and QBench2 . We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench , MultiNews , and WCEP-10 . Notably, our method requires no additional training or inference cost.",
      "summary_en": "Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.",
      "summary_zh": "缩放视觉-语言模型中分隔符token的隐藏状态，可减少跨图像信息泄漏并提升多图像推理性能。",
      "hf_url": "https://huggingface.co/papers/2602.01984",
      "arxiv_url": "https://arxiv.org/abs/2602.01984",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01984",
      "github_url": "https://github.com/MYMY-young/DelimScaling",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:37:25.927516+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00759",
      "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning",
      "authors": [
        "Zhipeng Chen",
        "Xiaobo Qin",
        "Wayne Xin Zhao",
        "Youbin Wu",
        "Ji-Rong Wen"
      ],
      "abstract": "Adaptive Ability Decomposing (A²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model. Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration , which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions . Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer , revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner 's exploration and exploitation abilities.",
      "summary_en": "Adaptive Ability Decomposing (A²D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.",
      "summary_zh": "A²D（自适应能力分解）通过将复杂问题分解为更简单的子问题来增强可验证奖励强化学习，通过引导式探索提升LLM推理能力，无需教师模型。",
      "hf_url": "https://huggingface.co/papers/2602.00759",
      "arxiv_url": "https://arxiv.org/abs/2602.00759",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00759",
      "github_url": "",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:30.170477+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22674",
      "title": "VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration",
      "authors": [
        "Hanxun Yu",
        "Wentong Li",
        "Xuan Qu",
        "Song Wang",
        "Junbo Chen",
        "Jianke Zhu"
      ],
      "abstract": "VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss. Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens , particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.",
      "summary_en": "VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.",
      "summary_zh": "VisionTrim是一种无需训练的框架，通过选择主导视觉token并将其与文本引导的补充信息相融合，加速多模态大语言模型，在不损失性能的情况下提升效率。",
      "hf_url": "https://huggingface.co/papers/2601.22674",
      "arxiv_url": "https://arxiv.org/abs/2601.22674",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22674",
      "github_url": "https://github.com/hanxunyu/VisionTrim",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:14.905394+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22599",
      "title": "A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation",
      "authors": [
        "Kai Li",
        "Jintao Cheng",
        "Chang Zeng",
        "Zijun Yan",
        "Helin Wang",
        "Zixiong Su",
        "Bo Zheng",
        "Xiaolin Hu"
      ],
      "abstract": "Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements. Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events . These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol . Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency , offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.",
      "summary_en": "Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.",
      "summary_zh": "利用来自真实场景数据集的高纯度单事件片段进行声音分离的自动化流程，以显著降低的数据需求实现了具有竞争力的性能。",
      "hf_url": "https://huggingface.co/papers/2601.22599",
      "arxiv_url": "https://arxiv.org/abs/2601.22599",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22599",
      "github_url": "https://github.com/ShandaAI/Hive",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:12.691886+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22588",
      "title": "Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry",
      "authors": [
        "Zhuochun Li",
        "Yong Zhang",
        "Ming Li",
        "Yuelyu Ji",
        "Yiming Zeng",
        "Ning Cheng",
        "Yun Zhu",
        "Yanmeng Wang",
        "Shaojun Wang",
        "Jing Xiao",
        "Daqing He"
      ],
      "abstract": "Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework. Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this \" LLM-as-a-Judge \" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states . This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge , a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.",
      "summary_en": "Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.",
      "summary_zh": "小型语言模型能够利用内部表征而非生成回复来有效评估输出，通过基于探针的框架实现更高效且可解释的评估方法。",
      "hf_url": "https://huggingface.co/papers/2601.22588",
      "arxiv_url": "https://arxiv.org/abs/2601.22588",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22588",
      "github_url": "https://github.com/zhuochunli/Representation-as-a-judge",
      "upvotes": 5,
      "fetched_at": "2026-02-19T05:36:10.479757+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01997",
      "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
      "authors": [
        "Safal Shrestha",
        "Anubhav Shrestha",
        "Aadim Nepal",
        "Minwu Kim",
        "Keith Ross"
      ],
      "abstract": "Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses. Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks . Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses . This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.",
      "summary_en": "Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.",
      "summary_zh": "层剪枝能够压缩大型语言模型并保持分类性能，但会导致生成式推理任务的显著退化，且通过自生成回复的监督微调仅能有限恢复。",
      "hf_url": "https://huggingface.co/papers/2602.01997",
      "arxiv_url": "https://arxiv.org/abs/2602.01997",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01997",
      "github_url": "https://github.com/safal312/on-the-limits-of-layer-pruning",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:37:28.103392+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01296",
      "title": "Interacted Planes Reveal 3D Line Mapping",
      "authors": [
        "Zeran Ke",
        "Bin Tan",
        "Gui-Song Xia",
        "Yujun Shen",
        "Nan Xue"
      ],
      "abstract": "LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments. 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch . We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping , not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization , establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.",
      "summary_en": "LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.",
      "summary_zh": "LiP-Map提出了一种线-面联合优化框架，显式建模可学习的线基元和平面基元，以在人造环境中实现精确的3D线地图构建。",
      "hf_url": "https://huggingface.co/papers/2602.01296",
      "arxiv_url": "https://arxiv.org/abs/2602.01296",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01296",
      "github_url": "https://github.com/calmke/LiPMAP",
      "upvotes": 4,
      "fetched_at": "2026-02-19T05:36:39.324302+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01842",
      "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
      "authors": [
        "Jinbin Bai",
        "Yixuan Li",
        "Yuchen Zhu",
        "Yi Xin",
        "Qingyu Shi",
        "Aosong Feng",
        "Xiaohong Liu",
        "Molei Tao",
        "Jianru Xue",
        "Xiangtai Li",
        "Ming-Hsuan Yang"
      ],
      "abstract": "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms. Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding , which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window , (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
      "summary_en": "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.",
      "summary_zh": "针对离散扩散语言模型，本文提出了一种名为Prism的新型测试时扩展框架，通过分层轨迹搜索、局部分支与部分重掩码及自验证反馈机制提升推理性能。",
      "hf_url": "https://huggingface.co/papers/2602.01842",
      "arxiv_url": "https://arxiv.org/abs/2602.01842",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01842",
      "github_url": "https://github.com/viiika/Prism",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:37:16.135176+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01077",
      "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
      "authors": [
        "Haopeng Li",
        "Shitong Shao",
        "Wenliang Zhong",
        "Zikai Zhou",
        "Lichen Bai",
        "Hui Xiong",
        "Zeke Xie"
      ],
      "abstract": "PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality. Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention . While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub- quadratic complexity . Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion . This design allows PISA to serve as a faithful proxy to full attention , effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality . Code is available at: https://github.com/xie-lab-ml/piecewise-sparse- attention .",
      "summary_en": "PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.",
      "summary_zh": "PISA是一种新颖的稀疏注意力方法，通过近似非关键注意力块而非直接丢弃，提升扩散Transformer的效率，在保持质量的同时实现更快处理。",
      "hf_url": "https://huggingface.co/papers/2602.01077",
      "arxiv_url": "https://arxiv.org/abs/2602.01077",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01077",
      "github_url": "https://github.com/xie-lab-ml/piecewise-sparse-attention",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:36:37.264517+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00130",
      "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks",
      "authors": [
        "Sumit Yadav"
      ],
      "abstract": "Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy. We investigate the relationship between representation geometry and neural network performance . Analyzing 52 pretrained ImageNet models across 13 architecture families , we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance , computed entirely without labels.",
      "summary_en": "Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.",
      "summary_zh": "有效维度是一种无监督几何度量，可强预测不同架构与领域中的神经网络性能，并揭示表示几何与准确率之间的双向因果关系。",
      "hf_url": "https://huggingface.co/papers/2602.00130",
      "arxiv_url": "https://arxiv.org/abs/2602.00130",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00130",
      "github_url": "",
      "upvotes": 3,
      "fetched_at": "2026-02-19T05:36:20.243558+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01983",
      "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
      "authors": [
        "Xintian Shen",
        "Jiawei Chen",
        "Lihao Zheng",
        "Hao Ma",
        "Tao Wei",
        "Kun Zhan"
      ],
      "abstract": "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation. Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
      "summary_en": "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.",
      "summary_zh": "一种无需训练的框架使语言模型智能体能够在推理过程中自动创建和优化工具，通过自我进化和记忆巩固提升其推理能力。",
      "hf_url": "https://huggingface.co/papers/2602.01983",
      "arxiv_url": "https://arxiv.org/abs/2602.01983",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01983",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:37:24.084259+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01970",
      "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models",
      "authors": [
        "Yun Qu",
        "Qi Wang",
        "Yixiu Mao",
        "Heming Zou",
        "Yuhang Jiang",
        "Weijie Liu",
        "Clive Bai",
        "Kai Yang",
        "Yangkun Chen",
        "Saiyong Yang",
        "Xiangyang Ji"
      ],
      "abstract": "Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance. Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization . Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency . However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency , final performance, and test-time efficiency over superior baseline methods.",
      "summary_en": "Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.",
      "summary_zh": "可泛化预测性提示选择（GPS）利用贝叶斯推断和轻量级生成模型，为强化学习增强的语言模型高效选择信息丰富的提示，从而提升训练效率和性能。",
      "hf_url": "https://huggingface.co/papers/2602.01970",
      "arxiv_url": "https://arxiv.org/abs/2602.01970",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01970",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:37:21.870713+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01618",
      "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia",
      "authors": [
        "Panuthep Tasawong",
        "Jian Gang Ngui",
        "Alham Fikri Aji",
        "Trevor Cohn",
        "Peerat Limkonchotiwat"
      ],
      "abstract": "Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance. Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.",
      "summary_en": "Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.",
      "summary_zh": "研究人员开发了一种新型智能体数据生成框架，用于为东南亚创建具有文化根基的安全数据集，由此构建的多语言安全防护模型在检测区域性敏感内容方面优于现有方法，同时保持了通用安全性能。",
      "hf_url": "https://huggingface.co/papers/2602.01618",
      "arxiv_url": "https://arxiv.org/abs/2602.01618",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01618",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:37:02.879281+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.23000",
      "title": "Mano: Restriking Manifold Optimization for LLM Training",
      "authors": [
        "Yufei Gu",
        "Zeke Xie"
      ],
      "abstract": "A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements. While large language models ( LLMs ) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizer s, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs , which may address both optimizer s' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold , we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizer s. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity , respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.",
      "summary_en": "A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.",
      "summary_zh": "提出了一种名为Mano的新型优化器，它结合了流形优化与切空间上的动量投影，在性能上优于AdamW和Muon，同时减少了内存和计算需求。",
      "hf_url": "https://huggingface.co/papers/2601.23000",
      "arxiv_url": "https://arxiv.org/abs/2601.23000",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.23000",
      "github_url": "https://github.com/xie-lab-ml/Mano-Restriking-Manifold-Optimization-for-LLM-Training",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:36:18.427272+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22801",
      "title": "Clipping-Free Policy Optimization for Large Language Models",
      "authors": [
        "Ömer Veysel Çağatan",
        "Barış Akgün",
        "Gözde Gül Şahin",
        "Xuandong Zhao"
      ],
      "abstract": "Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss. Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking , and training instability . We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.",
      "summary_en": "Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.",
      "summary_zh": "无裁剪策略优化使用凸二次惩罚替代启发式裁剪，在稳定大语言模型强化学习训练的同时避免性能损失。",
      "hf_url": "https://huggingface.co/papers/2601.22801",
      "arxiv_url": "https://arxiv.org/abs/2601.22801",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22801",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:36:16.755891+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.21968",
      "title": "OVD: On-policy Verbal Distillation",
      "authors": [
        "Jing Xiong",
        "Hui Shen",
        "Shansan Gong",
        "Yuxin Cheng",
        "Jianghan Shen",
        "Chaofan Tao",
        "Haochen Tan",
        "Haoli Bai",
        "Lifeng Shang",
        "Ngai Wong"
      ],
      "abstract": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints. Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models ; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models , which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning . We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models . OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment , allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io",
      "summary_en": "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints.",
      "summary_zh": "策略内语言蒸馏（OVD）通过以离散语言评分进行轨迹匹配来替代token级概率匹配，实现从教师模型到学生模型的高效知识迁移，降低内存消耗，并支持无token对齐约束的自由探索。",
      "hf_url": "https://huggingface.co/papers/2601.21968",
      "arxiv_url": "https://arxiv.org/abs/2601.21968",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21968",
      "github_url": "",
      "upvotes": 2,
      "fetched_at": "2026-02-19T05:36:04.617166+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02354",
      "title": "Implicit neural representation of textures",
      "authors": [
        "Albert Kwok",
        "Zheyuan Hu",
        "Dounia Hammou"
      ],
      "abstract": "Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks. Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space . Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality , with considerable memory usage and rendering inference time . We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation .",
      "summary_en": "Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.",
      "summary_zh": "隐式神经表示在UV坐标空间上连续运算，在平衡内存使用与渲染时间的同时展现出良好的图像质量，可应用于实时渲染和下游任务。",
      "hf_url": "https://huggingface.co/papers/2602.02354",
      "arxiv_url": "https://arxiv.org/abs/2602.02354",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02354",
      "github_url": "https://github.com/PeterHUistyping/INR-Tex",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:37:55.211040+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.02287",
      "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages",
      "authors": [
        "Isaac Chung",
        "Linda Freienthal"
      ],
      "abstract": "Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks. Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences. This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages , motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data , and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.",
      "summary_en": "Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.",
      "summary_zh": "受控跨语言评估显示，针对形态丰富语言时，LLM评估方法存在不稳定性，表明零样本评判迁移在话语级任务中并不可靠。",
      "hf_url": "https://huggingface.co/papers/2602.02287",
      "arxiv_url": "https://arxiv.org/abs/2602.02287",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.02287",
      "github_url": "https://github.com/isaac-chung/cross-lingual-stability-judges",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:37:49.562313+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01815",
      "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery",
      "authors": [
        "Yunhui Jang",
        "Seonghyun Park",
        "Jaehyung Kim",
        "Sungsoo Ahn"
      ],
      "abstract": "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches. Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal , critique , and voting phases . Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.",
      "summary_en": "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.",
      "summary_zh": "使用基于发表论文和分子历史的个性化科学家档案的分子发现多智能体系统优于传统的基于角色的方法。",
      "hf_url": "https://huggingface.co/papers/2602.01815",
      "arxiv_url": "https://arxiv.org/abs/2602.01815",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01815",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:37:14.394286+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00192",
      "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange",
      "authors": [
        "Elif Nebioglu",
        "Emirhan Bilgiç",
        "Adrian Popescu"
      ],
      "abstract": "VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance. Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks . Our findings highlight the need for content-aware detection . Indeed, training on our dataset yields better generalization and localization than standard inpainting . Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.",
      "summary_en": "VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.",
      "summary_zh": "基于VAE的修复会产生频谱偏移，从而欺骗检测系统；通过Inpainting Exchange可缓解该现象，进而提升内容感知检测性能。",
      "hf_url": "https://huggingface.co/papers/2602.00192",
      "arxiv_url": "https://arxiv.org/abs/2602.00192",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00192",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:24.323074+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00168",
      "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation",
      "authors": [
        "Ranjan Sapkota",
        "Manoj Karkee"
      ],
      "abstract": "YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities. This paper presents YOLOE -26, a unified framework that integrates the deployment-optimized YOLO26 (or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free , end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE -26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation , followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head , which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE -26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.",
      "summary_en": "YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.",
      "summary_zh": "YOLOE-26集成YOLO26架构与开放词汇学习以实现实时实例分割，采用卷积骨干网络、端到端回归及具备文本与视觉提示能力的目标嵌入头。",
      "hf_url": "https://huggingface.co/papers/2602.00168",
      "arxiv_url": "https://arxiv.org/abs/2602.00168",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00168",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:22.323200+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.22296",
      "title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
      "authors": [
        "Matteo Pinna",
        "Giacomo Lagomarsini",
        "Andrea Ceni",
        "Claudio Gallicchio"
      ],
      "abstract": "Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains. Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing . However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space , enabling parallel processing of temporal data . We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.",
      "summary_en": "Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.",
      "summary_zh": "并行回声状态网络（ParalESN）通过使用对角线性递归实现并行时序处理，解决了储备池计算的局限性，在保持理论保证的同时实现了显著的计算效率提升。",
      "hf_url": "https://huggingface.co/papers/2601.22296",
      "arxiv_url": "https://arxiv.org/abs/2601.22296",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.22296",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:08.327948+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.21759",
      "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers",
      "authors": [
        "Meet Doshi",
        "Vishwajeet Kumar",
        "Yulong Li",
        "Jaydeep Sen"
      ],
      "abstract": "An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs. General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models . We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2 , even when starting from expert-assigned weights on a large pool of training datasets.",
      "summary_en": "An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.",
      "summary_zh": "基于强化学习的采样框架通过自适应重加权训练数据集，在提升嵌入模型性能的同时降低GPU成本。",
      "hf_url": "https://huggingface.co/papers/2601.21759",
      "arxiv_url": "https://arxiv.org/abs/2601.21759",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.21759",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:36:02.489694+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.16513",
      "title": "Competing Visions of Ethical AI: A Case Study of OpenAI",
      "authors": [
        "Melissa Wilfley",
        "Mengting Ai",
        "Madelyn Rose Sanfilippo"
      ],
      "abstract": "",
      "summary_en": "",
      "summary_zh": "",
      "hf_url": "https://huggingface.co/papers/2601.16513",
      "arxiv_url": "https://arxiv.org/abs/2601.16513",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.16513",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-19T05:23:42.091308+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01897",
      "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
      "authors": [
        "Sungheon Jeong",
        "Sanggeon Yun",
        "Ryozo Masukawa",
        "Wenjun Haung",
        "Hanning Chen",
        "Mohsen Imani"
      ],
      "abstract": "Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model. Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce internal flow signatures that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring , then summarizes trajectories in compact moving readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport , yielding depth-comparable transported step lengths , turning angles , and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement : the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. Code is available at github.com/EavnJeong/Internal-Flow-Signatures-for- Self-Checking -and-Refinement-in-LLMs.",
      "summary_en": "Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.",
      "summary_zh": "内部流特征分析大语言模型的深度动态，以在不修改基础模型的情况下实现自我检查与针对性优化。",
      "hf_url": "https://huggingface.co/papers/2602.01897",
      "arxiv_url": "https://arxiv.org/abs/2602.01897",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01897",
      "github_url": "https://github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:37:20.012786+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.01418",
      "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas",
      "authors": [
        "Christoffer Koo Øhrstrøm",
        "Rafael I. Cabral Muchacho",
        "Yifei Dong",
        "Filippos Moumtzidellis",
        "Ronja Güldenring",
        "Florian T. Pokorny",
        "Lazaros Nalpantidis"
      ],
      "abstract": "Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles. We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures . Given a set of vision tokens -such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities . Prior works have largely extended position encoding s from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance , rotation invariance (PaPE-RI), distance decay , directionality , and context awareness . We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding . Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.",
      "summary_en": "Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.",
      "summary_zh": "抛物线位置编码（PaPE）是一种面向视觉模态的新型位置编码方法，通过融合平移不变性、旋转不变性、距离衰减、方向性和上下文感知原则，改进了现有方法。",
      "hf_url": "https://huggingface.co/papers/2602.01418",
      "arxiv_url": "https://arxiv.org/abs/2602.01418",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.01418",
      "github_url": "https://github.com/DTU-PAS/parabolic-position-encoding",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:36:48.384472+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2602.00521",
      "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory",
      "authors": [
        "Junhyuk Choi",
        "Sohhyung Park",
        "Chanhee Cho",
        "Hyeonchu Park",
        "Bugeun Kim"
      ],
      "abstract": "A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment. While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge , grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency , defined as the stability of measurement behavior under prompt variations, and (2) human alignment , capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of un reliability .",
      "summary_en": "A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.",
      "summary_zh": "提出了一种基于项目反应理论和等级反应模型的两阶段诊断框架，通过检验内在一致性和人类对齐来评估 LLM-as-a-Judge 的可靠性。",
      "hf_url": "https://huggingface.co/papers/2602.00521",
      "arxiv_url": "https://arxiv.org/abs/2602.00521",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.00521",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:36:28.470769+00:00"
    },
    {
      "date": "2026-02-03",
      "paper_id": "2601.14691",
      "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation",
      "authors": [
        "Muhammad Khalifa",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Sungryull Sohn",
        "Yunxiang Zhang",
        "Moontae Lee",
        "Hao Peng",
        "Lu Wang",
        "Honglak Lee"
      ],
      "abstract": "Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations. Large language models (LLMs) are increasingly used as judges to evaluate agent performance , particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute , which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.",
      "summary_en": "Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.",
      "summary_zh": "用作智能体性能评估评判器的大语言模型易受推理轨迹操纵，且基于内容的伪造比基于风格的修改更有效。",
      "hf_url": "https://huggingface.co/papers/2601.14691",
      "arxiv_url": "https://arxiv.org/abs/2601.14691",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2601.14691",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-19T05:35:57.482881+00:00"
    }
  ]
}