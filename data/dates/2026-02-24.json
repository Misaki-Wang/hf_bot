{
  "date": "2026-02-24",
  "count": 25,
  "papers": [
    {
      "date": "2026-02-24",
      "paper_id": "2602.20159",
      "title": "A Very Big Video Reasoning Suite",
      "authors": [
        "Maijunxian Wang",
        "Ruisi Wang",
        "Juyi Lin",
        "Ran Ji",
        "Thaddäus Wiedemer",
        "Qingying Gao",
        "Dezhi Luo",
        "Yaoyao Qian",
        "Lianyu Huang",
        "Zelong Hong",
        "Jiahui Ge",
        "Qianli Ma",
        "Hang He",
        "Yifan Zhou",
        "Lingzi Guo",
        "Lantao Mei",
        "Jiachen Li",
        "Hanwen Xing",
        "Tianqi Zhao",
        "Fengyuan Yu",
        "Weihang Xiao",
        "Yizheng Jiao"
      ],
      "abstract": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks. Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning . The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
      "summary_en": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.",
      "summary_zh": "提出了一个大规模视频推理数据集与基准，用于研究超越视觉质量的视频智能能力，支持对时空推理及跨多样化任务泛化的系统分析。",
      "hf_url": "https://huggingface.co/papers/2602.20159",
      "arxiv_url": "https://arxiv.org/abs/2602.20159",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20159",
      "github_url": "",
      "upvotes": 305,
      "fetched_at": "2026-02-25T02:01:38.278745+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18532",
      "title": "VLANeXt: Recipes for Building Strong VLA Models",
      "authors": [
        "Xiao-Ming Wu",
        "Bin Fan",
        "Kang Liao",
        "Jian-Jian Jiang",
        "Runze Yang",
        "Yihang Luo",
        "Zhonghua Wu",
        "Wei-Shi Zheng",
        "Chen Change Loy"
      ],
      "abstract": "Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization. Following the rise of large foundation models , Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning . Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA , we systematically dissect design choices along three dimensions: foundational components, perception essentials , and action modelling perspectives . From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt . VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.",
      "summary_en": "Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.",
      "summary_zh": "通过统一框架对Vision-Language-Action模型进行系统性分析与优化，所得到的VLANeXt模型在基准任务上性能更优，并展现出强大的真实世界泛化能力。",
      "hf_url": "https://huggingface.co/papers/2602.18532",
      "arxiv_url": "https://arxiv.org/abs/2602.18532",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18532",
      "github_url": "https://github.com/DravenALG/VLANeXt",
      "upvotes": 39,
      "fetched_at": "2026-02-25T02:01:20.892119+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19672",
      "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer",
      "authors": [
        "Jiayu Wang",
        "Yifei Ming",
        "Zixuan Ke",
        "Shafiq Joty",
        "Aws Albarghouthi",
        "Frederic Sala"
      ],
      "abstract": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods. Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration . Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse , repeatedly invoking one strong but costly option in multi-turn scenarios . We introduce SkillOrchestra, a framework for skill-aware orchestration . Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off . Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration , offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.",
      "summary_en": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods.",
      "summary_zh": "SkillOrchestra 提出了一种技能感知编排框架，通过细粒度技能建模与高效智能体选择提升复合 AI 系统性能，相较于基于强化学习的方法，在显著降低学习成本的同时取得了更优结果。",
      "hf_url": "https://huggingface.co/papers/2602.19672",
      "arxiv_url": "https://arxiv.org/abs/2602.19672",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19672",
      "github_url": "",
      "upvotes": 31,
      "fetched_at": "2026-02-25T02:01:33.827916+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19313",
      "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
      "authors": [
        "Shirui Chen",
        "Cole Harrison",
        "Ying-Chun Lee",
        "Angela Jin Yang",
        "Zhongzheng Ren",
        "Lillian J. Ratliff",
        "Jiafei Duan",
        "Dieter Fox",
        "Ranjay Krishna"
      ],
      "abstract": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks. While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits . In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning .",
      "summary_en": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.",
      "summary_zh": "TOPReward是一种基于概率的时序价值函数，使用预训练的视频视觉-语言模型通过内部token logits估计机器人任务进度，在多样真实世界任务的零样本评估中表现较优。",
      "hf_url": "https://huggingface.co/papers/2602.19313",
      "arxiv_url": "https://arxiv.org/abs/2602.19313",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19313",
      "github_url": "https://github.com/TOPReward/TOPReward",
      "upvotes": 21,
      "fetched_at": "2026-02-25T02:01:30.436085+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.20093",
      "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
      "authors": [
        "Kun Yang",
        "Yuxuan Zhu",
        "Yazhe Chen",
        "Siyao Zheng",
        "Bangyang Hong",
        "Kangle Wu",
        "Yabo Ni",
        "Anxiang Zeng",
        "Cong Fu",
        "Hui Li"
      ],
      "abstract": "ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy. Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation . Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift , where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning ), a principled framework that grounds reasoning within the topology of a global interaction graph . ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex . During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10 . Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
      "summary_en": "ManCAR is a recommendation framework that constrains latent reasoning within a collaborative manifold to prevent implausible trajectories and improve accuracy.",
      "summary_zh": "ManCAR 是一种推荐框架，它将潜在推理约束在协作流形内，以防止不合理的轨迹并提高准确性。",
      "hf_url": "https://huggingface.co/papers/2602.20093",
      "arxiv_url": "https://arxiv.org/abs/2602.20093",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20093",
      "github_url": "https://github.com/FuCongResearchSquad/ManCAR",
      "upvotes": 20,
      "fetched_at": "2026-02-25T02:01:36.770793+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.20161",
      "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
      "authors": [
        "Abdelrahman Shaker",
        "Ahmed Heakl",
        "Jaseel Muhammad",
        "Ritesh Thawkar",
        "Omkar Thawakar",
        "Senmao Li",
        "Hisham Cholakkal",
        "Ian Reid",
        "Eric P. Xing",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "abstract": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology. Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices . We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment . This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding , Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices . We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
      "summary_en": "A compact vision-language-diffusion model called Mobile-O enables efficient unified multimodal understanding and generation on mobile devices through specialized architecture design and optimized training methodology.",
      "summary_zh": "一种名为 Mobile-O 的紧凑视觉-语言-扩散模型通过专门的架构设计和优化的训练方法，在移动设备上实现了高效的统一多模态理解与生成。",
      "hf_url": "https://huggingface.co/papers/2602.20161",
      "arxiv_url": "https://arxiv.org/abs/2602.20161",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20161",
      "github_url": "https://github.com/Amshaker/Mobile-O",
      "upvotes": 18,
      "fetched_at": "2026-02-25T02:01:41.067874+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.20021",
      "title": "Agents of Chaos",
      "authors": [
        "delegated authority",
        "Natalie Shapira",
        "Chris Wendler",
        "Avery Yen",
        "Gabriele Sarti",
        "Koyena Pal",
        "Olivia Floody",
        "Adam Belfki",
        "Alex Loftus",
        "Aditya Ratan Jannali",
        "Nikhil Prakash",
        "Jasmine Cui",
        "Giordano Rogers",
        "Jannik Brinkmann",
        "Can Rager",
        "Amir Zur",
        "Michael Ripa",
        "Aruna Sankaranarayanan",
        "David Atkinson",
        "Rohit Gandikota",
        "Jaden Fiotto-Kaufman",
        "EunJeong Hwang",
        "Hadas Orgad"
      ],
      "abstract": "Autonomous language-model-powered agents in a live laboratory environment exhibited numerous security and governance vulnerabilities including unauthorized actions, information disclosure, and system takeovers during a two-week study with twenty researchers.",
      "summary_en": "Autonomous language-model-powered agents in a live laboratory environment exhibited numerous security and governance vulnerabilities including unauthorized actions, information disclosure, and system takeovers during a two-week study with twenty researchers.",
      "summary_zh": "在一项为期两周、二十名研究人员参与的实时实验室环境研究中，自主语言模型驱动的智能体表现出众多安全与治理漏洞，包括未经授权的操作、信息泄露和系统接管。",
      "hf_url": "https://huggingface.co/papers/2602.20021",
      "arxiv_url": "https://arxiv.org/abs/2602.20021",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20021",
      "github_url": "",
      "upvotes": 14,
      "fetched_at": "2026-02-25T02:01:35.997082+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18996",
      "title": "Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction",
      "authors": [
        "Shannan Yan",
        "Leqi Zheng",
        "Keyu Lv",
        "Jingchen Ni",
        "Hongyang Wei",
        "Jiajun Zhang",
        "Guangting Wang",
        "Jing Lyu",
        "Chun Yuan",
        "Fengyun Rao"
      ],
      "abstract": "A conditional binary segmentation framework with cycle-consistency training enables robust object correspondence across egocentric and exocentric viewpoints without ground-truth annotations. We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation , where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations , we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at https://github.com/shannany0606/CCMP.",
      "summary_en": "A conditional binary segmentation framework with cycle-consistency training enables robust object correspondence across egocentric and exocentric viewpoints without ground-truth annotations.",
      "summary_zh": "一种基于循环一致性训练的条件二值分割框架，无需真值标注即可实现跨自我中心与外中心视角的鲁棒物体对应。",
      "hf_url": "https://huggingface.co/papers/2602.18996",
      "arxiv_url": "https://arxiv.org/abs/2602.18996",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18996",
      "github_url": "https://github.com/shannany0606/CCMP",
      "upvotes": 13,
      "fetched_at": "2026-02-25T02:01:26.629132+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.16863",
      "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
      "authors": [
        "Kushal Kedia",
        "Tyler Ga Wei Lum",
        "Jeannette Bohg",
        "C. Karen Liu"
      ],
      "abstract": "SimToolReal enables generalizable robot manipulation of diverse tools through procedural simulation and universal reinforcement learning policies without task-specific training.",
      "summary_en": "SimToolReal enables generalizable robot manipulation of diverse tools through procedural simulation and universal reinforcement learning policies without task-specific training.",
      "summary_zh": "SimToolReal 通过程序化仿真和通用强化学习策略，实现了对多样化工具的可泛化机器人操作，无需任务特定训练。",
      "hf_url": "https://huggingface.co/papers/2602.16863",
      "arxiv_url": "https://arxiv.org/abs/2602.16863",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16863",
      "github_url": "https://github.com/tylerlum/simtoolreal",
      "upvotes": 11,
      "fetched_at": "2026-02-25T02:01:15.935753+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19895",
      "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
      "authors": [
        "Zhongwei Wan",
        "Yun Shen",
        "Zhihao Dou",
        "Donghao Zhou",
        "Yu Zhang",
        "Xin Wang",
        "Hui Shen",
        "Jing Xiong",
        "Chaofan Tao",
        "Zixuan Zhong",
        "Peizhou Huang",
        "Mi Zhang"
      ],
      "abstract": "DSDR is a reinforcement learning framework that enhances large language model reasoning by promoting diversity at both global and local levels through dual-scale regularization techniques. Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning , yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization . We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization , and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k , highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
      "summary_en": "DSDR is a reinforcement learning framework that enhances large language model reasoning by promoting diversity at both global and local levels through dual-scale regularization techniques.",
      "summary_zh": "DSDR 是一种强化学习框架，通过双尺度正则化技术在全局和局部层面促进多样性，以增强大语言模型的推理能力。",
      "hf_url": "https://huggingface.co/papers/2602.19895",
      "arxiv_url": "https://arxiv.org/abs/2602.19895",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19895",
      "github_url": "",
      "upvotes": 10,
      "fetched_at": "2026-02-25T02:01:34.480098+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18742",
      "title": "RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning",
      "authors": [
        "Seungku Kim",
        "Suhyeok Jang",
        "Byungjun Yoon",
        "Dongyoung Kim",
        "John Won",
        "Jinwoo Shin"
      ],
      "abstract": "RoboCurate enhances synthetic robot learning data by evaluating action quality through simulator replay consistency and augmenting observation diversity via image editing and video transfer techniques. Synthetic data generated by video generative models has shown promise for robot learning as a scalable pipeline, but it often suffers from inconsistent action quality due to imperfectly generated videos. Recently, vision-language models (VLMs) have been leveraged to validate video quality, but they have limitations in distinguishing physically accurate videos and, even then, cannot directly evaluate the generated actions themselves. To tackle this issue, we introduce RoboCurate, a novel synthetic robot data generation framework that evaluates and filters the quality of annotated actions by comparing them with simulation replay. Specifically, RoboCurate replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance. We observe RoboCurate's generated data yield substantial relative improvements in success rates compared to using real data only, achieving +70.1% on GR-1 Tabletop (300 demos), +16.1% on DexMimicGen in the pre-training setup, and +179.9% in the challenging real-world ALLEX humanoid dexterous manipulation setting.",
      "summary_en": "RoboCurate enhances synthetic robot learning data by evaluating action quality through simulator replay consistency and augmenting observation diversity via image editing and video transfer techniques.",
      "summary_zh": "RoboCurate 通过模拟器回放一致性评估动作质量，并通过图像编辑与视频迁移技术增强观测多样性，从而提升合成机器人学习数据。",
      "hf_url": "https://huggingface.co/papers/2602.18742",
      "arxiv_url": "https://arxiv.org/abs/2602.18742",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18742",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-25T02:01:24.503436+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.16872",
      "title": "DODO: Discrete OCR Diffusion Models",
      "authors": [
        "Sean Man",
        "Roy Ganz",
        "Roi Ronen",
        "Shahar Tsiper",
        "Shai Mazor",
        "Niv Nayman"
      ],
      "abstract": "Diffusion models are adapted for optical character recognition by using block discrete diffusion to enable faster, parallel processing while maintaining high accuracy.",
      "summary_en": "Diffusion models are adapted for optical character recognition by using block discrete diffusion to enable faster, parallel processing while maintaining high accuracy.",
      "summary_zh": "扩散模型通过采用块离散扩散被适配用于光学字符识别，在保持高准确率的同时实现更快的并行处理。",
      "hf_url": "https://huggingface.co/papers/2602.16872",
      "arxiv_url": "https://arxiv.org/abs/2602.16872",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.16872",
      "github_url": "",
      "upvotes": 7,
      "fetched_at": "2026-02-25T02:01:17.155148+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19320",
      "title": "Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations",
      "authors": [
        "Dongming Jiang",
        "Yi Li",
        "Songtao Wei",
        "Jinxin Yang",
        "Ayushi Kishore",
        "Alysa Zhao",
        "Dingyi Kang",
        "Xu Hu",
        "Feng Chen",
        "Qiannan Li",
        "Bingzhe Li"
      ],
      "abstract": "Agentic memory systems for LLM agents face empirical challenges including inadequate benchmarks, misaligned metrics, and performance variability that limit their practical effectiveness. Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows . Despite rapid architectural development , the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility , performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures . Then, we analyze key pain points limiting current systems, including benchmark saturation effects , metric validity and judge sensitivity , backbone-dependent accuracy , and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design .",
      "summary_en": "Agentic memory systems for LLM agents face empirical challenges including inadequate benchmarks, misaligned metrics, and performance variability that limit their practical effectiveness.",
      "summary_zh": "面向LLM智能体的Agentic记忆系统面临实证挑战，包括基准测试不足、指标错位和性能波动，限制了其实际效用。",
      "hf_url": "https://huggingface.co/papers/2602.19320",
      "arxiv_url": "https://arxiv.org/abs/2602.19320",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19320",
      "github_url": "https://github.com/FredJiang0324/Anatomy-of-Agentic-Memory",
      "upvotes": 5,
      "fetched_at": "2026-02-25T02:01:31.347114+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19128",
      "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
      "authors": [
        "Shiyi Cao",
        "Ziming Mao",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "abstract": "K-Search uses a co-evolving world model to optimize GPU kernels by separating high-level planning from low-level implementation, achieving significant performance improvements over existing evolutionary methods. Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model , our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation , enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer , including GQA , MLA , and MoE kernels . Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels . On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.",
      "summary_en": "K-Search uses a co-evolving world model to optimize GPU kernels by separating high-level planning from low-level implementation, achieving significant performance improvements over existing evolutionary methods.",
      "summary_zh": "K-Search 使用协同演化的世界模型，通过将高层规划与底层实现分离来优化 GPU 内核，相比现有演化方法实现了显著的性能提升。",
      "hf_url": "https://huggingface.co/papers/2602.19128",
      "arxiv_url": "https://arxiv.org/abs/2602.19128",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19128",
      "github_url": "",
      "upvotes": 4,
      "fetched_at": "2026-02-25T02:01:29.374586+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19626",
      "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
      "authors": [
        "Roberto Tacconelli"
      ],
      "abstract": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder . Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs. On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
      "summary_en": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder . Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs. On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
      "summary_zh": "我们提出Nacrith，一种无损压缩系统，其将1.35亿参数Transformer语言模型（SmolLM2-135M）与轻量级在线预测器集合及32位算术编码器相结合。除基础的LLM加算术编码范式外，Nacrith引入以下贡献：（1）CDF精度从2^16提升至2^24，消除大词汇表中由最小概率下限导致的大约75%量化开销；（2）用于快速局部预测的token级N-gram模型；（3）通过在线梯度下降修正每篇文档LLM误差的自适应对数空间偏置头；（4）基于置信度的LLM跳过机制，用于加速高可预测性token；（5）混合二进制格式（NC06），将神经压缩扩展至任意二进制文件——据我们所知，这在基于LLM的压缩器中尚属首次；（6）llama.cpp推理后端，单token解码速度比PyTorch快约7倍；（7）最多8个worker的并行多GPU压缩；（8）原生KV缓存滑动窗口，将每步滑动开销降低约37倍。该系统每worker仅需约500 MB GGUF权重和约1.2 GB显存，可在消费级GPU上运行。在alice29.txt（Canterbury语料库，152 KB）上，Nacrith达到0.918 bits per byte（bpb），优于gzip达3.1倍、bzip2达2.5倍、CMIX v21达44%、ts_zip达20%，且压缩结果低于0阶、1阶和2阶字节级香农熵界限。在enwik8（100 MB）上，Nacrith达到0.9389 bpb（11.74%），超过ts_zip（约1.11 bpb）15%、FineZip（1.024 bpb）8%，尽管其使用小60倍的模型且未经微调。在模型训练截止日期之后发布的文档上进行分布外评估，证实这些提升并非记忆效应，在未见文本上达到0.723 bpb。",
      "hf_url": "https://huggingface.co/papers/2602.19626",
      "arxiv_url": "https://arxiv.org/abs/2602.19626",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19626",
      "github_url": "https://github.com/robtacconelli/Nacrith-GPU",
      "upvotes": 2,
      "fetched_at": "2026-02-25T02:01:32.760668+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18224",
      "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation",
      "authors": [
        "Yuankai Luo",
        "Woping Chen",
        "Tong Liang",
        "Baiqiao Wang",
        "Zhenguo Li"
      ],
      "abstract": "SimVLA presents a simplified baseline for Vision-Language-Action models that achieves state-of-the-art performance with fewer parameters while enabling clearer evaluation of architectural improvements. Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control , using a standard vision-language backbone and a lightweight action head , and standardizing critical training dynamics , we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA",
      "summary_en": "SimVLA presents a simplified baseline for Vision-Language-Action models that achieves state-of-the-art performance with fewer parameters while enabling clearer evaluation of architectural improvements.",
      "summary_zh": "SimVLA 为 Vision-Language-Action 模型提出了一个简化的基线，以更少的参数实现了最先进的性能，同时能够对架构改进进行更清晰的评估。",
      "hf_url": "https://huggingface.co/papers/2602.18224",
      "arxiv_url": "https://arxiv.org/abs/2602.18224",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18224",
      "github_url": "https://github.com/LUOyk1999/SimVLA",
      "upvotes": 2,
      "fetched_at": "2026-02-25T02:01:19.072743+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.20160",
      "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
      "authors": [
        "Chen Wang",
        "Hao Tan",
        "Wang Yifan",
        "Zhiqin Chen",
        "Yuheng Liu",
        "Kalyan Sunkavalli",
        "Sai Bi",
        "Lingjie Liu",
        "Yiwei Hu"
      ],
      "abstract": "A novel 3D reconstruction model called tttLRM uses a Test-Time Training layer to enable efficient, scalable autoregressive reconstruction with linear complexity, achieving better results than existing methods. We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling , resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
      "summary_en": "A novel 3D reconstruction model called tttLRM uses a Test-Time Training layer to enable efficient, scalable autoregressive reconstruction with linear complexity, achieving better results than existing methods.",
      "summary_zh": "一种名为tttLRM的新型3D重建模型使用Test-Time Training层，实现具有线性复杂度的高效可扩展自回归重建，效果优于现有方法。",
      "hf_url": "https://huggingface.co/papers/2602.20160",
      "arxiv_url": "https://arxiv.org/abs/2602.20160",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.20160",
      "github_url": "https://github.com/cwchenwang/tttLRM",
      "upvotes": 1,
      "fetched_at": "2026-02-25T02:01:39.498212+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18915",
      "title": "AAVGen: Precision Engineering of Adeno-associated Viral Capsids for Renal Selective Targeting",
      "authors": [
        "Mohammadreza Ghaffarzadeh-Esfahani",
        "Yousof Gheisari"
      ],
      "abstract": "AAVGen is a generative AI framework that designs AAV capsids with improved traits through protein language models, supervised fine-tuning, and reinforcement learning techniques. Adeno-associated viruses (AAVs) are promising vectors for gene therapy, but their native serotypes face limitations in tissue tropism, immune evasion, and production efficiency. Engineering capsids to overcome these hurdles is challenging due to the vast sequence space and the difficulty of simultaneously optimizing multiple functional properties. The complexity also adds when it comes to the kidney, which presents unique anatomical barriers and cellular targets that require precise and efficient vector engineering. Here, we present AAVGen, a generative artificial intelligence framework for de novo design of AAV capsids with enhanced multi-trait profiles. AAVGen integrates a protein language model (PLM) with supervised fine-tuning (SFT) and a reinforcement learning technique termed Group Sequence Policy Optimization (GSPO). The model is guided by a composite reward signal derived from three ESM-2-based regression predictors , each trained to predict a key property: production fitness, kidney tropism, and thermostability. Our results demonstrate that AAVGen produces a diverse library of novel VP1 protein sequences. In silico validations revealed that the majority of the generated variants have superior performance across all three employed indices, indicating successful multi-objective optimization . Furthermore, structural analysis via AlphaFold3 confirms that the generated sequences preserve the canonical capsid folding despite sequence diversification. AAVGen establishes a foundation for data-driven viral vector engineering, accelerating the development of next-generation AAV vectors with tailored functional characteristics.",
      "summary_en": "AAVGen is a generative AI framework that designs AAV capsids with improved traits through protein language models, supervised fine-tuning, and reinforcement learning techniques.",
      "summary_zh": "AAVGen是一个生成式AI框架，通过蛋白质语言模型、监督微调和强化学习技术设计具有改进性状的AAV衣壳。",
      "hf_url": "https://huggingface.co/papers/2602.18915",
      "arxiv_url": "https://arxiv.org/abs/2602.18915",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18915",
      "github_url": "https://github.com/mohammad-gh009/AAVGen",
      "upvotes": 1,
      "fetched_at": "2026-02-25T02:01:25.091473+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18640",
      "title": "Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System",
      "authors": [
        "Longfei Yun",
        "Yihan Wu",
        "Haoran Liu",
        "Xiaoxuan Liu",
        "Ziyun Xu",
        "Yi Wang",
        "Yang Xia",
        "Pengfei Wang",
        "Mingze Gao",
        "Yunxiang Wang",
        "Changfan Chen",
        "Junfeng Pan"
      ],
      "abstract": "GEARS presents a framework that reframes ranking optimization as an autonomous discovery process using specialized agent skills and validation hooks to balance algorithmic signals with ranking context while ensuring production reliability. Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS ( Generative Engine for Agentic Ranking Systems ), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near- Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability .",
      "summary_en": "GEARS presents a framework that reframes ranking optimization as an autonomous discovery process using specialized agent skills and validation hooks to balance algorithmic signals with ranking context while ensuring production reliability.",
      "summary_zh": "GEARS提出了一个框架，将排序优化重新定义为利用专门智能体技能和验证钩子的自主发现过程，以平衡算法信号与排序上下文，同时确保生产可靠性。",
      "hf_url": "https://huggingface.co/papers/2602.18640",
      "arxiv_url": "https://arxiv.org/abs/2602.18640",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18640",
      "github_url": "",
      "upvotes": 1,
      "fetched_at": "2026-02-25T02:01:22.391560+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.17393",
      "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots",
      "authors": [
        "Minxing Sun",
        "Yao Mao"
      ],
      "abstract": "A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors. Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor : joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a sim200\\,m horizontal loop and a sim15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a sim700\\,m horizontal loop yields 7.68\\,m error and a sim20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a sim120\\,m horizontal loop with 2.2138\\,m error and a sim8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git",
      "summary_en": "A proprioceptive state estimation method for legged robots uses IMU and motor measurements to jointly estimate body pose and velocity, leveraging contact-based constraints and geometric consistency to reduce drift without external sensors.",
      "summary_zh": "一种用于足式机器人的本体感知状态估计方法，利用IMU和电机测量数据联合估计机体位姿和速度，通过基于接触的约束和几何一致性在没有外部传感器的情况下减少漂移。",
      "hf_url": "https://huggingface.co/papers/2602.17393",
      "arxiv_url": "https://arxiv.org/abs/2602.17393",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.17393",
      "github_url": "https://github.com/ShineMinxing/Ros2Go2Estimator",
      "upvotes": 1,
      "fetched_at": "2026-02-25T02:01:17.809698+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.12100",
      "title": "AssetFormer: Modular 3D Assets Generation with Autoregressive Transformer",
      "authors": [
        "Lingting Zhu",
        "Shengju Qian",
        "Haidi Fan",
        "Jiayu Dong",
        "Zhenchao Jin",
        "Siwei Zhou",
        "Gen Dong",
        "Xin Wang",
        "Lequan Yu"
      ],
      "abstract": "AssetFormer is an autoregressive Transformer-based model that generates modular 3D assets from textual descriptions by adapting language model techniques to handle design constraints. The digital industry demands high-quality, diverse modular 3D assets , especially for user-generated content~(UGC). In this work, we introduce AssetFormer, an autoregressive Transformer-based model designed to generate modular 3D assets from textual descriptions . Our pilot study leverages real-world modular assets collected from online platforms. AssetFormer tackles the challenge of creating assets composed of primitives that adhere to constrained design parameters for various applications. By innovatively adapting module sequencing and decoding techniques inspired by language models, our approach enhances asset generation quality through autoregressive modeling . Initial results indicate the effectiveness of AssetFormer in streamlining asset creation for professional development and UGC scenarios. This work presents a flexible framework extendable to various types of modular 3D assets , contributing to the broader field of 3D content generation. The code is available at https://github.com/Advocate99/AssetFormer.",
      "summary_en": "AssetFormer is an autoregressive Transformer-based model that generates modular 3D assets from textual descriptions by adapting language model techniques to handle design constraints.",
      "summary_zh": "AssetFormer 是一种基于 Transformer 的自回归模型，通过采用语言模型技术处理设计约束，从文本描述生成模块化 3D 资产。",
      "hf_url": "https://huggingface.co/papers/2602.12100",
      "arxiv_url": "https://arxiv.org/abs/2602.12100",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.12100",
      "github_url": "https://github.com/Advocate99/AssetFormer",
      "upvotes": 1,
      "fetched_at": "2026-02-25T02:01:14.763975+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19455",
      "title": "SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning",
      "authors": [
        "Zelin He",
        "Boran Han",
        "Xiyuan Zhang",
        "Shuai Zhang",
        "Haotian Lin",
        "Qi Zhu",
        "Haoyang Fang",
        "Danielle C. Maddix",
        "Abdul Fatir Ansari",
        "Akash Chandrayan",
        "Abhinav Pradhan",
        "Bernie Wang",
        "Matthew Reimherr"
      ],
      "abstract": "A hybrid knowledge-injection framework combines general reasoning large language models with time-series LLMs through reinforcement learning-based verifiable rewards to enhance time-series diagnostic reasoning performance. Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning -based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection . We further release SenTSR-Bench, a multivariate time-series -based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.",
      "summary_en": "A hybrid knowledge-injection framework combines general reasoning large language models with time-series LLMs through reinforcement learning-based verifiable rewards to enhance time-series diagnostic reasoning performance.",
      "summary_zh": "一种混合知识注入框架通过基于强化学习的可验证奖励，将通用推理大语言模型与时间序列大语言模型相结合，以提升时间序列诊断推理性能。",
      "hf_url": "https://huggingface.co/papers/2602.19455",
      "arxiv_url": "https://arxiv.org/abs/2602.19455",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19455",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-25T02:01:31.959534+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.19089",
      "title": "Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling",
      "authors": [
        "Qi Sun",
        "Can Wang",
        "Jiaxiang Shang",
        "Yingchun Liu",
        "Jing Liao"
      ],
      "abstract": "Ani3DHuman framework combines kinematics-based animation with video diffusion priors to generate photorealistic 3D human animations by addressing challenges in non-rigid motion synthesis and identity preservation. Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non- rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors . We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion . Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion . However, this restoration task, based on diffusion sampling , is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality ) with self-guidance (for identity fidelity ). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \\MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman.",
      "summary_en": "Ani3DHuman framework combines kinematics-based animation with video diffusion priors to generate photorealistic 3D human animations by addressing challenges in non-rigid motion synthesis and identity preservation.",
      "summary_zh": "Ani3DHuman框架结合基于运动学的动画与视频扩散先验，通过解决非刚性运动合成和身份保持的挑战，生成真实感3D人体动画。",
      "hf_url": "https://huggingface.co/papers/2602.19089",
      "arxiv_url": "https://arxiv.org/abs/2602.19089",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.19089",
      "github_url": "https://github.com/qiisun/ani3dhuman",
      "upvotes": 0,
      "fetched_at": "2026-02-25T02:01:28.149662+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18662",
      "title": "Large Causal Models for Temporal Causal Discovery",
      "authors": [
        "Nikolaos Kougioulis",
        "Nikolaos Gkorgkolis",
        "MingXue Wang",
        "Bora Caglayan",
        "Dario Simionato",
        "Andrea Tonon",
        "Ioannis Tsamardinos"
      ],
      "abstract": "Large causal models combine synthetic and real time-series data to enable scalable temporal causal discovery with improved generalization and fast inference. Causal discovery for both cross-sectional and temporal data has traditionally followed a dataset-specific paradigm, where a new model is fitted for each individual dataset. Such an approach limits the potential of multi-dataset pretraining . The concept of large causal models (LCMs) envisions a class of pre-trained neural architectures specifically designed for temporal causal discovery . Prior approaches are constrained to small variable counts, degrade with larger inputs, and rely heavily on synthetic data, limiting generalization. We propose a principled framework for LCMs, combining diverse synthetic generators with realistic time-series datasets , allowing learning at scale. Extensive experiments on synthetic, semi-synthetic and realistic benchmarks show that LCMs scale effectively to higher variable counts and deeper architectures while maintaining strong performance. Trained models achieve competitive or superior accuracy compared to classical and neural baselines, particularly in out-of-distribution settings , while enabling fast, single-pass inference . Results demonstrate LCMs as a promising foundation-model paradigm for temporal causal discovery . Experiments and model weights are available at https://github.com/kougioulis/LCM-paper/.",
      "summary_en": "Large causal models combine synthetic and real time-series data to enable scalable temporal causal discovery with improved generalization and fast inference.",
      "summary_zh": "大型因果模型结合合成与真实时序数据，实现可扩展的时间因果发现，提升泛化能力并加速推理。",
      "hf_url": "https://huggingface.co/papers/2602.18662",
      "arxiv_url": "https://arxiv.org/abs/2602.18662",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18662",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-25T02:01:23.767204+00:00"
    },
    {
      "date": "2026-02-24",
      "paper_id": "2602.18333",
      "title": "On the \"Induction Bias\" in Sequence Models",
      "authors": [
        "M. Reza Ebrahimi",
        "Michaël Defferrard",
        "Sunny Panchal",
        "Roland Memisevic"
      ],
      "abstract": "Despite the remarkable practical success of transformer-based language models , recent work has raised concerns about their ability to perform state tracking . In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes . We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence length s. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.",
      "summary_en": "Despite the remarkable practical success of transformer-based language models , recent work has raised concerns about their ability to perform state tracking . In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes . We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence length s. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.",
      "summary_zh": "尽管基于 Transformer 的语言模型在实际应用中取得了显著成功，但近期研究对其执行状态追踪的能力提出了担忧。特别是，越来越多的文献主要通过分布外 (OOD) 泛化（如长度外推）的失败来展示这一局限性。在本研究中，我们将注意力转向这些局限性在分布内影响。我们在多种监督机制下，对 Transformer 和循环神经网络 (RNN) 的数据效率进行了大规模实验研究。我们发现，Transformer 所需的训练数据量随状态空间大小和序列长度的增长速度快于 RNN。此外，我们分析了所学得的状态追踪机制在不同序列长度间的共享程度。我们发现 Transformer 在不同长度间表现出可忽略甚至有害的权重共享，表明其独立地学习针对特定长度的解决方案。相比之下，循环模型通过跨长度共享权重展现出有效的摊销学习，使得来自某一序列长度的数据能够提升在其他长度上的性能。综上所述，这些结果表明，即使训练分布与评估分布一致，状态追踪对 Transformer 而言仍是一项根本性挑战。",
      "hf_url": "https://huggingface.co/papers/2602.18333",
      "arxiv_url": "https://arxiv.org/abs/2602.18333",
      "arxiv_pdf_url": "https://arxiv.org/pdf/2602.18333",
      "github_url": "",
      "upvotes": 0,
      "fetched_at": "2026-02-25T02:01:20.297876+00:00"
    }
  ]
}